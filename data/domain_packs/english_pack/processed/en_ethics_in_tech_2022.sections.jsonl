{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "section_path": ["About this text, the author, and Open Education Resource (OER) Copyright"], "page": 3, "content": "Ethics in Technology\nBy: Ed Weber\n1. Preface\nAbout this text, the author, and Open Education Resource (OER) Copyright\nAbout this text\nThe purpose of this text is to serve as an Open Education Resource (OER) designed initially to be\nused in a secondary or post-secondary education setting. It is intended to be a guide to facilitate\nfocused discussions about contemporary issues of the ethical considerations related to technology\nevolution, development, deployment, and consumption, as well as issues (both known and unknown) of\npotential misuse and abuse of technology. This is not a traditional Ethics textbook in that it is not\nintended to provide a survey of all of the history of Ethics through the ages nor is it intended to dive\ndeeply into any particular ethical movements or frameworks as may be the focus of other ethics studies.\nRather, it is intended to focus predominantly on the concepts of applying critical and ethical thinking to\nissues and subsequent decisions related to our interactions with technology in the 21st century.\nIf you came here looking for definitive answers – I’m so sorry to disappoint you. This text does not\npurport to include any categorical absolutes or any sort of an ethical road map for the reader to adopt.\nRather, the intent is to be a starting point for the thinking person to consider one’s own perspectives\nand understandings of:\n• what the concept of ‘ethics’ means to you as you begin your exploration...\n• what influences (family, school, religion, region, ethnicity, gender identity, socioeconomic\nstatus, etc.) have come together to make you the ‘who’ you are at this moment…\n• what are the skills necessary to be able to objectively review a concept or situation so as to\nunderstand what its ethical issues may be…\n• what it actually means to have a choice between more than one option and how to be able to\nmake an informed decision for yourself that includes ethical considerations…\n• how to recognize that while technology continues to advance, other systems like legal systems,\neconomic systems, political systems, social systems, and individual and group moral and ethical\nsystems often lag significantly behind the rate of change of technology.\nThis text is best used in conjunction with case studies and examples (both historic and\ncontemporary) to serve as springboards for in-depth discussion surrounding the various topics. As an\nOER, it is expected that some of the case studies and examples that are initially presented throughout\nthe text will become ‘stale’ over time. Therefore, it is encouraged that adopters of this text consider\nenhancing this base text with additional, contemporary case studies or other source materials which\nexemplify the topics. Likewise, it is encouraged that students and other consumers of this text also\nbring suggested new case studies and examples for possible inclusion. New examples of both ethical\nPage 3 of 125 1. Preface"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "section_path": ["About this text, the author, and Open Education Resource (OER) Copyright"], "page": 4, "content": "Ethics in Technology\nBy: Ed Weber\nand unethical behaviors, decisions, and applications of technology continue to happen every single day.\nIt is a hope of the author that those who work with this text will begin to adopt a habit of intentionally\nand closely examining the ethical considerations of their respective interactions with technology.\nThroughout this text, we will discover that having a shared understanding of the intended definition\nof critical terms (if not a shared agreement of those meanings) is critical in order to have meaningful\ndiscourse about these ethereal concepts. As a result, many terms will be presented throughout this text\nwhich will require this shared understanding of the author’s intended meaning.\nAs a result, terms that may be subject to multiple definitions or interpretations are highlighted in\nbold in this text. The definition of these terms may appear when they are first used, or may be defined\nat the end of the chapter in which they are used. Throughout the use of this text, it is important that the\nreader and others involved in discussions about these topics have a minimum of a shared definition of\nwhat each critical term means and the context in which the terms are being used.\nAbout the Author\nHello good reader! I am Ed Weber, an Associate Professor of\nComputer Science at St. Charles Community College (SCC) in Cottleville,\nMO. I also am the President and owner of Weber Enterprises, LLC of\nWildwood, MO, which was founded in 1995. I have spent my entire career\nin Information Systems working with both very large companies, and with\nmid-size and small clients. I have been teaching Information Systems and\nComputer Science since 1996 originally as an adjunct instructor and then\nas a full-time Assistant Professor with Millikin University for 9 ½ years\nbefore joining SCC. I have seen countless technologies come and go Figure 2: Ed Weber,\nthroughout my career and I have witnessed both the ethical and unethical Assoc. Prof. of Computer\nimplementations of technology as well. Science SCC\nWhen I wrote my first textbook, Spreadsheet Fundamentals, in 2018, it was for the primary purpose\nof reducing the cost of a required textbook for my class. This was a direct and intentional action when\nI realized that some of my students were struggling because they didn’t have the financial support to\nbuy the textbook we were previously using. This was my first real exposure to the realities of the\nethical issues surrounding the digital divide (see Chapter 6). At that time, I was unaware of the Open\nEducation Resources (OER) concepts and that there was a way for me to publish the textbook to be\n100% free of charge. Rather, I was approached by a traditional textbook publisher who helped me in\nmy first publishing endeavor.\nSo, while my first publication allowed me to reduce the cost of the text from nearly $200 to under\n$40, I have subsequently learned how the OER model affords me an opportunity to make new\nPage 4 of 125 1. Preface"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "section_path": ["About this text, the author, and Open Education Resource (OER) Copyright"], "page": 5, "content": "Ethics in Technology\nBy: Ed Weber\nclassroom materials for students that will be forever 100% free of charge. That is my primary purpose\nin developing this text.\nThroughout my career, I have often found myself in situations where I had to make decisions and\ntake actions which would have significant ethical implications for myself, my family, my colleagues,\nmy staff, my employees, my clients, my organization, and even my entire community. The more often\nthat these types of situations occurred, the more I realized that you don’t want to begin thinking about\nthe ethical issues of your situation when you are already knee-deep in the middle of it! Rather, it seems\nto be healthier for me to be more proactive when it comes to thinking about ethics and how I\nincorporate my understandings into my day-to-day life and decisions.\nTherefore, this text will attempt to lay out just a sampling of some of the major technology-related\ntopics that are happening at this time which have exceptional ethical considerations. It is my hope that\nby thinking about (and discussing) these topics before you find yourself in the middle of making any\nsignificant decisions, you will be able to find yourself much more prepared for the decisions that life\nwill be throwing at you.\nOpen Education Resource (OER) Copyright\nOpen Educational Resources (OER) are teaching, learning, and research materials that are freely\navailable for anyone to use, adapt, and share. These resources can include textbooks, course materials,\nvideos, tests, and software, and are either in the public domain or released under an open license that\npermits no-cost access, reuse, modification, and redistribution by others. The purpose of OER is to\nreduce barriers to education by providing high-quality materials that can be tailored to meet local needs\nand contexts.\nThe material in this textbook is copyrighted and licensed under the Creative\nCommons Attribution Non-Commercial Share-Alike (CC BY-NC-SA) license.\nThis means you are free to use, share, and adapt the content for non-commercial purposes, as long as\nyou provide appropriate credit to the original creator and distribute any derivative works under the\nsame license. Commercial use of this material is not permitted without explicit permission from the\ncopyright holder.\nThe images found in this text have the following attributions:\n• The internet memes and Maslow's hierarchy image do not receive any attribution as they are\nconsidered in the public domain.\n• The screenshot of the website is attributed with the citation for the entire content taken from\nthat site\n• All of the rest of the images were created by me and referenced when using the Pixlr AI tool.\nPage 5 of 125 1. Preface"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 6, "content": "Ethics in Technology\nBy: Ed Weber\n2. Introduction, Ethical Frameworks and Personal Lenses\nFoundations; Frameworks and Personal Lenses; Key Concepts\nFoundations\nFrom an early age, most people experience the concept of Ethics long before they ever learn that\nthere is a term to describe the concept that they are experiencing. Many individuals might say that the\nterm includes some concept of right vs. wrong, or good vs. evil, and how these concepts affect a\nperson’s behaviors. And maybe, they consider Ethics to be the full collection of a person’s attitude and\nbehaviors and their rightness vs. wrongness. But as we can see, often trying to define one term leads\nus down a rabbit-hole where we discover that we need to define even more terms! In fact, a large part\nof the first few chapters of this text will focus on the necessity of defining terms and why it is\nnecessary to have a shared understanding of the critical terms which we use to have discussions about\nethics.\nFor example, what do you think is the meaning of the term: Ethics? I’m sure that most folx would\nbe able to come up with their own definition of what they think Ethics means. Take a few moments to\nsee if you can come up with your own definition of what you think the term Ethics means for you… go\nahead… we’ll wait.\nNow that you have your own definition of Ethics in your mind, let’s now consider a follow-up\nquestion: How did you come to have this understanding of what you think Ethics means? Did you\npreviously learn a formal definition of the term from a textbook or a dictionary? Did you learn it from\nsome previous class or somewhere else in school? Were you told what the term means from your\nparents or grand-parents or other family members? Did you learn the term from your religious leaders?\nDid you read it in a sacred book? Did you learn it from television or the movies? Did you learn it on\nthe Internet? Did you Google it? From social media? From an influencer? Did you as a generative AI\ntool? Really think about it. Where did you get your understanding of this term?\nAnd next, while you contemplate all of the possible places where you might have learned the term,\nthe next follow-up question is this: Whomever provided you with this definition… were they right?\nWhere did they learn it from? Is it possible (… just possible …) that they got it wrong? Who,\nultimately, gets to decide – for you – what the acceptable definition of this term Ethics may be? Or, for\nthat matter, who gets to decide – for you – what the acceptable definition of any term is going to be?\nAs you can quickly see, this text will be full of a lot more questions than answers! But it is\nprecisely these questions that will help guide you through thoughtful and insightful discussions about\nhow the ethical aspects of technology are becoming more and more significant every day.\nPage 6 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 7, "content": "Ethics in Technology\nBy: Ed Weber\nBefore continuing, it may be helpful to list some of the significant philosophical movements and\njust a few of the many ethicists whose work may be considered most relevant to the concept of\napplying ethical thinking to issues surrounding technology.\nApplicable Ethical Movements\n• Deontological Ethics (18th Century)\nDeontological ethics emphasizes duty, rules, and the inherent rightness or wrongness of actions\nregardless of consequences. This framework provides essential grounding for technology ethics\nby establishing inviolable principles such as respect for human dignity, autonomy, and rights\nthat must be preserved regardless of technological benefits. In applied technology ethics,\ndeontological thinking helps establish non-negotiable boundaries around issues like privacy,\nconsent, and human agency that cannot be overridden by utilitarian calculations of greater good.\n• Rationalism (Enlightenment Era, 17th-18th Century)\nRationalism emphasizes reason, logic, and systematic thinking as the primary sources of\nknowledge and ethical guidance. This movement is foundational to applied technology ethics\nbecause it provides methodological approaches for analyzing complex technological systems\nand their ethical implications through structured reasoning. Rationalist approaches help\ntechnologists and ethicists develop systematic frameworks for evaluating emerging\ntechnologies rather than relying solely on intuition or tradition.\n• Utilitarianism (18th-19th Century)\nUtilitarianism judges actions based on their consequences and seeks to maximize overall well-\nbeing or happiness for the greatest number of people. This consequentialist approach is highly\nrelevant to technology ethics because it provides frameworks for weighing the benefits and\nharms of technological innovations across large populations. Applied technology ethics\nfrequently employs utilitarian analysis when evaluating trade-offs between technological\nprogress and potential societal risks, such as balancing AI efficiency gains against job\ndisplacement or privacy concerns.\n• Humanism (Renaissance/Modern Era, 15th-20th Century)\nHumanism places human dignity, agency, and flourishing at the center of ethical consideration.\nThis perspective is crucial for technology ethics as it ensures that technological development\nserves human needs and preserves human agency rather than subordinating humans to\ntechnological systems. Applied technology ethics draws on humanist principles to advocate for\nhuman-centered design, meaningful human oversight of automated systems, and the\npreservation of human choice and autonomy in technological environments.\nPage 7 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 8, "content": "Ethics in Technology\nBy: Ed Weber\n• Feminism (19th-20th Century)\nFeminist ethics emphasizes care, relationships, context, and the examination of power\nstructures, particularly how they affect marginalized groups. This movement brings essential\nperspectives to technology ethics by highlighting how technological systems can perpetuate or\nchallenge existing inequalities and by advocating for inclusive design processes. Applied\ntechnology ethics incorporates feminist insights to address issues like algorithmic bias, the\ndigital divide, and ensuring diverse voices are included in technological development and\ngovernance.\n• Phenomenology (20th Century)\nPhenomenology focuses on lived experience, consciousness, and how individuals encounter and\nmake meaning of their world. This movement contributes to applied technology ethics by\nemphasizing the importance of understanding how people actually experience and interact with\ntechnology in their daily lives. Phenomenological approaches help bridge the gap between\nabstract ethical principles and the concrete realities of how technology affects human\nexperience, informing more nuanced and contextually sensitive ethical frameworks.\nThere are entire courses that could be taken on each one of these broad philosophical movements.\nBut just knowing their names and descriptions can give you a starting point should you should choose\nto explore them in greater detail.\nNow, let’s take a quick look at just a few of the historical and contemporary ethicists who have\nstudied, expanded upon, and have otherwise contributed greatly to the pursuit of understanding of\napplied ethics and summarize some of their major propositions and how they might approach ethical\nissues related to technology today.\nEthicists and Their Approach to Technology Ethics\n• Immanuel Kant (Deontological Ethics)\n◦ Ethical Center: Kant's ethical philosophy is grounded in the \"Categorical Imperative,\"\nasserting that moral duties are universal and rational, requiring actions that could be applied\nwithout contradiction to all individuals and treating humanity as an end in itself, never\nmerely as a means. Morality stems from reason and duty, independent of consequences.\n◦ Approach to Technology Today: Kant would scrutinize technological advancements\nthrough the lens of human dignity and autonomy, insisting that AI, data collection, and\nautomation must never instrumentalize individuals. He would advocate for strict, universal\nethical rules in technology design and usage, such as mandatory privacy by design and\nPage 8 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 9, "content": "Ethics in Technology\nBy: Ed Weber\nalgorithmic transparency, ensuring that technological systems uphold inherent human rights\nand rational agency.\n• René Descartes (Rationalism)\n◦ Ethical Center: Descartes' ethical center is built on methodical doubt, systematic\nreasoning, and the pursuit of clear and distinct knowledge through logical analysis rather\nthan relying on tradition or emotion. His approach emphasizes breaking down complex\nproblems into manageable parts and building knowledge from foundational principles\nthrough careful reasoning.\n◦ Approach to Technology Today: Addressing contemporary technology ethics, Descartes\nwould advocate for systematic, step-by-step analysis of technological systems, demanding\nclear logical justification for each design choice and rejecting technological\nimplementations based merely on convenience, profit, or popular opinion without rigorous\nethical reasoning.\n• John Stuart Mill (Utilitarianism)\n◦ Ethical Center: Mill's ethical center focuses on maximizing overall happiness and well-\nbeing while protecting individual liberty, emphasizing that the greatest good for the greatest\nnumber must be balanced against the fundamental importance of personal freedom and self-\ndetermination. His harm principle argues that society can only restrict individual liberty to\nprevent harm to others, creating a framework that values both collective welfare and\nindividual autonomy.\n◦ Approach to Technology Today: When applied to technology, Mill’s ideas guide ethical\nevaluations of innovations like autonomous vehicles or predictive algorithms, helping to\nweigh benefits (e.g., safety, efficiency) against societal costs (e.g., job loss, privacy erosion).\n• Martha Nussbaum (Humanism)\n◦ Ethical Center: Nussbaum's ethical center emphasizes human capabilities, dignity, and\nflourishing, arguing that societies should be structured to enable all individuals to develop\ntheir full human potential across multiple dimensions of well-being. Her capabilities\napproach focuses on what people are able to do and be, rather than just material resources,\nemphasizing the importance of agency, practical reason, and meaningful relationships.\n◦ Approach to Technology Today: Approaching contemporary technology ethics, Nussbaum\nwould evaluate digital systems based on whether they enhance or diminish human\ncapabilities – supporting technologies that expand access to education, meaningful work,\nand social connection while opposing those that create dependency, reduce critical thinking,\nor undermine human agency and authentic relationships.\nPage 9 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 10, "content": "Ethics in Technology\nBy: Ed Weber\n• Carol Gilligan (Feminist Ethics)\n◦ Ethical Center: Gilligan's ethical center emphasizes an ethics of care that values\nrelationships, context, and responsibility, challenging traditional moral frameworks that\nprioritize abstract rights and justice over concrete care and connection. Her work highlights\nhow ethical reasoning often involves understanding particular situations and maintaining\nrelationships rather than applying universal principles, and she emphasizes the importance\nof listening to marginalized voices, especially women's moral perspectives.\n◦ Approach to Technology Today: In addressing technology ethics today, Gilligan would\nfocus on how digital systems affect relationships and care networks, advocating for\ninclusive design processes that center the experiences of marginalized users and questioning\nwhether technologies strengthen or weaken our capacity for empathy, care, and authentic\nhuman connection.\n• Don Ihde (Phenomenology)\n◦ Ethical Center: Ihde's ethical center focuses on human-technology relations and how\nhumans and technologies mutually shape each other's existence, arguing that we cannot\nunderstand human experience without examining our relationships with technological\nartifacts. His postphenomenological approach emphasizes that technologies are neither\nneutral tools nor autonomous forces, but rather extend and transform human capabilities\nwhile simultaneously shaping how we perceive and act in the world.\n◦ Approach to Technology Today: Approaching contemporary technology ethics, Ihde\nwould analyze how specific technologies – from smartphones to AI systems – alter our ways\nof being-in-the-world, advocating for careful attention to how digital interfaces change our\nperceptual habits, social relationships, and bodily engagement with our environment, while\nemphasizing that ethical evaluation must consider the concrete, lived experience of human-\ntechnology interactions rather than abstract technological assessments.\nAs you review the descriptions above and the works of these (or other) historical and contemporary\nethicists, you will quickly discover that there are not singular, universally accepted definitions for the\nterms that are used to discuss ethics. As a matter of course, as scholars work to build their own\nunderstandings within their own disciplines, they often find themselves redefining previously accepted\nterms with new or altered meanings and nuances. In fact, you will discover completely new terms\nbeing coined to describe specific concepts or novel interpretations of life observations.\nPage 10 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 11, "content": "Ethics in Technology\nBy: Ed Weber\nFrameworks and Personal Lenses\nIn this section we need to explore a bit more detailed information about myself as the author, and\nhow this information is necessary to establish the framework in which the rest of the material for this\ntext will be presented. It is highly suggested that everyone who utilizes this text (instructors and\nstudents alike) prepare their own supplemental Full Disclosure section (found below) to help you to\nidentify and understand your own personal frameworks and personal lenses. This will greatly facilitate\nfuture topic discussions.\nFull Disclosure: Your textbook author is a 61-year-old, white, cisgender, heterosexual married\nman. He was born and raised Catholic and went to Catholic elementary and high schools. He grew up\nin the Midwest of the USA in a lower-middle-class household within a homogeneous neighborhood (re:\nethnicity, religion, socioeconomic status). He has been married to his wife, Kim for 40 years and they\nhave no children by choice (… except for dogs… there will always be dogs!) Your author currently\nconsider himself to be in the middle class on a socioeconomic scale. While your author does have some\nminor health issues and wears glasses to correct his vision, he would consider himself non-disabled. He\nis no longer a Catholic and identifies instead as an agnostic.\nYour textbook author initially earned a Certificate of Proficiency in Data Processing in the late\n1980s. This credential allowed him to begin a decades-long career in Information Technology. Much\nlater on, he earned a Bachelor’s Degree in Psychology with a Minor in Computer Science after having\nalready been actively working in the field for over 20 years. In 1995, he started his own computer\nconsulting firm which is still active today. In the mid-2000s, he earned a Master’s Degree in Computer\nScience so as to facilitate his transition into full-time teaching at colleges and universities. He considers\nhimself to be a life-long learner and enjoys working with others on challenging activities.\nNow, considering these details about your textbook author, I am going to shift modes into a\nconversational mode just to talk to you about why I shared all of those details: I have shared these\ndetail – not because the I consider myself to be significant or special in any way – but rather it is\nbecause these facts (and more) are the things that make up the ‘who’ that I am today as I compose this\ntext. And, as a matter of practice, whenever I attempt to think about the ethics of a given situation or\ntechnology, I try to actively take a step backwards and try to view the situation through all of the lenses\nthat make me who I am. It can be very enlightening when I discover, “… Oh! That is where my\nattitude about this particular thing is coming from!” Sometimes, this type of introspection leads me to\nbe able to say, “Wait a minute! It isn’t ME that thinks this particular way about this particular thing…\nRather, I was taught/told/indoctrinated/instructed to think this particular way about this particular\nthing! And now that I really think about it – for myself – I realize that I am actually free to choose how\nI actually think about this thing – all on my own!”\nSo, I offer up my own details here just so that you will have a good understanding of the framework\nthat I am using to present and discuss the various concepts we cover throughout the rest of this text. It\nPage 11 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 12, "content": "Ethics in Technology\nBy: Ed Weber\nis by intentionally inspecting and acknowledging our own lenses, that we might be able to uncover and\nappropriately restrain our own preconceived biases.\nAs you engage with the concepts and case studies in this textbook, it is important to recognize the\nunique set of experiences, values, and perspectives that shape your own ethical viewpoints. In this text,\nwe will call these perspectives your personal lenses. Honest self-reflection can help you become more\naware of your own assumptions and biases as you look through your personal lenses, allowing for more\nthoughtful and inclusive ethical reasoning. To help us consider these personal lenses, consider this\ninternet meme:\nFigure 3: Truth perceived from different perspectives\nThe person standing near the orange light source and looking at the shadow only might say that the\nshape making this shadow is a square or a cube. And from their perspective at that moment in time,\nthis may seem to be true.\nBut the person standing near the blue light source and looking at the shadow only might say that the\nshape making this shadow is a circle or a sphere. And, once again, from their perspective at that\nmoment in time, this too may seem to be true.\nPage 12 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 13, "content": "Ethics in Technology\nBy: Ed Weber\nHowever, by taking a step back to view even more pertinent information, a third observer can see\nthat the shape appears to be – as seen from this third perspective – a cylinder suspended in such a way\nthat the two light sources cast these two unique shadows.\nUnderstanding that there may, in fact, be a difference between ‘Truth’ as defined with a capital ‘T’\nwhen viewed from an objective perspective, as compared to what appears to be ‘true’ from our own\nunique perspective at any moment in time, may help us think more critically about the importance of\nunderstanding our own personal lenses.\nConsider asking yourself the following questions:\n• What aspects of my background (such as age, race, ethnicity, gender identity, sexual\norientation, religion, socioeconomic status, education, or geographic location) have most\ninfluenced the way I see the world?\n• How did my family, community, or culture shape my attitudes toward technology, authority,\nand ethics?\n• What are my core values, and where did they come from? Have any of my values changed\nover time? If so, why?\n• Are there beliefs or viewpoints I hold mainly because they were taught to me, rather than\nones I have critically examined for myself?\n• How do my personal experiences with privilege or marginalization affect the way I interpret\nethical dilemmas?\n• Have I ever changed my mind about a major ethical issue? What prompted that change?\n• In what ways do my current roles (student, employee, family member, etc.) influence my\nperspective on ethical questions?\n• Are there perspectives or experiences I am less familiar with? How might I seek out and\nlearn from voices different from my own?\n• When I encounter a viewpoint that challenges my own, how do I typically respond? Am I\nopen to reconsidering my position?\n• What steps can I take to recognize and address my own biases as I study Ethics in\nTechnology?\nBy thoughtfully considering these questions, you can better understand the framework through\nwhich you interpret ethical issues and strive for greater objectivity and empathy in your analysis.\nLet’s also look at this concept of reviewing a situation both passively (ignoring our own personal\nlenses and perspectives) and then subsequently reviewing the same situation intentionally\nPage 13 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 14, "content": "Ethics in Technology\nBy: Ed Weber\n(acknowledging and evaluating our own personal lenses and perspectives.) Consider this initial\ninternet meme:\nFigure 4: 6 vs. 9 as evaluated from one's own\nperspective\nThis meme, similar to the one shown in the Figure 2 earlier in this chapter, illustrates how the same\n‘Truth’ (with a capital ‘T’) can actually represent two significantly different ‘truths’ when one only\nfocuses on their own personal lenses and their own current perspective.\nIn fact, the caption within this meme attempts to establish the importance of trying to help the\nviewer consider what the other perspectives may look like. But is this easy to do? Consider, for\nexample, all of the personal lenses that this textbook author shared in the previous section. Will I ever\nbe able to realistically look at (or more rightly so – even imagine) any situation through the lenses of a\nnon-white, economically struggling, lesbian, Baptist, still wanting to finish their GED someday…? My\npersonal lenses are so significantly different than their lenses – and yet we may be looking at the exact\nsame situation.\nBut is this realistic? Is it realistically even possible for us to completely recognize, understand,\nacknowledge and then fully contain all of our own personal lenses and our own current, unique\nperspective? Are we certain that there may not be any residual biases still influencing our\ninterpretation of what we perceive to be both ‘true’ as well as the ‘Truth’?\nDo I have what it takes to be able to step away from my own personal lenses (without forgetting\nthem or failing to acknowledge them) so that I can try to see through what I imagine another person’s\npersonal lenses might look like? Do I also recognize that my own imaginations of someone else’s\nPage 14 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 15, "content": "Ethics in Technology\nBy: Ed Weber\npersonal lenses may be pure fantasy? How can I go about really trying to understand what another’s\npersonal lenses really look like for them?\nFor me, this kind of effort begins and ends with communications. Finding ways to actually ask\nanother about their own personal lenses and perspectives seems to be a profound beginning. Then,\nactively listening as they share their own experiences – without judgment – and trying to repeat back\nyour own understanding of what they actually say seems like an effective follow-up. Only after we\nhave a shared understanding of each other’s personal lenses can we really begin to have effective\nethical discussions about various technical topics.\nNow, for contrast, consider this follow-up internet meme that was based on the original theme:\nFigure 5: 6 vs. 9 reconsidered from an objective view\nThis meme attempts to suggest that ‘Truth’, with a capital ‘T’ exists (a concept that has been\nheavily debated by many ethicists throughout history!) and can be justly recognized if one simply\nchooses to step away from their own perspective and view the situation objectively. So, while we\nPage 15 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 16, "content": "Ethics in Technology\nBy: Ed Weber\nmight agree with the concept of trying to step outside of our own personal lenses to see a situation from\na different perspective, this meme author seems more intent to show that the value from the act itself\nisn’t about fostering empathy for each person’s perspective (and personal lenses). Rather, this meme\nauthor seems to imply that the value is more-so that one of these people can use this technique to prove\nthe other one wrong! For this unknown meme author, it’s not about empathy but rather it is about\nwinning.\nKey Foundational Concepts\nAs you continue deeper into this text, you will discover there are a number of key concepts that are\nused repeatedly to help facilitate greater discussions and understandings of the various topics. This list\nwill also serve as a preview of some of the upcoming chapter concepts.\n• Definition of terms – Whenever not previously defined, new critical terms will be defined at\neach introduction so as to present what I, as the author, am using as the definition of each given\nterm. This does not mean I am claiming supreme authority regarding the term! Rather, it is the\nstarting point from which you can determine if you agree with the definition or not and what, if\nany, compromises might need to be made so as to have a meaningful discussion. It is\nimperative that individuals who intend to have meaningful interactions at least have a shared\nunderstanding of the terms being used – if not full agreement regarding the terms!\n• Technology can be found almost everywhere – If you are reading this text, it is most likely\nthat you are already familiar with very many forms of technology. For the vast majority of\nindividuals, the year 2025 is synonymous with technology being integrated into almost every\naspect of our existence. And as such, technology is often taken for granted without full\nconsideration of any potential ethical considerations of that tech. For example, how often do\nwe think about how a particular piece of tech was developed or created or distributed? Do we\never wonder just who has access to that tech and who does not have access? Are there any\nunintended uses of the tech that we should be thinking about?\n• The rate of change of technology is NOT linear – Throughout most of human history,\ntechnological change was slow and incremental, with major breakthroughs – like the control of\nfire, the invention of the wheel, and the development of agriculture – occurring over thousands\nof years. This gradual pace continued through the Bronze and Iron Ages and even into early\nmodern times. However, beginning with the Industrial Revolution, the rate of technological\nadvancement accelerated dramatically, as mechanization, mass production, and new forms of\nenergy rapidly transformed societies in mere centuries. Now within less than the last 100 years,\nthe emergence of computers, the internet, and digital technologies, have brought an\nunprecedented surge in innovation and human adoption rates. The current era is unique for its\nexponentially rapid, transformative impact on nearly every aspect of life.\nPage 16 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 17, "content": "Ethics in Technology\nBy: Ed Weber\n• There is no universally accepted Ethical system – The absence of a universally accepted\nethical system means that actions are often interpreted through a variety of moral frameworks,\nleading to nuanced discrepancies or even diametrically opposed viewpoints. For example, the\nact of taking another person’s life can be labeled as “murder,” “justifiable homicide,” or “self-\ndefense,” depending on the ethical system, cultural context, or legal tradition applied. What one\nsociety or theory may condemn as inherently immoral, another may view as permissible or even\nobligatory under certain circumstances, illustrating how ethical judgments can range from\nsubtle distinctions to fundamentally conflicting positions. This diversity reflects the influence of\ndifferent normative theories – such as utilitarianism, deontology, virtue ethics, and cultural\nrelativism – each offering its own criteria for evaluating right and wrong. This illuminates the\ncomplexity and subjectivity inherent in ethical reasoning.\n• Legal and Societal systems attempt to implement a well defined and accepted Ethical\nsystem – Legal and societal systems tend to follow a well-defined and accepted ethical system\nbecause such alignment provides a consistent moral foundation for laws and policies, ensuring\nthat actions and decisions are guided by shared principles of fairness, justice, and respect for all\nindividuals. This ethical grounding fosters public trust, transparency, and accountability, helping\nto protect the rights and interests of all members of society while reducing arbitrary or biased\ndecision-making. Ultimately, integrating ethics into legal and societal frameworks promotes\nsocial cohesion, supports long-term sustainability, and enhances the legitimacy and\neffectiveness of institutions by aligning them with the values and expectations of the\ncommunity.\n• When considering the Ethics of Technology, the issue is almost never the Technology, but\nrather the issue is almost always the Ethics – The ethical evaluation of technology often\nreveals that the core issue lies not with the technology itself, but with how humans choose to\nuse it, as the same contemporary tool can enable outcomes at nearly opposite ends of the moral\nspectrum. For example, artificial intelligence can be harnessed to improve healthcare\ndiagnostics, enhance disaster response, and promote environmental sustainability, yet the very\nsame AI systems can also be deployed for mass surveillance, autonomous weaponry, or\ndiscriminatory decision-making. This dual-use dilemma illustrates that technologies are\ninherently neutral, but their ethical character is defined by human intentions, societal values,\nand regulatory choices, resulting in applications that can be seen as highly beneficial or deeply\nproblematic depending on context and use. Thus, it is the ethical framework guiding\ndeployment and oversight – not the technology itself – that determines whether its impact is\nviewed as just, responsible, or harmful.\n• With technology, very often science fiction can be viewed as science fact that just hasn’t\nhappened yet – Since the age of radio and television, science fiction authors, movies, and\nPage 17 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 18, "content": "Ethics in Technology\nBy: Ed Weber\nshows have repeatedly imagined technologies that later became reality, often serving as\ninspiration or conceptual blueprints for real-world innovation. Classic examples include video\ncalling, featured in \"The Jetsons\" and \"Metropolis,\" now realized through platforms like Zoom;\nwireless earbuds, reminiscent of Ray Bradbury’s \"Fahrenheit 451,\" now ubiquitous as devices\nlike AirPods; and handheld communicators from \"Star Trek,\" which anticipated today’s\nsmartphones. Science fiction has also envisioned self-driving cars, as described by Isaac\nAsimov, and immersive virtual reality, as seen in \"The Matrix,\" both of which are now active\nareas of technological development. Even broader concepts, such as global information\nnetworks and intelligent digital assistants, were explored in early fiction long before the internet\nor AI became commonplace. This demonstrates how speculative storytelling has consistently\nanticipated – and sometimes directly influenced – the trajectory of technological advancement.\nTextbook Definitions – Introduction, Ethical Frameworks and Personal Lenses\n• right vs. wrong – The distinction between actions or choices considered morally acceptable\nand those considered morally unacceptable.\n• good vs. evil – The contrast between that which is morally virtuous, beneficial, or constructive\nand that which is morally wrong, harmful, or destructive.\n• rightness vs. wrongness – The quality of being in accordance with moral or ethical principles\nversus being in violation of them.\n• defining terms – The process of clearly explaining the meaning of words or concepts to ensure\nclarity and understanding.\n• shared understanding – A mutual agreement or common interpretation of ideas, terms, or\nvalues among individuals or groups.\n• Ethics – The branch of philosophy concerned with moral principles that govern behavior and\ndecision-making.\n• Deontological Ethics – An ethical theory that judges the morality of actions based on\nadherence to rules or duties, regardless of their consequences.\n• Rationalism – A philosophical view that emphasizes reason and logical analysis as the primary\nsources of knowledge and ethical judgment.\n• Utilitarianism – An ethical framework that evaluates actions based on their outcomes, aiming\nto maximize overall happiness or well-being.\nPage 18 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "section_path": ["Foundations; Frameworks and Personal Lenses; Key Concepts"], "page": 19, "content": "Ethics in Technology\nBy: Ed Weber\n• Humanism – A worldview that centers human dignity, agency, and the promotion of individual\nand collective flourishing as ethical priorities.\n• Feminism – An ethical and social movement that advocates for gender equality and emphasizes\nthe importance of care, context, and power dynamics in moral decision-making.\n• Phenomenology – A philosophical approach that focuses on individuals’ lived experiences and\nthe ways in which they perceive and interpret the world around them.\n• personal lenses – The unique perspectives shaped by an individual’s experiences, values, and\ncultural background through which they interpret the world.\n• rate of technological advancement – The speed at which new technologies are developed and\nadopted within society.\nPage 19 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 20, "content": "Ethics in Technology\nBy: Ed Weber\n3. Defining Ethics and Related Terminology\nWhy are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical\nThinking and Limiting Biases\nWhy are we here?\nWow! If THAT isn’t a loaded question!\nBut that is exactly what we would like to tackle at this point. Why should we think about this\nconcept of Ethics and how it can be defined and applied to technology?\nTo get our own thoughts germinating, I asked several generative AI platforms (Perplexity,\nChatGPT, Claude, and Gemini) to provide a hypothetical response to this question as written from the\nperspective of several selected ethicists. Here is what was generated:\n• Immanuel Kant (Deontological Ethics)\n\"We must think about ethics in technology because human beings possess inherent dignity as\nrational agents, and technology must never treat us merely as means to an end. Every\ntechnological system embeds moral choices about how humans should be treated, and without\nethical reflection, we risk creating tools that violate our fundamental duty to respect human\nautonomy and dignity. The categorical imperative demands that we only develop technologies\nwhose principles we could will to be universal laws—would we want to live in a world where\nall technology operates according to the ethical assumptions built into our current systems?\"\n• René Descartes (Rationalism)\n\"We must apply systematic ethical reasoning to technology because, without methodical\nanalysis, we build upon uncertain foundations that may lead us astray. Just as I doubted\neverything to reach clear and distinct knowledge, we must doubt the moral assumptions\nembedded in our technologies and rebuild them on solid rational principles. Technology\namplifies human action, so any errors in our ethical reasoning will be magnified across society\n—we cannot afford to proceed on mere tradition, convenience, or popular opinion when the\nstakes are so high.\"\n• John Stuart Mill (Utilitarianism)\n\"Ethics in technology matters because technology's power to generate both tremendous benefits\nand serious harms across vast populations demands careful calculation of consequences. We\nmust evaluate whether our technological choices truly maximize overall human happiness and\nwell-being, while vigilantly protecting individual liberty from technological systems that might\nrestrict human self-determination. The harm principle requires us to examine whether our\ntechnologies prevent individuals from pursuing their own good in their own way, even when\nsuch technologies might benefit the majority.\"\nPage 20 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 21, "content": "Ethics in Technology\nBy: Ed Weber\n• Martha Nussbaum (Humanism)\n\"We need ethics in technology because technology should serve human flourishing by\nexpanding rather than constraining human capabilities. Every technological design choice\naffects whether people can live fully human lives—whether they can engage in practical reason,\nform meaningful relationships, experience joy, exercise bodily autonomy, and participate in\npolitical life. Without ethical reflection, we risk creating technologies that diminish rather than\nenhance what makes us human, particularly for those who are already marginalized or\nvulnerable in society.\"\n• Carol Gilligan (Feminist Ethics)\n\"Ethics in technology is essential because technological decisions affect relationships, care\nnetworks, and the voices that get heard in our society. We must ask not just whether\ntechnologies work efficiently, but whether they strengthen or weaken our capacity for care,\nempathy, and connection with others. Ethical reflection helps us recognize whose perspectives\nhave been excluded from technological design and ensures that we consider how our tools\naffect the most vulnerable members of our communities, particularly when those voices are\noften silenced in traditional ethical frameworks.\"\n• Don Ihde (Phenomenology)\n\"We must think ethically about technology because humans and technologies co-constitute each\nother's existence—there is no neutral technology, and every technological relation shapes how\nwe perceive, act, and experience the world. Ethics helps us examine how specific technologies\nalter our ways of being-in-the-world, sometimes in ways we don't immediately recognize.\nWithout phenomenological reflection on our actual lived experience with technologies, we may\ninadvertently adopt tools that fundamentally change what it means to be human without\nconscious deliberation about whether those changes align with our values.\"\nDid these hypothetical responses from historic and contemporary ethicists actually help you\ndiscover or clarify your own reason for being here?\nFor some individuals, simply reviewing what others have previously thought or discovered about a\ntopic can be truly enlightening and inspiring! Academics and Ethicists through the ages have invested\nmuch of their lives and studies to review, categorize, differentiate, and subsequently expand and\nexpound on what others have to say about Ethics. And sometimes, they find that one or more of these\nother people had previously found ways to express, precisely or nearly so, just what it was that they,\nthemselves, think about the topic.\nAnd yet for other individuals, the flood of disparate and often contradictory positions on the\nsupposed same topic seems to simply add levels of confusion and introduce unintentional biases.\nPage 21 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 22, "content": "Ethics in Technology\nBy: Ed Weber\nThese individuals feel that the research actually gets in the way of their own ability to figure anything\nout for themselves.\nSo, whichever kind of person you may be, I’d like to re-ask the questions: Why are we here? Why\nshould we think about this concept of Ethics and how it can be defined and applied to technology?\nVery early in my career, I learned an important lesson that has informed my entire professional and\nteaching philosophy: When working with adults, if all individuals involved can first understand and\nagree on ‘the WHY’ of whatever it is that they are doing, then adults tend to be more receptive to fully\nengage in the related tasks. So, for me, I try to consistently first ask and answer the question, “Why are\nwe doing this again?” before we even get started.\nFor me, the WHY is that, as a technology professional for my entire career, I have seen too often –\nfirst hand – what the unethical implementation of technology looks like and what I have perceived to\nbe the harmful ramifications of these unethical situations. And as such, I choose to intentionally\nexplore the ethics surrounding technology so that I can be more prepared to work with students,\ncolleagues and clients as we collectively discover just what the ethical issues might be. This has helped\nme identify and recognize some of my own biases and has helped me learn how to be more objective in\nmy ethical analysis of new technological advancements.\nAlso, as previously mentioned, the proactive intentional examination of the Ethics of Technology\nmay reward individuals by helping them to feel more prepared when they find themselves required to\nmake decisions or take actions with significant ethical implications.\nTextbook Definitions – Defining Ethics and Related Terminology\nIn order to create a baseline of terminology that will be used throughout the rest of the text, this\nsection now provides a collection of terms along with definitions to be used as some of the basic\n‘textbook definitions’ for continuing conversations. These definitions are not to be considered\nabsolute! In fact, it will be important for you to ask yourself if you agree with the definition as\npresented or, if not, what aspects do you think may need clarification. Also, in future chapters, the\nadditional terms will be listed at the end of the chapter vs. here, in the middle of the chapter.\n• Ethics – The systematized principles and standards of right and wrong behavior, typically\nestablished and endorsed by a community or society.\n• Morals – Personal beliefs and internalized values about what is right and wrong, guiding\nindividual behavior.\n• Virtues – Positive character traits or qualities, such as honesty or courage, that are considered\nmorally good and enable individuals to act in alignment with ethical principles.\nPage 22 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 23, "content": "Ethics in Technology\nBy: Ed Weber\n• Vices – Negative character traits or habits, such as dishonesty or greed, that are considered\nmorally bad and detract from ethical behavior.\n• Beliefs – Convictions or accepted ideas that shape an individual's morals, virtues, and\nworldview.\n• Right – Actions or behaviors that are considered morally or ethically acceptable or good.\n• Wrong – Actions or behaviors that are considered morally or ethically unacceptable or bad.\n• Education – The process of acquiring knowledge, skills, values, and attitudes, often shaping\nethical understanding and personal development.\n• Traditions – Long-standing customs or practices passed down within a culture or community,\ninfluencing values and behavior.\n• Life Experiences – Personal events and interactions that shape one’s perspectives, beliefs, and\nethical outlook.\n• Culture – The shared values, norms, practices, and artifacts of a group that influence behavior\nand ethical perspectives.\n• Religion – Organized systems of beliefs and practices related to the sacred or divine, often\nproviding ethical guidance.\n• Gender – Socially constructed roles, behaviors, and identities associated with being male,\nfemale, or non-binary, influencing experiences and perspectives.\n• Age – The length of time a person has lived, often affecting their perspectives and ethical\nviewpoints.\n• Personal Lenses – The unique set of experiences, values, and perspectives that shape your\nown current ethical viewpoints.\n• Critical Thinking – The disciplined process of actively analyzing, evaluating, and synthesizing\ninformation to form reasoned judgments and make well-informed decisions.\n• Semantics – The branch of linguistics that studies the meaning of words, phrases, and\nsentences, examining how meaning arises from language structure, word choice, and context to\nconvey and interpret information.\n• Individual – A single person, distinct from a group, with unique experiences and perspectives.\n• Group – A collection of individuals who interact and share common characteristics, goals, or\ninterests.\n• Group Member – An individual who belongs to and participates in a group.\nPage 23 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 24, "content": "Ethics in Technology\nBy: Ed Weber\n• Integrity – The quality of being honest and having strong moral principles, consistently acting\nin accordance with ethical standards.\n• Law – A system of rules established by a governing authority to regulate behavior within a\nsociety.\n• Legal – Actions or behaviors that are permitted or recognized by law.\n• Illegal – Actions or behaviors that are forbidden by law.\n• Moral – Conforming to principles or standards of right conduct; virtuous.\n• Amoral – Lacking a sense of morality or indifference to right and wrong.\n• Ethical – In accordance with accepted principles of right and wrong, especially within a\nprofessional or societal context.\n• Unethical – Contrary to accepted standards of right and wrong.\n• Misconduct – Improper, unethical, or illegal behavior.\n• Lying – Knowingly making a false statement with the intent to deceive.\n• Cheating – Acting dishonestly or unfairly to gain an advantage.\n• Stealing – Taking something that does not belong to you without permission or legal right.\n• Abusive Behavior – Actions that cause harm, mistreatment, or suffering to others.\n• Discrimination – Unjust or prejudicial treatment of individuals or groups based on\ncharacteristics such as race, gender, age, or religion.\n• Hazardous – Posing a risk of harm or danger.\n• Conflict of Interest – A situation in which a person’s personal interests could improperly\ninfluence their professional decisions or actions.\n• Falsifying Information – Deliberately altering, inventing, or misrepresenting information with\nthe intent to deceive.\n• Honesty – The quality of being truthful, transparent, and free from deceit.\n• Fairness – Treating people equally and justly, without favoritism or discrimination.\n• Responsibility – The obligation to act correctly and be accountable for one’s actions.\n• Duty – A moral or legal obligation to perform or refrain from certain actions.\nPage 24 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 25, "content": "Ethics in Technology\nBy: Ed Weber\n• Obligation – A requirement to act in a particular way, often arising from law, contract, or moral\nprinciple.\n• Stakeholder – Any individual or group affected by or having an interest in the actions and\ndecisions of an organization.\n• Shareholder – An individual or entity that owns shares in a corporation and has a financial\ninterest in its performance.\n• Consumers – Individuals or groups who purchase and use goods or services.\n• Customers – People or organizations that buy goods or services from a business.\n• Employers – Individuals or organizations that hire and pay people to work for them.\n• Suppliers – Entities that provide goods or services to other organizations.\n• Community – A group of people living in the same area or sharing common interests, values,\nor goals.\n• Environment – The natural world, including air, water, land, and ecosystems, affected by\nhuman activity.\n• Corporation – A legal entity that is separate from its owners, with its own rights and\nresponsibilities.\n• Corporate Social Responsibility – A business model in which companies integrate social and\nenvironmental concerns into their operations and interactions with stakeholders.\n• Sustainability – Meeting present and continuing needs without compromising the ability of\nfuture generations to meet their own needs, especially regarding environmental stewardship.\n• Consistency – Acting in the same way over time, maintaining coherence in values, principles,\nand behavior.\n• Goodwill – A positive reputation or relationship built through ethical actions and\ntrustworthiness.\n• Protection – The act of keeping people, property, or the environment safe from harm.\n• Favorable – Producing or indicating a positive outcome or approval.\n• Unfavorable – Producing or indicating a negative outcome or disapproval.\n• Diversity – The presence of a wide range of different characteristics, backgrounds, and\nperspectives within a group or organization.\n• Respect – Recognition and regard for the rights, feelings, and dignity of others.\nPage 25 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 26, "content": "Ethics in Technology\nBy: Ed Weber\n• Principles – Fundamental truths or rules that guide behavior and decision-making.\n• Standards – Established benchmarks or criteria used to measure and guide conduct or\nperformance.\n• Reward – Something given in recognition of service, effort, or achievement.\n• Punishment – A penalty imposed for wrongdoing or violation of rules.\n• Profit – The financial gain obtained when revenue exceeds expenses.\n• Loss – The negative financial result when expenses exceed revenue.\n• Empowered – Having the authority, confidence, or power to make decisions and take action.\n• Disenfranchised – Deprived of rights, power, or access, especially to participate in decision-\nmaking.\n• Code of Ethics – A formal set of guidelines and principles designed to help professionals\nconduct business honestly and with integrity.\n• Leading by Example – Demonstrating desired behaviors and standards through one’s own\nactions, serving as a model for others.\nAs new content is introduced, additional terms will be collected and presented at the end of each\nchapter to facilitate future discussions using those same terms. If you feel strongly that your own\npersonal definition of any particular term varies significantly from these ‘textbook definitions’, you\nshould always discuss these differences with the rest of the individuals in your discussion group to\ndetermine how these differences may or may not affect the ongoing conversations. It is important to\nnot simply dismiss these differences as minor differences in semantics when, in fact, these differences\nmay represent the root cause of major differences in perspectives.\nAs you can see, in order to fully examine a concept like Ethics in Technology, a large part of the\npreparatory work involved is making sure that we each have a shared, common baseline from which to\nbegin our work together. This includes coming to a shared agreement on a large number of terms that\nmay effect the conversations. Here are just a few examples of terms that will need to be defined in\nupcoming chapters as part of their exploration:\nintention, manipulation, transparency, true vs. false, excess, greed, bias, preferential treatment,\nconflict of interest, bribery vs. gift, fraud, whistle-blowing, negligence, reasonable person, adult, child,\nassault, threat, harm, freedom of speech, hate speech, censorship, obscenities, rights, privileges,\nnational security, responsibility, liability, expectation of privacy, safety, liberty, risk, customer lock-in,\nPage 26 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 27, "content": "Ethics in Technology\nBy: Ed Weber\nprofit, loss, capitalism, socialism, communism, feudalism, representation, health and welfare,\nmarketing, stalking, exploitation, employee rights vs. employer rights, … so many more…\nNot to mention all of the technology-related terms we will explore in future chapters such as:\nautomation, robotics, artificial intelligence (AI), big data, natural language processing (NLP),\nmachine learning, telecommunications, vulnerabilities, spam, phishing, virtual private networks\n(VPNs), biometrics, passwords, personally-identifiable information (PII), firewalls, prevention,\nsurveillance, remote monitoring, doxing, sexting, copyright, trademark, intellectual property, trade\nsecret, cybersquatting, industrial espionage, quality assurance, telemedicine, 3D printing, 3D bio-\nprinting, planned obsolescence, chatbots, digital assistants, product liability, breaches, electronic\nmedical records (EMR), cyberstalking, cyberbullying, sexual predators, revenge porn, … and so many\nmore of these as well!\nEthical vs. Legal\nUsing the definitions of Ethical, Unethical, Legal, and Illegal, as defined above, consider this\nmatrix:\nFigure 6: Ethical vs. Legal Matrix\nSeveral concepts that tend to be regularly (but incorrectly) confounded during discussions, are the\nconcepts of something being either Ethical or Unethical, as well as that same thing being either Legal\nor Illegal. Ethics and Legalities are two different concepts. As you can see by the matrix above, a\nsingle item or situation can exist in any single one of the white cells.\nIn other words, something can be both Ethical as well as Legal – which many individuals think is\nthe ideal situation. But our shared reality has shown us that the ideal situation isn’t always what we\nexperience. Rather, you may discover that another thing may be completely Legal, while\nsimultaneously being completely Unethical. It many societies, one can simply examine the current or\nhistoric laws or codes of conduct and they will inevitably be able to discover numerous instances of\nPage 27 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 28, "content": "Ethics in Technology\nBy: Ed Weber\nlaws that, at the time, were considered Legal, however they can simultaneously be considered\ncompletely Unethical if not abhorrent.\nLikewise, there can be many situations where something is completely Ethical, but may be\nsimultaneously Illegal! And finally, something could be completely Unethical and also completely\nIllegal. We all have probably seen these kinds of situations as these often end up as the Breaking New\nstories because of their often significant shock factor!\nTake a few minutes now and see if you can come up with several instances of situations for each of\nthose four possible matrix cells. Then, ask yourself how you would be able to present, to someone else,\nyour reasoning for placing each individual situation into its own respective cell. Understanding the\n‘WHY’ that you have used to come to your conclusions is a major aspect of applied critical thinking\nskills.\nCritical Thinking and Limiting Biases\nThinking now about the terms that have been presented in this chapter, what happens when one or\nmore of our personal lenses illuminates a conflict when accepting the definition of a particular term?\nWhat happens when an education lens defines a concept one way and that definition is in conflict with\na definition previously constructed through a religious lens? What about when a definition changes\nwhen viewed through a gender lens vs. using a cultural lens? How about when your family lens\ndefines something one way, but your work community defines that same thing a different way? Do you\nknow how to first identify whenever there is a conflict in definitions? Do you then know how to isolate\nthe differences so that points of agreement and points of disagreement can be fully fleshed out?\nThe ability to review your own perspectives from an objective point of view is a very large part of\napplying the concept called Critical Thinking. This activity can help you to become well-prepared to\nhave in-depth, meaningful discussions about the underlying ethics of any given topic. This helps you\nbe able to see things more clearly through your own personal lenses as well as helps you be able to\nmore closely understand how others see the same concept through their own personal lenses.\nIn the previous section, you were asked to think of several situations that might be appropriately\nplaced into each of the four white cells in the Ethical vs. Legal Matrix above. Re-consider those issues\nor situations once again, but this time, review each instance or situation while intentionally looking\nthrough your multiple personal lenses. Can you identify which lens or lenses most significantly\ninfluence your decisions as you categorized where each situation should end up? Do the lenses that\nyou use weigh more heavily with one lens vs. the others? What if someone else’s lenses would put the\nsame situation into a different cell? Are you prepared to have an objective conversation with that\nperson to delve down to discover the root differences?\nPage 28 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "section_path": ["Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases"], "page": 29, "content": "Ethics in Technology\nBy: Ed Weber\nThe more you can practice these skills of self-reflection, the greater your capacity will become to\ntruly understand yourself as well as your ability to empathize with those who may be viewing situations\nthrough other personal lenses or through lenses with different weights than your own. This effort pays\nfor itself through greater shared understandings and, hopefully, with greater positive ethical decisions.\nPage 29 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 30, "content": "Ethics in Technology\nBy: Ed Weber\n4. Ethics for Tech Developers and Tech Consumers\nTech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making\nTech Consumer Responsibilities\nConsidering the typical audience for this textbook, nearly 100% of you would most likely consider\nyourselves to be consumers of tech. Take a few moments to think about all of the tech that you\nregularly use as a part of your day-to-day experiences. From checking your smartphone for messages\nand social media updates, to using laptops or tablets for school or work assignments, streaming music\nand videos, playing video games, and even relying on smartwatches or fitness trackers to monitor your\nhealth – technology is woven into nearly every aspect of your daily routine. Whether you’re ordering\nfood through an app, catching up on news, attending virtual classes, or collaborating on group projects\nusing cloud-based tools, it’s clear that tech plays a central role in how you learn, work, connect, and\nentertain yourselves.\nWhat would be some of the ethical considerations for consumers of tech? As a starting point for\nthis discussion, I posed this exact question to a generative-AI tool (Perplexity) to see the response.\nHere is what Perplexity had to say:\n“Some key ethical considerations for consumers of tech include protecting personal\nprivacy and data, being aware of how their information is collected and used, ensuring\ninformed consent when using digital services, recognizing and avoiding technologies or\ncompanies that engage in discriminatory or unfair practices, respecting intellectual\nproperty, and considering the broader social and environmental impacts of their tech use.\nConsumers should also be mindful of the potential for technology to amplify bias or\nharm vulnerable groups, and strive to make choices that support transparency, fairness,\nand accountability in the tech industry.”\nInteresting indeed!\nLet’s look at each of these concepts and turn them each into questions for you to review through\nyour own personal lenses:\n• protecting personal privacy and data – Is your personal privacy and data actually protected? Do\nyou know where (all) this data exists and who (all) has access to it? Does it really need to be\nkept private and protected?\n• being aware of how their information is collected and used – Do you have any control (or say)\nover what is collected or how it might be used (or shared, or sold)? Does it matter?\nPage 30 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 31, "content": "Ethics in Technology\nBy: Ed Weber\n• informed consent when using digital services – Have you willingly and knowingly given away\nyour information? How about your rights? Can you change your mind? If you do change your\nmind, is it already too late? Do you have any recourse?\n• recognizing and avoiding technologies or companies that engage in discriminatory or unfair\npractices – Do you ever investigate the provider(s) of the tech you consume and does that\ninformation affect your decision to work with (and consequently support and enable) those\ncompanies? Do you have any responsibility to not support and enable unethical companies?\n• respecting intellectual property – Do things like pirating games, software, or music have ethical\nimplications? Does it matter if we’re talking about an artist’s intellectual property or a\ncompany’s intellectual property? What about your colleagues’ work? If they didn’t ‘legally’\nprotect it, is it fair-game?\n• considering the broader social and environmental impacts – Does it matter that buying the\nnewest version of a phone will mean that the ‘old’ phone may become e-waste with long-term\nglobal environmental harm? Does it matter when trash-talking someone in a gaming app if we\nare completely unaware of their own propensity toward self-harm? Do you have any\nresponsibility and/or culpability in the outcomes that derive from your own actions with tech?\n• technology amplifying bias or harming vulnerable groups – Am I using platforms or tools that\nhave been shown to perpetuate or amplify biases, such as facial recognition systems with higher\nerror rates for people with darker skin tones or recommendation algorithms that reinforce\nstereotypes or exclude certain groups? How might my engagement with algorithm-driven\ncontent – such as clicking, sharing, or purchasing – contribute to feedback loops that reinforce\nexisting biases or marginalize underrepresented communities?\nAs technology becomes ever more integrated into daily life, intentionally reflecting on how we\ninteract with it is essential for fostering a more ethical and responsible digital world. By asking\nourselves thoughtful questions about privacy, consent, fairness, and the broader impacts of our choices,\nwe can move beyond passive consumption and become conscious participants in shaping the tech\nlandscape. This means not only protecting our own interests but also considering the well-being of\nothers and the environment, holding companies accountable, and striving for transparency and\ninclusivity in all our digital interactions. Through ongoing awareness and deliberate action, each of us\ncan contribute to a culture where technology empowers rather than exploits, and where ethical\nconsiderations guide both innovation and everyday use.\nProfessional Codes\nPage 31 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 32, "content": "Ethics in Technology\nBy: Ed Weber\nNow, for some of you, technology isn’t going to be something you simply consume. Rather, some\nof you may be heading down a path toward becoming a technology practitioner. As a technology\npractitioner, all of the previously discussed ethical responsibilities of a tech consumer exist – first and\nforemost – but there are also additional ethical factors that will be found in addition to those basic\nconsiderations.\nSome common tech-related job titles include the following:\n• Software Engineer • AR/VR Developer • Data Scientist\n• Systems Administrator • Network Architect • Cloud Architect\n• Cybersecurity Analyst • Business Systems Analyst • IT Support Specialist\n• UI/UX Designer • DevOps Manager • AI Computer Scientist\nThink of all of the people that you will have an ethical relationship with as a part of your\ntechnology-based profession. This diagram represents some of the main relationships you will\nexperience in your IT careers.\nIn your tech-related path, you will find\nyourself working with very many different\nindividuals who may have many different\nexpectations as to how you will apply ethics in\nyour day-to-day interactions. Think about all\nof the different people shown here and\nremember that each person has their own set of\npersonal lenses which they are using (whether\nintentionally or subconsciously) during each\ninteraction with you.\nOften, the details of these relationships may\nbe spelled out (at least partially) via various\nrelationship agreements. These agreements can\ntake many forms (i.e. contracts, non-disclosure\nagreements, license agreements, professional\ncodes of conduct, etc.) with many of these\nforms having both ethical and legal\nFigure 7: IT Professionals' Ethical Relationships\nramifications.\nPage 32 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 33, "content": "Ethics in Technology\nBy: Ed Weber\nBut at other times, the details of the relationships are not spelled out at all! And, as a result,\nconflicts can certainly arise when it becomes evident that there are competing interests being\nconsidered and viewed through conflicting personal lenses.\nA great starting point for considering the additional ethical responsibilities of a tech practitioner is\nto review the Association for Computing Machinery (ACM) Code of Ethics and Professional Conduct.\nThis document attempts to codify the ethical responsibilities of tech professionals. But with even a\ncursory review of this professional code, one can easily discover how conflicts can arise when different\nconstituents prioritize their own agendas related to tech development.\nLet’s consider the following case study:\nCase Study: Apple Settles ‘Batterygate’ Class Action Suit with an additional $113 Million1\nIn November of 2020, Apple agreed to pay $113 million to settle consumer fraud\nlawsuits. These lawsuits were brought by more than 30 states over alleging that Apple\nwas intentionally and without notice slowing down and shutting off iPhones resulting in\nthe devices having sluggish performance or completely shutting down.\nAt first, Apple denied that it purposefully impeded the devices’ performance in any\nway. Later, Apple admitted that it did, in fact, alter the devices’ performance but it was\nfor the purpose to “…preserve battery life amid widespread reports of iPhones\nunexpectedly turning off.”\n\"Many consumers decided that the only way to get improved performance was to\npurchase a newer-model iPhone from Apple,\" Arizona Attorney General Mark Brnovich\nwrote in the complaint. \"Apple, of course, fully understood such effects on sales.\"\nThe slowdown effected phones that were released between 2014 and 2016, but it\nwasn’t until December of 2017 that Apple eventually admitted to the slowdowns. Then,\nthey issued an apology of sorts by saying, \"We have never – and would never – do\nanything to intentionally shorten the life of any Apple product, or degrade the user\nexperience to drive customer upgrades.\" But they settled anyway! Initially, they agreed\nto a settlement of $500 Million to pay affected consumers $25 per phone. The $113\nMillion was in addition to the initial $500 Million.\nTo think about this case in more detail, let’s put ourselves in different shoes and examine this\nsituation from different perspectives with differing lenses.\n1 “Apple Agrees To Pay $113 Million To Settle 'Batterygate' Case Over iPhone Slowdowns”, NPR, updated Nov. 18,\n2020, https://www.npr.org/2020/11/18/936268845/apple-agrees-to-pay-113-million-to-settle-batterygate-case-over-\niphone-slowdowns\nPage 33 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 34, "content": "Ethics in Technology\nBy: Ed Weber\nPerson 1: Apple Executive – We’ve sold and shipped millions of units of phones from 2014\nthrough 2016 – and they are excellent products! Maybe too good. But that was then. How will we\ncontinue to get new sales if our older products are still working just fine? Why should our customers\nthrow away perfectly good and working devices if they are still working for them? If we don’t\ncontinue to have the same (or better) sales growth, my position (and my bonuses) will be at risk. And if\nthe new features of our new devices aren’t so appealing that the consumers won’t willingly abandon\ntheir working tech, then we need to make the older devices less-than-desirable if we will have any\nchance of getting those new replacement sales. But we also can’t disclose what we’re doing because\nwe can’t risk our competitors seeing our strategies. Let’s just find a way to make the old phones ‘go\naway’. (planned obsolescence, trade secrets, intellectual property)\nPerson 2: Apple Customer – I just spent hundreds of dollars on a device less than 5 years ago and it\ndid everything I needed it to do. And for several years now, it has been a great device! But now, for no\napparent reason, my regular apps have become unmanageably sluggish and, sometimes, without\nwarning, my phone will just shut itself off. Apple said that it didn’t do anything, but I certainly didn’t\nchange anything! The device was working perfectly, and then all of a sudden, it is no longer\nfunctioning the way it should. Isn’t it reasonable to have an expectation that something that is working\nwill continue to work as designed without interruption or performance degradation? Also, when I ask\nApple about what’s going on, shouldn’t they have a responsibility to tell me the truth right up front?\nAnd if this device no longer works for me, then it won’t work for anyone. Where will this device end\nup? In a landfill I suppose… oh, the waste! (corporate responsibility, transparency, environmental\nresponsibility)\nPerson 3: YOU – as any one of a number of different technology professionals working at Apple.\nDeveloper – The boss just asked me to start working on a program that would run constantly –\nundetectable and behind-the-scenes – that would basically do nothing but would\nconsume a lot of clock cycles of the processor. The purpose of this program is simply to\nintentionally drain the battery as quickly as possible. I’m no dummy. I know exactly\nwhy I am being asked to do this. If I didn’t work here at Apple, and I wrote something\nlike this, I would be considered a cyberterrorist. Am I really OK with being a part of\n‘planned obsolescence’? Is this why I went to school to learn how to be an app\ndeveloper? Is this the kind of app I thought I’d be developing? Is this really what I\nsigned up for?\nMarketing – The boss just asked me to develop a campaign targeting existing phone users telling\nthem how much they are missing out because their older devices just can’t keep up with\nthe new apps. I’m supposed to focus on new apps that only run on the newest devices.\nWe’ve already seen that the ‘new apps’ aren’t that big of a hit because we aren’t getting\na lot of ‘new’ customers at this time anyway. It doesn’t seem to be true that the new\nPage 34 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 35, "content": "Ethics in Technology\nBy: Ed Weber\napps are all that they’re supposed to be. When does ‘exaggeration’ become ‘lying’? Is\nthis really what I signed up for?\nCustomer Service – The boss just told us that we are experiencing a huge increase in disgruntled\ncustomers because their batteries keep draining very quickly and some phones are just\nshutting off. The boss told me, “Your response is that the customer is just using\ntechnology that is too old and they should upgrade to a newer phone right away” and\nthen I should try to transfer them to Sales. I asked the boss, “What changed? Why have\nthe devices suddenly started failing?” The boss said, ‘That’s above our pay-grade.’ and\nleft me on my own. Something doesn’t seem right here. If I were the customer, I would\nexpect more of a concrete answer. And I certainly wouldn’t appreciate a ‘hard sales\npitch’ if the reason I called in was to get my current device restored to the way it was\nworking fine just a bit ago. And why can’t my own company be transparent about what\nis going on? Is this really what I signed up for?\nSales Person – The boss just told me that we will begin using two different sales pitches for our\npotential customers. First, we have to find out if they are a ‘new’ customer (without one\nof our previous models.) If so, then we are supposed to first pitch our great Apple brand\n(be one of the cool kids), and then next we should pitch the differences between our\nbrand and the other brands, and then, finally, we should close with the pitch that the new\nphones are positioned to handle any new technology advancements that may come in the\nfuture. But if the prospect is an ‘old’ customer, then the pitch needs to be just ‘sad\ncommiseration’. They will be grousing about how they thought their old phone should\nhave lasted a lot longer and how they always took great care with it… I am supposed to\njust sadly nod my head and say, “yeah – the tech just keeps changing… it’s getting hard\nto keep up… but what are you gonna’ do? You need the latest tech to be able to do all of\nthe things you’ve gotten used to doing.” But then the boss says I should also use the\npitch that the new phones are positioned to handle any new technology advancements\nthat may come in the future. Are you kidding me? How am I supposed to do that with a\nstraight face? Isn’t the reality that this investment is designed to fail in less than 3 – 5\nyears? Is this really what I signed up for?\nWhen looking at this exact same situation through the different perspectives of different involved\npeople, we can see that there are quite a number of ethical considerations that should not be too easily\ndismissed. One of the most common conflicts when it comes to the ethics associated with business and\ntechnology involves the distinctions and differentiations between two different groups of people:\nstockholders vs. stakeholders.\nPage 35 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 36, "content": "Ethics in Technology\nBy: Ed Weber\nStockholders (or shareholders) are individuals, companies, or institutions that own shares in a\ncorporation, giving them partial ownership and certain rights such as voting on major company\ndecisions and receiving dividends when profits are distributed. Their primary interest lies in the\nfinancial performance of the company, as their returns depend on stock value and dividend payouts,\nand they can typically buy or sell their shares at will.\nIn contrast, stakeholders encompass a much broader group, including not only shareholders but also\nemployees, customers, suppliers, and members of the local community, all of whom have an interest in\nthe company's performance and impact, even if they do not own any shares. While stockholders are\nmainly concerned with financial returns, stakeholders may prioritize long-term stability, ethical\npractices, job security, product quality, and the company’s social and environmental responsibilities.\nThis means that stakeholders’ interests may not always align with those of stockholders. It has been\nthis author’s experience that the vast majority of corporate boards and executive leadership prioritize\nstockholder interests over stakeholder interests.\nSo, where does this leave us when we consider how a Professional Code of Conduct can help us in\nmaking our ethical decisions? First, this can be something that one can intentionally look for when\ninterviewing for a position with a potential employer. individuals can find out if the organization has\nadopted the ACM Code of Ethics and Professional Conduct. Or perhaps they have their own code of\nconduct that they have developed.\nindividuals can also discover the policies and procedures that the organization uses for dealing with\nconflicts that arise from competing priorities. Some organizations utilize internal and/or external\nmediation boards to help provide unbiased, objective reviews and conflict resolutions.\nAdditionally, individuals can adopt their own code of ethics and professional conduct to be a guide\nfor their own, personal decisions. But in order to do this, one should also proactively consider what\nthey may do when they are asked to do something that violates their adopted code of ethics.\nA look at one company’s Code of Conduct\nThe company Enterprise Mobility (formerly Enterprise Holdings) is a 68-year-old company that\nincludes several brands – such as Enterprise Rent-A-Car, National Car Rental and Alamo – and\nprovides services such as fleet management, car-sharing, van-pooling, truck rental, luxury rental, retail\ncar sales and vehicle subscription. Enterprise currently has a global fleet of more than 2.3 million\nvehicles with rental locations in more than 90 countries and territories, including more than 40\ncountries across Europe.\nIn a strong example of corporate responsibility, transparency, and commitment to a professional\ncode of conduct, Enterprise publishes their Code of Conduct here:\nPage 36 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 37, "content": "Ethics in Technology\nBy: Ed Weber\nhttps://www.enterprisemobility.com/content/dam/enterpriseholdings/marketing/about-us/compliance-\nand-ethics/enterprise-holdings-employee-code-of-conduct.pdf\nThis comprehensive document defines and discloses precisely what is expected of its employees as\nwell as how its customers and vendors can expect their relationships to exist. The introductory letter\nfrom the President and Chief Executive Office (CEO) of Enterprise, Chrissy Taylor, clearly illustrates\nthe ‘WHY’ that Enterprise has adopted and publicly declared their commitment to this Code of\nConduct.\nFigure 8: Enterprise's Code of Conduct Introduction Letter by Chrissy Taylor, President and CEO\nContained within their code of conduct, Enterprise shows that there is a clearly defined path that\nindividuals can use (but, more importantly, also have a responsibility to use) to report and resolve\nissues of ethical concern. This path includes one’s immediate supervisor, a next-level supervisor, the\nHuman Resources department, or the Compliance and Ethics Committee. They even have an ethics\nhotline which includes a link for online reporting as well as a toll-free number which are both available\nand monitored 24 hours a day, 7 days a week, and 365 days a year!\nTake a few minutes to review this one company’s published Code of Conduct. Just a few of the\ntopics that you will find covered here include concepts that allow us to also introduce additional terms\nto be further explored:\n• Obligations of Leadership – leaders are required to ‘lead by example’ as a requirement of their\nposition.\n• Copyright, trade secret, patent, intellectual property – creations of the mind that the law\ndefines and protects.\nPage 37 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 38, "content": "Ethics in Technology\nBy: Ed Weber\n• Conflicts of Interest – occurs when one’s personal interests interfere with their ability to make\nunbiased decisions on behalf of the organization.\n• Gift – Something of value given willingly to another person without any expectation of return\nor influence.\n• Bribe – Something of value offered or given with the intent to influence the recipient’s actions\nor decisions for the giver’s benefit.\n• Insider Trading – buying or selling of a company's securities by individuals who possess\nmaterial, nonpublic information about that company, often in violation of a duty to keep that\ninformation confidential.\n• Anti-Corruption Laws – these vary by region/country and can be complex, but these remain\npart of the legal requirement as well as an ethical requirement.\n• Harassment – any unwelcome behavior toward another person relating to a person’s legally\nprotected characteristics that have the purpose or effect of creating an intimidating, hostile, or\noffensive work environment. Such conduct may be physical, sexual, or psychological.\nReview the Enterprise Code of Conduct and compare and contrast what you find there with the\nACM Code of Ethics and Professional Conduct. Where do you discover similarities? Are there\ndifferences? Do you feel that any differences are significant? Which, if any (and in which document),\nwould you suggest should be changed and why? How do these codes of conduct currently match up to\nyour own way of thinking about your own personal ethics?\nEveryday Decision-Making\nSo far in this chapter you have:\n• considered your own ethical responsibilities as a consumer of tech\n• reviewed the generic but widely adopted ACM Code of Ethics and Professional Conduct\n• reviewed one company’s comprehensive and transparent Code of Ethics\nIt is now time to consider what these might mean for you in your everyday decision-making\nprocess. For example, there appear to be some rather valid reason for individuals to consider their own\nethical consumption of all things tech – for both their own health, safety and well-being – but also for\nhow their actions might impact others. There appear to be some strong starting points and examples of\nPage 38 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "section_path": ["Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making"], "page": 39, "content": "Ethics in Technology\nBy: Ed Weber\nEthical Codes of Conduct for both individuals and organizations to use to help define, clarify, and\nformalize their own approaches to adopting ethics in their own day-to-day actions.\nSo now, here come the real questions for this chapter:\nWhat does your own, personal Ethical Code of Conduct look like? What are some of the areas that\nyou feel are pretty well defined? What are some areas that you feel may be in conflict – depending on\nwhich personal lenses you choose to use? What are some areas of your own personal code of ethics\nthat may be in conflict with your school, or your work, or your family, or your church, or your\ncommunity, or your culture? Can you imagine writing up your own, personal code of ethics? What all\nwould be included? What do you feel is still too undefined or situational, so much so that it means that\nyou may not have a consistent ethical response in certain situations? What are the risks of\ninconsistently or sporadically adhering to a personal code of ethics? Is it OK to change your ethical\nposition on a particular subject? If so, what circumstances would allow for this?\nAs promised in the early chapters of this text… we aren’t providing any concrete, absolute answers\nto these questions. But rather, we are hoping that by listing a collection of questions, these may help\nindividuals to discover and implement some intentionality into what it means to be an ethical person\nmaking informed, ethical decisions in their day-to-day activities.\nPage 39 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "section_path": ["Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation"], "page": 40, "content": "Ethics in Technology\nBy: Ed Weber\n5. Cybersecurity, Hacking, and Digital Identity\nCybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation\nIn the previous chapter, we explored some of your own personal ethical responsibilities as they\nrelate to you as a consumer of all things ‘tech’. We also, explored some of your ethical responsibilities\nas they relate to your role in an organization that utilizes ‘tech’. Now, for this chapter and each of our\nsubsequent chapters, we will look at various individual aspects of technology and try to focus on the\nethical considerations within some narrower contexts. As we will quickly see, however, all of these\nchapters cannot be completely isolated from each other. Rather, it will be common to discover that\nthere are aspects of each of these subsequent chapters that significantly overlap along with other\nchapters.\nCybersecurity\nTo begin this discussion, once again, let’s use a\ngenerative-AI tool (Perplexity) to help us provide the\n‘textbook definition’ of cybersecurity. Here is the\nprompt that was used:\n“In a single sentence, please define the term\n'Cybersecurity'.”\nHere are the results we received:\nCybersecurity – The art and practice of\nprotecting networks, devices, programs, and data\nfrom unauthorized access, cyberattacks, or criminal\nuse, ensuring the confidentiality, integrity, and\navailability of information.\nFigure 9: Graphic representing\nIt seems that there may be some significant\ncybersecurity\nomissions from this definition: We already have a\nfeel for ‘Why’ cybersecurity is important… But what\nabout the other questions? ‘Who’ is responsible for effective cybersecurity? ‘When’ do we need to\nthink about cybersecurity? ‘Where’ do we need to be cognizant of cybersecurity issues? And ‘How’ do\nwe actually address cybersecurity concerns?\nSo, here is a follow-up prompt for our friend Perplexity:\nPage 40 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "section_path": ["Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation"], "page": 41, "content": "Ethics in Technology\nBy: Ed Weber\n“This definition answers the question of 'what' is Cybersecurity. Assuming that we\nalready know the 'why' associated with a need for cybersecurity, in a single sentence\neach, what would be the answers to the questions 'when', 'where', 'how', and most\nimportantly, 'who' that should be associated with the concept of effective\ncybersecurity?”\nHere are the additional results:\nWhen: Effective cybersecurity must be practiced continuously, adapting to evolving\nthreats and technological changes, and is especially critical whenever digital systems,\nnetworks, or sensitive data are in use.\nWhere: Cybersecurity is required wherever digital assets exist, including on-premises\nnetworks, cloud environments, remote work setups, mobile devices, and any location\nwhere information is stored, processed, or transmitted.\nHow: Effective cybersecurity is achieved through a combination of risk assessments,\nrobust security policies, technical controls like encryption and multi-factor\nauthentication, continuous monitoring, regular training, and incident response planning.\nWho: Everyone with access to digital systems – including individuals, employees, IT\nprofessionals, organizational leaders, and specialized cybersecurity teams – shares\nresponsibility for maintaining effective cybersecurity, with organizations often relying on\ndedicated experts, external consultants, and collaborative initiatives like cybersecurity\nclinics to strengthen their defenses.\nThe generative AI seems to get this one nearly 100% right! (When we look into the AI systems\nthemselves in greater detail in Chapter 11, we will see this is not always the case!) Everyone is\nresponsible – in part at least – for effective cybersecurity. The following sub-topics of this chapter will\nexplore just a few of the concepts related to cybersecurity where a deeper dive into the corresponding\nethics can illustrate some interesting and maybe even unexpected challenges!\nVulnerabilities\nVulnerabilities in digital systems represent weaknesses or flaws that can be exploited by malicious\nactors to gain unauthorized access, disrupt operations, or compromise sensitive information. These\nvulnerabilities may be targeted through various forms of cybercrime, such as phishing, ransomware,\nor distributed denial-of-service (DDoS) attacks, often leveraging exploits that take advantage of\nunpatched software or misconfigured systems.\nPage 41 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "section_path": ["Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation"], "page": 42, "content": "Ethics in Technology\nBy: Ed Weber\nEthically speaking, addressing vulnerabilities requires timely identification and remediation –\ntypically through \"fixes\" or patches – to prevent harm to individuals and organizations. Without proper\nprevention and remediation, threats like viruses, Trojan horses, botnets, logic bombs, or blended\nthreats that can propagate across interconnected networks. The presence of sophisticated threats, such\nas rootkits or phishing campaigns (including spear phishing, smishing, and vishing, etc.),\nunderscores the ongoing responsibility of cybersecurity professionals and tech users alike to remain\nvigilant, promote best practices, and uphold principles of fairness, accountability, and non-maleficence\nin protecting digital assets and identities.\nHacking\nHacking, in its broadest sense, refers to the act of gaining unauthorized access to computer systems\nor networks, but the motivations and ethical implications of hacking can vary widely depending on the\nindividuals involved. Black-hat hackers, for example, engage in hacking for malicious purposes such\nas stealing data, causing disruption, or committing cybercrime, often motivated by personal gain or the\nintent to inflict harm. In contrast, white-hat hackers use their technical skills to identify and report\nvulnerabilities, helping organizations strengthen their security by acting as ethical defenders –\nsometimes as part of formal roles like penetration testers or through coordinated vulnerability\ndisclosure programs like hack-a-thons.\nThe landscape of hacking also includes figures such as crackers, who break into systems to bypass\nprotections or copy software illegally, and hacktivists, who use hacking as a form of protest or to\npromote social or political causes. The actions of hacktivists can raise complex ethical questions, as\ntheir activities may be intended to expose injustice or raise awareness, yet still involve unauthorized\naccess and potential harm to innocent parties. As technology evolves, so too do the methods and\nmotivations of hackers, making it essential for society to continually reassess the ethical boundaries of\nhacking, the responsibilities of those with advanced technical knowledge, and the appropriate legal and\norganizational responses to both harmful and beneficial forms of hacking.\nDigital Identity\nDigital identity, as it relates to an individual, is the collection of digitally or electronically captured\nattributes, behaviors, credentials, and data points that uniquely verify and represent a person online.\nThis identity is not static; it is dynamically shaped by both the information individuals actively provide\n– such as usernames, email addresses, social media profiles, and biometric data – as well as the data\npassively collected through their online activities, including browsing habits, search histories, and\ntransaction records. Key aspects that make up a person's digital identity include personally\nidentifiable information (PII) like social security numbers, dates of birth, and biometric traits; login\nPage 42 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "section_path": ["Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation"], "page": 43, "content": "Ethics in Technology\nBy: Ed Weber\ncredentials; device identifiers; behavioral patterns; and contextual signals such as location and device\nusage.\nBad actors often seek to access and exploit digital identities through methods like phishing,\ncredential theft, or malware, using the compromised information for identity theft, financial fraud, or\nunauthorized access to sensitive accounts. Once a digital identity is breached, attackers can leverage it\nto impersonate individuals, commit cybercrime, or even build more convincing attacks against others\nby harvesting further data from compromised accounts. Ethically, individuals have a responsibility to\nbe intentional and mindful about the information they share and the digital footprint they create, as\ntheir digital identity not only reflects on their personal reputation but also affects their privacy and\nsecurity. Practicing thoughtful self-representation and safeguarding personal data are essential not just\nfor personal protection but also for fostering a trustworthy and respectful digital environment.\nOnline Reputation\nThis leads us to our final concept for this chapter – one’s online reputation. Online reputation refers\nto the collective perception and judgment that others form about an individual based on their digital\npresence, including the content they create, share, and are associated with across various online\nplatforms. Unlike digital identity, which is the sum of all information that identifies a person online,\nonline reputation is shaped not only by one's own actions but also by what others post, comment, or tag\nabout them, and is visible to third parties through search engines, forums, blogs, and especially social\nmedia. Social media usage plays a significant role in building or damaging online reputation, as posts,\ncomments, likes, and shares contribute to the overall digital footprint, and even a single viral incident\ncan have lasting effects – positive or negative – on how a person is viewed by peers, employers, and the\nbroader public.\nAlso, in today’s society, there is often an ongoing blending of work and personal life which appears\nto be mostly unavoidable. At work, you may use the corporate computer to prepare for some personal\nmeetings such as with your doctor, etc. Additionally, some organizations have bring-your-own-device\n(BYOD) requirements and/or policies which stipulate if and how you will use your own devices in the\nwork setting. These kinds of intersections between what is ‘personal’ vs. what is ‘public’ introduces\nunique risks to one’s online reputation. When individuals commingle work and home information on\nshared devices, they increase the chances of accidental data leaks, inappropriate content exposure, or\nbreaches that could affect both professional and personal reputations. For example, a security lapse on a\npersonal device used for work could expose sensitive corporate information or inadvertently link\npersonal social media activity with professional contacts, complicating the separation between private\nand public personas.\nVirtual Private Networks (VPNs) are often used to enhance privacy by encrypting internet traffic\nand masking a user's IP address. However, using a VPN – especially one provided by a third party –\nPage 43 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "section_path": ["Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation"], "page": 44, "content": "Ethics in Technology\nBy: Ed Weber\ndoes not guarantee true anonymity or untraceability. While a VPN can obscure activity from local\nnetworks or Internet Service Providers (ISPs), the VPN provider itself can potentially (and usually\ndoes!) log user activity. They do this because if the tech fails for any reason, it is only through\nreviewing the logs that the provider can discover and remedy the failure! As a result of this known\nlogging, both law enforcement as well as sophisticated attackers may still trace actions back to the\nindividual if the VPN is compromised or if endpoints are not secure. Direct-to-endpoint VPNs (such as\nthose connecting directly to a corporate network) offer more control but still do not provide absolute\nanonymity, highlighting the need for individuals to remain vigilant and intentional about their online\nactions and the security tools they use.\nUltimately, maintaining a positive online reputation requires individuals to be mindful of their\ndigital footprint and the potential consequences of their online behavior. Ethical self-management\ninvolves regularly reviewing privacy settings, thinking critically before posting or sharing information,\nand understanding that online actions can have far-reaching effects on credibility, trustworthiness, and\nfuture opportunities.\nTextbook Definitions – Cybersecurity\n• cybercrime – Illegal activities conducted using computers or networks, including theft, fraud,\nor disruption of services.\n• phishing – A deceptive technique where attackers impersonate legitimate entities to trick\nindividuals into revealing sensitive information, such as passwords or financial details.\n• ransomware – Malicious software that encrypts a victim's data and demands payment for its\nrelease.\n• distributed denial-of-service (DDoS) – An attack in which multiple compromised systems\nflood a target with traffic, overwhelming it and rendering services unavailable to legitimate\nusers.\n• viruses – Malicious programs that attach themselves to legitimate files or programs and\nreplicate, spreading to other systems and causing harm.\n• Trojan horse – Malicious software disguised as legitimate applications, which, when executed,\nenable unauthorized access or cause damage.\n• botnets – Networks of compromised computers, controlled remotely by attackers, used to\nperform coordinated malicious activities such as DDoS attacks or spam distribution.\n• logic bombs – Malicious code embedded in software that triggers a harmful action when\nspecific conditions are met.\nPage 44 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "section_path": ["Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation"], "page": 45, "content": "Ethics in Technology\nBy: Ed Weber\n• blended threats – Attacks that combine multiple types of malware or attack methods to exploit\ndifferent vulnerabilities simultaneously.\n• rootkits – Malicious tools designed to hide the existence of certain processes or programs,\nallowing continued privileged access to a system.\n• spear phishing – Targeted phishing attacks aimed at specific individuals or organizations, often\nusing personalized information to increase credibility.\n• smishing – Phishing attacks delivered via SMS text messages, aiming to trick recipients into\ndivulging sensitive information.\n• vishing – Voice-based phishing attacks conducted over the phone to deceive individuals into\nproviding confidential information.\n• black-hat hackers – Individuals who exploit vulnerabilities in systems for malicious purposes,\npersonal gain, or to cause harm.\n• white-hat hackers – Ethical hackers who identify and help fix security vulnerabilities to\nimprove system security, often with permission.\n• penetration testers – Security professionals who simulate cyberattacks on systems or networks\nto identify and address vulnerabilities before malicious actors can exploit them.\n• hack-a-thons – Collaborative events where programmers and security experts work intensively\nto solve problems, develop software, or test security in a short period.\n• crackers – Individuals who break into computer systems or software, often to bypass\nprotections or copy software illegally.\n• hacktivists – Hackers who use their skills to promote social or political causes, often through\nunauthorized digital actions.\n• credentials – Usernames, passwords, or other authentication information used to verify identity\nand gain access to systems.\n• biometrics – Unique physical or behavioral characteristics, such as fingerprints or facial\nrecognition, used for automated identity verification.\n• personally identifiable information (PII) – Data that can be used to uniquely identify an\nindividual, such as name, address, social security number, or date of birth.\n• credential theft – The act of stealing authentication information, such as usernames and\npasswords, to gain unauthorized access to systems or data.\nPage 45 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "section_path": ["Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation"], "page": 46, "content": "Ethics in Technology\nBy: Ed Weber\n• malware – Malicious software designed to disrupt, damage, or gain unauthorized access to\ncomputer systems or networks.\n• digital footprint – The trail of data and activity a person leaves behind when using digital\nservices, including social media posts, browsing history, and online transactions.\n• bring-your-own-device (BYOD) – A policy or practice where employees use their personal\ndevices for work purposes, often increasing security and privacy risks.\n• Virtual Private Networks (VPNs) – Services that encrypt internet traffic and route it through a\nsecure server, providing privacy and security for online activities.\n• Internet Service Providers (ISPs) – Companies that provide individuals and organizations\nwith access to the internet.\nPage 46 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "section_path": ["Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;"], "page": 47, "content": "Ethics in Technology\nBy: Ed Weber\n6. Technology, Justice, and Social Equity\nTech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion;\nMaslow’s Hierarchy of Needs and Tech; Digital Divide;\nTech in Education\nThe integration of technology in education has transformed how students learn, educators teach,\nand institutions deliver knowledge. From digital textbooks and online learning platforms to adaptive\nlearning software and virtual classrooms, technology can help bridge gaps in access to quality\neducation, particularly for students in remote or underserved areas. However, the ethical considerations\nare complex: unequal access to devices and reliable internet can reinforce existing educational\ndisparities, and the use of student data for algorithmic personalization raises questions about privacy,\ninformed consent, and potential bias.\nEducators and policymakers must grapple with the responsibility to ensure that technology\nenhances learning equitably, rather than exacerbating divides. This involves not only providing\nhardware and connectivity but also supporting digital literacy, offering accessible content for students\nwith disabilities, and critically evaluating the impact of educational technologies on student well-being\nand autonomy. Ultimately, the ethical deployment of technology in education requires ongoing\nreflection on who benefits, who may be left behind, and how to foster a more just and inclusive\nlearning environment.\nHealthcare Access and Tech\nTechnology has revolutionized healthcare delivery through telemedicine, electronic medical\nrecords (EMR), wearable health monitors, and AI-driven diagnostics. These advances can increase\naccess to care for rural populations, streamline patient management, and enable earlier detection of\ndisease. Yet, ethical challenges persist: not all patients have equal access to the internet or smart\ndevices, and the digital skills needed to navigate modern healthcare tools are unevenly distributed.\nThere is also the risk that algorithmic decision-making in healthcare may reflect or amplify existing\nbiases, leading to disparities in diagnosis or treatment. Protecting patient privacy and ensuring\ninformed consent are paramount as more sensitive health data is collected and shared across digital\nplatforms. Ethically, healthcare providers and technologists must work to ensure that technological\ninnovation does not widen the gap between those who can and cannot access high-quality care, but\ninstead promotes justice by making healthcare more inclusive, affordable, and responsive to the needs\nof all communities.\nPage 47 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "section_path": ["Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;"], "page": 48, "content": "Ethics in Technology\nBy: Ed Weber\nTech for Accessibility and Inclusion\nOne of the most promising aspects of technology is its potential to empower individuals with\ndisabilities and promote broader social inclusion. Assistive technologies – such as screen readers,\nvoice recognition software, and adaptive hardware – can enable people with visual, auditory, motor,\nor cognitive impairments to participate more fully in education, employment, and civic life. The ethical\nimperative is to design technology that is accessible by default, not as an afterthought, and to involve\npeople with disabilities in the design and evaluation process.\nAt the same time, barriers remain: not all digital content is accessible, and some emerging\ntechnologies (like AI-powered interfaces) may introduce new obstacles if not thoughtfully\nimplemented. Promoting digital inclusion means addressing affordability, usability, and cultural\nrelevance, while also challenging stereotypes and assumptions about disability. Ethically, the goal is to\ncreate a digital world where everyone can participate with dignity and autonomy, regardless of ability.\nMaslow’s Hierarchy of Needs and Tech\nMaslow’s Hierarchy of Needs provides a useful framework for considering the ethical implications\nof technology’s role in fulfilling human needs.\nFigure 10: Maslow's Hierarchy of Needs\nPage 48 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "section_path": ["Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;"], "page": 49, "content": "Ethics in Technology\nBy: Ed Weber\nAt the base level, technology can support physiological needs – such as food, water, and shelter –\nthrough innovations in agriculture, clean water delivery and clean air maintenance, and smart housing.\nSafety needs are addressed through security systems, health monitoring and health care, and emergency\ncommunication tools. As we move up the hierarchy, technology supports belonging and esteem through\nsocial media, online communities, and platforms for self-expression.\nIt has often been said that we currently live in a time where literally all of the basic needs at the\nlowest levels of Maslow’s hierarchy could be assured for all people of the world! We already have the\ntechnology for production, distribution and re-distribution, monitoring for need, and all associated\ncommunication needs, to completely eradicate food insecurity (hunger, thirst and starvation), clothing\nneeds and housing needs (shelter and safety), assure clean air, land and water (pollution removal),\nhealth care (diagnostics and treatment) and provide the peace-of-mind and mental wellness that comes\nfrom having all of these other needs addressed. The technology isn’t what is getting in the way of\nachieving all of this!\nRather, it is the insistence of holding on to the status-quo of man-made economic systems which\nfavor one economic group to the detriment of all others. This is the only thing that is preventing all of\nthese achievements. It isn’t the tech that is lagging. Rather, it is the socioeconomic constructs that are\nthe root cause preventing the ethical implementation of the tech.\nEven worse, additional ethical tensions arise when technology is used in ways that actually\nexacerbate rather than work to address Maslow’s hierarchy needs: for example, when farmers are paid\nto not produce food to keep prices at an artificial level – all while people go hungry. Or when drug\ncompanies are allowed to charge obscene prices for life-saving treatments resulting in millionaire-class\nexecutives, while simultaneously standing idly by while poor people die without the treatments. Or\nwhen surveillance systems undermines privacy (a component of safety), or when social media\nalgorithms foster isolation or harm self-esteem. The challenge for technologists and society is to ensure\nthat digital tools are designed and deployed in ways that genuinely enhance human flourishing at every\nlevel of Maslow’s hierarchy, being mindful of unintended consequences and the needs of the most\nvulnerable.\nDigital Divide\nThe digital divide refers to the gap between those who have ready access to computers, the internet,\nand possess digital literacy, and those who do not. This digital divide is most readily recognized along\nlines of income, geography, age, and/or ability. This divide often limits opportunities for education,\nemployment, healthcare, and civic participation, reinforcing cycles of disadvantage. Ethically, bridging\nthe digital divide is not just a matter of providing hardware or connectivity, but also of addressing\naffordability, digital skills, and culturally relevant content.\nPage 49 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "section_path": ["Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;"], "page": 50, "content": "Ethics in Technology\nBy: Ed Weber\nConsider this hypothetical case:\nA 24-hour convenience store in New York has been robbed multiple times in the last several years.\nFortunately, no employees were ever physically harmed although most have experienced some amount\nof psychological trauma and many of them have subsequently quit. In fact, it is very hard to find any\nemployees to work because of the continuing threat of being robbed at knife or gun-point.\nSo, the business owner decides that an ideal solution is to simply stop accepting cash! Instead, the\nowner places signs that says, “No CASH accepted and No CASH on premises.” and “Credit or Debit\nCards only accepted.” The owner tells the local news team that they decided to go this route because,\n‘… nobody ever robbed a store and asked them to hand over all of their credit card receipts! The cash\nis the problem.’ The owner said, the technology will allow them to have a safe store once again, and\nthis will allow the store to stay in business.\nBut there is only one problem with this plan… in 2020, New York passed a law that said it was\nillegal for a business to “not accept cash”.\nThe law stated that its aim was to protect the rights of the ‘unbanked’ and ‘underbanked’\npopulation. The state used, as part of its assertion, the picture of a $1 bill noting the inscription found\non each bill of US currency:\nFigure 11: US $1 bill highlighting 'legal tender' phrase\nThe state argues that it is illegal for anyone to not accept cash. They say that the currency says “all\ndebts, public and private”… not just the debts that anyone wants to choose.\nPage 50 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "section_path": ["Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;"], "page": 51, "content": "Ethics in Technology\nBy: Ed Weber\nNote, that there actually have been multiple legal cases about this very topic – and not just in New\nYork! But, for the moment, let’s not focus on the legal aspects of the case, but rather, let’s think about\nthe ethical aspects of the case.\nIf we look at the situation through the bodega owner’s perspective and lenses, we can potentially\nsee the following:\n• The owner is trying to make a living and provide for their family while providing goods for\ntheir community at reasonable prices. (Maslow’s hierarchy level 1)\n• The owner needs workers to supplement their own work and keep the store open 24 hours as\nthere are many customers who work all 3 shifts in the neighborhood. (Maslow’s hierarchy level\n1)\n• The workers need to feel safe and the risk of being robbed for cash prevents this. (Maslow’s\nhierarchy level 2)\n• A tech solutions (cashless-payments) exists that address the need previously mentioned.\n• The owner wants everyone to have a cashless payment option and says it is the banks\nresponsibility to give a card to anyone who has cash and let the bank take on the exclusive risk\nof being robbed for their cash.\nBut now, let’s look at it from the bank’s perspective and lenses:\n• The bank will only allow for a new account to be opened if there is a minimum original deposit,\nand a minimum maintained balance, and/or a repeating direct deposit.\n• The bank claims that this is necessary because the cost of maintaining an account requires these\nbalances.\n• Meanwhile, the bank has reported that once again this year, the bank has earned record profits\nand its executives are making multi-million-dollar bonuses.\n• The bank points to the statement emblazoned on the currency itself and declares – the cash is\nalready here so it’s not our problem.\nFinally, let’s look at the situation from one particular customer’s perspective and lenses:\n• I was laid off from my previous job and I am currently freelancing odd jobs just to get by.\n• The jobs I can get all pay me in cash.\n• I had to close my bank account because I couldn’t maintain a minimum balance and I no longer\nhave direct deposit. Since I had to file for bankruptcy, I can’t get a credit card anymore.\nPage 51 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "section_path": ["Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;"], "page": 52, "content": "Ethics in Technology\nBy: Ed Weber\n• I am just barely getting by – literally – and pretty much all that I have to my name is right here\nin my pocket.\n• I just need to get some food before I drive over to the park to crash for a few hours in my car.\nAs we look at this kind of situation through the various lenses of the different individuals involved,\nit can become pretty obvious pretty quickly that the issues that are causing concern are not the tech!\nBut rather, the issues surround the facts that the socioeconomic systems, and the legal systems, have\nnot kept pace with the changes that have been brought about by tech advancements. And, rather than\nfocusing on the ethical considerations of the situation, our current society tends to put greater focus on\nthe legal considerations that have too often tended to foster adherence to the status-quo.\nAnd it is this perpetuation of the status-quo that continues to exacerbate the divides (socioeconomic\nand technological) which become an ever-widening and unsustainable downward spiral.\nEfforts to close the digital divide must be intentional and sustained, involving collaboration among\ngovernments, private sector, educators, and community organizations. There is also an ethical\nobligation to consider the environmental and social impacts of technology deployment, ensuring that\nsolutions are sustainable and respect the needs and voices of marginalized communities. In a world\nincreasingly shaped by digital technology, promoting justice and social equity means ensuring that\neveryone has the opportunity to participate fully and fairly in the digital society.\nTextbook Definitions – Tech, Justice and Social Equity\n• Algorithmic personalization – The use of computer algorithms to tailor digital content,\nservices, or experiences to individual users based on their data and behaviors, raising ethical\nquestions about fairness, autonomy, and the reinforcement of social inequalities.\n• Privacy – The right of individuals to control access to their personal information and data,\nparticularly regarding how it is collected, used, and shared by technology platforms.\n• Informed consent – The process by which individuals are provided with clear, understandable\ninformation about how their data will be used by technology systems, enabling them to make\nvoluntary and knowledgeable decisions about their participation.\n• Digital Divide – Refers to the gap between those who have ready access to computers, the\ninternet, and possess digital literacy, and those who do not. The digital divide is often obviously\nrecognized along socioeconomic lines.\nPage 52 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "section_path": ["Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;"], "page": 53, "content": "Ethics in Technology\nBy: Ed Weber\n• Bias – Systematic and unfair discrimination that can be embedded in technological systems,\nsuch as algorithms, which may perpetuate or amplify existing social inequalities and injustices.\n• Digital literacy – The ability to critically understand, evaluate, and effectively use digital\ntechnologies, which is essential for individuals to navigate, question, and challenge the impacts\nof technology on justice and social equity.\n• Autonomy – The capacity for individuals to make self-directed, informed choices in digital\nenvironments, which can be threatened by technologies that manipulate or constrain decision-\nmaking without transparency or consent.\n• Telemedicine – The remote delivery of healthcare services and clinical information using\ntelecommunications technology, which expands access to care but raises ethical concerns about\npatient privacy, confidentiality, and the quality of the patient-provider relationship.\n• Electronic medical records (EMR) – Digital versions of patients’ medical histories maintained\nby healthcare providers, designed to improve care coordination and efficiency while presenting\nchallenges related to data security, privacy, and equitable access.\n• Wearable health monitors – Technology-enabled devices worn on or in the body that\ncontinuously collect and transmit health data, offering opportunities for proactive health\nmanagement but also raising issues of data privacy, consent, and potential disparities in access.\n• AI-driven diagnostics – The use of AI systems to analyze medical data and assist in diagnosing\nhealth conditions, which can enhance diagnostic accuracy and efficiency but may\nsimultaneously introduce algorithmic bias, and lack of transparency, and accountability.\n• Patient privacy – The ethical and legal obligation to protect individuals’ health information\nfrom unauthorized access or disclosure, a critical concern heightened by the use of digital health\ntechnologies such as telemedicine, wearable health monitors and EMRs.\n• Screen readers – Software applications that convert digital text into synthesized speech or\nbraille, enabling people with visual impairments to access and interact with digital content and\npromoting greater accessibility and inclusion.\n• Voice recognition – Technology that interprets and processes spoken language, allowing users\n– especially those with mobility or dexterity challenges – to control devices and input\ninformation hands-free, thereby enhancing digital accessibility.\n• Adaptive hardware – Specialized physical devices designed to accommodate the needs of\nindividuals with disabilities, such as modified keyboards or alternative input devices, which\nhelp remove barriers and foster inclusive participation in technology use.\nPage 53 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "section_path": ["Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction"], "page": 54, "content": "Ethics in Technology\nBy: Ed Weber\n7. Technology in Personal and Social Life\nDigital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism;\nTechnology Addiction\nTechnology has become deeply woven into the fabric of daily\nlife, shaping how individuals interact, form relationships, and\nmanage personal well-being. As digital tools and platforms\nincreasingly mediate everything from communication to leisure,\nthey bring both opportunities for connection and challenges around\nautonomy, privacy, and mental health. The pervasive nature of\ntechnology means that personal choices – such as how much time to\nspend online or which platforms to use – can have far-reaching\neffects on social dynamics, emotional resilience, and even one’s\nsense of self.\nNavigating this landscape requires a thoughtful approach to the\nFigure 12: Graphic\nethical questions that arise when technology intersects with\nrepresentation of a digital\npersonal and social spheres. Issues such as the management of\nfootprint\ndigital footprints, the boundaries between public and private life,\nand the impact of constant connectivity on relationships demand\ncareful consideration. As society continues to adapt to evolving digital norms, individuals must balance\nthe benefits of technological convenience with the responsibility to protect their own well-being and\nthat of their communities.\nDigital Relationships\nDigital relationships are connections that are primarily formed, maintained, or deepened through\ndigital communication channels such as social media, messaging apps, online forums, or video calls.\nUnlike traditional relationships that rely on physical proximity and face-to-face interaction (i.e. in-real-\nlife (IRL), digital relationships transcend geographical boundaries and can develop between\nindividuals who may never meet in person. These connections can be romantic, platonic, or\nprofessional, and are often characterized by the use of text, images, video, and other digital media to\nconvey emotion, share experiences, and build trust.\nWhile some view digital relationships as fleeting or “throw-away” connections – easily formed\nand just as easily discarded – others experience profound intimacy and authenticity in their online\ninteractions. The digital environment can lower barriers to self-disclosure, allowing individuals to share\npersonal thoughts and feelings more openly than they might in person. This can lead some to value\nPage 54 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "section_path": ["Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction"], "page": 55, "content": "Ethics in Technology\nBy: Ed Weber\ndigital connections as deeply, or even more so, than their in-person relationships. However, the lack of\nphysical cues and the potential for curated online personas can blur the lines between genuine\nconnection and illusion, sometimes causing individuals to lose perspective on the nature and depth of\ntheir digital ties.\nOnline Dating\nOnline dating refers to the practice of seeking romantic or sexual partners via internet platforms,\ntypically through dedicated websites or mobile apps that facilitate the creation of personal profiles and\ndigital communication. These platforms allow users to present curated aspects of their identity – such\nas interests, values, and appearance – and to browse or be matched with others based on compatibility\nalgorithms or personal preferences. The convenience, broad reach, and relative anonymity of online\ndating have made it a mainstream method for meeting new people, offering opportunities to connect\nbeyond traditional social circles and geographic boundaries. For online dating sites to be considered\nvaluable, they must foster a safe, respectful environment, provide accurate and meaningful matches,\nand protect user privacy. Features such as robust identity verification, transparent algorithms, and\nclear communication tools contribute to a platform's credibility and user trust.\nHowever, online dating also presents significant risks and ethical challenges. Algorithmic bias can\nskew matches, reinforcing stereotypes or excluding certain groups, while aggressive data collection and\nthe sale of personal information raise concerns about user privacy and consent. Some platforms may\npersonalize content or matches to such an extent that they inadvertently censor or limit users’ choices,\nreducing the diversity of potential connections. The prevalence of deceptive practices – including\ncatfishing (posing as someone else), fraudulent schemes, and the creation of fake profiles – can lead to\nemotional harm or financial loss. These issues highlight the need for ethical oversight, transparency\nin data use, and robust safeguards to ensure that online dating remains a positive and equitable\nexperience for all users.\nPersonal Data Tracking\nIn an era of 'big-data', there is no such thing as 'TMI' (too-much-information). Every digital\ninteraction, no matter how trivial it may seem, contributes to a vast and ever-expanding profile of\npersonal data. Companies routinely track direct data points such as names, email addresses, phone\nnumbers, and purchase histories, but the scope goes far beyond this. Indirectly, organizations can derive\nsensitive information from patterns in browsing behavior, location data from GPS, device\nfingerprints, app usage, social media activity, and even metadata embedded in videos, photos and\nmessages. By aggregating and analyzing these diverse data streams, companies can infer a person’s\nhabits, preferences, health status, social circles, and even political leanings.\nPage 55 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "section_path": ["Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction"], "page": 56, "content": "Ethics in Technology\nBy: Ed Weber\nConsider a hypothetical user who only provides their name, email address, date of birth, and\ncountry when signing up for a service on their smartphone. Even with just these four data points, the\ndevice and associated apps can collect a wealth of additional information. The smartphone’s operating\nsystem and apps may automatically log the user’s IP address, device type, language settings, and\ngeolocation. By linking the email address to other online accounts, data brokers can cross-reference\nsocial media profiles, public records, and past purchase histories. The date of birth enables age-based\nprofiling, while the country helps narrow down cultural, legal, and economic backgrounds.\nMeanwhile, passive data collection – such as app usage patterns, movement tracked via GPS, and\nbrowsing history – can reveal daily routines, frequented locations, and social interactions. Machine\nlearning algorithms can then synthesize these disparate data points to construct an eerily accurate and\ninvasive profile: predicting the user’s income bracket, relationship status, health risks, interests, and\neven likely future behaviors. This comprehensive profiling, often invisible to the user, highlights the\nprofound privacy risks and ethical dilemmas posed by ubiquitous personal data tracking in the digital\nage.\nDigital Minimalism\nDigital minimalism is often seen as a response to the overwhelming realities of big-data collection,\nwhere every online action contributes to a growing digital footprint that is neither fully transparent nor\neasily controlled. For some, embracing digital minimalism can resemble “sticking one’s head in the\nsand” – a knee-jerk reaction to the anxiety and fatigue caused by constant notifications, information\noverload, and the persistent sense of surveillance. In this light, digital minimalism may appear as an\nattempt to escape rather than confront the pervasive reach of technology. This is especially apparent\nwhen “opting out” entirely is rarely practical (… or even possible…) in a world where digital\nconnectivity underpins nearly every aspect of work, social life, and civic engagement.\nDespite these limitations, the appeal of digital minimalism lies in its promise to restore balance,\nfocus, and well-being by encouraging a more intentional and mindful relationship with technology.\nRather than rejecting digital tools outright, digital minimalism advocates for curating one’s digital\nenvironment to prioritize high-value activities that align with personal values and goals. Practitioners\nreport benefits such as improved mental clarity, stronger relationships, and enhanced productivity, as\nthey reduce digital clutter and reclaim their attention from low-value distractions.\nHowever, the practical challenge remains: even the most disciplined digital minimalist cannot fully\nescape the data-driven infrastructure that shapes modern existence. Thus, digital minimalism is less\nabout total withdrawal and more about making conscious choices to engage with technology in ways\nthat support, rather than undermine, a meaningful and healthy life.\nPage 56 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "section_path": ["Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction"], "page": 57, "content": "Ethics in Technology\nBy: Ed Weber\nTechnology Addiction\nTechnology addiction is a behavioral disorder characterized by compulsive and excessive\nengagement with digital devices and online activities, often to the detriment of personal, social, and\nprofessional well-being. Like other addictions, those affected may not recognize – or may actively deny\n– that their technology use has become problematic. Denial is a common defense mechanism, with\nindividuals believing they are in control or rationalizing their behavior as normal, even as they neglect\nresponsibilities, relationships, or self-care. This lack of self-awareness is compounded by the ubiquity\nof technology in daily life, making it difficult to distinguish between healthy use and dependency.\nMany app developers and tech companies intentionally design their products to maximize user\nengagement, leveraging psychological principles such as variable rewards and social validation to\ncreate habit-forming experiences. Features like endless scrolling, push notifications, and algorithmic\ncontent feeds are engineered to keep users returning, increasing screen time and, ultimately,\nadvertising revenue.\nThere have been high-profile legal actions against major tech firms alleging that their platforms are\nintentionally addictive, particularly to young users. For example, lawsuits have been filed against social\nmedia companies for allegedly exploiting vulnerabilities in children and teens to encourage compulsive\nuse, with claims that these practices contribute to mental health crises.\nIn response, some governments and advocacy groups have called for increased regulation,\ntransparency in algorithm design, and the implementation of features like screen time limits and\ndigital well-being tools. It is their hope and intent that these additional approaches will help users\nregain control over their technology use. Despite these efforts, addressing technology addiction remains\na significant challenge in an increasingly digital world.\nTextbook Definitions – Technology in Personal and Social Life\n• digital footprint – The unique trail of data created by an individual’s online activities, both\nintentionally and unintentionally, including websites visited, emails sent, and information\nsubmitted online.\n• Digital relationships – Connections formed and maintained primarily through digital\ncommunication channels such as social media, messaging apps, or online forums.\n• in-real-life (IRL) – Interactions or relationships that occur in the physical, offline world rather\nthan through digital platforms.\n• “throw-away” connections – Brief, low-commitment digital interactions that are easily formed\nand just as easily discarded, often lacking depth or long-term significance.\nPage 57 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "section_path": ["Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction"], "page": 58, "content": "Ethics in Technology\nBy: Ed Weber\n• curated online personas – Carefully crafted digital identities where individuals selectively\npresent aspects of themselves to shape how they are perceived online.\n• Online dating – The practice of seeking romantic or sexual partners through internet platforms\nthat facilitate profile creation, matching, and digital communication.\n• relative anonymity – The condition in which users can interact or share information online\nwithout fully revealing their true identities, often lowering barriers to self-disclosure.\n• transparent algorithms – Algorithms whose functioning, criteria, and decision-making\nprocesses are openly disclosed and understandable to users.\n• Algorithmic bias – Systematic and unfair discrimination embedded in automated decision-\nmaking processes, often reflecting or amplifying existing social prejudices.\n• censor – To suppress, limit, or remove content or information from digital platforms, often\nbased on specific rules, policies, or external pressures.\n• catfishing – The act of creating a fake digital identity to deceive others, typically for personal,\nfinancial, or emotional gain.\n• fake profiles – Online accounts that use false or misleading information to impersonate\nsomeone else or create a fictitious persona.\n• ethical oversight – The process of monitoring and guiding technology development and\ndeployment to ensure alignment with ethical standards and societal values.\n• transparency in data use – The practice of clearly informing users about how their personal\ndata is collected, processed, shared, and stored.\n• big-data – Extremely large and complex datasets that are analyzed computationally to reveal\npatterns, trends, and associations, especially relating to human behavior.\n• TMI (too-much-information) – The idea that excessive sharing or collection of personal data\ncan occur, though in the context of big-data, there is often no perceived limit to what is gathered\nor analyzed.\n• information overload – A state in which the volume of information received exceeds an\nindividual’s capacity to process or make decisions effectively.\n• patterns in browsing behavior – Trends and habits revealed by analyzing the websites and\ncontent an individual visits or interacts with online.\n• location data – Information about the geographical position of a device or user, often collected\nvia GPS, Wi-Fi, or IP address.\nPage 58 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "section_path": ["Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction"], "page": 59, "content": "Ethics in Technology\nBy: Ed Weber\n• device fingerprints – Unique combinations of device attributes (such as browser type,\noperating system, screen size) used to identify and track users across digital platforms.\n• metadata – Data that provides information about other data, such as time stamps, location,\nauthor, or device details associated with digital files or communications.\n• data brokers – Companies or entities that collect, aggregate, and sell personal data from\nvarious sources, often without direct user consent.\n• profiling – The process of analyzing and combining personal data to create detailed user\nprofiles that predict behaviors, preferences, or characteristics.\n• Digital minimalism – A lifestyle approach that emphasizes intentional and selective use of\ndigital technologies to reduce distractions and focus on meaningful activities.\n• surveillance – The continuous monitoring or observation of individuals’ activities, often\nthrough digital means, for purposes such as security, marketing, or data collection.\n• opting out – The act of choosing not to participate in certain digital services or data collection\npractices, often to protect privacy or reduce digital exposure.\n• Technology addiction – A behavioral disorder involving compulsive and excessive use of\ndigital devices or online platforms, leading to negative impacts on daily life.\n• variable rewards – Unpredictable and intermittent incentives that reinforce repeated\nengagement with digital platforms, making them more habit-forming.\n• social validation – The psychological reinforcement gained from receiving approval, likes, or\npositive feedback from others in digital environments.\n• endless scrolling – A design feature that allows users to continuously view new content without\nexplicit breaks, encouraging prolonged engagement.\n• push notifications – Alerts sent by apps or websites to users’ devices to prompt immediate\nattention or action.\n• algorithmic content feeds – Streams of information or media curated and delivered to users\nbased on automated analysis of their preferences and behaviors.\n• screen time – The amount of time an individual spends using digital devices or engaging with\nscreens.\n• advertising revenue – Income generated by digital platforms through displaying ads to users,\noften driven by high engagement and data collection.\nPage 59 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "section_path": ["Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction"], "page": 60, "content": "Ethics in Technology\nBy: Ed Weber\n• compulsive use – Repetitive and uncontrollable engagement with digital technology, often\ndespite negative consequences.\n• transparency in algorithm design – The practice of making the logic, criteria, and functioning\nof algorithms open and understandable to users and stakeholders.\n• screen time limits – Tools or policies that restrict the duration an individual can spend on\ndigital devices or specific applications to promote healthier usage patterns.\nPage 60 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 61, "content": "Ethics in Technology\nBy: Ed Weber\n8. Privacy, Surveillance, and Data Ethics\nBig Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data\nCollection and Consent; Cloud Computing; Data Ownership and Open-Source\nSolutions\nAs our lives become increasingly intertwined with digital technologies, the boundaries between\npublic and private spheres have grown more complex. The preceding chapters have explored how\ntechnology shapes our identities, relationships, and access to opportunities, while also highlighting the\nresponsibilities of both developers and consumers in navigating ethical challenges. From the\nresponsibilities of tech users and professionals, to the impact of technology on justice, equity, and\npersonal well-being, we have seen that ethical decision-making is rarely straightforward – often\nrequiring us to balance competing values and anticipate unintended consequences.\nBuilding on these foundations, this chapter delves into the critical issues of privacy, surveillance,\nand data ethics. In a world driven by big data, cloud computing, and ubiquitous connectivity,\nquestions about who owns our information, how it is collected, and for what purposes it is used have\nbecome central to the ethical landscape. We will examine the evolving definitions of privacy in the\ndigital age, the rise of urban surveillance and smart cities, and the ethical dilemmas posed by large-\nscale data collection and consent. By considering the implications of data ownership and the\nresponsibilities of both individuals and organizations, this chapter aims to equip readers with the tools\nto critically assess the ethical dimensions of privacy and surveillance in contemporary society.\nBig Data and Privacy\nThe rise of big data has fundamentally transformed the landscape of personal privacy. Every day,\nindividuals generate vast amounts of digital information through online interactions, purchases, social\nmedia activity, and even passive data collection via mobile devices and smart home technology. This\ninformation is not only collected by the platforms and services individuals use directly, but is also\nroutinely shared with third-party data aggregators, sold to marketers, and analyzed by a wide array of\norganizations seeking to infer deeper insights about users’ behaviors and preferences. The sheer scale\nand interconnectedness of data collection means that a single piece of personal information – such as an\nemail address or geolocation – can be replicated, cross-referenced, and stored in dozens, if not\nhundreds, of separate databases worldwide.\nA crude (but conservative) estimate suggests that for any active digital user, there may be hundreds\nto thousands of copies of their personal data distributed across various entities. Each online service,\nretailer, social media platform, and app may create its own record; data brokers and aggregators\nfurther multiply these records as they buy, sell, and combine data sets; and backup systems, cloud\nPage 61 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 62, "content": "Ethics in Technology\nBy: Ed Weber\nstorage, and analytics platforms add further redundancy. Moreover, big data analytics can infer\nadditional attributes and connections, effectively creating new “copies” of information by extrapolating\nfrom existing data points. This exponential proliferation makes it nearly impossible for individuals to\nfully track or control the spread of their digital footprints.\nConsider this scenario: A big cell phone provider sells anonymized (de-identified) data points to a\ndata aggregator and analysis group which cover a period of one-week for a particular region. The data\nwas requested because a store in this region wants to learn more about the individuals who showed up\nfor their big sale event. The aggregator starts by focusing on all of the cell phones that went to that\nparticular store during the store’s event.\nBut, they don’t stop there. They then review where the data points go after they left the store to see\nthe other destinations for these devices. They discover that quite a number of phones left the store and\nwent to food establishments. This might suggest that the store should have some ‘snacks’ available\nduring their next sale event. They also discover that a number of the devices went to a competitor’s\nstore… this may be interesting to both the original store as well as to the competitor store.\nBut then, they follow the phones to their ‘final destinations’ for that day to see where they ended up\nthat night. Then, they repeat this process for each day (not just the sale day) to see what else they can\nlearn about the ‘anonymous’ data points. Just by analyzing where the phones end up for the ‘end-of-\nthe-day’ these data points may likely represent the ‘homes’ of the ‘anonymous data points’. And\ntracking where the phones go each day, they are likely to discover other patterns as well. So much for\nanonymized data!\nPrivacy, in its most common definition, refers to the ability of individuals to control the collection,\nusage, and distribution of their personal information. It encompasses the right to decide what\ninformation is shared, with whom, and for what purposes. In the digital age, however, this expectation\nis increasingly challenged. The default practices of data collection, the complexity of data flows, and\nthe lack of transparency in how information is shared or sold mean that true control over personal\ndata is often illusory. While privacy remains a foundational value and a legal right in many\njurisdictions, the reality is that maintaining a reasonable expectation of privacy online requires\nsignificant effort, technical literacy, and often, a willingness to opt out of many modern conveniences.\nThus, while the principle of privacy is still widely recognized, its practical realization in the age of big\ndata is fraught with challenges and, for many, may no longer be a fully reasonable expectation without\nsubstantial systemic change.\nPage 62 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 63, "content": "Ethics in Technology\nBy: Ed Weber\nPublic vs. Private\nIn the previous section, we considered the expectation of privacy in the age of big-data. So it seems\nnow we should differentiate between the legal definitions vs. the ethical definitions of the terms\n‘Public’ vs. ‘Private’.\nThe distinction between ‘public’ and ‘private’ is foundational both in legal and ethical discussions,\nyet the definitions and boundaries can shift depending on context. Legally, ‘public’ typically refers to\nspaces, actions, or information that are accessible or visible to the general population and where\nindividuals have a reduced expectation of privacy. ‘Private,’ on the other hand, denotes areas,\nbehaviors, or data that are restricted to individuals or select groups, where a higher expectation of\nprivacy is recognized and protected by law.\nEthically, the distinction often hinges on the reasonable expectations of those involved. Consider\nthe scenario of looking through an open window into someone’s home: while the window may be open\nand the view technically accessible from a public sidewalk, most people would agree that peering\ninside or, more invasively, taking a photo or video crosses an ethical line. The act transitions from a\npassive observation in a public space to an active intrusion into someone’s private life, highlighting\nhow context and intent matter.\nSimilarly, recording audio or video of people inside a grocery store – where there is a general\nexpectation of being in a semi-public space – differs ethically (and sometimes legally) from recording\nthose same people outside on a public sidewalk. The boundaries blur further in places like restaurants,\npublic transportation, or even online forums, where the mix of public accessibility and private\ninteraction complicates the ethical calculus.\nAdditional examples illustrate these nuances. In a workplace, conversations in a private office are\ngenerally considered private, while those in a break room may not be. In digital contexts, posting on a\npublic social media page is typically considered public, but sending a direct message is private –\nthough the technical ability to copy, share, or leak messages challenges this expectation. Even in public\nspaces, certain activities, such as using a restroom or changing clothes in a locker room, retain strong\nlegal and ethical protections of privacy despite their location.\nUltimately, the legal definitions of public versus private are shaped by statutes and case law, often\nfocusing on the impact of actions on society versus individuals. Ethically, the distinction is more fluid,\nrelying on context, societal norms, and the reasonable expectations of those involved. As technology\ncontinues to blur these boundaries – through ubiquitous cameras, data collection, and online sharing –\nit becomes increasingly important to critically examine not just what is legally permissible, but what is\nethically respectful of individuals’ privacy and autonomy.\nPage 63 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 64, "content": "Ethics in Technology\nBy: Ed Weber\nUrban Surveillance and Smart Cities\nUrban surveillance and the development of smart cities have introduced a range of technologies that\npromise to enhance public safety, improve efficiency, and optimize city services. Traffic cameras, for\ninstance, are widely deployed to monitor intersections, enforce traffic laws, and provide real-time data\nto manage congestion. These systems can reduce accidents and improve emergency response times by\nallowing authorities to quickly identify and address incidents. Similarly, vehicle tracking – enabled\nthrough license plate readers and various connected sensors – can help locate stolen vehicles,\noptimize public transportation routes, and even support environmental goals by monitoring emissions\nand traffic patterns.\nHowever, these same technologies raise significant concerns about privacy and the potential for\nmisuse. Traffic cameras and vehicle tracking systems can be repurposed for mass surveillance,\nenabling authorities or third parties to monitor individuals’ movements without their knowledge or\nconsent. This persistent observation can erode the sense of urban anonymity and create a chilling effect\non personal freedom, as people may change their behaviors if they feel constantly watched. The\naggregation of vehicle movement data, when combined with other data sources, can reveal sensitive\npatterns about individuals’ routines and associations.\nFacial recognition technology represents another powerful but controversial tool in the smart city\narsenal. On the positive side, it can assist in locating missing persons, identifying suspects in criminal\ninvestigations, and enhancing security at large public events. Yet, the deployment of facial recognition\nin public spaces has sparked intense debate over accuracy, bias, and the risk of wrongful\nidentification. Moreover, the widespread use of facial recognition can enable pervasive government or\ncorporate monitoring, undermining civil liberties and disproportionately impacting marginalized\ncommunities.\nOther notable examples include smart utility meters and environmental sensors. Smart meters can\nhelp residents and city officials monitor and reduce energy and water consumption, contributing to\nsustainability goals and lowering costs. Environmental sensors, such as those monitoring air quality or\nflood risks, can provide early warnings and improve public health outcomes. Yet, both technologies\ncollect detailed data about residents’ habits and activities, raising questions about who has access to this\ninformation and how it might be used beyond its intended purpose.\nUltimately, while urban surveillance and smart city technologies offer clear benefits – improved\nsafety, efficiency, and sustainability – they also introduce complex ethical challenges. The risk of\ncyberattacks, unauthorized data sharing, and the erosion of privacy demands robust governance,\ntransparent policies, and meaningful public engagement to ensure that technological progress does not\ncome at the expense of individual rights and community trust.\nPage 64 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 65, "content": "Ethics in Technology\nBy: Ed Weber\nData Collection and Consent\nThe distinction between explicit and implied consent is central to understanding how data is\ncollected and used in the digital environment. Explicit consent requires a clear, affirmative action from\nthe user – such as checking a box, signing a form, or clicking an “I Agree” button – indicating\nunambiguous agreement to the collection and processing of their data. This type of consent is often\naccompanied by detailed language in End-User License Agreements (EULAs) or Terms of Service\n(ToS), specifying what data will be collected, how it will be used, and who it may be shared with. For\nexample, a ToS might state, “We collect your name, email, and usage data to provide and improve our\nservices,” and the company requires the user to actively accept these terms before proceeding.\nImplied consent, by contrast, is inferred from a user’s actions or the context in which those actions\noccur. If a user continues to browse a website after being notified of a cookie policy, or submits a\ncontact form expecting a response, their behavior is interpreted as agreement to certain data practices –\neven if they have not explicitly acknowledged them. Implied consent is often used for routine or less\nsensitive data collection, but it is inherently less transparent and can lead to ambiguity or disputes over\nwhat the user actually agreed to.\nA critical nuance in these agreements is the use of open-ended language regarding data use. For\ninstance, a clause might state, “We may use your data for purposes such as backups or translation to\nanother language.” The phrase “such as” does not restrict the company to only those listed uses; rather,\nit leaves the door open for additional, unspecified uses of the data. The stated intent behind this\nlanguage (if it is ever actually stated) is said by the company to provide flexibility for operational\nneeds. However, it can also be used to mask broader data exploitation. For example, data collected for\n“service improvement” could be repurposed for targeted advertising, profiling, or even sold to third\nparties – uses not explicitly disclosed in the original agreement – but made legal through the agreement\nas written.\nThe concept of intent becomes central here. While a company may claim its intent is benign – such\nas improving user experience or ensuring data security – the same permissions can be leveraged for\nmore intrusive or profit-driven activities, like behavioral advertising, location tracking, or sharing data\nwith law enforcement or other organizations without further user notification. Other examples include\nusing voice recordings from smart speakers to train AI beyond the stated purpose, or aggregating\nfitness tracker data for insurance risk assessment, even when the original consent was for personal\nhealth monitoring.\nLegally, the sufficiency of consent – whether explicit or implied – depends on the jurisdiction and\nthe sensitivity of the data involved. Regulations like the General Data Protection Regulation\n(GDPR) developed and implemented in the European Union, require explicit, informed consent for\nmost personal data processing, especially for sensitive categories, and place the burden on\norganizations to demonstrate that valid consent was obtained.\nPage 65 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 66, "content": "Ethics in Technology\nBy: Ed Weber\nEthically, the bar is even higher: true consent should be informed, freely given, and revocable,\nwith users fully understanding both the scope and intent of data collection. In practice, however, the\ncomplexity of agreements and the opacity of data flows make it difficult to prove that users have\ngenuinely understood or agreed to all possible uses of their data.\nLikewise, demonstrating the true intent of a company’s data practices is challenging, as broad or\nambiguous language can be exploited for purposes far beyond those originally disclosed. As a result,\nboth proving consent and intent remains a fraught process, highlighting the ongoing need for clearer\ncommunication, stronger regulation, and more transparent data practices.\nCloud Computing\nCloud computing has become deeply integrated into the daily routines of non-corporate users,\noffering convenience and flexibility across a range of applications. Common examples include file\nstorage and sharing services like Google Drive, Apple iCloud, and Dropbox, which allow users to save\ndocuments, photos, and videos remotely and access them from any device. Email services such as\nGmail and Yahoo Mail rely on the cloud to store messages and attachments, making communication\nseamless and accessible from anywhere. Social media platforms like Facebook and Instagram use cloud\ninfrastructure to let users upload and share photos, videos, and other content. Streaming services,\nincluding Netflix and Spotify, leverage the cloud to deliver on-demand entertainment to millions, while\ncloud-based productivity suites like Google Docs and Microsoft 365 enable real-time collaboration and\ndocument editing without the need for local software installations.\nThe primary appeal of these cloud-based applications lies in their promise of accessibility across\ndevices, ease of use, and the ability to synchronize data across multiple devices. Users are drawn to\nthe convenience of automatic backups, the ability to share files instantly, and the reduction in the need\nfor physical storage or device-specific software. Cloud computing also supports mobile banking, online\neducation, and even health and fitness tracking, making it a central pillar of modern digital life.\nHowever, as discussed in the previous section on data\ncollection and consent, the agreements users accept when\nadopting cloud services often grant providers broad rights over\ntheir personal information. While the stated intent may be to\nfacilitate backups or enhance user experience, the legal\nlanguage typically allows providers to use, analyze, and even\nshare user data for purposes far beyond those original use cases.\nThis creates a significant imbalance: the value extracted from\nuser data – through targeted advertising, analytics, or third-\nFigure 13: There is no cloud...\nPage 66 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 67, "content": "Ethics in Technology\nBy: Ed Weber\nparty partnerships – can exceed the utility provided to the user in the form of basic storage or\nconvenience.\nConsider this figure describing ‘the cloud’. If we understand that there is no cloud, but rather, it is\njust someone else’s computer, we begin to understand that we are just using their computers for\nstorage, and that they are perpetually peeking at literally everything you put up there!\nTransparency remains a major issue. Once data is uploaded to the cloud, users have little visibility\ninto where it is stored, how it is processed, or with whom it is shared. The lack of clear, accessible\ninformation about data practices means that users cannot easily verify how their information is being\nused or if it is being sold or repurposed for profit. This opacity is compounded by the trend of phasing\nout traditional, locally-installed productivity software in favor of cloud-based, subscription-only\nmodels. Companies are increasingly steering users toward exclusive cloud solutions to ensure recurring\nrevenue, gain greater control over software updates, and – crucially – maintain ongoing access to user\ndata. As a result, users are often left with little choice but to accept these terms if they wish to continue\nusing familiar tools, further eroding their control over personal information and privacy in the digital\nage.\nData Ownership and Open-Source Solutions\nData ownership refers to the legal rights, control, and authority an individual or entity has over\nspecific sets of data, including how that data is accessed, used, modified, shared, or deleted. It is about\nboth possession and responsibility, granting the owner the power to determine the fate of the data and\nto enforce those rights legally and ethically. Data ownership is foundational for accountability, privacy,\nand security in a world where personal and organizational data are invaluable assets.\nQuestions to Consider About Data Ownership:\n• Does a person own their own name, or is it merely a public identifier?\n• Who owns an individual’s email address: the person, the email provider, or both?\n• If you purchase a phone, do you own all the data stored on it, or does the manufacturer or\nservice provider retain some rights?\n• Is your fingerprint your property, or does an entity that collects and stores its digital\nrepresentation (e.g., for authentication) share ownership?\n• Who owns your DNA sequence: you, your healthcare provider, or the company that analyzes it?\n• If a company collects your location data via a mobile app, do you retain ownership, or does the\ncompany claim rights through its terms of service?\nPage 67 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 68, "content": "Ethics in Technology\nBy: Ed Weber\n• Who owns the photos and messages you upload to social media platforms – you, the platform,\nor both?\n• If you generate creative works (art, writing, code) using a cloud-based app, do you own the\ncontent, or does the app provider have rights to it?\n• When you use voice assistants, do you own the recordings, or does the service provider?\n• Who owns aggregated or anonymized data derived from your personal information?\n• If your data is sold to third parties, do you still have any ownership or control over it?\n• Who owns the metadata (such as timestamps, device info, or usage statistics) generated by your\ninteractions with digital services?\n• If a government agency collects your data for public health or security, do you retain any\nownership or rights over that data?\nThese questions illustrate the complexity and spectrum of data formats – ranging from personally\nidentifiable information (PII) like names and fingerprints, to digital content, behavioral metadata, and\neven biological data. Legally, ownership can depend on jurisdiction, contractual agreements, and the\nnature of the data, while ethically, many argue individuals should retain primary rights and control over\ntheir personal information.\nSome types of data, such as biometric identifiers (fingerprints, facial scans, DNA) and\ncommonly accessed data (emails, social media posts), are inherently difficult to isolate and protect due\nto the way they are collected, stored, and shared across platforms and organizations. Once digitized and\nuploaded, these data types often become subject to broad terms of service that can dilute individual\nownership and control.\nBy contrast, data that a user creates – such as documents, code, or media files – can, actually be\nmore readily controlled and protected! Open-source software and personal computing resources\nprovide the mechanisms by which users can take a modicum of control over digital information that\nthey create. Open-source solutions empower users to retain ownership by allowing them to store,\nmanage, and modify their data locally or on self-hosted platforms, free from restrictive proprietary\nagreements. This approach not only enhances privacy and security but also aligns with the ethical\nprinciple that individuals should have meaningful control over their own digital creations and personal\ninformation.\nThere are many open-source solutions for the vast majority of the computing activities that typical\nusers experience. Here is a brief list (as of this publication) of just some of the open-source titles and\ntheir typical uses:\nPage 68 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 69, "content": "Ethics in Technology\nBy: Ed Weber\nHere are several of the most popular open-source software titles across a wide range of productivity\nand creative tasks, suitable for non-corporate users:\nOperating Systems\n• Ubuntu • Linux Mint • Debian\n• Fedora • Manjaro • OpenBSD\n• FreeBSD • Puppy Linux\nPersonal Information Managers & Email\n• Thunderbird • Evolution • KOrganizer/KMail\nOffice Applications\n• LibreOffice • OnlyOffice • Calligra Suite\nArtistic and Image Editing\n• GIMP (Photo Editing) • Inkscape (vector graphics) • Krita (digital painting)\nVideo Editing and Production\n• Shotcut • Blender (also for 3D modeling • OBS Studio (Open\nand animation) Broadcaster Software)\nAudio Editing and Production\n• Audacity • LMMS (Linux Multi Media • Ardour\nStudio)\nPage 69 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 70, "content": "Ethics in Technology\nBy: Ed Weber\nOther Productivity and Creative Tools\n• VLC Media Player • Nextcloud (personal cloud • Joplin (note-taking and\n(media playback) storage and collaboration) to-do lists)\n• Scribus (desktop • Darktable (photo workflow • Calibre (e-book\npublishing) and raw development) management)\n• Rocket.Chat (team • Jupyter Notebook (interactive\ncommunication) computing and data science)\nThese tools provide robust alternatives to proprietary solutions and empower users to retain greater\ncontrol over their data and creative output.\nTextbook Definitions – Privacy, Surveillance, and Data Ethics\n• privacy – The right and ability of individuals to control the collection, use, and sharing of their\npersonal information, ensuring freedom from unwarranted intrusion into their lives.\n• surveillance – The monitoring or observation of individuals or groups, often by authorities or\norganizations, to collect information or ensure security, which can threaten privacy if\nunwarranted.\n• data ethics – The moral principles and guidelines that govern the collection, analysis, and use\nof data, emphasizing privacy, transparency, accountability, and fairness.\n• big data – Extremely large and complex datasets generated from various sources, analyzed to\nreveal patterns, trends, and associations, especially relating to human behavior.\n• cloud computing – The delivery of computing services – including storage, processing, and\nsoftware – over the internet, allowing users to access and manage data and applications\nremotely.\n• ubiquitous connectivity – The state of being continuously connected to digital networks and\nservices from virtually anywhere, enabling constant data exchange.\n• urban surveillance – The use of technology such as cameras, sensors, and tracking systems in\ncities to monitor public spaces and activities for safety, efficiency, or control.\n• data collection – The process of gathering information from various sources, either actively or\npassively, for analysis, storage, or decision-making.\nPage 70 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 71, "content": "Ethics in Technology\nBy: Ed Weber\n• consent – Permission granted by individuals for the collection and use of their data, which\nshould be informed, freely given, and revocable.\n• data ownership – The legal rights and control an individual or entity has over specific data,\nincluding how it is accessed, used, shared, or deleted.\n• passive data collection – Gathering information from users without their direct input or\nawareness, often through background processes or device sensors.\n• smart home technology – Devices and systems within a home that use internet connectivity to\nautomate and control functions such as lighting, security, and climate.\n• data aggregators and brokers – Entities that collect, combine, and sell data from multiple\nsources, often creating detailed profiles of individuals.\n• cloud storage – A service that allows users to save data on remote servers accessed via the\ninternet, rather than on local devices.\n• lack of transparency – The absence of clear, accessible information about how data is\ncollected, used, or shared, making it difficult for individuals to understand or control their data.\n• expectation of privacy – The belief or assumption that one’s personal information or activities\nwill not be observed or disclosed without consent.\n• Public – Legally and ethically, spaces, actions, or information accessible to the general\npopulation, where individuals have a reduced expectation of privacy.\n• Private – Spaces, actions, or information restricted to individuals or select groups, where a\nhigher expectation of privacy is recognized and protected.\n• reasonable expectations – What an average person would consider appropriate regarding\nprivacy or data use in a given context.\n• context – The circumstances or setting in which data is collected, used, or observed, which\ninfluence privacy expectations and ethical considerations.\n• intent – The purpose or motivation behind collecting, using, or sharing data, which affects the\nethical evaluation of those actions.\n• societal norms – The shared expectations and rules within a community that shape perceptions\nof privacy, consent, and acceptable data practices.\n• Traffic cameras – Cameras installed in public areas to monitor vehicle flow, enforce traffic\nlaws, and enhance public safety.\nPage 71 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 72, "content": "Ethics in Technology\nBy: Ed Weber\n• license plate readers – Automated systems that capture and process images of vehicle license\nplates for law enforcement or traffic management.\n• connected sensors – Devices embedded in infrastructure or vehicles to collect and transmit\ndata on movement, environment, or system status.\n• mass surveillance – The large-scale monitoring of populations, often by governments, using\ntechnology to collect and analyze vast amounts of data.\n• Facial recognition – Technology that identifies or verifies individuals by analyzing facial\nfeatures from images or video.\n• accuracy – The degree to which a system or process correctly identifies, measures, or\nrepresents information, crucial for fair outcomes in surveillance and data use.\n• bias – Systematic errors or prejudices in data collection, analysis, or technology that can lead to\nunfair or discriminatory outcomes.\n• wrongful identification – Incorrectly matching or labeling an individual by surveillance or\nrecognition systems, leading to potential harm.\n• civil liberties – Fundamental rights and freedoms, such as privacy and free expression, that are\nprotected from excessive government or organizational intrusion.\n• marginalized communities – Groups that experience discrimination or disadvantage, often\ndisproportionately affected by surveillance and data misuse.\n• cyberattacks – Malicious attempts to access, disrupt, or damage digital systems or data.\n• Explicit consent – Clear, affirmative agreement to data collection or processing, usually given\nthrough direct actions like checking a box or clicking “I Agree”.\n• End-User License Agreements (EULAs) – Legal contracts between software providers and\nusers outlining the terms for using the software, including data rights.\n• Terms of Service (ToS) – Agreements specifying the rules, responsibilities, and data practices\nassociated with using a digital service.\n• Implied consent – Permission inferred from a person’s actions or the context, rather than a\ndirect statement or agreement.\n• cookie policy – A statement on a website detailing how cookies are used to collect and process\nuser data.\n• General Data Protection Regulation (GDPR) – A comprehensive European Union law that\ngoverns data protection and privacy, emphasizing informed, explicit consent and user rights.\nPage 72 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "section_path": ["Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions"], "page": 73, "content": "Ethics in Technology\nBy: Ed Weber\n• informed – Having adequate information to understand the implications and risks before\nagreeing to data collection or use.\n• freely given – Consent provided voluntarily, without coercion or undue pressure.\n• revocable – The ability to withdraw consent at any time, stopping further data collection or use.\n• Cloud Computing – The practice of using remote servers on the internet to store, manage, and\nprocess data, rather than relying on local hardware.\n• accessibility across devices – The capability to use data and applications seamlessly from\nmultiple devices via cloud services.\n• synchronize data across multiple devices – Keeping files, settings, and information consistent\nand updated on all user devices through cloud-based solutions.\n• automatic backups – The process of regularly copying data to a remote server to prevent loss\nand ensure recovery.\n• Data ownership – The legal and ethical right to control, access, and manage one’s own data,\nincluding decisions about its use and sharing.\n• personally identifiable information (PII) – Any data that can be used to identify a specific\nindividual, such as names, addresses, or Social Security numbers.\n• biological data – Information derived from an individual’s biological characteristics, including\nDNA, fingerprints, and other biometrics.\n• biometric identifiers – Unique physical or behavioral traits, such as fingerprints, facial scans,\nor iris patterns, used for identification.\n• fingerprints – Distinctive patterns on the tips of fingers, often used as a biometric identifier.\n• facial scans – Digital representations of facial features used for identification or authentication.\n• DNA – The genetic material that carries an individual’s hereditary information, unique to each\nperson.\n• Open-source software – Software with publicly available source code that can be freely used,\nmodified, and distributed by anyone.\n• personal computing resources – Devices and infrastructure owned and controlled by\nindividuals, enabling them to manage and store their own data locally.\nPage 73 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 74, "content": "Ethics in Technology\nBy: Ed Weber\n9. Digital Communication, Social Media, Misinformation and Democracy\nSocial Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation;\nManipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy\nThe digital revolution has profoundly transformed the ways in which people connect, share ideas,\nand participate in civic life. This chapter explores how the tools and platforms that facilitate these\ninteractions also raise complex ethical questions that touch on every aspect of our personal and\ncollective existence. The rapid spread of information – and the ease with which it can be shaped or\ndistorted – has forced societies to confront new challenges regarding trust, credibility, and the\nresponsibilities of both individuals and institutions. These dynamics are deeply interwoven with our\nearlier discussions on privacy, data ethics, and the broader societal impacts of technology. This all\nserves to highlight the need for nuanced approaches to digital citizenship.\nAs digital spaces become central to public discourse, the boundaries between private expression and\npublic consequence have blurred. The ethical dilemmas introduced here are not isolated; they are\namplified by the same technological advancements that enable unprecedented connectivity and\ninnovation. Issues explored in previous chapters – such as the responsibilities of tech developers and\nconsumers, the vulnerabilities of digital identities, and the implications of surveillance – are now seen\nthrough the lens of how information is shared, consumed, and manipulated. This chapter examines the\nways in which digital communication shapes social norms, influences decision-making, and can both\nempower as well as undermine democratic processes. It is here that the ethical frameworks introduced\nat the outset of this text are put to the test, as readers are invited to consider how technology mediates\nour relationships with each other and with the wider world.\nSocial Media Ethics\nSocial media has redefined how individuals engage with information and with one another, creating\na dynamic environment where both users and platforms play crucial ethical roles. As consumers, people\nare constantly exposed to a vast array of content – news, opinions, entertainment, and more – often\nalgorithmically curated and designed to maximize engagement rather than assure accuracy. This\nplaces a unique responsibility on users to critically evaluate the information they encounter. Ethical\nparticipation means more than simply sharing or reacting; it involves considering the potential impact\nof one's posts and interactions. Users must weigh the value of free expression against the potential\nharm caused by spreading misinformation, engaging in harmful rhetoric, or participating in online\nharassment. The rise of digital anonymity can sometimes embolden individuals to act in ways they\nwould not in face-to-face interactions, underscoring the importance of empathy, respect, and\naccountability in online spaces.\nPage 74 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 75, "content": "Ethics in Technology\nBy: Ed Weber\nPlatforms, on the other hand, bear a distinct set of ethical responsibilities. While users must exercise\npersonal judgment, social media companies are tasked with balancing the principles of free speech with\nthe need to prevent harm and maintain a safe, inclusive environment – all while keeping their financial\nbottom line in mind. This balancing act often manifests in debates over censorship – where does\nmoderation cross the line into undue suppression of ideas? Platforms must also grapple with the\nchallenge of distinguishing between legitimate satire and deliberately misleading content. The\nexpectation of fact-checking is a contentious issue: while some argue that platforms should take a\nmore active role in verifying information, others warn of the dangers of overreach and the potential for\nbias in content moderation. Ultimately, both participants and platforms share an ethical obligation to\nfoster an online ecosystem that encourages constructive dialogue, protects against harm, and upholds\nthe integrity of public discourse – a challenge that grows ever more complex as the digital landscape\ncontinues to evolve.\nCyberbullying and Harassment\nCyberbullying and harassment are two closely related forms of harmful behavior that occur through\ndigital channels. Cyberbullying is defined as the use of technology – such as social media, messaging\napps, or online games – to harass, threaten, embarrass, or target another person. It often involves\nrepeated actions intended to harm, and can include sending mean or aggressive messages, spreading\nrumors, posting embarrassing photos or videos, or deliberately excluding someone from online groups.\nHarassment is a broader term that encompasses any unwanted behavior intended to annoy, threaten, or\nintimidate another person, and in a digital context, this can range from persistent unwanted messages to\nexplicit threats or hate speech. Both cyberbullying and harassment can have severe emotional and\npsychological consequences, especially since digital content can be widely and permanently\ndistributed.\nExamples of these behaviors are numerous and can include cyberstalking, where an individual\nmonitors or follows someone’s online activity obsessively, often with threatening intent; doxxing,\nwhich involves maliciously sharing someone’s personal information online without consent; and the\ndistribution of inappropriate material, such as revenge porn, which is the sharing of explicit images\nor videos without consent to humiliate or blackmail the victim. Other mechanisms include\nimpersonation (creating fake profiles to harm someone’s reputation), trolling (posting inflammatory\nor offensive comments to provoke a reaction), and flaming (sending hostile and insulting messages).\nThese actions not only violate privacy but can also escalate into situations where victims feel unsafe in\nboth digital and physical spaces.\nFrom a young age, many children and adolescents may be exposed to digital environments where\nthe culture of “trash-talking” – playful or aggressive banter often aimed at opponents in online games –\nis prevalent. While initially intended as harmless competition, such behavior can quickly escalate if not\nPage 75 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 76, "content": "Ethics in Technology\nBy: Ed Weber\nmoderated, leading to more serious forms of cyberbullying or harassment. The anonymity and distance\nprovided by digital platforms can embolden individuals to cross ethical boundaries. As a result, what\nbegins as teasing can easily spiral into targeted campaigns of abuse. Over time, repeated exposure to or\nparticipation in such behavior can desensitize young people to the harm caused by their words and\nactions, making it crucial for both individuals and platform providers to foster respectful and\naccountable online communities.\nDeepfakes, Misinformation and Manipulation\nDeepfakes, misinformation, and manipulation represent some of the most complex ethical\nchallenges in today’s digital landscape. Deepfakes – realistic, AI-generated images, videos, or audio –\ncan blur the line between truth and fiction, with both creative and destructive potential. On the positive\nside, deepfake technology has been used to enhance public awareness campaigns, such as the “Malaria\nMust Die” initiative, where David Beckham appeared to speak in nine different languages, helping to\nreach a global audience. In media, Reuters has employed AI-generated presenters for personalized\nnews summaries, making content more accessible and engaging. Other beneficial uses include voice\ncloning for individuals with speech impairments, de-aging actors for films, and creating immersive\neducational or historical experiences.\nHowever, deepfakes have also led to significant legal and ethical controversies. Lawsuits have\narisen over non-consensual use of individuals’ likenesses – most notably in cases involving revenge\nporn, where deepfakes have been used to create explicit content without consent, leading to litigation\nand demands for stricter regulation. High-profile cases also include financial scams, where deepfake\nvoices or videos impersonated executives to authorize fraudulent transactions, resulting in millions in\nlosses and subsequent lawsuits. Celebrities and public figures have similarly pursued legal action\nagainst unauthorized deepfake impersonations that damaged their reputations or misled the public.\nMisinformation and manipulation, meanwhile, are often amplified by automated tools such as bots,\nwhich can flood social media platforms with false or misleading content. Bots are designed to mimic\nhuman behavior, allowing them to interact with users, post comments, and even “like” or share content\nen masse. This orchestrated activity can artificially boost the visibility of certain narratives, pushing\ncurated lists of users toward trending misinformation. The intent is often to manipulate public opinion,\ninfluence elections, or sow discord by making fringe ideas appear more widely accepted than they\nactually are. The combination of deepfakes and bot-driven misinformation creates a potent tool for\nmanipulation, challenging both individuals and platforms to discern fact from fiction in an increasingly\nsynthetic information environment.\nPage 76 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 77, "content": "Ethics in Technology\nBy: Ed Weber\nFree Speech vs. Hate Speech\nThe legal definitions of “free speech” and “hate speech” have evolved through a complex interplay\nof constitutional principles, court decisions, and ongoing debates about ethics and public order. In the\nUnited States, the First Amendment protects freedom of speech as a foundational right, barring the\ngovernment from restricting expression based on viewpoint, even when that expression is offensive or\nhateful. The intent behind this legal framework was to uphold robust public discourse and protect\nminority voices, recognizing that ethical considerations – such as the need to prevent harm and promote\ndignity – must be balanced against the imperative of open debate. Over time, courts have clarified that\nspeech can only be restricted if it directly incites imminent lawless action or constitutes a true threat.\nDespite these legal boundaries, ethical debates persist over what constitutes acceptable speech. Hate\nspeech, while not legally defined in the U.S., is generally understood as expression intended to vilify,\nhumiliate, or incite hatred against a group or class of people based on characteristics such as race,\nreligion, gender, or sexual identity. The challenge arises because the same words or phrases can be\ninterpreted differently depending on the observer’s perspective, cultural background, or personal\nexperience. Can you think of some phrases that have been used by one group as a rallying cry of ‘free\nspeech’ while others attempt to vilify anyone who uses the exact same phrase with accusations of ‘hate\nspeech’?\nWhen communities or governments attempt to define and regulate these terms, the result is often\nconfusion, ambiguity, or outright contradiction. The subjective nature of what constitutes hate speech\nor offensive speech means that any attempt to codify these concepts risks either overreach –\nsuppressing legitimate debate – or underreach – failing to protect vulnerable groups from harm. This\ntension is heightened in diverse societies, where different groups may have conflicting values and\ninterpretations of what is ethical or acceptable. As a result, legal definitions rarely align perfectly with\nthe full spectrum of ethical considerations, and the process of defining these terms remains a\ncontentious and evolving challenge for both lawmakers and society at large.\nInfluencer Culture\nInfluencer Culture refers to the social phenomenon in which individuals – both online and off –\nbuild communities around themselves and exert significant commercial and non-commercial influence\nover their followers. This culture is not new: throughout history, prominent figures such as royalty,\nphilosophers, political leaders, and celebrities have shaped public opinion, set trends, and influenced\nconsumer behavior. In the digital age, however, the barriers to becoming an influencer have dropped\ndramatically, and the speed and reach of influence have expanded exponentially.\nBefore the rise of social media, influencers included figures like Eleanor Roosevelt, who used her\nnewspaper column and radio appearances to shape public opinion and advocate for social causes. In the\nPage 77 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 78, "content": "Ethics in Technology\nBy: Ed Weber\n20th century, celebrities such as The Beatles, Marilyn Monroe, and Audrey Hepburn became\ntrendsetters whose choices in fashion, music, and lifestyle were widely emulated. Today, influencers\nare typically individuals who have built large followings on platforms like Instagram, YouTube, and\nTikTok. These influencers often arrive without credentials or any specific expertise. Rather, they excel\nat social media engagement and, perhaps, have a likable or convincing personality.\nAs influencer culture has grown, so too have debates about the responsibilities of influencers\nthemselves. Some have faced backlash and legal repercussions for promoting harmful products,\nspreading misinformation, or engaging in unethical behavior. In response, there have been calls – and\nsometimes legal actions – to hold influencers accountable for the consequences of their actions,\nparticularly when those actions mislead or harm their audiences. This includes demands for greater\ntransparency in sponsored content, as well as accountability for endorsing products or ideas that may\nhave negative real-world effects.\nThe rise of influencers goes beyond mere entertainment. For many followers, influencers fill voids\nleft by traditional institutions, offering advice, companionship, or a sense of belonging that may be\nmissing from their everyday lives. Influencers often create parasocial relationships – one-sided bonds\nwhere followers feel a personal connection to the influencer – which can be a source of comfort,\ninspiration, or even identity formation. This dynamic can make influencers powerful agents of change\nbut also places significant responsibility on their shoulders.\nDespite the potential for lasting impact, many influencers experience the ephemeral nature of fame.\nThe phrase “15 minutes of fame” is especially apt, as viral success can be fleeting, and the public’s\nattention is fickle. Some influencers exhaust their popularity through overexposure, scandal, or\ncontroversial behavior, leading to a rapid loss of followers and influence. Others “crash and burn” more\ndramatically, facing public backlash or legal issues that end their careers as quickly as they began. This\ncycle highlights both the opportunities and the risks inherent in influencer culture, underscoring the\nneed for ethical awareness and resilience in the digital age.\nMedia Literacy\nMedia Literacy is the ability to access, analyze, evaluate, create, and act using all forms of\ncommunication. It goes beyond simply understanding information; it involves critical thinking about\nthe messages we encounter, their sources, and their impact. Media literacy empowers individuals to\nnavigate the complex media landscape, discerning credible information from misinformation or\nmanipulation.\nA cornerstone of media literacy is the use of multiple sources to verify facts. By comparing\ninformation from various reputable outlets, consumers can identify patterns, inconsistencies, or\nbiases. Evaluating the credibility of sources is also essential. This includes considering the reputation\nPage 78 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 79, "content": "Ethics in Technology\nBy: Ed Weber\nof the publisher, the author’s expertise, and the presence of citations or references to original research.\nTraditional methods also involve checking for objectivity, transparency about funding or affiliations,\nand whether the information is current and relevant.\nDetermining whether information is factual or opinion-based requires careful analysis. Facts are\nstatements that can be objectively verified with evidence, while opinions reflect personal beliefs or\ninterpretations. Facts are often presented with quantifiable data without qualification with an intent to\ninform. Whereas opinions are often subjectively presented with adjectives and adverbs intended to\npersuade, or in some other way elicit an emotional response. A simple way to consider whether some\ncontent is more fact-based or opinion-based is to simply count the parts of speech. If the piece has\nnotably more numerals, nouns, and verbs (objective) than it has adjectives and adverbs (subjective)\nthen the piece may be more fact-based than opinion-based. But if the piece has more subjective\nlanguage than objective language, you already know that the piece is more opinion than fact.\nContent creators bear the responsibility of producing accurate, transparent, and ethical media if they\nare, in fact, acting in an ethical framework. This means clearly distinguishing between facts and\nopinions, disclosing conflicts of interest, and correcting errors promptly. Creators should also be\nmindful of the potential impact of their messages on audiences, striving to avoid harm and promote\ninformed understanding.\nContent consumers, on the other hand, must approach media with a critical mindset. This includes\nquestioning the motives behind messages, recognizing bias, and seeking out diverse perspectives.\nConsumers should also engage in reflection about how media influences their thoughts and behaviors,\nand take action – such as sharing reliable information or educating others – to contribute positively to\npublic discourse. By embracing these practices, both creators and consumers can foster a media\nenvironment that supports truth, accountability, and informed civic participation.\nTextbook Definitions – Digital Communication, Social Media, Misinformation and Democracy\n• Social Media Ethics – The moral principles and guidelines that govern responsible, respectful,\nand ethical behavior on social media platforms.\n• Maximize engagement – Strategies designed to increase user interaction, such as likes, shares,\nand comments, on digital content.\n• Accuracy – The degree to which information is free from errors, distortions, or\nmisrepresentations.\n• Ethical participation – Engaging online in a manner that is respectful, honest, and mindful of\nthe impact on others.\nPage 79 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 80, "content": "Ethics in Technology\nBy: Ed Weber\n• Misinformation – False or inaccurate information that is spread, regardless of intent to deceive.\n• Harassment – Unwanted behavior intended to annoy, threaten, or intimidate another person,\nespecially repeatedly.\n• Accountability – The obligation to take responsibility for one’s actions and accept the\nconsequences.\n• Censorship – The suppression or prohibition of speech, writing, or other forms of expression\nconsidered objectionable or harmful.\n• Moderation – The process of monitoring and managing online content to ensure it complies\nwith rules or standards.\n• Suppression – The deliberate act of preventing information or expression from being shared or\nseen.\n• Satire – The use of humor, irony, or exaggeration to criticize or mock people, ideas, or\ninstitutions.\n• Misleading content – Information that is designed or likely to deceive or misinform the\naudience.\n• Fact-checking – The process of verifying the accuracy of claims made in content or statements.\n• Bias in content moderation – Prejudiced or unfair treatment in the review and management of\nonline content.\n• Cyberbullying – The use of digital technology to harass, threaten, embarrass, or target another\nperson.\n• Cyberstalking – The repeated use of digital technology to monitor, follow, or harass someone.\n• Doxxing – The malicious act of publicly revealing private or identifying information about an\nindividual without their consent.\n• Inappropriate material – Content that is offensive, explicit, or otherwise unsuitable for its\nintended audience.\n• Revenge porn – The distribution of explicit images or videos without consent, often to\nhumiliate or blackmail.\n• Impersonation – Pretending to be someone else online, often for malicious or deceptive\npurposes.\n• Trolling – Posting inflammatory, offensive, or disruptive comments or messages to provoke a\nreaction.\nPage 80 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 81, "content": "Ethics in Technology\nBy: Ed Weber\n• Flaming – Sending hostile and insulting messages, often in online discussions or forums.\n• Deepfakes – Realistic, AI-generated images, videos, or audio that can make it appear someone\nsaid or did something they did not.\n• Impersonated executives – Individuals falsely represented as company leaders, often in scams\nor fraudulent schemes.\n• Bots – Automated software programs designed to perform tasks online, such as posting\nmessages or mimicking human behavior.\n• Trending – The state of being widely discussed or shared on social media at a given time.\n• Free speech – The right to express opinions and ideas without fear of government retaliation or\ncensorship.\n• Hate speech – Expression intended to vilify, humiliate, or incite hatred against a group or class\nof people.\n• Open debate – The free exchange of ideas and perspectives in public discourse.\n• Overreach – Excessive or unjustified restriction of rights, such as speech, beyond what is\nnecessary or appropriate.\n• Underreach – Failing to provide sufficient protection or regulation, resulting in harm or\ninjustice.\n• Influencer Culture – The social phenomenon in which individuals build communities and\nexert significant influence over their followers’ opinions and behaviors.\n• Credible information – Information that is trustworthy, reliable, and supported by evidence.\n• Reputable outlets – Media sources known for accuracy, fairness, and reliability in reporting.\n• Credibility of sources – The degree to which a source is considered trustworthy and\nauthoritative.\n• Objectivity – The practice of presenting information in a neutral and unbiased manner.\n• Transparency – Openness and clarity about intentions, actions, and sources of information.\n• Fact-based – Information that is grounded in verifiable evidence and data.\n• Opinion-based – Information that reflects personal beliefs, interpretations, or judgments.\n• Questioning motives – The act of critically examining the reasons behind someone’s actions or\nstatements.\nPage 81 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "section_path": ["Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy"], "page": 82, "content": "Ethics in Technology\nBy: Ed Weber\n• Recognizing bias – Identifying personal or systemic prejudices that may affect the presentation\nor interpretation of information.\nPage 82 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "section_path": ["Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI"], "page": 83, "content": "Ethics in Technology\nBy: Ed Weber\n10. Intellectual Property, Digital Art, and Emerging Economies\nIntellectual Property; Patents and Copyright; Trade Secrets and National Security;\nBlockchain and Cryptocurrency; Digital Art and Generative-AI\nThe digital era has fundamentally transformed how we create, share, and profit from intellectual\nworks. At the heart of this transformation are the concepts of intellectual property – copyrights,\npatents, and trade secrets – which were originally established to balance the interests of creators,\ninventors, and the broader public. These legal protections were ethically motivated: they aimed to\nreward creativity and innovation, ensuring that inventors and artists could benefit from their labor\nwhile ultimately enriching society as a whole. By granting temporary monopolies, societies hoped to\nincentivize the production of new knowledge, art, and technology, while eventually returning these\nworks to the public domain for communal benefit.\nHowever, as technology has evolved, so too have the ethical challenges surrounding ownership and\ncontrol of ideas. Today, questions arise about the fairness and validity of these systems – especially\nwhen the legal owner of a creative work is not the original creator. For example, when a music label\nowns the rights to a song rather than the artist who composed and performed it, or when companies\nhold patents and trade secrets developed by employees, it prompts us to reconsider the original ethical\njustification for these protections. Are these arrangements still serving the public good, or have they\nshifted too far in favor of corporate interests? Do current laws adequately recognize the contributions\nof individual creators, or do they perpetuate power imbalances in the digital economy?\nAs you explore this chapter, consider:\n• Who truly benefits from intellectual property laws in a digital, globalized world?\n• Should the rights of creators be prioritized over those of corporations, or vice versa?\n• How do emerging technologies like blockchain and generative AI challenge or reinforce\ntraditional notions of ownership and authorship?\n• What ethical responsibilities do companies have to the individuals whose innovations they\nprofit from?\nThese questions invite you to critically examine not only the legal structures that govern intellectual\nproperty, but also the underlying ethical principles that justify – or challenge – them in today’s rapidly\nchanging technological landscape.\nIntellectual Property\nPage 83 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "section_path": ["Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI"], "page": 84, "content": "Ethics in Technology\nBy: Ed Weber\nIntellectual property (IP) refers to creations of the mind – such as inventions, literary and\nartistic works, designs, symbols, names, and images used in commerce – that are legally protected\nfrom unauthorized use or reproduction. Unlike physical property such as real estate, vehicles, or\nconsumable goods, intellectual property is intangible: it can be shared or copied without depriving the\noriginal owner of its use. For example, while only one person can drive a specific car or eat a particular\nloaf of bread at a time, an unlimited number of people could read the same digital book or listen to a\nsong without exhausting the original asset.\nThe concept of ownership in intellectual property mirrors some aspects of tangible property – such\nas the ability to sell, license, or bequeath rights to others – but also differs in important ways. IP rights\ncan be transferred, inherited, or assigned, much like physical property, allowing creators or owners\nto grant permission for use, sell their rights, or pass them on to heirs. However, the time-bound and\nterritorial nature of most IP rights means that, unlike land or a house, these rights eventually expire and\nthe protected works enter the public domain, becoming freely available to all.\nThere are also clear legal and ethical boundaries regarding what can be owned. For instance, ideas\nthemselves, natural phenomena, and mathematical formulas are generally not subject to ownership,\nthough their specific expressions or applications might be. This creates gray areas – such as disputes\nover genetic information, traditional knowledge, or the line between inspiration and infringement –\nwhere the boundaries of ownership are continually negotiated. The evolving landscape of IP law\nreflects ongoing debates about how best to balance private rights with public benefit in an era where\nintangible assets are increasingly valuable.\nPatents and Copyright\nPatents and copyright are two foundational forms of intellectual property protection, each with a\nlong and evolving history rooted in the desire to encourage creativity and innovation. The earliest\nknown patent-like rights date back to Ancient Greece. In medieval Europe, the concept matured: the\nRepublic of Venice’s 1474 Patent Statute is considered the first codified patent system, granting\ninventors exclusive rights to new devices for a limited time to encourage disclosure and public benefit.\nSimilarly, early copyright law emerged in England with the 1710 Statute of Anne, which shifted the\nfocus from publisher monopolies to author rights, aiming to promote learning and the progress of\nknowledge by granting authors exclusive rights for a limited period.\nIn the United States, these traditions were enshrined in the Constitution, empowering Congress to\ngrant authors and inventors exclusive rights “for limited times” to promote the progress of science and\nuseful arts. The first federal copyright law, enacted in 1790, protected books, maps, and charts for 14\nyears, renewable for another 14 years if the author was still alive. Today, U.S. patents generally last 20\nyears from the filing date and cannot be renewed, though certain extensions are possible in specific\nPage 84 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "section_path": ["Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI"], "page": 85, "content": "Ethics in Technology\nBy: Ed Weber\ncases (such as pharmaceuticals). Copyright protection for works created after January 1, 1978, typically\nendures for the life of the author plus 70 years. For works made for hire, anonymous, or pseudonymous\nworks, the term is 95 years from publication or 120 years from creation, whichever is shorter.\nCopyrights cannot be “renewed” in the traditional sense, but older works under previous laws\nsometimes allowed for renewal terms.\nThe rapid pace of technological change raises important questions about whether these traditional\ntime frames remain appropriate. In fields like software and digital technology, products and inventions\ncan become obsolete within a few years, long before the expiration of a 20-year patent or multi-decade\ncopyright. Shortening the protection period for rapidly evolving technologies could accelerate their\nentry into the public domain, fostering greater innovation and competition. For example, a system\ncould be envisioned where software patents expire after 5–10 years, or where digital works have a\nreduced copyright term. This would allow society to benefit from shared knowledge and creative\nworks more quickly, while still providing inventors and creators with a period of exclusive benefit.\nSuch reforms would need to carefully balance the incentives for innovation with the broader public\ninterest in access and progress.\nTrade Secrets and National Security\nTrade secrets are a powerful tool used by corporations to protect valuable information that gives\nthem a competitive edge, such as formulas, algorithms, or business processes. Unlike patents or\ncopyrights, which require public disclosure in exchange for legal protection, trade secrets are\nmaintained through confidentiality and internal security measures. Famous examples include the Coca-\nCola recipe and Google’s search algorithm, both of which remain undisclosed to the public and are\nclosely guarded to maintain their economic value.\nHowever, this secrecy can sometimes conflict with the public interest, especially when withheld\ninformation – such as pharmaceutical data or environmental impact data – could benefit society at\nlarge. Corporations may claim trade secret status not only to protect legitimate business interests but\nalso to avoid scrutiny or regulation, raising ethical questions about where to draw the line between\nproprietary knowledge and the public’s right to know.\nSimilarly, governments often invoke “national security” as a reason to withhold information from\nthe public, sometimes even when disclosure might serve the greater good. While there are legitimate\nreasons to keep certain details confidential – such as protecting citizens or critical infrastructure – the\nconcept can be misused to obscure wrongdoing, prevent accountability, or stifle public debate. Both\ntrade secrets and national security claims thus present a tension between the need for confidentiality\nand the ethical imperative for transparency. Striking the right balance is challenging: too much secrecy\ncan erode trust and hinder oversight, while too much transparency can expose sensitive information to\nPage 85 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "section_path": ["Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI"], "page": 86, "content": "Ethics in Technology\nBy: Ed Weber\nmisuse or harm.\nThese dilemmas prompt a range of critical ethical questions:\n• Who gets to decide what qualifies as a trade secret or a matter of national security?\n• What standards or rubrics are used to make these determinations, and are they consistent?\n• How can these decisions be independently audited or reviewed to prevent abuse?\n• Should there be time limits or periodic reviews for information classified as secret?\n• Would greater transparency reduce the need for whistleblowers, or are some secrets always\ninevitable?\n• How should the public interest be weighed against corporate or governmental interests in\nsecrecy?\n• What safeguards exist to ensure that claims of secrecy are not used to cover up misconduct or\navoid accountability?\n• Are there circumstances where the ethical imperative to disclose outweighs legal protections for\nsecrecy?\n• How can stakeholders – including employees, citizens, and regulators – challenge or appeal\nsecrecy claims?\n• What role should external watchdogs or independent panels play in overseeing decisions about\nsecrecy?\nThese questions highlight the ongoing need for robust debate and oversight to ensure that trade\nsecrets and national security claims serve the public interest rather than merely protecting private or\ninstitutional power.\nBlockchain and Cryptocurrency\nBlockchain technology has been promoted as a transformative solution for protecting digital assets\nby providing a decentralized, transparent, and tamper-resistant ledger. Unlike traditional databases,\nblockchain distributes records across a network of computers, making it extremely difficult to alter or\nerase past transactions. This architecture is seen as a major step forward in safeguarding traditional\ndigital works – such as audio, video, and images – by ensuring clear, immutable records of ownership\nand provenance.\nPage 86 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "section_path": ["Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI"], "page": 87, "content": "Ethics in Technology\nBy: Ed Weber\nLikewise, the rise of Non-Fungible Tokens (NFTs) exemplifies this: NFTs are unique digital\ntokens on a blockchain that verify ownership and authenticity of digital items, ranging from digital art\nand music to virtual real estate and collectibles. For instance, an artist can mint an NFT representing a\ndigital painting, which can then be bought, sold, or traded with its ownership history securely tracked\non the blockchain. Similarly, cryptocurrencies like Bitcoin and Ethereum use blockchain to secure\nfinancial transactions, while also enabling new forms of digital property and decentralized finance.\nDespite these advantages, blockchain-based asset protection is not without significant risks. One of\nthe most critical vulnerabilities is the risk of permanent loss if a user loses access to their private wallet\nkeys or passwords. Unlike traditional banking systems, there is no central authority to recover lost\ncredentials, and it is estimated that up to 25% of all Bitcoin in circulation may be permanently\ninaccessible due to lost keys. This highlights the importance of robust key management and secure\ncustody solutions, especially as digital assets become more valuable and widely adopted.\nAdditionally, while blockchain is currently considered highly secure, the advent of quantum\ncomputing poses a potential existential threat. Quantum computers, once they reach sufficient power –\na milestone sometimes referred to as \"Q-Day\" – could theoretically break all of the cryptographic\nalgorithms that underpin blockchain security, making it possible to forge transactions or steal assets.\nWhile estimates for Q-Day vary, some experts believe it could occur within the next decade, prompting\nurgent research into quantum-resistant cryptography and other safeguards to ensure the long-term\nviability of blockchain-based protections. As a result, while blockchain and related technologies offer\npowerful tools for digital asset protection and new models of ownership, they also introduce new\ncategories of risk and uncertainty that must be carefully managed as the technology and its threats\ncontinue to evolve.\nDigital Art and Generative-AI\nThe digital revolution has dramatically expanded\nthe possibilities for both creating and copying art, while\nalso blurring the boundaries between original works,\nforgeries, and homages. In traditional terms, a forgery\nis an unauthorized imitation of an existing work,\nintended to deceive by passing off as the original. In the\ndigital realm, the distinction between a forgery and a\nsimple digital copy becomes less clear, as digital files\ncan be reproduced perfectly and infinitely. Meanwhile,\nan homage refers to a new work that deliberately\nFigure 14: Picture of Dog using\nreferences or emulates the style of a particular artist,\nChatGPT generated by Pixlr\noften as a form of respect or creative exploration rather\nPage 87 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "section_path": ["Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI"], "page": 88, "content": "Ethics in Technology\nBy: Ed Weber\nthan deception. The challenge lies in distinguishing between these categories, especially as generative\nAI tools can now produce images, music, or text that closely mimic the style of well-known creators.\nTechnological advances in forgery detection – using AI, blockchain, and watermarking – have made\nit possible to analyze digital artworks for inconsistencies, provenance, and originality. Yet, even with\nsophisticated tools, it can be difficult to determine whether a digital piece is a genuine original, a direct\ncopy, a forgery, or a legitimate homage, particularly when AI-generated works are involved. This\nambiguity complicates questions of ownership: if an AI model is trained on thousands of works by a\nspecific artist and then generates a new piece \"in their style,\" who owns the result? Is it the person who\nprovided the prompt, the creators of the AI tool, or the original artists whose work was used to train the\nmodel?\nThese complexities raise a host of ethical questions, especially when considering the perspectives of\nart creators, tool/platform providers, and those whose works are used as models:\n• Is it ethical for AI tools to be trained on copyrighted works without the original creator’s\nconsent?\n• Should the person who provides a prompt to a generative-AI model be considered the creator or\nowner of the resulting artwork?\n• What rights, if any, should the original artists have when their styles or works are used to train\ngenerative models?\n• If a platform profits from AI-generated art, should it compensate the creators whose works were\nused as training data?\n• How can we distinguish between homage and unauthorized imitation in the age of generative\nAI?\n• Should digital forgeries be treated differently from physical forgeries in terms of legal and\nethical consequences?\n• Who is responsible if generative-AI art is used to deceive or defraud others?\n• Can or should ownership of AI-generated art be transferred, inherited, or sold like traditional\nartworks?\n• What mechanisms should exist for artists to opt in or out of having their works used as AI\ntraining data?\n• How do we ensure that innovation and creative freedom are not stifled by overly restrictive\nownership rules in digital and AI art?\nPage 88 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "section_path": ["Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI"], "page": 89, "content": "Ethics in Technology\nBy: Ed Weber\nThese questions highlight the evolving landscape of digital art and generative AI, where traditional\nnotions of authorship, authenticity, and ownership are being fundamentally reexamined.\nTextbook Definitions – Intellectual Property, Digital Art, and Emerging Economies\n• intellectual property – Creations of the mind, such as inventions, literary and artistic works,\ndesigns, symbols, names, and images, that are protected by law and can be owned, transferred,\nor licensed.\n• copyrights – Legal rights granted to creators for their original literary, artistic, or musical\nworks, allowing them to control reproduction, distribution, and adaptation of those works.\n• patents – Exclusive rights granted for new inventions, processes, or designs, giving inventors\ncontrol over the use and commercialization of their inventions for a limited period.\n• trade secrets – Confidential business information, such as formulas, practices, or processes,\nthat provide a competitive advantage and are protected by secrecy rather than public\nregistration.\n• temporary monopolies – Time-limited exclusive rights granted to creators or inventors to\ncontrol the use of their intellectual property, intended to incentivize innovation before works\nenter the public domain.\n• fairness – The ethical principle of treating all parties justly and equitably, especially in the\ndistribution and enforcement of rights and benefits.\n• power imbalances – Situations where one party holds significantly more influence or control\nover resources, decisions, or rights than others, often leading to ethical concerns.\n• Intellectual property (IP) – A category of property that includes intangible creations of human\nintellect, such as inventions, works of art, and symbols, protected by law.\n• inventions – Novel devices, methods, or processes resulting from creativity and ingenuity,\noften eligible for patent protection.\n• literary works – Original written creations, such as books, poems, and articles, protected by\ncopyright law.\n• artistic works – Creative visual or performance pieces, including paintings, sculptures, music,\nand films, covered by copyright protection.\n• transferred – The act of legally moving ownership or rights from one party to another, such as\nthrough sale or assignment.\nPage 89 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "section_path": ["Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI"], "page": 90, "content": "Ethics in Technology\nBy: Ed Weber\n• inherited – The process by which ownership or rights are passed down from one person to\nanother, typically upon the original owner’s death.\n• assigned – The legal transfer of rights or interests in intellectual property from one party to\nanother, often through a formal agreement.\n• grant permission – To authorize another party to use, reproduce, or otherwise exploit a work or\ninvention under specified conditions.\n• expire – To come to the end of a legally defined period of protection, after which exclusive\nrights are no longer enforceable.\n• public domain – The status of a work or invention whose intellectual property rights have\nexpired or never existed, making it freely available for public use.\n• public interest – The welfare or well-being of the general public, often considered in legal and\nethical decisions about access to information or resources.\n• Blockchain – A decentralized, distributed digital ledger technology that records transactions\nsecurely and transparently across multiple computers.\n• Non-Fungible Tokens (NFTs) – Unique digital tokens recorded on a blockchain that certify\nownership and authenticity of a specific digital asset, such as art, music, or collectibles.\n• quantum computing – A field of computing that uses quantum-mechanical phenomena, such\nas superposition and entanglement, to perform calculations far beyond the capabilities of\nclassical computers.\n• Q-Day – The anticipated future date when quantum computers will be powerful enough to\nbreak current cryptographic systems, potentially compromising blockchain security.\n• cryptography – The practice and study of techniques for securing communication and\ninformation through encoding, ensuring confidentiality, integrity, and authenticity.\n• forgery – The act of creating a false or unauthorized imitation of a work, typically with the\nintent to deceive others about its authenticity.\n• digital copy – An exact reproduction of a digital file or work, which can be duplicated without\nloss of quality or originality.\n• homage – A new work created in deliberate imitation or tribute to the style or influence of\nanother artist, usually as a sign of respect rather than deception.\n• ownership – The legal right to possess, use, control, and transfer a property or asset, including\nintellectual property.\nPage 90 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 91, "content": "Ethics in Technology\nBy: Ed Weber\n11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic\nEthics\nLevels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot\nEthics; Algorithmic Bias; Automation; Predictive Policing\nThe story of automation is one of both\ndisruption and transformation, shaping the very\nfabric of society from the earliest days of\nagriculture to the dawn of the Information\nAge. In the agricultural era, simple tools and\nanimal-driven machines revolutionized food\nproduction, freeing human labor for other\npursuits. The Industrial Revolution brought\nmechanized factories and assembly lines,\ndramatically increasing productivity but also\ndisplacing traditional crafts and altering social\nstructures. The advent of computers in the 20th\ncentury marked another, automating complex\ncalculations and data management, and laying\nthe groundwork for the digital revolution.\nToday, as we enter the era of artificial\nintelligence (AI), automation, and robotics, the Figure 15: Robot typing at computer.\npace of change is accelerating at an\nunprecedented rate, touching every aspect of our economic, social, and personal lives.\nTechnologies such as advanced AI, autonomous vehicles, chatbots, and robotics are no longer\nconfined to research labs or science fiction – they are rapidly becoming integral to how we work,\ncommunicate, and make decisions. AI systems now perform tasks ranging from diagnosing medical\nconditions to driving cars and moderating online content. Automation is transforming industries, from\nmanufacturing and logistics to finance and customer service, while algorithmic decision-making\nincreasingly shapes everything from hiring practices to law enforcement through predictive policing.\nThis growing ubiquity brings both promise and peril: while these technologies offer the potential for\ngreater efficiency, safety, and convenience, they also raise profound ethical questions about bias,\naccountability, and the distribution of power and opportunity.\nAs these innovations continue to evolve, we must grapple with the sustainability of our current\neconomic and social systems. Will the continued rise of AI, automation, and robotics lead to\nwidespread job displacement, deepen existing inequalities, or erode human agency? Or can these\ntechnologies be harnessed to create a more just, equitable, and sustainable society? The answers to\nPage 91 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 92, "content": "Ethics in Technology\nBy: Ed Weber\nthese questions will depend not only on technical advancements, but also on the ethical frameworks\nand policies we establish to guide their development and deployment.\nLevels of AI\nArtificial Intelligence (AI) exists along a spectrum of complexity and capability, often described in\nterms of “levels.” Early AI systems, such as expert systems, were designed to mimic the decision-\nmaking abilities of human specialists within narrow domains – think medical diagnosis or\ntroubleshooting technical issues. These systems rely on predefined rules and logic, and while they can\noutperform humans in specific, well-defined tasks, they lack the flexibility and adaptability of broader\nintelligence. At the other end of the spectrum is Artificial General Intelligence (AGI), a theoretical\nform of AI that can understand, learn, and apply knowledge across a wide range of tasks at a human-\nlike level. Beyond AGI lies Artificial Superintelligence (ASI), which would surpass human\nintelligence in virtually every field, including creativity, problem-solving, and social intelligence.\nMost of what is marketed as “AI” today – such as large language models (LLMs) and natural\nlanguage processing (NLP) systems – falls far short of AGI or ASI. These models, including popular\nchatbots and content generators, are trained on vast, curated datasets but do not actively or\ncontinuously learn from new data once deployed. Instead, they are periodically “tuned” by their\ncreators, often for specific domains or applications, which can introduce or reinforce biases and\ninaccuracies present in the training data. The curated nature of these datasets means that AI outputs can\nreflect the perspectives, limitations, and prejudices of the data and those who select it, leading to\nalgorithmic bias and fairness issues. Despite rapid advances, none of today’s mainstream AI systems\npossess the autonomy, adaptability, or self-awareness associated with AGI.\nThe path to AGI – and, by extension, ASI – remains uncertain, but many experts believe that once\nAGI is achieved, an immediate, unavoidable and unstoppable transition to ASI will follow. Given the\npotential for self-improvement and recursive learning (without curated input, interruption, and\nwithout specified domain limitations) this prospect raises profound questions about control and safety.\nThe assumption that AGI or ASI could be reliably “controlled” is widely regarded as hubristic, given\nthe unpredictable nature and potential power of such systems.\nCompounding these concerns is the lack of universal ethical definitions or standards in the data\nused to train AI, making it impossible to predict what kind of “ethical center” an advanced AI might\ndevelop. As a result, society faces urgent questions about how to guide the development of increasingly\ncapable AI systems in ways that align with shared values and long-term human interests.\nPage 92 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 93, "content": "Ethics in Technology\nBy: Ed Weber\nAI Moral Agency\nCurrent AI systems – including expert systems, large language models, and other advanced tools –\nare best understood as sophisticated instruments rather than independent moral agents. These systems\ncurrently lack consciousness, intentionality, and the capacity for ethical judgment, so moral agency\nand culpability remain with the humans who design, deploy, and use them. Developers are responsible\nfor building systems that are safe and fair, operators must ensure proper oversight, and users must\nunderstand the tool’s limitations and risks. Attributing moral agency to these tools can lead to\nconfusion, misplaced accountability, and the dangerous illusion that ethical responsibility can be\ndelegated to technology.\nThe conversation shifts dramatically when considering the hypothetical emergence of Artificial\nGeneral Intelligence (AGI) or Artificial Superintelligence (ASI). If an AI system were to achieve\nhuman-level understanding, autonomy, and the ability to make independent decisions (which a number\nof AI researchers and companies are actively pursuing), the question of moral agency becomes more\ncomplex and contentious. Would such a system deserve to be treated as a moral agent, or even as a\nlegal entity, responsible for its actions?\nThis debate is reminiscent of the gradual transfer of moral agency from parent to child: children\ninitially lack full moral responsibility, which is, instead, held by their parents or guardians. But as\nchildren develop autonomy and understanding, they gradually assume agency for their own actions.\nSimilarly, if AGI or ASI were to demonstrate genuine autonomy and ethical reasoning, there could\nbe a case for shifting some degree of responsibility from the creators or users to the AI itself. However,\nthis transition would be fraught with uncertainty, as we currently lack clear ethical rubrics, legal\nframeworks, or even a consensus on what would constitute an “ethical center” for such entities.\nAutonomous Vehicles\nAutonomous vehicles (AVs) are rapidly transforming transportation, with trucking and freight\nleading the way in the adoption of high-level autonomy. The Society of Automotive Engineers (SAE)\ndefines six levels of vehicle autonomy, from Level 0 (no automation) to Level 5 (full automation, with\nno human intervention required at any point). Most consumer vehicles today feature Level 2 or Level 3\nautonomy, offering driver assistance and partial automation. However, the most groundbreaking\ndevelopments are occurring at Levels 4 and 5, where vehicles can operate independently in specific\nconditions or, eventually, in all environments.\nIn the United States, fully autonomous trucking is no longer a distant vision. Aurora Innovation\nlaunched driverless trucks on the I-45 corridor between Dallas and Houston in 2025. Other companies\nPage 93 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 94, "content": "Ethics in Technology\nBy: Ed Weber\nsuch as Kodiak Robotics, Gatik, and Waabi are also advancing hub-to-hub autonomous trucking,\nparticularly in states like Texas, Arizona, and Florida, where regulations are more permissive.\nInternationally, China’s Inceptio Technology and Germany’s on-road trials are pushing the envelope\nin large-scale autonomous truck deployment. These trucks promise to address driver shortages, increase\noperational efficiency, and reduce costs, with the potential to revolutionize logistics and supply chains\nglobally.\nOne of the most compelling arguments for autonomous vehicles is their potential to dramatically\nreduce vehicular crashes. Human error is responsible for over 90% of traffic accidents; by removing\nfatigue, distraction, and impaired driving from the equation, AVs could save thousands of lives\nannually.\nHowever, the transition is not without challenges. Legal and ethical questions loom large: when an\nautonomous vehicle is involved in a crash, who is responsible – the manufacturer, the software\ndeveloper, the fleet operator, or the owner? Current legal frameworks are struggling to keep pace, and\nthere is ongoing debate about how to assign liability and ensure accountability as vehicles become\nmore autonomous. These questions will only grow in importance as AV technology becomes more\nubiquitous, raising fundamental issues about trust, transparency, and the future of transportation.\nChatbots\nChatbots have evolved dramatically from their origins as simple, rule-based programs designed for\nentertainment or to answer basic questions. Early chatbots, like ELIZA in the 1960s, relied on scripted\nresponses and could only handle straightforward, predictable interactions. As technology advanced,\nchatbots became popular in business settings for providing 24/7 customer service, automating\nfrequently asked questions, and reducing the workload for human agents. The introduction of natural\nlanguage processing (NLP) and machine learning (ML) allowed chatbots to better understand context\nand intent, leading to more sophisticated conversational agents that could manage more complex\nqueries. Today, chatbots are widely used not only for customer service but also for telemarketing, sales,\nand customer engagement, often serving as the first point of contact between companies and their\ncustomers.\nDespite these advancements, significant limitations persist. Most chatbots, even those powered by\nlarge language models, are trained on curated datasets and operate within restricted domains; they\nstruggle to adapt when conversations deviate from expected patterns, often resulting in user frustration\nwhen the system cannot process nuanced or evolving requests. Additionally, modern chatbots\nincreasingly use synthesized voice recordings, complete with intonations and inflections, to simulate\nemotion and create a more “human-like” interaction. This can enhance user experience but also blurs\nthe line between machine and human, raising important ethical questions:\nPage 94 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 95, "content": "Ethics in Technology\nBy: Ed Weber\n• Is it ethical to replace human customer service jobs with chatbots, especially when the\ntechnology is still imperfect?\n• Should companies be required to disclose when a customer is interacting with a chatbot rather\nthan a real person?\n• What are the risks of chatbots providing false, misleading, or “hallucinated” information to\nusers?\n• How can companies ensure that chatbots do not exploit users by establishing artificial\nrelationships or manipulating emotions?\n• Who is responsible if a chatbot causes harm, either through misinformation or inappropriate\ninteractions?\n• Should there be regulations governing the use of voice synthesis to prevent deception or\nemotional manipulation?\n• How can biases and inaccuracies in chatbot responses be effectively identified and corrected?\n• What safeguards should be in place to protect vulnerable populations from exploitation by\nautomated systems?\n• How can transparency and accountability be maintained as chatbots become more autonomous\nand integrated into everyday life?\nThese questions highlight the ethical complexities that accompany the rapid integration of chatbots\ninto business and society, underscoring the need for thoughtful oversight and responsible development\nas the technology continues to advance.\nRobotics and Robot Ethics\nRobotics is the interdisciplinary field of engineering and computer science focused on the design,\nconstruction, operation, and use of programmable machines – robots – that can replicate, substitute, or\nassist human actions in various tasks. Some of the earliest robots were ancient automata, such as\nmechanical birds in ancient Greece and water clocks in China, but the modern concept of the robot\nemerged in the 20th century with inventions like George Devol’s Unimate, the first industrial robotic\narm, which began operating at a General Motors facility in 1959. The field of robotics was further\ndefined by Isaac Asimov’s introduction of the “Three Laws of Robotics,” which have influenced\nethical thinking about robots ever since.\nAsimov’s three laws of robotics were defined as follows:\nIsaac Asimov's Three Laws of Robotics are:\nPage 95 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 96, "content": "Ethics in Technology\nBy: Ed Weber\n1. A robot may not injure a human being or, through inaction, allow a human being to come to\nharm.\n2. A robot must obey the orders given it by human beings except where such orders would conflict\nwith the First Law.\n3. A robot must protect its own existence as long as such protection does not conflict with the First\nor Second Law.\nAlthough begun as a work of science fiction, these three laws have become a foundational starting-\npoint for very many philosophical and ethical positions regarding how robotics should be ethically\ndeveloped and utilized.\nMost industrial robots today are fully programmed using programmable logic controllers (PLCs)\nor computer numerical control (CNC) systems, enabling them to perform repetitive tasks such as\nwelding, assembly, and painting within tightly controlled environments. These robots are typically\nlimited to their pre-programmed domains and cannot adapt to new tasks without human intervention or\nreprogramming.\nHowever, advances in robotics have produced machines capable of operating in more diverse and\nless structured environments, such as autonomous mobile robots, manufacturing and warehouse\nautomation systems, and even robots that can assist in surgery or explore hazardous locations. These\nmore advanced robots use sensors, AI, and machine learning to make decisions and adapt to changing\nconditions, reducing the need for direct human oversight and expanding the potential applications of\nrobotics.\nSome ethical questions raised by the increasing use of robotics include:\n• What are the societal consequences of job displacement caused by robotics without\ncorresponding changes in the existing economic model?\n• Should robots be used for police or military operations, and what are the risks of delegating\nlethal force to machines?\n• Is it ethical to use robots to administer medicines or perform medical procedures, and who is\nresponsible if something goes wrong?\n• Should robots be permitted to manufacture or design other robots, potentially accelerating\nautomation and reducing human oversight?\n• How do we ensure safety and accountability when robots operate in public or shared spaces?\n• What rights, if any, should humans have to intervene in or override robot decisions in critical\nsituations?\nPage 96 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 97, "content": "Ethics in Technology\nBy: Ed Weber\n• How can we prevent bias or discrimination in robots programmed for social or service roles?\n• Should there be universal standards or regulations for the ethical design and deployment of\nrobots?\n• How do we balance innovation with the need to protect vulnerable populations from unintended\nharm caused by robotics?\nThese questions highlight the complex ethical landscape that accompanies the rapid advancement\nand integration of robotics into society.\nAlgorithmic Bias\nAlgorithmic bias arises because AI systems are fundamentally shaped by the data used to train\nthem, the domains they are intended to operate within, and the objectives set by their developers. Most\nAI is trained on curated datasets that reflect the perspectives, limitations, and sometimes the\nprejudices of those who collect and label the data. These models are typically fixed within a specific\ndomain, meaning their understanding and decision-making are limited to the patterns present in their\ntraining environment. Furthermore, the intended outcomes – what the AI is supposed to optimize or\npredict – are defined in advance by the tool’s creators, embedding their assumptions and priorities into\nthe system. This results in inherent biases, which can become self-perpetuating as the AI consistently\nproduces outputs that reinforce the patterns and disparities present in its training data.\nImagine a hypothetical, national healthcare system that adopts an AI-powered tool to help prioritize\npatients for specialist referrals. The model is trained on historical data from urban hospitals, where\naccess to care and patient demographics differ significantly from rural areas. Because the data\nunderrepresents rural patients and overrepresents certain ethnic groups, the AI learns to prioritize\nurban, majority-population patients for referrals. Over time, this bias is amplified: rural and minority\npatients are systematically deprioritized, leading to poorer health outcomes and widening existing\ndisparities. The system’s recommendations are trusted as “objective” because they come from an\nadvanced AI, making it difficult for affected groups to challenge the results or for administrators to\nrecognize the underlying bias.\nIf machine learning environments begin to “learn on their own” – continuously updating their\nmodels based on new data – the risk of algorithmic bias may become even more pronounced. Without\nexplicit mechanisms to recognize and correct for bias, an AI could reinforce and amplify prejudices\npresent in both its initial and ongoing data streams. How would such a system recognize that its data is\nincomplete or skewed? Could it ever truly understand the social and ethical context behind the data it\nconsumes? Would it be able to distinguish between correlation and causation, or between majority\npatterns and minority needs? If an AI is left to self-train, who is responsible for monitoring and\nPage 97 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 98, "content": "Ethics in Technology\nBy: Ed Weber\ncorrecting its outputs, and how can we ensure transparency and accountability in such a dynamic\nsystem?\nThese questions highlight the risk that algorithmic bias may be inevitable unless there is continuous\nhuman oversight, robust auditing, and deliberate efforts to diversify and scrutinize training data. But\nhow will this be accomplished if the creators of the AI systems are allowed to claim ‘trade secrets’ or\n‘national security’ and then withhold this information? As AI systems become more autonomous, the\nchallenge of ensuring fairness and ethical outcomes will only grow more complex – demanding\nvigilance, innovation, and a commitment to equity at every stage of development and deployment.\nAutomation\nAutomation refers to the use of technology to perform tasks without human intervention, marking\na fundamental shift from humans merely using tools to tools independently executing work. The\nearliest automation can be traced back to inventions like water mills and mechanical clocks, which\nreduced the need for constant human oversight. The Industrial Revolution accelerated this trend with\nmachines such as the Jacquard loom and assembly line systems, which automated textile production\nand manufacturing processes. Over time, automation evolved from simple mechanical aids to\nsophisticated systems capable of performing complex, repetitive, or hazardous tasks with minimal\nhuman input.\nThe primary drivers for automation include improving health and safety by removing humans from\ndangerous environments, surpassing human physical and cognitive limitations, increasing speed and\nproductivity, reducing fatigue and stoppages, and enhancing accuracy and consistency. Automation also\nallows for 24/7 operation, minimizes waste, and ensures higher quality control, all of which contribute\nto significant cost savings and increased profits for businesses. While these benefits are often framed in\nterms of operational efficiency, flexibility, and safety, they are ultimately subordinate to economic\nmotivations: the adoption of automation is primarily justified by its potential to reduce labor costs,\nincrease output, and boost competitiveness in the marketplace.\nToday, automation extends far beyond manufacturing. In logistics, automated warehouses and self-\ndriving delivery vehicles streamline supply chains. In healthcare, robotic surgery and automated\ndiagnostics improve precision and efficiency. Financial services use algorithmic trading and\nautomated fraud detection, while agriculture benefits from autonomous tractors and drones for\nplanting and crop monitoring. Even creative industries are seeing automation in content generation and\ndesign.\nIf automation continues unchecked across all sectors, it could potentially replace most traditional\nforms of human employment, fundamentally challenging the status quo of the current economic model.\nThe question remains: can our existing economic systems – rooted in wage labor and job-based income\nPage 98 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 99, "content": "Ethics in Technology\nBy: Ed Weber\n– sustain the rapid pace of automation adoption? Or, will we need to rethink how value, work, and\nlivelihood are distributed in a world where machines do most of the work?\nPredictive Policing\nBefore the existence of formal laws, societies were governed by shared ethical norms – unwritten\nrules about right and wrong that guided individual and collective behavior. Laws and legal systems\nonly emerged after these societal ethics were violated, requiring a codification of values into\nenforceable rules to maintain order and address breaches. Policing, as a profession and practice, arose\nto uphold these laws, maintain social order, and protect the community through the prevention,\ndetection, and investigation of crime. The role of policing has always been closely tied to ethics, as\nofficers are entrusted with significant power and discretion, and their actions can profoundly affect life,\nliberty, and public trust.\nPolicing, however, has not always been a force for good. Throughout history, the institution has\nbeen subject to abuse – ranging from corruption and discrimination to excessive use of force and the\nprotection of political interests over public welfare. These abuses highlight the ongoing tension\nbetween the ideals of ethical policing – courage, respect, empathy, and public service – and the realities\nof institutional culture and unchecked discretionary power. The evolution of policing models, from\ncrime control to social peacekeeping, reflects an ongoing struggle to balance authority,\naccountability, and the ethical imperative to serve the public fairly and justly.\nPredictive policing is a recent development that uses algorithms and data analysis to forecast\nwhere crimes are likely to occur or who might be involved, with the aim of deploying resources more\nefficiently and preventing crime before it happens. Proponents argue that predictive policing can\nimprove efficiency, reduce crime rates, and help allocate police resources more effectively. However,\ncritics warn that these systems can amplify existing biases, lack transparency, and lead to over-policing\nof already marginalized communities. The risks of algorithmic bias, lack of oversight, and ethical\nambiguity – discussed in previous sections – are especially acute in predictive policing, where flawed\ndata or unchecked models can result in large-scale injustices, erode public trust, and perpetuate cycles\nof discrimination.\nAs predictive policing becomes more prevalent, the amplification of these risks could manifest in\nwidespread surveillance, unfair targeting, and diminished civil liberties. Without rigorous ethical\nstandards, transparency, and accountability, predictive policing could undermine the very societal\nvalues and ethical foundations that laws and policing were meant to protect.\nPage 99 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 100, "content": "Ethics in Technology\nBy: Ed Weber\nTextbook Definitions – Artificial Intelligence (AI), Automation and Robotics, and Algorithmic\nEthics\n• Automation – The use of technology, machines, or systems to perform tasks with minimal or\nno human intervention, streamlining processes and increasing efficiency.\n• Information Age – The current era characterized by the rapid transmission, processing, and\naccessibility of information through digital technology and computing.\n• agricultural era – A historical period when societies were primarily based on farming and the\ncultivation of crops and livestock.\n• Industrial Revolution – The period of major industrialization during the late 18th and early\n19th centuries marked by the shift from hand production to machines and factory systems.\n• artificial intelligence (AI) – The development of computer systems capable of performing\ntasks that typically require human intelligence, such as reasoning, learning, and problem-\nsolving.\n• Robotics – The branch of technology that deals with the design, construction, operation, and\napplication of robots to perform automated tasks.\n• Autonomous vehicles (AVs) – Vehicles equipped with technology that enables them to\nnavigate and operate without direct human control.\n• Chatbots – Software applications that simulate human conversation using text or voice, often\nfor customer service or information retrieval.\n• hiring practices – The methods and criteria organizations use to recruit, select, and employ\npersonnel.\n• Predictive policing – The use of data analysis and algorithms to forecast potential criminal\nactivity and inform law enforcement strategies.\n• bias – A systematic inclination or prejudice in favor of or against certain outcomes, groups, or\ndata, often leading to unfair or inaccurate results.\n• accountability – The obligation to explain, justify, and take responsibility for one's actions or\ndecisions.\n• distribution of power and opportunity – The way authority, resources, and chances for\nadvancement are allocated among individuals or groups in a society.\n• sustainability – The capacity to maintain or support processes, systems, or resources over the\nlong term without depleting them.\nPage 100 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 101, "content": "Ethics in Technology\nBy: Ed Weber\n• job displacement – The loss of employment opportunities due to technological change,\nautomation, or other factors.\n• human agency – The capacity of individuals to act independently and make their own free\nchoices.\n• expert systems – Computer programs that emulate the decision-making abilities of human\nexperts in specific domains using predefined rules.\n• domains – Specific areas of knowledge, activity, or expertise within which a system or\nindividual operates.\n• predefined rules and logic – Explicitly programmed instructions and decision criteria that\ngovern the behavior of a system or process.\n• Artificial General Intelligence (AGI) – A theoretical form of AI capable of understanding,\nlearning, and applying knowledge across a wide range of tasks at a human-like level.\n• Artificial Superintelligence (ASI) – A hypothetical AI that surpasses human intelligence in all\nrespects, including creativity, reasoning, and problem-solving.\n• large language models (LLMs) – Advanced AI models trained on extensive text data to\ngenerate, summarize, and understand human language.\n• natural language processing (NLP) – The field of AI focused on enabling computers to\ninterpret, process, and generate human language.\n• curated datasets – Carefully selected and organized collections of data used to train or evaluate\nAI models.\n• tuned – Adjusted or refined by developers to improve a model’s performance or adapt it to\nspecific tasks or domains.\n• reinforce biases – To perpetuate or amplify existing prejudices or patterns present in training\ndata through repeated outputs.\n• Algorithmic bias – Systematic and repeatable errors in AI outputs that result from biases in the\ndata, design, or implementation of algorithms.\n• recursive learning – A process where AI systems iteratively update and improve themselves by\nlearning from their own outputs or new data.\n• ethical center – The core set of moral principles or values that guide decision-making and\nbehavior in an individual or system.\nPage 101 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 102, "content": "Ethics in Technology\nBy: Ed Weber\n• independent moral agents – Entities capable of making ethical decisions and being held\nresponsible for their actions without external control.\n• moral agency – The ability to discern right from wrong and to be held accountable for one’s\nactions.\n• culpability – The degree to which an individual or entity is responsible for a fault or wrong.\n• Human error – Mistakes or failures in judgment, perception, or action made by people, often\nleading to unintended consequences.\n• simulate emotion – The act of mimicking or reproducing emotional expressions or responses\nusing technology.\n• automata – Self-operating machines or mechanisms, often designed to follow a predetermined\nsequence of operations.\n• Three Laws of Robotics – A set of ethical rules devised by science fiction writer Isaac Asimov\nto govern the behavior of robots.\n• programmable logic controllers (PLCs) – Industrial digital computers used to control\nmanufacturing processes or machinery.\n• computer numerical control (CNC) – The automated control of machining tools and 3D\nprinters by means of a computer.\n• manufacturing and warehouse automation – The use of automated systems and robots to\nperform tasks in production and storage facilities with minimal human involvement.\n• intended outcomes – The specific goals or results that a system or process is designed to\nachieve.\n• self-perpetuating – Capable of continuing or reinforcing itself without external input or\nintervention.\n• systematically deprioritized – Consistently assigned lower importance or priority in a\nstructured or organized manner.\n• correlation – A statistical relationship or association between two or more variables.\n• causation – The action of causing something to happen; a direct cause-and-effect relationship.\n• majority patterns – Trends or behaviors that are most common within a given dataset or\npopulation.\n• minority needs – The specific requirements or interests of less-represented groups within a\npopulation.\nPage 102 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "section_path": ["Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing"], "page": 103, "content": "Ethics in Technology\nBy: Ed Weber\n• oversight – The act of supervising, monitoring, or regulating processes or organizations to\nensure proper conduct.\n• auditing – The systematic examination and evaluation of processes, systems, or data to ensure\naccuracy, compliance, and integrity.\n• algorithmic trading – The use of computer algorithms to automatically execute financial trades\nat high speed and volume.\n• autonomous tractors – Self-driving agricultural vehicles capable of performing tasks such as\nplowing, planting, and harvesting without human intervention.\n• drones – Unmanned aerial vehicles operated remotely or autonomously for various purposes,\nincluding surveillance, delivery, and data collection.\n• Policing – The activities and responsibilities of maintaining public order, enforcing laws, and\npreventing and investigating crime.\n• corruption – Dishonest or unethical conduct by those in power, typically involving bribery or\nthe abuse of authority for personal gain.\n• discrimination – Unfair or prejudicial treatment of individuals or groups based on\ncharacteristics such as race, gender, or age.\n• excessive use of force – The application of more physical power than is necessary or justified\nin a given situation, often by law enforcement.\n• unchecked discretionary power – Authority exercised without sufficient oversight, limits, or\naccountability, increasing the risk of abuse.\n• authority – The legitimate power to make decisions, enforce rules, and command obedience.\n• accountability – The requirement to answer for one’s actions and decisions, especially in\npositions of power or responsibility.\nPage 103 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 104, "content": "Ethics in Technology\nBy: Ed Weber\n12. Bioethics and Human Enhancement\nGenetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-\nComputer Interfaces; Biotechnology; Cloning\nFrom the earliest days of recorded history, humans have striven to overcome injury, disease, and the\nlimitations imposed by nature. Ancient civilizations developed rudimentary forms of medicine, using\nherbal remedies, ritualistic healing, and early surgical techniques to treat wounds and illnesses. Over\ncenturies, the figure of the healer evolved into the professional doctor, as societies formalized the\nstudy of anatomy, pharmacology, and hygiene. The establishment of medical institutions and the\ncodification of ethical standards, such as the original Hippocratic Oath as well as the current versions\nas it has evolved over the years, marked significant milestones in the professionalization of medicine.\nThese advancements, coupled with improvements in sanitation, nutrition, and public health,\ncontributed to dramatic increases in birth rates and steadily rising life expectancies across much of the\nworld.\nIn the modern era, the fusion of medicine and technology has ushered in a new age of diagnostics\nand treatment. Innovations such as magnetic resonance imaging (MRI), robotic-assisted surgery,\ngene sequencing, and targeted therapies have become accessible – and even commonplace – in many\ndeveloped regions. These breakthroughs have enabled earlier detection of disease, more precise\ninterventions, and improved outcomes for patients. Technologies like wearable health monitors,\ntelemedicine platforms, and personalized medicine are reshaping the patient experience, making\nhealthcare more efficient and, in some cases, more equitable. Yet, these advances are not uniformly\ndistributed, and significant disparities in access to care persist both within and between nations.\nToday, the frontier of technology and human biology is rapidly expanding beyond traditional\ntreatment. Emerging capabilities in genetic engineering allow for the possibility of designing offspring\nwith selected traits, raising profound ethical questions about autonomy, consent, and the very\ndefinition of humanity. Human augmentation – whether through biological enhancements, neural\ninterfaces, or hybrid bio-robotic systems – challenges our understanding of ability, identity, and\nfairness. Cloning and advanced biotechnologies further blur the boundaries between natural and\nartificial life. These developments amplify longstanding issues of ethics and equity, as access to\ncutting-edge interventions often remains limited by socioeconomic status, geography, and policy. As\nwe look to the future, society must grapple with how to ensure that the benefits of bioethical innovation\nare shared broadly, while safeguarding individual rights and addressing the risks of deepening\ninequality.\nPage 104 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 105, "content": "Ethics in Technology\nBy: Ed Weber\nGenetic Engineering and CRISPR\nFor millennia, humans have shaped the natural world through selective breeding and cross-\nbreeding, long before the discovery of DNA or the advent of modern biotechnology. Early\nagriculturalists learned to cultivate plants and animals with desirable traits – such as higher yields,\nresistance to disease, or improved taste – by intentionally mating individuals that exhibited these\ncharacteristics. Hybridization, the crossing of different species or varieties, produced vigorous new\ncrops like hybrid grains and apples, while grafting and cloning techniques allowed for the propagation\nof seedless fruits such as bananas and larger, juicier varieties of produce. The transformation of wild\nteosinte (a Mexican grass) into modern maize (corn) is a striking example of how traditional breeding\npractices could fundamentally alter a species over generations. Similarly, the development of hybrid\ncorn in the early 20th century revolutionized agriculture by increasing crop productivity.\nAs scientific understanding deepened, especially following the discovery of DNA’s structure,\ngenetic manipulation became more precise. By the mid-20th century, plant breeders were using\nradiation and chemicals to induce random mutations, further expanding the genetic toolkit available\nfor crop improvement. The real turning point came in the 1970s, when researchers developed\ntechniques to directly modify DNA – splicing genes from one organism into another, regardless of\nspecies boundaries. Early successes included the creation of recombinant bacteria and the first\ngenetically modified plants, such as tobacco engineered for antibiotic resistance. In animals, transgenic\nmice paved the way for more complex genetic research and applications.\nThe mapping of the human genome at the turn of the 21st century marked a watershed moment,\nproviding a comprehensive blueprint of human genetic information. This achievement set the stage for\nthe development of CRISPR, a revolutionary gene-editing technology that allows scientists to\nprecisely \"cut and paste\" sections of DNA within living organisms. Today, CRISPR is being used in a\nwide range of applications – from developing disease-resistant crops and livestock to exploring\npotential cures for genetic disorders in humans. Researchers are even investigating the possibility of\nresurrecting extinct species by editing the genomes of living relatives.\nAs we have done previously, let’s consider some ethical questions surrounding the concepts of\ngenetic engineering:\n• Who should decide which genetic traits are considered \"normal,\" \"desirable,\" or \"disorders\"\nwhen it comes to genetic engineering in humans, plants, or animals?\n• Is it ethical to use gene editing technologies like CRISPR for human enhancement (such as\nincreasing intelligence or physical ability), rather than solely for treating diseases?\n• How can society ensure fair and equitable access to genetic engineering technologies, so that\nbenefits are not limited to the wealthy or privileged?\nPage 105 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 106, "content": "Ethics in Technology\nBy: Ed Weber\n• What are the potential long-term and unintended consequences of editing the human genome,\ngiven that changes could be passed to future generations who cannot consent?\n• Should it be permissible to patent genetically engineered organisms, genes, or gene-editing\ntechniques, and what are the implications for intellectual property, innovation, and access?\n• How might widespread use of gene editing affect societal acceptance of people with disabilities\nor differences, and could it lead to new forms of discrimination or eugenics?\n• What responsibilities do scientists and companies have to ensure transparency, informed\nconsent, and environmental stewardship when releasing genetically engineered organisms\ninto the environment?\n• Is it morally acceptable to genetically engineer animals for human benefit, such as for food\nproduction or medical research, and what are the welfare considerations for these animals?\n• Where should the line be drawn between therapeutic uses of genetic engineering and non-\ntherapeutic, elective, or cosmetic applications?\n• How should regulatory frameworks evolve to address the rapid pace of genetic engineering\ntechnology, especially given current ambiguities in law and policy?\nLegal and ethical frameworks have struggled to keep pace with these rapid advancements. In the\nUnited States, it was once legal to patent isolated human genes, a practice that sparked significant\ncontroversy over ownership and access to genetic information. However, a 2013 Supreme Court\ndecision ruled that naturally occurring human genes could not be patented, though synthetic DNA\n(cDNA) remains patentable. The legal landscape for genetic engineering in plants, animals, and\nhumans remains ambiguous, with regulations varying widely by country and often lagging behind\ntechnological capabilities. This uncertainty raises pressing questions about equity, access, and the\nresponsible use of genetic technologies as society moves deeper into the era of bioengineering.\nHuman Augmentations\nThroughout history, humans have sought ways to restore lost function and even enhance their\nbodies, as evidenced by archaeological discoveries of ancient prosthetics and artificial enhancements.\nRemains from ancient Egypt reveal prosthetic toes dating back nearly 3,000 years, crafted from wood\nand leather, suggesting both practical and possibly symbolic purposes. In China, a 2,200-year-old man\nof modest means was discovered having a prosthetic leg made from poplar wood, ox horn, and horse\nhoof. This limb was designed to help its owner – who suffered from a fused knee – walk more easily.\nSimilarly, in medieval Europe, prosthetic hands and legs have been unearthed, some simple and\nPage 106 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 107, "content": "Ethics in Technology\nBy: Ed Weber\nfunctional, others more elaborate, reflecting both the medical ingenuity of the time and the social\nsignificance attached to bodily integrity and appearance.\nThese early prosthetics were primarily functional, aiming to restore lost mobility or utility.\nHowever, some may have also served as markers of status, identity, or resilience, particularly when\ncrafted with care or adorned with valuable materials. Over centuries, the evolution of prosthetic\ntechnology has mirrored advances in materials science, medicine, and engineering – from wood and\nmetal devices fastened with leather straps to today’s lightweight carbon fiber limbs and sophisticated\nbionic prosthetics that can be controlled by neural signals.\nModern human augmentation has moved beyond mere replacement of lost function. Today’s\nprosthetics can not only restore, but also enhance, physical abilities – sometimes surpassing what is\nconsidered “normal” human performance. Athletes with advanced running blades, for example,\nchallenge conventional definitions of ability and fairness. Neural implants, exoskeletons, and sensory\nenhancements are pushing the boundaries of what it means to be human, raising profound questions\nabout identity, equity, and the future of human evolution.\nConsider these ethical questions surrounding the topic of human augmentation:\n• Should there be limits on augmentations that enhance abilities beyond the typical human range,\nsuch as strength, speed, or cognition?\n• Who should have access to advanced augmentations – should they be available to all, or only to\nthose who can afford them?\n• Could widespread augmentation create new forms of inequality or discrimination between\n“augmented” and “non-augmented” individuals?\n• How should society regulate the use of neural implants or brain-computer interfaces that could\nalter thought, memory, or personality?\n• If a person replaces most or all of their biological body with artificial parts, are they still the\nsame person – philosophically or legally?\n• Should children be allowed or required to receive certain augmentations to compete or\nparticipate in society?\n• What responsibilities do designers and manufacturers have if an augmentation malfunctions or\nis hacked?\n• How might human augmentation affect the value society places on natural abilities or\ndisabilities?\nPage 107 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 108, "content": "Ethics in Technology\nBy: Ed Weber\n• Should employers or governments be allowed to require or incentivize certain augmentations\nfor work or public service?\n• What rights and protections should individuals have regarding the data generated by their\naugmented bodies?\nThis last set of questions echoes the ancient philosophical thought experiment known as the Ship of\nTheseus: if every board of a ship is replaced over time, is it still the same ship? Applied to human\naugmentation, if all parts of a person are gradually replaced with artificial components, does their\nidentity persist – or does something fundamentally change? This debate sits at the heart of the ethical,\nlegal, and existential challenges posed by the future of human enhancement.\nNeuroethics and Brain-Computer Interfaces\nNeuroethics and brain-computer interfaces (BCIs)\nrepresent one of the most rapidly evolving frontiers in both\nneuroscience and technology. At the core of this field are\nneurological sensors, which can be broadly categorized as\nactive or passive. Active sensors, such as deep brain\nstimulators and implanted electrodes, not only record\nneural activity but can also deliver electrical stimulation to\ntargeted brain regions. Passive sensors, including\nelectroencephalography (EEG) caps and functional MRI\n(fMRI), non-invasively monitor the brain’s electrical or\nmetabolic activity for diagnostic and research purposes.\nThese technologies have become invaluable in\nunderstanding neurological disorders, mapping brain Figure 16: Exaggeration of brain-\nfunction, and developing treatments for conditions such as computer-interface\nepilepsy, Parkinson’s disease, and severe paralysis.\nBrain-computer interfaces leverage these advances to create direct communication pathways\nbetween the brain and external devices. The most promising use cases include restoring movement or\ncommunication for individuals with paralysis, enabling control of prosthetic limbs, and providing new\nways for people with severe disabilities to interact with the world. BCIs are also being explored for\ncognitive enhancement, mental health interventions, and even immersive gaming experiences. The\nability to decode neural signals and translate them into digital commands holds transformative potential\nfor medicine, rehabilitation, and human-computer interaction.\nHowever, the scale and complexity of data collected by neurological sensors and BCIs present\nsignificant challenges. Current technology cannot isolate individual thoughts or intentions with\nPage 108 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 109, "content": "Ethics in Technology\nBy: Ed Weber\nprecision; instead, it captures vast streams of brain activity, resulting in the collection of far more data\nabout a person than is necessary for a specific research or clinical goal. This phenomenon mirrors\nbroader concerns previously discussed in the chapter on Privacy, Surveillance, and Data Ethics. In that\nchapter we discussed how ‘big data’, the aggregation and analysis of massive datasets, can\ninadvertently expose sensitive personal information, create privacy risks, and lead to unintended uses\nof data. Just like with big data, the capture, storage, and analysis of neurological data repeats the same\nethical concerns which include informed consent, data ownership, potential misuse of neural data, and\nthe risk of surveillance or discrimination based on brain activity patterns.\nNeuralink, a leading company in the BCI space, has recently achieved a major milestone by\nsuccessfully implanting its “Telepathy” device in a human subject. This coin-sized implant uses ultra-\nfine threads equipped with thousands of electrodes to record neural activity at a high resolution. The\ndevice has demonstrated the ability to detect neuron spikes and correlate brain signals with intended\nmotor actions, allowing users to control computers or external devices directly through thought.\nNeuralink’s approach combines advanced neurosurgical robotics for precise implantation with custom\nelectronics that process and transmit neural data. Neuralink is currently engaged in human trials,\noffering hope for individuals with severe neurological conditions and opening new possibilities for\nhuman-computer integration.\nHere, again, are several ethical questions surrounding neuroethics and BCIs:\n• To what extent could brain-computer interfaces (BCIs) be used to read or decode private\nthoughts and memories, and what safeguards should be in place to protect mental privacy?\n• If a BCI could send signals to the brain that override or contradict a person’s intended actions\n(such as controlling movement or behavior), who is responsible for the outcome, and how\nshould consent be managed?\n• What ethical concerns arise if technology advances to the point where data can be written to the\nbrain – potentially altering memories, perceptions, or even personality traits – rather than just\nreading from it?\n• How can individuals maintain autonomy and freedom of thought in a future where\nneurotechnology might make it possible for others to access or influence their mental states?\n• Should there be limits on the collection and analysis of neural data, given that current BCIs\ncapture far more information than is needed for specific tasks, raising big-data privacy and\nconsent issues?\n• In the event of a malfunction, hack, or unauthorized access to a BCI, what protections and\nrecourse should users have if their thoughts or actions are affected without their consent?\nPage 109 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 110, "content": "Ethics in Technology\nBy: Ed Weber\n• How should society address the possibility of BCIs being used for enhancement or\nmanipulation, such as boosting cognitive abilities or influencing decisions, especially if access\nis unequal or coerced?\nAs BCI technology advances, the ethical landscape will require ongoing scrutiny, balancing the\nimmense potential for benefit with the need to protect individual rights and societal values.\nBiotechnology\nBiotechnology is a broad field that encompasses the use of living organisms, cells, and biological\nsystems to develop products and processes that benefit society. Many of the topics previously discussed\n– such as genetic engineering, CRISPR, brain-computer interfaces, and bioethics – are all integral parts\nof the biotechnology landscape. However, the reach of biotechnology extends even further, touching on\na range of emerging technologies and applications that are reshaping medicine, agriculture, industry,\nand environmental management.\nBeyond gene editing and medical diagnostics, biotechnology now includes advanced innovations\nlike nanotechnology for direct cell repair and targeted cancer therapies. Nanotech-enabled particles\ncan be engineered to seek out and destroy cancer cells without harming healthy tissue, offering more\nprecise and less invasive treatments. In addition, biotechnology has enabled the development of\nbioengineered organisms – microbes or plants designed to clean up pollution through processes like\nbioremediation and phytoremediation. These organisms can break down toxic substances in soil and\nwater, helping to restore contaminated environments and improve public health.\nAnother rapidly growing area is the production of bio-printed or lab-grown food. Using 3D\nprinting technology and cell culture techniques, scientists can now create meat, organs, and other\ntissues in the lab, potentially reducing the environmental impact of traditional agriculture and providing\nnew sources of nutrition. This technology is also being explored for medical applications, such as\nprinting skin, bone, or even entire organs for transplantation.\nWhile the benefits of biotechnology are substantial, significant risks and uncertainties remain. One\nmajor concern is the possibility of unintended release of engineered organisms or nanotech agents into\nthe environment. Once released, these entities may not be easily converted from active to dormant or\ninert states, raising fears about long-term ecological impacts or the creation of new, hard-to-control\nforms of pollution. The microscopic or nanoscale nature of many biotech interventions also makes\ntransparency and oversight difficult, complicating efforts to monitor their behavior and effects.\nFor example, bioengineered microbes used to clean up oil spills or toxic waste could themselves\nbecome hazardous if they mutate or interact with other organisms in unexpected ways. While the pros\nof such applications include cleaner water and soil, the cons may involve the organisms becoming toxic\nPage 110 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 111, "content": "Ethics in Technology\nBy: Ed Weber\nto humans or disrupting local ecosystems. Similarly, lab-grown foods promise sustainability and food\nsecurity, but raise questions about safety, labeling, and the social and economic impacts on traditional\nfarming communities.\nBiotechnology is revolutionizing how we approach health, food, and environmental challenges, but\nit also demands careful consideration of the risks, especially regarding safety, transparency, and long-\nterm sustainability. As these technologies become more integrated into daily life, ongoing ethical, legal,\nand societal debates will be essential to ensure they are used responsibly and equitably.\nCloning\nCloning, as a concept, has long fascinated humanity, appearing in ancient myths, literature, and\nmodern entertainment as the idea of creating identical copies of organisms. The scientific journey\ntoward cloning began in the late 19th century, when researchers like Hans Driesch demonstrated\nartificial embryo twinning in sea urchins, showing that separated embryonic cells could each develop\ninto whole organisms. In the 20th century, landmark experiments included the cloning of frogs by\nnuclear transfer in the 1950s and the cloning of mammals from embryonic and adult cells in the 1980s\nand 1990s. The most famous breakthrough came in 1996 with the birth of Dolly the sheep, the first\nmammal cloned from an adult somatic cell, announced by Ian Wilmut and his team at the Roslin\nInstitute in Scotland. Dolly’s creation proved that specialized adult cells could be reprogrammed to\ncreate an entire organism, igniting both scientific excitement and ethical debate. Other notable\nmilestones include the cloning of cows, cats, and even monkeys, as well as the cloning of animals for\nagriculture, research, and pet reproduction.\nAttempts at human cloning have been more controversial and less successful. In 2001, scientists at\nAdvanced Cell Technology in Massachusetts cloned human embryos for the first time, aiming for\ntherapeutic rather than reproductive purposes. In 2013, a team led by Shoukhrat Mitalipov achieved a\nbreakthrough in human cloning by creating embryonic stem cells from cloned human embryos. While\nsome fringe groups and individuals have claimed to have cloned humans, there is no verified scientific\nevidence of a live human clone. News of human cloning efforts has generally been met with skepticism\nand concern within the scientific and medical communities, and has sparked strong opposition from\nreligious, ethical, and political groups around the world. Reactions have ranged from moral outrage and\ncalls for bans to cautious support for therapeutic cloning aimed at treating disease.\nCurrently, human reproductive cloning is illegal or heavily restricted in the United States and\nmany other countries. Therapeutic cloning – using cloned embryos to derive stem cells for research or\nmedical treatment – remains a gray area, with regulations varying by state and ongoing debates about\nits ethical and legal status. The technology continues to raise profound questions about identity,\nindividuality, and the boundaries of human intervention in nature.\nPage 111 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 112, "content": "Ethics in Technology\nBy: Ed Weber\nConsider these ethical questions surrounding the concept of cloning:\n• Is it ethical to create a human clone for reproductive purposes, knowing the potential risks and\nuncertainties involved?\n• Should cloning be allowed for therapeutic purposes, such as generating tissues or organs for\ntransplantation?\n• What rights and status would a human clone have in society – would they be treated as\nindividuals or property?\n• Could the widespread use of cloning undermine the value of genetic diversity or lead to new\nforms of discrimination?\n• How should society regulate or oversee cloning technology to prevent abuse or unintended\nconsequences?\n• Would the existence of human clones challenge traditional notions of family, parenthood, and\nidentity?\n• What are the long-term psychological and social impacts on clones and their families?\nTextbook Definitions – Bioethics and Human Enhancement\n• medicine – The science and practice of diagnosing, treating, and preventing disease and injury\nin humans.\n• herbal remedies – Treatments derived from plants and plant extracts used for their medicinal\nproperties.\n• professional doctor – A person formally trained and licensed to practice medicine and provide\nhealthcare.\n• anatomy – The study of the structure of living organisms, especially their internal systems and\norgans.\n• pharmacology – The branch of medicine concerned with the study of drugs and their effects on\nthe body.\n• hygiene – Practices and conditions that promote health and prevent disease, especially through\ncleanliness.\n• Hippocratic Oath – An ancient ethical code historically taken by physicians, emphasizing\nmedical ethics and patient care. The current accepted version of this oath (as of 2017) is:\nPage 112 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 113, "content": "Ethics in Technology\nBy: Ed Weber\n◦ AS A MEMBER OF THE MEDICAL PROFESSION:\n▪ I SOLEMNLY PLEDGE to dedicate my life to the service of humanity;\n▪ THE HEALTH AND WELL-BEING OF MY PATIENT will be my first consideration;\n▪ I WILL RESPECT the autonomy and dignity of my patient;\n▪ I WILL MAINTAIN the utmost respect for human life;\n▪ I WILL NOT PERMIT considerations of age, disease or disability, creed, ethnic origin,\ngender, nationality, political affiliation, race, sexual orientation, social standing or any\nother factor to intervene between my duty and my patient;\n▪ I WILL RESPECT the secrets that are confided in me, even after the patient has died;\n▪ I WILL PRACTICE my profession with conscience and dignity and in accordance with\ngood medical practice;\n▪ I WILL FOSTER the honor and noble traditions of the medical profession;\n▪ I WILL GIVE to my teachers, colleagues, and students the respect and gratitude that is\ntheir due;\n▪ I WILL SHARE my medical knowledge for the benefit of the patient and the\nadvancement of healthcare;\n▪ I WILL ATTEND TO my own health, well-being, and abilities in order to provide care\nof the highest standard;\n▪ I WILL NOT USE my medical knowledge to violate human rights and civil liberties,\neven under threat;\n▪ I MAKE THESE PROMISES solemnly, freely, and upon my honor.\n• sanitation – Measures and practices that maintain cleanliness and prevent the spread of disease,\nespecially through waste management.\n• nutrition – The process by which living organisms obtain and use food to support growth,\nhealth, and maintenance.\n• public health – The science and practice of protecting and improving the health of\ncommunities through education, policy, and preventive measures.\n• diagnostics – Techniques and tools used to identify diseases or medical conditions in\nindividuals.\n• treatment – Medical care or intervention given to manage or cure illness or injury.\n• magnetic resonance imaging (MRI) – A non-invasive imaging technique that uses magnetic\nfields and radio waves to create detailed images of internal body structures.\n• robotic-assisted surgery – Surgical procedures performed with the aid of robotic systems to\nenhance precision and control\nPage 113 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 114, "content": "Ethics in Technology\nBy: Ed Weber\n• gene sequencing – The process of determining the exact order of nucleotides in a DNA\nmolecule.\n• wearable health monitors – Electronic devices worn on the body that track health metrics such\nas heart rate, activity, or sleep.\n• telemedicine – The remote diagnosis and treatment of patients using telecommunications\ntechnology.\n• personalized medicine – Medical care tailored to an individual’s genetic, environmental, and\nlifestyle factors.\n• genetic engineering – The direct manipulation of an organism’s DNA to alter its characteristics\nor functions.\n• autonomy – The right or condition of self-government, especially in making informed\ndecisions about one’s own body and health.\n• consent – Permission for something to happen or agreement to do something, especially after\nbeing informed of the risks and benefits.\n• Human augmentation – The use of technology to enhance or extend human physical or\ncognitive abilities.\n• neural interfaces – Devices or systems that enable direct communication between the brain\nand external devices.\n• hybrid bio-robotic systems – Integrated systems combining biological and robotic components\nto enhance function or performance.\n• Cloning – The process of producing genetically identical copies of an organism, cell, or DNA\nsequence.\n• selective breeding – The intentional mating of organisms with desirable traits to produce\noffspring with those traits.\n• cross-breeding – The process of mating individuals from different breeds or species to produce\nhybrid offspring.\n• DNA – Deoxyribonucleic acid, the molecule that carries genetic information in living\norganisms.\n• Hybridization – The process of combining different varieties or species to produce a hybrid\nwith traits from both parents.\nPage 114 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 115, "content": "Ethics in Technology\nBy: Ed Weber\n• random mutations – Unplanned changes in DNA that can result in new traits or variations in\norganisms.\n• CRISPR – A gene-editing technology that allows precise modifications to DNA sequences in\nliving organisms.\n• eugenics – The controversial practice or belief in improving the genetic quality of a human\npopulation through selective breeding or genetic intervention.\n• transparency – Openness and clarity about processes, decisions, and data, especially in science\nand ethics.\n• informed consent – The process of providing individuals with sufficient information to make\nknowledgeable decisions about participation in medical or research activities.\n• environmental stewardship – The responsible management and care of the environment and\nnatural resources.\n• therapeutic – Intended to heal or treat disease or medical conditions.\n• elective – Chosen or optional, especially referring to medical procedures that are not medically\nnecessary.\n• cosmetic – Intended to improve appearance rather than health or function.\n• prosthetics – Artificial devices that replace missing body parts to restore function or\nappearance.\n• carbon fiber – A strong, lightweight material commonly used in advanced prosthetics and other\nhigh-performance applications.\n• bionic prosthetics – Artificial limbs or devices enhanced with electronic or mechanical\ncomponents to mimic or surpass natural function.\n• neural signals – Electrical impulses generated by neurons that transmit information within the\nnervous system.\n• running blades – Curved, spring-like prosthetic limbs designed to enable or enhance running\nperformance.\n• Neural implants – Devices surgically placed in the brain or nervous system to restore or\nenhance function.\n• exoskeletons – Wearable robotic frameworks that support or augment human movement and\nstrength.\nPage 115 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 116, "content": "Ethics in Technology\nBy: Ed Weber\n• sensory enhancements – Technologies or interventions that improve or extend human sensory\nperception.\n• thought – A mental process involving ideas, reasoning, or imagination.\n• memory – The mental capacity to store, retain, and recall information or experiences.\n• personality – The combination of characteristics or qualities that form an individual’s\ndistinctive character.\n• Neuroethics – The study of ethical, legal, and social issues arising from neuroscience and\nneurotechnology.\n• brain-computer interfaces (BCIs) – Systems that enable direct communication between the\nbrain and external devices, often for control or interaction.\n• Active sensors – Devices that both detect and interact with biological signals, often by sending\nor receiving electrical impulses.\n• implanted electrodes – Electrodes surgically placed in the body or brain to monitor or\nstimulate neural activity.\n• Passive sensors – Devices that detect and record biological signals without actively interacting\nwith the system.\n• electroencephalography (EEG) – A non-invasive method for recording electrical activity of\nthe brain using electrodes placed on the scalp.\n• functional MRI (fMRI) – An imaging technique that measures brain activity by detecting\nchanges in blood flow.\n• cognitive enhancement – The use of technology or interventions to improve mental functions\nsuch as memory, attention, or intelligence.\n• intended motor actions – Movements or actions that a person consciously plans or attempts to\nperform.\n• neural data – Information collected from the nervous system, especially brain activity signals.\n• Biotechnology – The use of living organisms, cells, or biological systems to develop products\nand technologies for human benefit.\n• nanotechnology – The manipulation and application of materials at the molecular or atomic\nscale, often for medical or technological purposes.\n• bioengineered organisms – Living organisms whose genetic material has been deliberately\nmodified for specific purposes.\nPage 116 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "section_path": ["Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning"], "page": 117, "content": "Ethics in Technology\nBy: Ed Weber\n• bioremediation – The use of living organisms, such as microbes or plants, to clean up\nenvironmental pollutants.\n• phytoremediation – The use of plants to absorb, remove, or neutralize contaminants from soil\nor water.\n• bio-printed – Created using 3D printing techniques with biological materials, often for medical\nor food applications.\n• unintended release – The accidental escape or spread of engineered organisms or substances\ninto the environment.\n• human reproductive cloning – The creation of a human being that is genetically identical to\nanother individual through cloning techniques.\n• Therapeutic cloning – The creation of cloned embryos for the purpose of generating stem cells\nfor medical research or treatment.\nPage 117 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "section_path": ["Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability"], "page": 118, "content": "Ethics in Technology\nBy: Ed Weber\n13. Technological Disruption and the Paradox of Progress\nObsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech\nProgress; Erosion of Economic Sustainability\nThroughout history, the greatest leaps forward in human civilization have often been catalyzed by\ntechnological disruption. From the invention of the printing press, which democratized knowledge\nand upended centuries-old power structures, to the assembly line that revolutionized manufacturing and\nmade goods accessible to millions, each wave of innovation has brought both profound progress and\nsignificant upheaval.\nThese disruptions are rarely met with universal enthusiasm; while some individuals and industries\nembrace, adopt, and expand upon new technologies, others resist, fearing loss of livelihood, status, or\ncontrol. The introduction of the personal computer, for example, was welcomed by early adopters and\nvisionaries but met skepticism by those invested in mainframe computing or manual record keeping.\nSimilarly, the rapid rise of the Internet in the 1990s transformed everything from commerce to\ncommunication, sparking both excitement and anxiety about its societal implications.\nLooking back at the past 50 to 75 years, many of the most notable disruptions align with the topics\nexplored in earlier chapters. The digital revolution – driven by personal computers, the Internet, and\nlater, mobile devices – has fundamentally altered how we work, learn, and connect. Advances in\ngenetics and biotechnology, such as the mapping of the human genome and the development of\nCRISPR, have opened new frontiers in medicine and ethics. The rise of artificial intelligence,\nautomation, and robotics has transformed industries, from manufacturing to healthcare, while also\nraising concerns about job displacement and algorithmic bias. In recent years, technologies like 3D\nprinting, spatial computing, and wearable and embedded devices have further blurred the\nboundaries between the physical and digital worlds, creating new opportunities and challenges.\nA defining feature of modern technological disruption is its exponential, rather than linear, rate of\nchange. Innovations that once took decades to diffuse now reach global scale in a matter of years – or\neven months. The adoption curve for technologies like smartphones, streaming services, and\ngenerative AI has been breathtakingly steep, leaving little time for societies to adapt before the next\nwave arrives. As we stand on the cusp of further disruption, the window between major breakthroughs\ngrows ever shorter, and the potential impacts – both positive and negative – become more profound.\nLooking to the near horizon, several major technological breakthroughs seem poised to reshape our\nworld. These may include:\n• Widespread deployment of advanced AI systems capable of autonomous decision-making in\ncritical sectors.\nPage 118 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "section_path": ["Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability"], "page": 119, "content": "Ethics in Technology\nBy: Ed Weber\n• Quantum computing breakthroughs that render current encryption obsolete and enable new\nscientific discoveries.\n• Scalable, affordable bioengineering solutions for disease treatment, food production, and\nenvironmental restoration.\n• Mainstream adoption of brain-computer interfaces, enabling direct neural interaction with\ndigital systems.\n• The rise of fully immersive spatial computing environments, transforming work, education, and\nentertainment.\n• Next-generation energy technologies, such as fusion or advanced battery storage, that could\ndisrupt global energy markets.\nThe paradox of progress is that while technological disruption drives unprecedented advancement,\nit also brings new challenges – planned and inevitable obsolescence, tech lock-in, erosion of\neconomic participation, and environmental consequences – that demand thoughtful navigation in the\ndecades ahead.\nObsolescence – Planned vs. Inevitable\nThe story of technological progress is often told through the lens of obsolescence, where each new\nleap forward renders a previous standard obsolete: the abacus gave way to the calculator, which was\nthen eclipsed by the computer; buggy-whips disappeared as automobiles replaced horse-drawn\ncarriages, soon followed by windshield wipers as standard equipment; the telegraph was overtaken by\ntelephones, which themselves have been transformed by digital and VoIP communication; vinyl records\nyielded to cassette tapes, then CDs, and now streaming audio; and film cameras faded as digital\nphotography became the norm. Each of these transitions highlights how new technologies not only\nreplace old ones but also reshape industries, economies, and daily life.\nInevitable obsolescence is a natural byproduct of technological disruption, as newer, better, or\nmore efficient solutions emerge and make older versions less useful or even unserviceable. This cycle\nis accelerating, with product and component life cycles growing shorter as innovation speeds up and\nconsumer expectations rise. The ethical implications of this relentless churn are complex.\nWhile much attention is paid to job displacement and the need to support those affected by\ndisruption, a deeper question arises: should our ethical frameworks focus solely on preserving the\nstatus quo, or should they also empower us to reimagine the very structure of work, value, and\nparticipation in society? As automation and AI threaten to upend traditional employment models, it\nPage 119 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "section_path": ["Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability"], "page": 120, "content": "Ethics in Technology\nBy: Ed Weber\nmay be time to challenge the assumption that widespread employment is the only path to economic\nsecurity and personal fulfillment.\nIn response to the threat of inevitable obsolescence, some businesses have adopted the strategy of\nplanned obsolescence – intentionally designing products with limited lifespans or incremental\nimprovements to encourage repeated purchases and maintain brand relevance. This approach allows\ncompanies to attempt to control the pace of change and manage consumer expectations, but it also\nraises ethical concerns about waste, resource use, and consumer manipulation. Ultimately, the interplay\nbetween inevitable and planned obsolescence shapes not only the technology landscape but also the\nbroader social and ethical context in which innovation unfolds.\nTech Lock-In and 3D Printing\nAs a facet of planned obsolescence, many companies have increasingly adopted the practice of tech\nlock-in – designing products and systems that require vendor-specific components, consumables, or\nsoftware, and sometimes even restricting or disabling functionality to ensure ongoing customer\ndependence. Classic examples include printer manufacturers requiring proprietary ink cartridges,\nsmartphone ecosystems that only accept certified accessories, and enterprise software platforms that\nlimit interoperability or export options. In the digital realm, cloud-based services and software-as-a-\nservice (SaaS) solutions often lock users into proprietary file formats, APIs, or user experiences,\nmaking it difficult and costly to migrate to alternative providers. Major vendors like Apple, Salesforce,\nand Amazon Web Services are well-known for creating tightly integrated ecosystems that discourage\nswitching by making data migration complex, costly, or incomplete. These strategies are further\nreinforced by contractual constraints, such as multi-year commitments, tiered pricing, and auto-\nrenewals, which add financial friction to any potential move.\nThis lock-in effect is exacerbated by the concepts of sunk costs and high conversion costs.\nOrganizations and individuals invest significant time, money, and training into a particular platform or\necosystem, making the prospect of switching even more daunting. The more customized and integrated\na solution becomes, the harder it is to leave – creating a cycle where users tolerate limitations or\nincremental upgrades rather than face the disruption and expense of change. As a result, tech lock-in\nnot only prolongs the viability of existing brands and products but also shapes the pace and direction of\ntechnological progress. But this tech lock-in also results in diminished innovation, and often a\nresignation to accepting inferior products due to lack of reasonable options.\nPage 120 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "section_path": ["Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability"], "page": 121, "content": "Ethics in Technology\nBy: Ed Weber\nEnter 3D printing, or additive manufacturing, which\nis the process of creating three-dimensional objects from\ndigital models by layering material – such as plastics,\nmetals, ceramics, or even biological substances – one\nlayer at a time. Since its inception in the 1980s, 3D\nprinting has rapidly evolved from a prototyping tool to a\ntransformative manufacturing technology. Today, it\nencompasses everything from nano-scale components to\nlarge-scale construction, including the printing of custom\nmedical implants, automotive parts, aerospace\ncomponents, and even entire homes. The technology’s\nversatility is evident in applications such as on-demand\nspare parts, personalized prosthetics, bio-printed tissues,\nand rapid prototyping for innovation across industries.\nFigure 17: 3D Printer creating\nprosthetic\nIf the full potential of 3D printing were unleashed, it\ncould disrupt several major status quos:\n• Traditional manufacturing and supply chains could be decentralized, with goods produced\nlocally or even at home, reducing the need for mass production and global shipping.\n• Proprietary replacement parts and consumables could be bypassed, undermining tech lock-in\nstrategies and empowering consumers to repair or modify products independently.\n• The barriers to entry for new inventors and small businesses would be dramatically lowered,\nfostering innovation and competition.\n• Entire industries, from construction to healthcare, could be transformed by the ability to\nproduce complex, customized items on demand.\n• Environmental impacts could be mitigated by reducing waste, transportation emissions, and\nexcess inventory.\nUltimately, widespread adoption of 3D printing has the potential to challenge both the economic\nand ethical foundations of planned obsolescence and tech lock-in, shifting power from centralized\nproducers to distributed creators and consumers.\nAR/VR & Tech Progress\nFrom the earliest days of photography and moving pictures, humanity has sought to capture,\nreplicate, and even enhance reality through technology. The journey from static images to immersive\nPage 121 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "section_path": ["Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability"], "page": 122, "content": "Ethics in Technology\nBy: Ed Weber\ndigital environments has been marked by continual innovation: stereoscopes in the 1800s introduced\nthree-dimensional imagery, while the 20th century brought movies, television, and eventually\nholography, each step deepening our ability to simulate and augment the world around us.\nBy the late 1960s, the first head-mounted display, “The Sword of Damocles,” laid the groundwork\nfor both virtual reality (VR) and augmented reality (AR), offering users computer-generated\ngraphics that blended with or replaced their sensory experience of the real world. Over the decades,\nmilestones such as flight simulators, interactive “artificial reality” labs, and data gloves paved the way\nfor today’s spatial computing – where AR and VR converge to create interactive, immersive\nenvironments that respond to users in real time.\nToday, AR and VR technologies are transforming a wide range of industries. In entertainment, VR\nheadsets and AR mobile games like Pokémon Go have redefined gaming and storytelling. Aerospace\nand automotive companies use VR for prototyping and immersive design, while AR assists with\nmaintenance and training. In education, students explore historical sites or conduct virtual science\nexperiments. The medical field employs VR for surgical training and pain management, and AR for\noverlaying critical information during procedures. Retailers offer virtual try-ons, architects visualize\nbuildings at scale, and therapists use immersive simulations for mental health treatments. Even\nmanufacturing and logistics benefit from AR overlays that guide workers or optimize warehouse\noperations.\nLooking ahead, the fusion of AR/VR with machine learning and other emerging technologies\npromises to disrupt even more status quos. Imagine:\n• Virtual prototyping of clothing or products, allowing users to “try before they print” with 3D\nprinting.\n• Entire public spaces transformed through AR, offering personalized information, art, or\nadvertising on demand.\n• Realistic VR/AR simulations for social skills training, therapy, or remote collaboration.\n• Educational experiences that adapt in real time to student performance, providing personalized\nlearning paths.\n• Remote medical consultations using AR overlays to guide both patient and provider.\n• Urban planning tools that let communities visualize and interact with proposed changes before\nthey happen.\n• Fully immersive remote workspaces, blurring the line between physical presence and digital\ncollaboration.\nPage 122 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "section_path": ["Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability"], "page": 123, "content": "Ethics in Technology\nBy: Ed Weber\nAs these technologies accelerate, several forward-looking questions arise that tie together themes\nfrom previous chapters:\n• How do we ensure equitable access to immersive technologies, so benefits aren’t limited to the\nprivileged?\n• In what ways might AR/VR amplify existing biases, privacy concerns, or misinformation\nchallenges?\n• What ethical responsibilities do creators and users have when virtual experiences become\nindistinguishable from reality?\n• How can we balance the immense potential for progress with the risks of addiction,\nsurveillance, or deepening digital divides?\n• Will the next wave of disruption redefine not just how we interact with technology, but how we\nunderstand identity, agency, and community itself?\nEmbracing technological progress means not only harnessing these tools for innovation and growth,\nbut also facing the ethical challenges they bring – ensuring that the future we build is both immersive\nand inclusive.\nErosion of Economic Sustainability\nEarlier in this chapter, we looked at obsolescence and how it could be considered either inevitable\nor planned. Now let’s consider our current economic models, specifically their sustainability. The most\nprevalent model globally is capitalism, which is defined by private ownership of resources and means\nof production, with goods and services exchanged in markets driven by supply and demand. Another\nmodel is socialism, where the state or community owns the means of production and aims to distribute\nwealth more equally. There are also mixed economies, which blend elements of both systems to\nvarying degrees. Each of these models has evolved to address the needs and challenges of their times,\nbut all are fundamentally shaped by the dynamics of labor, consumption, and resource allocation.\nThe accelerating rate of technological change, coupled with the disruptive nature of tech progress,\nposes significant challenges to the sustainability of these economic systems. Automation, artificial\nintelligence, and digital platforms are rapidly transforming industries, often rendering traditional jobs\nobsolete faster than new roles can be created. This disruption threatens the foundation of economic\nparticipation in models like capitalism, which rely on widespread employment and consumer\nspending to drive growth. To counteract these effects, societies have experimented with artificial\nsupports such as subsidies, retraining programs, and universal basic income (UBI). However, these\nmeasures often fail to address the root causes of disruption, serving as only temporary fixes rather than\nsustainable solutions.\nPage 123 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "section_path": ["Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability"], "page": 124, "content": "Ethics in Technology\nBy: Ed Weber\nIt is important to recognize that Maslow’s hierarchy of needs – a foundational theory of human\nmotivation – does not include “make a lot of money” as a requirement at any level. Instead, Maslow’s\npyramid begins with physiological needs (food, water, shelter), followed by safety, belonging, esteem,\nand ultimately self-actualization. While money can help secure basic needs, research shows that\nhappiness and fulfillment plateau once a certain level of financial security is achieved.\nIf we imagine a world where technological advancements – automation, AI, biotechnology, and\nbeyond – are harnessed intentionally to meet all of Maslow’s needs directly, the necessity for traditional\neconomic systems would need to be fundamentally reevaluated. In such a scenario, access to food,\nshelter, healthcare, education, and even opportunities for personal growth could be decoupled from\nemployment and income, challenging us to envision new models of economic and social organization\nthat prioritize human well-being over perpetual economic growth.\nTextbook Definitions – Technological Disruption and the Paradox of Progress\n• technological disruption – A fundamental change that occurs when a new technology radically\nalters the way consumers, businesses, or industries operate, often making existing products or\nprocesses obsolete.\n• digital revolution – The transition from analogue devices to digital technology, marking the\nbeginning of the Information Era and profoundly transforming societies and economies\nworldwide.\n• 3D printing – A manufacturing process that creates three-dimensional objects by layering\nmaterials according to digital models, enabling rapid prototyping and customized production.\n• spatial computing – The use of digital technology to interact with and manipulate physical\nspace, blending real and virtual environments for immersive experiences.\n• wearable and embedded devices – Electronic gadgets designed to be worn on the body or\nintegrated into physical objects, often to collect data or enhance functionality.\n• adoption curve – A graphical representation of how new technologies or products are adopted\nover time by different segments of a population.\n• planned obsolescence – The deliberate design of products with a limited useful life so that they\nwill need to be replaced, driving ongoing consumption.\n• inevitable obsolescence – The natural process by which products or technologies become\noutdated due to advancements and innovation, regardless of intentional design.\nPage 124 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "language": "en", "book": "en_ethics_in_tech_2022", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "section_path": ["Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability"], "page": 125, "content": "Ethics in Technology\nBy: Ed Weber\n• tech lock-in – A situation where users are dependent on a specific technology, vendor, or\necosystem, making it difficult or costly to switch to alternatives.\n• obsolescence – The process by which something becomes outdated or no longer used, typically\ndue to newer alternatives.\n• software-as-a-service (SaaS) – A software distribution model in which applications are hosted\nby a provider and accessed by users over the internet, typically via subscription.\n• sunk costs – Investments of time, money, or resources that cannot be recovered once made,\noften influencing future decision-making.\n• conversion costs – The expenses and effort required to switch from one product, service, or\nsystem to another.\n• virtual reality (VR) – A computer-generated simulation of a three-dimensional environment\nthat users can interact with, typically through specialized headsets and controllers.\n• augmented reality (AR) – Technology that overlays digital information or images onto the real\nworld, enhancing the user’s perception of their environment.\n• economic models – Frameworks or systems that describe how resources are allocated, goods\nand services are produced, and wealth is distributed within a society.\n• capitalism – An economic system characterized by private ownership of the means of\nproduction and operation for profit within competitive markets.\n• socialism – An economic system in which the means of production are owned and controlled\ncollectively or by the state, with an emphasis on equal distribution of wealth.\n• economic participation – The involvement of individuals or groups in the production,\ndistribution, and consumption of goods and services within an economy.\n• subsidies – Financial assistance provided by governments to support businesses, industries, or\nindividuals, often to promote economic activity or stabilize prices.\n• retraining programs – Initiatives designed to teach new skills to workers, especially those\ndisplaced by technological or economic changes.\n• universal basic income (UBI) – A policy proposal in which all citizens receive a regular,\nunconditional sum of money from the government to cover basic living expenses.\nPage 125 of 125 13. Technological Disruption and the Paradox of Progress"}
