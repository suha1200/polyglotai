‫اﳌﺤﺘﻮﻳﺎت‬

‫ﺗﻤﻬﻴﺪ اﻟﺴﻠﺴﻠﺔ‬
‫ﺷﻜﺮ وﺗﻘﺪﻳﺮ‬
‫‪ -١‬أﻳﺘﻬﺎ املﺮآة ﻋﲆ اﻟﺤﺎﺋﻂ‬
‫‪ -٢‬اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻟﻮﺣﻮش وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫‪ -٣‬ﻛﻞ ﻣﺎ ﻟﻪ ﻋﻼﻗﺔ ﺑﺎﻟﺒﴩ‬
‫‪ -٤‬أﻫﻲ ٍّ‬
‫ﺣﻘﺎ ﻣﺠﺮد آﻻت؟‬
‫‪ -٥‬اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫‪ -٦‬ﻻ َ‬
‫ﺗﻨﺲ )ﻋﻠﻢ( اﻟﺒﻴﺎﻧﺎت‬
‫‪ -٧‬اﻟﺨﺼﻮﺻﻴﺔ وﻏريﻫﺎ ﻣﻦ اﻟﻘﻀﺎﻳﺎ‬
‫ُ‬
‫ﻻﻣﺴﺌﻮﻟﻴﺔ اﻵﻻت واﻟﻘﺮارات ﻏري ا ُملﱪرة‬
‫‪-٨‬‬
‫‪ -٩‬اﻟﺘﺤﻴﺰ وﻣﻌﻨﻰ اﻟﺤﻴﺎة‬
‫‪ -١٠‬اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ‬
‫‪ -١١‬اﻟﺘﺤﺪﻳﺎت اﻟﺘﻲ ﺗُﻮاﺟﻪ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‬
‫‪ -١٢‬ﺗﺤﺪﱢي ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ :‬ﺣﻮل اﻷوﻟﻮﻳﺎت وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‬
‫ﻣﴪد املﺼﻄﻠﺤﺎت‬
‫ﻣﻼﺣﻈﺎت‬
‫ﻗﺮاءات إﺿﺎﻓﻴﺔ‬
‫املﺮاﺟﻊ‬

‫‪9‬‬
‫‪11‬‬
‫‪13‬‬
‫‪19‬‬
‫‪31‬‬
‫‪41‬‬
‫‪51‬‬
‫‪63‬‬
‫‪71‬‬
‫‪79‬‬
‫‪89‬‬
‫‪101‬‬
‫‪113‬‬
‫‪123‬‬
‫‪135‬‬
‫‪139‬‬
‫‪143‬‬
‫‪147‬‬

‫إﱃ أرﻧﻮ‬

‫ﲤﻬﻴﺪ اﻟﺴﻠﺴﻠﺔ‬

‫ﺗُﻘﺪﱢم »ﺳﻠﺴﻠﺔ املﻌﺎرف اﻷﺳﺎﺳﻴﺔ« اﻟﺘﻲ ﺗَﻨﴩﻫﺎ ﻣﺆﺳﺴﺔ »إم آي ﺗﻲ ﺑﺮﻳﺲ« ُﻛﺘﺒًﺎ ﻣﻮﺟﺰ ًة‬
‫وﺷﻜﻞ أﻧﻴﻖ‪ ،‬وﺣﺠ ٍﻢ ﺻﻐري ﻳُﻼﺋﻢ اﻟﺠﻴﺐ‪ ،‬ﺗُﻨﺎﻗِ ﺶ املﻮﺿﻮﻋﺎت اﻟﺘﻲ‬
‫ﺑﻠُﻐﺔ ﺟَ ﺰﻟﺔ ﺳﻬﻠﺔ اﻟﻔﻬﻢ‪،‬‬
‫ٍ‬
‫ﺗُﺜري اﻻﻫﺘﻤﺎم ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ‪ .‬و ﱠملﺎ ﻛﺎﻧﺖ ُﻛﺘﺐ ﻫﺬه اﻟﺴﻠﺴﻠﺔ ﻣﻦ ﺗﺄﻟﻴﻒ ﻣُﻔ ﱢﻜﺮﻳﻦ ﺑﺎرزﻳﻦ‪،‬‬
‫ً‬
‫ٍ‬
‫إﺿﺎﻓﺔ‬
‫ﻣﻮﺿﻮﻋﺎت ﺗﺘﻨﻮﱠع ﺑني املﺠﺎﻻت اﻟﺜﻘﺎﻓﻴﺔ واﻟﺘﺎرﻳﺨﻴﺔ‪،‬‬
‫ﻓﺈﻧﻬﺎ ﺗُﻘﺪﱢم آراء اﻟﺨﱪاء ﺑﺸﺄن‬
‫إﱃ اﻟﻌِ ﻠﻤﻴﺔ واﻟﺘﻘﻨﻴﺔ‪.‬‬
‫إﺷﺒﺎع َﻟﺤﻈﻲ ﻟﻠﻤﻌﻠﻮﻣﺎت‪ ،‬أﺿﺤﻰ ﻟﺪى اﻟﺠﻤﻴﻊ‬
‫ﰲ ِﻇﻞ ﻣﺎ ﻳَﺸﻴﻊ ﰲ ﻫﺬا اﻟﻌﴫ ﻣﻦ‬
‫ٍ‬
‫ٍ‬
‫ﺑﴪﻋﺔ وﺳﻬﻮﻟﺔ‪ ،‬وأﺻﺒﺢ ﻣﻦ‬
‫اﻟﻘﺪر ُة ﻋﲆ اﻟﻮﺻﻮل إﱃ اﻵراء واﻷﻓﻜﺎر واﻟﴩوح اﻟﺴﻄﺤﻴﺔ‬
‫ً‬
‫ﺑﻤﻜﺎن أن ﻳَﺤﻈﻰ املﺮء ﺑﺎملﻌﺮﻓﺔ اﻷﺳﺎﺳﻴﺔ اﻟﺘﻲ ﺗُ ﱢ‬
‫ﺻﺎدﻗﺎ ﻟﻠﻌﺎ َﻟﻢ؛ وﻣﺎ‬
‫ﻴﴪ َﻓﻬﻤً ﺎ‬
‫اﻟﺼﻌﻮﺑﺔ‬
‫ٍ‬
‫ﺗﻔﻌﻠﻪ ﻛﺘﺐ ﻫﺬه اﻟﺴﻠﺴﻠﺔ ﻫﻮ أﻧﻬﺎ ﺗُ ﱢ‬
‫ﺤﻘﻖ ذﻟﻚ اﻟﻐﺮض‪ .‬وﻛ ﱡﻞ ﻛﺘﺎب ﻣﻦ ﻫﺬه اﻟﻜﺘﺐ ا ُمل َ‬
‫ﺨﺘﴫة‬
‫ُﻴﴪة ﻟﻠﻮﺻﻮل إﱃ اﻷﻓﻜﺎر ا ُمل ﱠ‬
‫ً‬
‫وﺳﻴﻠﺔ ﻣ ﱠ‬
‫ﻌﻘﺪة‪ ،‬ﻣﻦ ﺧﻼل ﺗﺒﺴﻴﻂ املﻮاد ا ُمل ﱢ‬
‫ﺘﺨﺼﺼﺔ‬
‫ﻳُﻘﺪﱢم ﻟﻠﻘﺎرئ‬
‫ٍ‬
‫ﻟﻐري ا ُمل ﱢ‬
‫ﻃﺮﻳﻘﺔ ﻣُﻤﻜﻨﺔ‪.‬‬
‫وﴍح املﻮﺿﻮﻋﺎت املﻬﻤﺔ ﺑﺄﺑﺴﻂ‬
‫ﺨﺘﺼني‪ْ َ ،‬‬
‫ﺑﺮوس ﺗﻴﺪور‬
‫أﺳﺘﺎذ اﻟﻬﻨﺪﺳﺔ اﻟﺒﻴﻮﻟﻮﺟﻴﺔ وﻋﻠﻮم اﻟﻜﻤﺒﻴﻮﺗﺮ‬
‫»ﻣﻌﻬﺪ ﻣﺎﺳﺎﺗﺸﻮﺳﺘﺲ ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ«‬

‫ﺷﻜﺮ وﺗﻘﺪﻳﺮ‬

‫ﻻ ﻳﻌﺘﻤﺪ ﻫﺬا اﻟﻜﺘﺎب ﻋﲆ ﻋﻤﲇ اﻟﺨﺎص ﰲ ﻣﻮﺿﻮع أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻓﺤﺴﺐ‪،‬‬
‫ﺑﻞ ﻳﻌﻜﺲ املﻌﺮﻓﺔ واﻟﺨﱪة ﰲ ﻫﺬا املﺠﺎل ﺑﺄﻛﻤﻠِﻪ‪ .‬وﺳﻴﻜﻮن ﻣﻦ ا ُملﺴﺘﺤﻴﻞ إدراج ﺟﻤﻴﻊ‬
‫اﻷﺷﺨﺎص اﻟﺬﻳﻦ ﻧﺎﻗﺸﺘُﻬﻢ وﺗﻌ ﱠﻠ ُ‬
‫ﻤﺖ ﻣﻨﻬﻢ ﻋﲆ ﻣﺪار اﻟﺴﻨﻮات املﺎﺿﻴﺔ‪ ،‬ﻟﻜﻦ املﺠﺘﻤﻌﺎت‬
‫ذات اﻟﺼﻠﺔ واﻟﴪﻳﻌﺔ اﻟﻨﻤﻮ اﻟﺘﻲ أﻋﺮﻓﻬﺎ ﺗﻀﻢ ﺑﺎﺣﺜني ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺜﻞ‬
‫ﺟﻮاﻧﺎ ﺑﺮﻳﺴﻮن وﻟﻮك ﺳﺘﻴﻠﺰ‪ ،‬وزﻣﻼﺋﻲ اﻟﻔﻼﺳﻔﺔ ﰲ ﻣﺠﺎل اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻣﺜﻞ ﺷﺎﻧﻮن ﻓﺎﻟﻮر‬
‫وﻟﻮﺗﺸﻴﺎﻧﻮ ﻓﻠﻮرﻳﺪي‪ ،‬وأﻛﺎدﻳﻤﻴني ﻳﺴﻌَ ﻮن إﱃ اﻻﺑﺘﻜﺎر املﺴﺌﻮل ﰲ ﻫﻮﻟﻨﺪا واملﻤﻠﻜﺔ املﺘﺤﺪة‪،‬‬
‫ﻣﺜﻞ ﺑريﻧﺪ ﺳﺘﺎل ﰲ ﺟﺎﻣﻌﺔ دي ﻣﻮﻧﺘﻔﻮرت‪ ،‬وﺑﻌﺾ اﻷﺷﺨﺎص اﻟﺬﻳﻦ ا ْﻟﺘَ ُ‬
‫ﻘﻴﺖ ﺑﻬﻢ ﰲ ﻓﻴﻴﻨﺎ‪،‬‬
‫ﻣﺜﻞ روﺑﺮت ﺗﺮاﺑﻞ‪ ،‬وﺳﺎرة ﺳﺒﻴﻜﺮﻣﺎن‪ ،‬ووﻟﻔﺠﺎﻧﺞ )ﺑﻴﻞ( ﺑﺮاﻳﺲ‪ ،‬وزﻣﻼﺋﻲ اﻷﻋﻀﺎء ﰲ‬
‫ﻌﻨﻲ ﺑﺎﻟﺬﻛﺎء‬
‫اﻟﻬﻴﺌﺎت اﻻﺳﺘﺸﺎرﻳﺔ ذات اﻟﺘﻮﺟﱡ ﻬﺎت اﻟﺴﻴﺎﺳﻴﺔ‪ ،‬ﻓﺮﻳﻖ اﻟﺨﱪاء اﻟﺮﻓﻴﻊ املﺴﺘﻮى ا َمل ﱢ‬
‫اﻻﺻﻄﻨﺎﻋﻲ )املﻔﻮﺿﻴﺔ اﻷوروﺑﻴﺔ( واملﺠﻠﺲ اﻟﻨﻤﺴﺎوي ﻟﻠﺮوﺑﻮﺗﺎت واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻣﻦ‬
‫ِﺿﻤﻨﻬﻢ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﻻ اﻟﺤﴫ راﺟﺎ ﺷﺎﺗﻴﻼ‪ ،‬وﻓريﺟﻴﻨﻴﺎ دﻳﺞ ﻧﻮم‪ ،‬وﺟريوﻳﻦ ﻓﺎن دﻳﻦ‬
‫ﻫﻮﻓﻦ‪ ،‬وﺳﺎﺑني ﻛﻮﺳﻴﺠﻲ‪ ،‬وﻣﺎﺗﻴﺎس ﺷﻮﺗﺰ‪ .‬أو ﱡد ً‬
‫أﻳﻀﺎ أن أﺷﻜﺮ ﺑﺤﺮارة زاﻛﺎري ﺳﺘﻮرﻣﺰ‬
‫ﻟﻠﻤﺴﺎﻋﺪة ﰲ اﻟﺘﺪﻗﻴﻖ اﻟﻠﻐﻮي ﻟﻠﻜﺘﺎب وﺗﻨﺴﻴﻘﻪ‪ ،‬وﻟﻴﻨﺎ ﺳﺘﺎرﻛﻞ وإﻳﺰاﺑﻴﻞ واﻟﱰ ﻋﲆ دﻋﻤﻬﻤﺎ‬
‫ﰲ اﻟﺒﺤﺚ ﻋﻦ اﻷدﺑﻴﺎت‪.‬‬

‫اﻟﻔﺼﻞ اﻷول‬

‫أﻳﺘﻬﺎ اﳌﺮآة ﻋﲆ اﳊﺎﺋﻂ‬

‫اﻟﻀﺠﺔ واملﺨﺎوف اﻟﺘﻲ ﻳُﺜريﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ :‬أﻳﺘﻬﺎ املﺮآة ﻋﲆ اﻟﺤﺎﺋﻂ‪:‬‬
‫ﻣَ ﻦ اﻷذﻛﻰ ﰲ اﻟﻌﺎ َﻟﻢ؟‬
‫ﻋﻨﺪﻣﺎ أُﻋﻠﻨﺖ اﻟﻨﺘﺎﺋﺞ‪ ،‬اﻏﺮورﻗﺖ ﻋﻴﻨﺎ اﻟﻼﻋﺐ ﱄ ﺳﻴﺪول ﺑﺎﻟﺪﻣﻮع‪ .‬ﱠ‬
‫ﺣﻘﻖ »أﻟﻔﺎ ﺟﻮ«‪ ،‬وﻫﻮ‬
‫ﺑﺮﻧﺎﻣﺞ ذﻛﺎء اﺻﻄﻨﺎﻋﻲ ﻃ ﱠﻮ َرﺗْﻪ ﴍﻛﺔ »دﻳﺐ ﻣﺎﻳﻨﺪ« اﻟﺘﺎﺑﻌﺔ إﱃ ﺟﻮﺟﻞ‪ ،‬ﻓﻮ ًزا ‪ ١-٤‬ﰲ‬
‫ﻟﻌﺒﺔ »ﺟﻮ« )ﻟﻌﺒﺔ »ﺟﻮ« ﻫﻲ ﻟﻌﺒﺔ اﺳﱰاﺗﻴﺠﻴﺔ ﻗﺪﻳﻤﺔ ﻇﻬﺮت ﰲ اﻟﺼني وﻳُﺸﺎرك ﻓﻴﻬﺎ‬
‫ﻻﻋﺒﺎن اﺛﻨﺎن(‪ .‬ﺗﺎرﻳﺦ اﻟﺤﺪث‪ :‬ﻣﺎرس ‪ .٢٠١٦‬ﻗﺒﻞ ﻋﻘﺪَﻳﻦ ﻣﻦ اﻟﺰﻣﺎن‪ ،‬ﺧﴪ ﻻﻋﺐ اﻟﺸﻄﺮﻧﺞ‬
‫ﺟﺎري ﻛﺎﺳﺒﺎروف اﻟﺤﺎﺻﻞ ﻋﲆ ﻟﻘﺐ »ﺟﺮاﻧﺪ ﻣﺎﺳﱰ« )اﻷﺳﺘﺎذ اﻟﻜﺒري( أﻣﺎم اﻵﻟﺔ »دﻳﺐ‬
‫ﺑﻠﻮ«‪ ،‬واﻵن ﻓﺎز ﺑﺮﻧﺎﻣﺞ ﻛﻤﺒﻴﻮﺗﺮ ﻋﲆ ﺑﻄﻞ اﻟﻌﺎﻟﻢ ﻟﺜﻤﺎﻧﻲ ﻋﴩة ﻣﺮة؛ ﱄ ﺳﻴﺪول‪ ،‬ﰲ ﻟﻌﺒﺔ‬
‫ﻣ ﱠ‬
‫ُﻌﻘﺪة ﻛﺎن ﻳُﻨ َ‬
‫ِ‬
‫ﺣﺪﺳﻬﻢ‬
‫ﻈﺮ إﻟﻴﻬﺎ ﻋﲆ أﻧﻬﺎ ﻟﻌﺒﺔ ﻻ ﻳﻤﻜﻦ أن ﻳﻠﻌﺒﻬﺎ إﻻ اﻟﺒﴩ‪ ،‬ﺑﺎﺳﺘﺨﺪام‬
‫وﺗﻔﻜريﻫﻢ اﻻﺳﱰاﺗﻴﺠﻲ‪ .‬اﻷدﻫﻰ ﻣﻦ ذﻟﻚ أن اﻟﻜﻤﺒﻴﻮﺗﺮ ﻟﻢ ُ‬
‫ﻳﻔﺰ ﺑﺎﺗﺒﺎع اﻟﻘﻮاﻋﺪ ا ُملﻌﻄﺎة ﻟﻪ‬
‫ﻣﻦ ﻗِ ﺒَﻞ ا ُملﱪﻣﺠني‪ ،‬وإﻧﻤﺎ ﻋﻦ ﻃﺮﻳﻖ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ اﻟﻘﺎﺋﻢ ﻋﲆ املﻼﻳني ﻣﻦ ﻣﺒﺎرﻳﺎت »ﺟﻮ«‬
‫اﻟﺴﺎﺑﻘﺔ وﻋﲆ اﻟﻠﻌﺐ ﺿ ﱠﺪ ﻧﻔﺴﻪ‪ .‬ﰲ ﻣﺜﻞ ﻫﺬه اﻟﺤﺎﻟﺔ‪ ،‬ﻳُﻌِ ﺪ املﱪﻣﺠﻮن ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت‬
‫وﻳ ِ‬
‫ُﻨﺸﺌﻮن اﻟﺨﻮارزﻣﻴﺎت‪ ،‬وﻟﻜﻦ ﻻ ﻳُﻤﻜﻨﻬﻢ ﻣﻌﺮﻓﺔ اﻟﺘﺤ ﱡﺮﻛﺎت اﻟﺘﻲ ﺳﻴﺄﺗﻲ ﺑﻬﺎ اﻟﱪﻧﺎﻣﺞ‪.‬‬
‫ﻓﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺘﻌ ﱠﻠﻢ ﻣﻦ ﺗﻠﻘﺎء ﻧﻔﺴﻪ‪ .‬وﺑﻌﺪ ﻋﺪدٍ ﻣﻦ اﻟﺘﺤ ﱡﺮﻛﺎت ﻏري املﻌﺘﺎدة واملﻔﺎﺟﺌﺔ‪،‬‬
‫اﺿ ُ‬
‫ﻄ ﱠﺮ ﺑﻄﻞ اﻟﻌﺎﻟﻢ ﱄ إﱃ اﻻﻧﺴﺤﺎب )‪.(Borowiec 2016‬‬
‫إﻧﻪ إﻧﺠﺎز راﺋﻊ ﱠ‬
‫ﺣﻘ َﻘﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻟﻜﻨﻪ‪ ،‬ﻣﻊ ذﻟﻚ‪ ،‬ﻳُﺜري املﺨﺎوف ﰲ ﻗﻠﻮﺑﻨﺎ‪ .‬إﻧﻨﺎ‬
‫ﻣُﻌﺠﺒﻮن ﺑﺠﻤﺎل اﻟﺤﺮﻛﺎت‪ ،‬وﻟﻜﻨﻨﺎ ً‬
‫أﻳﻀﺎ ﺣﺰاﻧﻰ‪ ،‬ورﺑﻤﺎ ﺣﺘﻰ ﺧﺎﺋﻔﻮن‪ .‬ﻧﺄﻣُﻞ ﰲ أن ﺗﺴﺎﻋﺪﻧﺎ‬
‫ﺣﻠﻮل‬
‫أﻧﻈﻤﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻷﻛﺜﺮ ذﻛﺎءً ﰲ إﺣﺪاث ﺛﻮرة ﰲ اﻟﺮﻋﺎﻳﺔ اﻟﺼﺤﻴﺔ أو ﰲ إﻳﺠﺎد‬
‫ٍ‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻟﺠﻤﻴﻊ أﻧﻮاع املﺸﻜﻼت املﺠﺘﻤﻌﻴﺔ‪ ،‬وﻟﻜﻦ ﻳُﺮاودﻧﺎ اﻟﻘﻠﻖ ﻣﻦ أن ﺗﺴﻴﻄﺮ اﻵﻻت ﻋﲆ زﻣﺎم‬
‫أﻣﻮرﻧﺎ‪ .‬ﻓﻬﻞ ﺗﺴﺘﻄﻴﻊ اﻵﻻت أن ﺗﺘﻔﻮﱠق ﻋﻠﻴﻨﺎ وﺗﺘﺤ ﱠﻜﻢ ﻓﻴﻨﺎ؟ ﻫﻞ ﻻ ﻳﺰال اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻣﺠﺮد أداة‪ ،‬أم إﻧﻪ ﺳﻴُﺼﺒﺢ روﻳﺪًا روﻳﺪًا ﺳﻴﺪﻧﺎ ﻻ ﻣﺤﺎﻟﺔ؟ ﺗُﺬ ﱢﻛﺮﻧﺎ ﻫﺬه املﺨﺎوف ﺑﻜﻠﻤﺎت‬
‫»ﻫﺎل« ﻛﻤﺒﻴﻮﺗﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﻓﻴﻠﻢ اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ اﻟﺬي أﺧﺮﺟﻪ ﺳﺘﺎﻧﲇ ﻛﻮﺑﺮﻳﻚ‪:‬‬
‫»‪ :٢٠٠١‬ﻣﻠﺤﻤﺔ اﻟﻔﻀﺎء« )‪ :٢٠٠١‬ﺳﺒﻴﺲ أودﻳﴘ(‪ ،‬ﺣني ﻗﺎل ردٍّا ﻋﲆ اﻷﻣﺮ اﻟﺒﴩي »اﻓﺘﺢ‬
‫أﺑﻮاب املﺮﻛﺒﺔ اﻟﺼﻐرية«‪» :‬أﺧﴙ أﻧﻨﻲ ﻻ أﺳﺘﻄﻴﻊ أن أﻓﻌﻞ ذﻟﻚ ﻳﺎ دﻳﻒ‪ «.‬وإذا ﻟﻢ ﻳ ُﻜﻦ ﻫﻨﺎك‬
‫ﺧﻮف‪ ،‬ﻓﻘﺪ ﻳﻜﻮن ﻫﻨﺎك ﺷﻌﻮر ﺑﺎﻟﺤﺰن أو ﺧﻴﺒﺔ اﻷﻣﻞ‪ .‬ﻟﻘﺪ أﻃﺎح داروﻳﻦ وﻓﺮوﻳﺪ ﺑﺈﻳﻤﺎﻧﻨﺎ‬
‫ﺑﺘﻤﻴﱡﺰﻧﺎ‪ ،‬وﺑﺈﺣﺴﺎﺳﻨﺎ ﺑﺎﻟﺘﻔﻮﱡق‪ ،‬وأﻃﺎﺣﺎ ﺑﺄوﻫﺎم اﻟﺴﻴﻄﺮة اﻟﺘﻲ ﻳﻌﻴﺶ ﻓﻴﻬﺎ اﻟﺒﴩ؛ واﻵن ﺟﺎء‬
‫ً‬
‫ﴐﺑﺔ أﺧﺮى إﱃ ﺻﻮرة اﻟﺒﴩ ﻋﻦ ذواﺗﻬﻢ‪ .‬إذا ﻛﺎﻧﺖ اﻵﻟﺔ‬
‫دور اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴُﻮﺟﱢ ﻪ‬
‫ﺗﺴﺘﻄﻴﻊ اﻟﻘﻴﺎم ﺑﺬﻟﻚ‪ ،‬ﻓﻤﺎذا ﱠ‬
‫ﺗﺒﻘﻰ ﻟﻨﺎ؟ ﻣﺎذا ﻧﺤﻦ؟ ﻫﻞ ﻧﺤﻦ ﻣﺠ ﱠﺮد آﻻت؟ ﻫﻞ ﻧﺤﻦ آﻻت‬
‫ردﻳﺌﺔ‪ ،‬ﺑﻬﺎ اﻟﻜﺜري ﻣﻦ اﻟﻌﻴﻮب واﻷﺧﻄﺎء؟ وﻣﺎذا ﺳﻴﺤﺪُث ﻟﻨﺎ؟ ﻫﻞ ﺳﻨُﺼﺒﺢ ﻋﺒﻴﺪًا ﻟﻶﻻت؟ أو‬
‫ﻣﺎ ﻫﻮ أﺳﻮأ‪ ،‬ﻣﺠﺮد ﻣﺼﺪر ﻟﻠﻄﺎﻗﺔ‪ ،‬ﻛﻤﺎ ﰲ ﻓﻴﻠﻢ »املﺼﻔﻮﻓﺔ« )ذا ﻣﺎﺗﺮﻳﻜﺲ(؟‬
‫اﻟﺘﺄﺛري اﻟﺤﻘﻴﻘﻲ واﻟﻮاﺳﻊ اﻟﻨﻄﺎق ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫وﻟﻜﻦ إﻧﺠﺎزات اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻻ ﺗﻘﺘﴫ ﻋﲆ اﻷﻟﻌﺎب أو ﻋﺎ َﻟﻢ اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ‪ .‬ﻓﺎﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻳﺤﺪث اﻵن وﻫﻮ ﻣ ﱢ‬
‫ُﺘﻮﻏﻞ ﰲ ﻛﻞ ﻣﺎ ﺣﻮﻟﻨﺎ‪ ،‬وﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﻜﻮن ﻣُﻀﻤﱠ ﻨًﺎ ﻋﲆ ﻧﺤﻮ‬
‫ﻏري ﻣﺮﺋﻲ ﰲ أدواﺗﻨﺎ اﻟﻴﻮﻣﻴﺔ وﺑﻜﻮﻧﻪ ﺟﺰءًا ﻣﻦ اﻷﻧﻈﻤﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ ﱠ‬
‫املﻌﻘﺪة )‪Boddington‬‬
‫‪ .(2017‬وﻧﻈ ًﺮا إﱃ اﻟﻨﻤﻮ اﻟﻬﺎﺋﻞ ﻟﻘﺪرة اﻟﻜﻤﺒﻴﻮﺗﺮ‪ ،‬وإﺗﺎﺣﺔ اﻟﺒﻴﺎﻧﺎت )اﻟﻀﺨﻤﺔ( ﺑﺴﺒﺐ‬
‫وﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ واﻻﺳﺘﺨﺪام اﻟﻬﺎﺋﻞ ملﻠﻴﺎرات اﻟﻬﻮاﺗﻒ اﻟﺬﻛﻴﺔ‪ ،‬وﺷﺒﻜﺎت املﺤﻤﻮل‬
‫اﻟﴪﻳﻌﺔ‪ ،‬أﺣ َﺮ َز اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﺧﺎﺻﺔ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ ،‬ﺗﻘﺪﱡﻣً ﺎ ﻛﺒريًا‪ .‬وﻗﺪ ﻣ ﱠﻜ َﻦ ﻫﺬا‬
‫اﻟﺨﻮارزﻣﻴﺎت ﻣﻦ ﱢ‬
‫ﺗﻮﱄ اﻟﻌﺪﻳﺪ ﻣﻦ أﻧﺸﻄﺘﻨﺎ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ اﻟﺘﺨﻄﻴﻂ واﻟﻜﻼم واﻟﺘﻌ ﱡﺮف ﻋﲆ‬
‫ٍ‬
‫ﺗﻄﺒﻴﻘﺎت ﰲ اﻟﻌﺪﻳﺪ ﻣﻦ املﺠﺎﻻت‪ ،‬ﺑﻤﺎ‬
‫اﻟﻮﺟﻮه واﺗﺨﺎذ اﻟﻘﺮار‪ .‬ﻳﻤﺘﻠﻚ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﰲ ذﻟﻚ اﻟﻨﻘﻞ واﻟﺘﺴﻮﻳﻖ واﻟﺮﻋﺎﻳﺔ اﻟﺼﺤﻴﺔ واﻟﺘﻤﻮﻳﻞ واﻟﺘﺄﻣني واﻷﻣﻦ واﻟﺠﻴﺶ واﻟﻌﻠﻮم‬
‫واﻟﺘﻌﻠﻴﻢ واﻟﻌﻤﻞ املﻜﺘﺒﻲ واملﺴﺎﻋﺪة اﻟﺸﺨﺼﻴﺔ )ﻣﺜﻞ ﺟﻮﺟﻞ دوﺑﻠﻜﺲ ‪ 1‬واﻟﱰﻓﻴﻪ واﻟﻔﻨﻮن‬
‫)ﻣﺜﻞ اﺳﱰﺟﺎع املﻮﺳﻴﻘﻰ وﺗﺄﻟﻴﻔﻬﺎ( واﻟﺰراﻋﺔ‪ ،‬وﺑﺎﻟﻄﺒﻊ اﻟﺘﺼﻨﻴﻊ‪.‬‬
‫ﺗﺘ ﱡﻢ ﻋﻤﻠﻴﺎت إﻧﺸﺎء اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﺳﺘﺨﺪاﻣﻪ ﻟﺪى ﴍﻛﺎت ﺗﻜﻨﻮﻟﻮﺟﻴﺎ املﻌﻠﻮﻣﺎت‬
‫واﻹﻧﱰﻧﺖ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻟﻄﺎملﺎ اﺳﺘﺨﺪﻣﺖ ﺟﻮﺟﻞ اﻟﺬﻛﺎءَ اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﻣُﺤ ﱢﺮك اﻟﺒﺤﺚ‬
‫اﻟﺨﺎص ﺑﻬﺎ‪ .‬ﻛﻤﺎ ﻳﺴﺘﺨﺪم ﻓﻴﺴﺒﻮك اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻹﻋﻼﻧﺎت املﺴﺘﻬﺪﻓﺔ وإﺷﺎرات‬
‫‪14‬‬

‫أﻳﺘﻬﺎ املﺮآة ﻋﲆ اﻟﺤﺎﺋﻂ‬

‫اﻟﺼﻮر‪ .‬ﻛﺬﻟﻚ ﺗﺴﺘﺨﺪم ﻣﺎﻳﻜﺮوﺳﻮﻓﺖ وأ ِﺑﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﺗﺸﻐﻴﻞ ﻣﺴﺎﻋﺪَﻳﻬﻤﺎ‬
‫اﻟﺮﻗﻤﻴني‪ .‬ﻟﻜﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻻ ﻳﻘﺘﴫ ﻋﲆ ﻗﻄﺎع ﺗﻜﻨﻮﻟﻮﺟﻴﺎ املﻌﻠﻮﻣﺎت ﺑﻤﻌﻨﺎه اﻟﻀﻴﱢﻖ‪.‬‬
‫ﻓﻬﻨﺎك‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﻟﻜﺜري ﻣﻦ ُ‬
‫اﻟﺨﻄﻂ املﻠﻤﻮﺳﺔ‪ ،‬واﻟﺘﺠﺎرب ﰲ ﻣﺠﺎل اﻟﺴﻴﺎرات اﻟﺬاﺗﻴﺔ‬
‫اﻟﻘﻴﺎدة‪ .‬ﻓﻬﺬه اﻟﺘﻘﻨﻴﺔ ﺗﻌﺘﻤﺪ ً‬
‫أﻳﻀﺎ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻛﻤﺎ ﺗﺴﺘﺨﺪِم اﻟﻄﺎﺋﺮات دون‬
‫ﻃﻴﺎر اﻟﺬﻛﺎءَ اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻣﺜﻠﻬﺎ ﻣﺜﻞ اﻷﺳﻠﺤﺔ اﻟﺬاﺗﻴﺔ اﻟﺘﺸﻐﻴﻞ اﻟﺘﻲ ﻳﻤﻜﻦ أن ﺗﻘﺘُﻞ دون‬
‫ﱡ‬
‫ﺗﺪﺧ ٍﻞ ﺑﴩي‪ .‬ﺑﻞ إن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﺪ اﺳﺘُﺨﺪِم ﺑﺎﻟﻔﻌﻞ ﰲ اﺗﺨﺎذ اﻟﻘﺮار ﰲ املﺤﺎﻛﻢ‪.‬‬
‫ﻓﻔﻲ اﻟﻮﻻﻳﺎت املﺘﺤﺪة‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﺳﺘُﺨﺪم ﻧﻈﺎم »ﻛﻮﻣﺒﺎس« ﻟﻠﺘﻨﺒﱡﺆ ﺑﺎﻟﺬﻳﻦ ﻳُﺤﺘﻤَ ﻞ أن‬
‫ﻳُﻌﺎودوا ارﺗﻜﺎب اﻟﺠﺮاﺋﻢ‪ .‬ﻳﺪﺧﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫ﻧﻌﺘﱪﻫﺎ ﻋﻤﻮﻣً ﺎ‬
‫أﻳﻀﺎ ﰲ املﺠﺎﻻت اﻟﺘﻲ ِ‬
‫أﻛﺜﺮ ﺷﺨﺼﻴﺔ أو ﺣﻤﻴﻤﻴﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻤﻜﻦ ﻟﻶﻻت اﻵن ﻗﺮاءة وﺟﻮﻫﻨﺎ‪ ،‬ﻟﻴﺲ ﻓﻘﻂ‬
‫ﻟﻠﺘﻌ ﱡﺮف ﻋﻠﻴﻨﺎ‪ ،‬وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﻟﻘﺮاءة اﻧﻔﻌﺎﻻﺗﻨﺎ واﺳﱰداد ﺟﻤﻴﻊ املﻌﻠﻮﻣﺎت املﺮﺗﺒﻄﺔ ﺑﻨﺎ‪.‬‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺤﺪث اﻵن وﻫﻮ ﻣ ﱢ‬
‫ﻧﺤﻮ ﻏري‬
‫ُﺘﻮﻏﻞ ﰲ ﻛ ﱢﻞ ﻣﺎ ﺣﻮﻟﻨﺎ‪ ،‬وﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﻜﻮن ﻣُﻀﻤﱠ ﻨًﺎ ﻋﲆ ٍ‬
‫ﻣﺮﺋﻲ ﰲ أدواﺗﻨﺎ اﻟﻴﻮﻣﻴﺔ‪.‬‬

‫اﻟﺤﺎﺟﺔ إﱃ ﻣﻨﺎﻗﺸﺔ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ واملﺠﺘﻤﻌﻴﺔ‬
‫ﻳﻤﻜﻦ أن ﻳﻜﻮن ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﻔﻮاﺋﺪ‪ .‬وﻳﻤﻜﻦ اﺳﺘﺨﺪاﻣﻪ ﻟﺘﺤﺴني اﻟﺨﺪﻣﺎت‬
‫اﻟﻌﺎﻣﺔ واﻟﺘﺠﺎرﻳﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳُﻌﺪ اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻟﺼﻮر ﺷﻴﺌًﺎ ﻣﻔﻴﺪًا ﰲ اﻟﻄﺐ؛ إذ رﺑﻤﺎ‬
‫أﻣﺮاض ﻣﺜﻞ اﻟﴪﻃﺎن وﻣﺮض أﻟﺰﻫﺎﻳﻤﺮ‪ .‬وﻟﻜﻦ ﻣﺜﻞ ﻫﺬه اﻟﺘﻄﺒﻴﻘﺎت‬
‫ﻳﺴﺎﻋﺪ ﰲ ﺗﺸﺨﻴﺺ‬
‫ٍ‬
‫ُ‬
‫ُ‬
‫ً‬
‫ﻈﻬﺮ أﻳﻀﺎ ﻛﻴﻒ ﺗﺜري اﻟﺘﻘﻨﻴﺎت اﻟﺠﺪﻳﺪة ﺗﺨﻮﱡﻓﺎت أﺧﻼﻗﻴﺔ‪.‬‬
‫اﻟﻴﻮﻣﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗ ِ‬
‫ٍ‬
‫أﺳﺌﻠﺔ ﺣﻮل أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫واﺳﻤﺤﻮا ﱄ أن أُﻗﺪﱢم ﺑﻌﺾ اﻷﻣﺜﻠﺔ ﻋﲆ‬
‫ﻫﻞ ﻳﺠﺐ أن ﺗﺤﺘﻮي اﻟﺴﻴﺎرات اﻟﺬاﺗﻴﺔ اﻟﻘﻴﺎدة ﻋﲆ ﻗﻴﻮ ٍد أﺧﻼﻗﻴﺔ ﻣﻀﻤﱠ ﻨﺔ؟ وإذا ﻛﺎن‬
‫اﻷﻣﺮ ﻛﺬﻟﻚ‪ ،‬ﻓﻤﺎ ﻧﻮع ﻫﺬه اﻟﻘﻴﻮد وﻛﻴﻒ ﻳﻨﺒﻐﻲ ﺗﺤﺪﻳﺪﻫﺎ؟ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إذا واﺟﻬﺖ‬
‫ً‬
‫ﱠ‬
‫ﺑﻄﻔﻞ أو ﺗﺼﻄﺪم‬
‫ﻳﺘﻌني ﻋﻠﻴﻬﺎ ﻓﻴﻪ اﻻﺧﺘﻴﺎر ﺑني أن ﺗﺼﻄﺪم‬
‫ﻣﻮﻗﻔﺎ‬
‫ﺳﻴﺎرة ذاﺗﻴﺔ اﻟﻘﻴﺎدة‬
‫ٍ‬
‫ﺑﺠﺪار ﻹﻧﻘﺎذ ﺣﻴﺎة اﻟﻄﻔﻞ‪ ،‬وﻟﻜﻦ ﻣﻊ اﺣﺘﻤﺎل ﻗﺘﻞ را ِﻛﺒﻬﺎ‪ ،‬ﻓﻤﺎذا ﺗﺨﺘﺎر؟ وﻫﻞ ﻳﻨﺒﻐﻲ ﺗﺮﺧﻴﺺ‬
‫ٍ‬
‫اﻷﺳﻠﺤﺔ اﻟﻔﺘﱠﺎﻛﺔ اﻟﺬاﺗﻴﺔ اﻟﺘﺸﻐﻴﻞ ﻣﻦ اﻷﺳﺎس؟ ﻛﻢ ﻋﺪد اﻟﻘﺮارات اﻟﺘﻲ ﻧُﺮﻳﺪ ﺗﻔﻮﻳﻀﻬﺎ إﱃ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻣﺎ َ‬
‫اﻟﻘﺪْر اﻟﺬي ﻧُﻔﻮﱢﺿﻪ ﻣﻨﻬﺎ؟ وﻣَ ﻦ ﺳﻴﻜﻮن املﺴﺌﻮل ﻋﻨﺪﻣﺎ ﻳﺤﺪُث ﺧﻄﺄ‬
‫ﻣﺎ؟ ﰲ إﺣﺪى اﻟﻘﻀﺎﻳﺎ‪َ ،‬‬
‫وﺿ َﻊ اﻟﻘﻀﺎة ﺛﻘﺘﻬﻢ ﰲ ﺧﻮارزﻣﻴﺔ »ﻛﻮﻣﺒﺎس« أﻛﺜﺮ ﻣﻦ ﺛِﻘﺘﻬﻢ ﰲ‬
‫‪15‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻻﺗﻔﺎﻗﺎت اﻟﺘﻲ ﱠ‬
‫ﺗﻮﺻﻞ إﻟﻴﻬﺎ اﻟﺪﻓﺎع واﻻدﻋﺎء‪ 2 .‬ﻓﻬﻞ ﺳﻨﻌﺘﻤﺪ ﻛﺜريًا ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟‬
‫ﺗُﻌﺪ ﺧﻮارزﻣﻴﺔ »ﻛﻮﻣﺒﺎس« ً‬
‫أﻳﻀﺎ ﻣُﺜرية ﻟﻠﺠﺪل إﱃ ﺣ ﱟﺪ ﻛﺒري؛ ﻧﻈ ًﺮا إﱃ أن اﻷﺑﺤﺎث أﻇﻬﺮت‬
‫أن اﻷﺷﺨﺎص اﻟﺬﻳﻦ ﺗﻨﺒﱠﺄَت اﻟﺨﻮارزﻣﻴﺔ ﺑﺄﻧﻬﻢ ﺳﻴُﻌﻴﺪون ارﺗﻜﺎب اﻟﺠﺮاﺋﻢ وﻟﻜﻨﻬﻢ ﻟﻢ ﻳﻔﻌﻠﻮا‬
‫ﻛﺎﻧﺖ اﻟﻨﺴﺒﺔ اﻟﻜﱪى ﻣﻨﻬﻢ ِﻣﻦ اﻟﺴﻮد )‪ .(Fry 2018‬وﺑﺎﻟﺘﺎﱄ ﻳﻤﻜﻦ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن‬
‫ﻳُﻌ ﱢﺰز اﻟﺘﺤﻴﱡﺰ واﻟﺘﻤﻴﻴﺰ ﻏري اﻟﻌﺎدل‪ .‬وﻳﻤﻜﻦ أن ﺗﻨﺸﺄ ﻣﺸﻜﻼت ﻣُﻤﺎﺛﻠﺔ ﻣﻊ اﻟﺨﻮارزﻣﻴﺎت اﻟﺘﻲ‬
‫ٍ‬
‫ﺑﻘﺮارات ﺑﺸﺄن ﻃﻠﺒﺎت اﻟﺮﻫﻦ اﻟﻌﻘﺎري وﻃﻠﺒﺎت اﻟﺘﻘﺪﱡم ﻟﻠﻮﻇﺎﺋﻒ‪ .‬أو ﻓﻠﻨُﻔﻜﺮ ﻓﻴﻤﺎ‬
‫ﺗُﻮﴆ‬
‫ﻳُﺴﻤﻰ ﺑﺎﻟﴩﻃﺔ اﻟﺘﻨﺒﱡﺆﻳﺔ‪ :‬ﺗُﺴﺘﺨﺪَم اﻟﺨﻮارزﻣﻴﺎت ﻟﻠﺘﻨﺒﺆ ﺑﺎملﻜﺎن ا ُملﺤﺘﻤَ ﻞ ﻻرﺗﻜﺎب اﻟﺠﺮاﺋﻢ‬
‫)ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬أي ﻣﻨﻄﻘﺔ ﰲ املﺪﻳﻨﺔ( وﻣَ ﻦ ﻗﺪ ﻳﺮﺗﻜِﺒﻬﺎ‪ ،‬وﻟﻜﻦ ﻗﺪ ﺗﻜﻮن اﻟﻨﺘﻴﺠﺔ أن‬
‫ٍ‬
‫ﺑﺪرﺟﺔ أﻛﱪ‬
‫ﺗُﺴﺘﻬﺪَف ﻣﺠﻤﻮﻋﺎت اﺟﺘﻤﺎﻋﻴﺔ واﻗﺘﺼﺎدﻳﺔ أو ﻋِ ﺮﻗﻴﺔ ﻣُﻌﻴﱠﻨﺔ ﻟﻠﻤﺮاﻗﺒﺔ اﻟﴩﻃﻴﺔ‬
‫ﻣﻦ ﻏريﻫﻢ ﻣﻦ املﺠﻤﻮﻋﺎت‪ .‬وﻗﺪ ﺟَ َﺮت اﻻﺳﺘﻌﺎﻧﺔ ﺑﺎﻟﻔﻌﻞ ﺑﺎﻟﴩﻃﺔ اﻟﺘﻨﺒﱡﺆﻳﺔ ﰲ اﻟﻮﻻﻳﺎت‬
‫املﺘﺤﺪة‪ ،‬وﻛﻤﺎ ﻳُﻈﻬﺮ ﺗﻘﺮﻳﺮ ﺣﺪﻳﺚ ملﻨﻈﻤﺔ »أﻟﺠﻮرﻳﺬم ووﺗﺶ« )‪ ،(٢٠١٩‬ﻓﻘﺪ اﺳﺘُﻌني‬
‫ﺑﻬﺎ ً‬
‫أﻳﻀﺎ ﰲ أوروﺑﺎ‪ 3 .‬وﻏﺎﻟﺒًﺎ ﻣﺎ ﺗُﺴﺘﺨﺪَم ﺗﻘﻨﻴﺔ اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻟﻮﺟﻮه اﻟﻘﺎﺋﻤﺔ ﻋﲆ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻷﻏﺮاض ا ُملﺮاﻗﺒﺔ‪ ،‬وﻣِﻦ ﺛَﻢ ﻳﻤﻜﻦ أن ﺗُﺸ ﱢﻜﻞ اﻧﺘﻬﺎ ًﻛﺎ ﻟﺨﺼﻮﺻﻴﺔ اﻷﻓﺮاد‪ .‬ﻛﻤﺎ‬
‫ﺑﺸﻜﻞ أو َ‬
‫ﺑﺂﺧﺮ اﻟﺘﻨﺒﺆ ﺑﺎملﻴﻮل اﻟﺠﻨﺴﻴﺔ ﻟﺪى اﻷﻓﺮاد‪ .‬اﻷﻣﺮ ﻻ ﻳﺘﻄ ﱠﻠﺐ أي ﻣﻌﻠﻮﻣﺎت‬
‫ﻳُﻤﻜﻨﻬﺎ‬
‫ٍ‬
‫ﻣﻦ ﻫﺎﺗﻔﻚ أو أي ﺑﻴﺎﻧﺎت ﺑﻴﻮﻣﱰﻳﺔ )ﺑﻴﺎﻧﺎت املﻘﺎﻳﻴﺲ اﻟﺤﻴﻮﻳﺔ(‪ .‬وﺗﻘﻮم اﻵﻟﺔ ﺑﻌﻤﻠﻬﺎ ﻋﻦ ﺑُﻌﺪ‪.‬‬
‫وﻣِﻦ ﺛَﻢ ﻓﺈﻧﻨﺎ ﺑﺎﺳﺘﺨﺪام اﻟﻜﺎﻣريات املﻮﺟﻮدة ﰲ اﻟﺸﻮارع واﻷﻣﺎﻛﻦ اﻟﻌﺎﻣﺔ اﻷﺧﺮى‪ ،‬ﻳﻤﻜﻦ‬
‫اﻟﺘﻌ ﱡﺮف ﻋﻠﻴﻨﺎ و»ﻗﺮاءﺗﻨﺎ«‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ اﻟﺘﻌﺮف ﻋﲆ ﺣﺎﻟﺘﻨﺎ املﺰاﺟﻴﺔ‪ .‬وﻋﻦ ﻃﺮﻳﻖ ﺗﺤﻠﻴﻞ‬
‫ﺑﻴﺎﻧﺎﺗﻨﺎ‪ ،‬ﻳﻤﻜﻦ اﻟﺘﻨﺒﺆ ﺑﺼﺤﱠ ﺘﻨﺎ اﻟﻌﻘﻠﻴﺔ واﻟﺠﺴﺪﻳﺔ؛ دون ﻋِ ﻠﻤﻨﺎ ﺑﺬﻟﻚ‪ .‬وﻳﻤﻜﻦ ﻷﺻﺤﺎب اﻟﻌﻤﻞ‬
‫اﺳﺘﺨﺪام اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ُملﺮاﻗﺒﺔ أداﺋﻨﺎ‪ .‬وﻳﻤﻜﻦ ﻟﻠﺨﻮارزﻣﻴﺎت اﻟﻨﺸﻄﺔ ﻋﲆ وﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ‬
‫اﻻﺟﺘﻤﺎﻋﻲ أن ﺗﻨﴩ ﺧﻄﺎب اﻟﻜﺮاﻫﻴﺔ أو املﻌﻠﻮﻣﺎت اﻟﺨﻄﺄ؛ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻤﻜﻦ أن ﺗﻈﻬﺮ‬
‫ُ‬
‫وﺗﻨﴩ ﻣﺤﺘﻮًى ﺳﻴﺎﺳﻴٍّﺎ‪ .‬إﺣﺪى اﻟﺤﺎﻻت‬
‫أﺷﺨﺎص ﺣﻘﻴﻘﻴﱢني‬
‫اﻟﺮوﺑﻮﺗﺎت اﻟﺴﻴﺎﺳﻴﺔ ﰲ ﻫﻴﺌﺔ‬
‫ٍ‬
‫املﻌﺮوﻓﺔ ﻫﻲ ﺑﺮﻧﺎﻣﺞ اﻟﺪردﺷﺔ اﻵﱄ ﻣﻦ ﻣﺎﻳﻜﺮوﺳﻮﻓﺖ ﻟﻌﺎم ‪ ٢٠١٦‬ا ُملﺴﻤﻰ »ﺗﺎي« ا ُملﺼﻤﱠ ﻢ‬
‫ﻹﺟﺮاء ﻣﺤﺎدﺛﺎت ﻣَ ِﺮﺣﺔ ﻋﲆ ﺗﻮﻳﱰ‪ ،‬وﻟﻜﻦ ﻋﻨﺪﻣﺎ أﺻﺒﺢ أﻛﺜﺮ ذﻛﺎءً‪ ،‬ﺑﺪأ ﰲ ﻧﴩ ﺗﻐﺮﻳﺪات‬
‫ٍ‬
‫ِ‬
‫دﻻﻻت ﻋﻨﴫﻳﺔ‪ .‬ﻳﻤﻜﻦ ﻟﺒﻌﺾ ﺧﻮارزﻣﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﻧﺸﺎء ﺧﻄﺎﺑﺎت ﻓﻴﺪﻳﻮ‬
‫ﺗﺤﻤﻞ‬
‫‪4‬‬
‫ﺑﺸﻜﻞ ﻣُﻀ ﱢﻠﻞ ﺧﻄﺎﺑًﺎ ﻟﺒﺎراك أوﺑﺎﻣﺎ‪.‬‬
‫ﻛﺎذﺑﺔ‪ ،‬ﻣﺜﻞ اﻟﻔﻴﺪﻳﻮ اﻟﺬي ﺟﺮى إﻧﺸﺎؤه ﻟﻴُﺸﺒﻪ‬
‫ٍ‬
‫ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﻜﻮن اﻟﻨﻮاﻳﺎ ﻃﻴﺒﺔ‪ .‬وﻟﻜﻦ ﻫﺬه املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ ﻋﺎد ًة ﻣﺎ ﺗﻜﻮن ﻧﺘﺎﺋﺞ ﻏري‬
‫ﻣﻘﺼﻮدة ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ :‬ﻓﻤﻌﻈﻢ ﻫﺬه اﻟﺘﺄﺛريات‪ ،‬ﻣﺜﻞ اﻟﺘﺤﻴﱡﺰ أو ﺧﻄﺎب اﻟﻜﺮاﻫﻴﺔ‪ ،‬ﻟﻢ ﻳﻘﺼﺪﻫﺎ‬
‫ﻣﻄﻮرو اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ أو ﻣُﺴﺘﺨﺪﻣﻮﻫﺎ‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻫﻨﺎك ﺳﺆال ﻣﻬﻢ ﻳﺠﺐ ﻃﺮﺣﻪ داﺋﻤً ﺎ‪:‬‬
‫‪16‬‬

‫أﻳﺘﻬﺎ املﺮآة ﻋﲆ اﻟﺤﺎﺋﻂ‬

‫ﻣﻦ أﺟﻞ ﻣَ ﻦ ﻳﺘﻢ اﻟﺘﺤﺴني؟ ﻣﻦ أﺟﻞ اﻟﺤﻜﻮﻣﺔ أم ﻣﻦ أﺟﻞ املﻮاﻃﻨني؟ ﻣﻦ أﺟﻞ اﻟﴩﻃﺔ أم ﻣﻦ‬
‫أﺟﻞ ﻣَ ﻦ ﺗﺴﺘﻬﺪﻓﻬﻢ اﻟﴩﻃﺔ؟ ﻣﻦ أﺟﻞ ﺑﺎﺋﻊ اﻟﺘﺠﺰﺋﺔ أم ﻣﻦ أﺟﻞ اﻟﺰﺑﻮن؟ ﻣﻦ أﺟﻞ اﻟﻘﻀﺎة‬
‫أم ﻣﻦ أﺟﻞ ا ُملﺘﻬﻤني؟ ﻛﻤﺎ ﺗﻈﻬﺮ اﻷﺳﺌﻠﺔ املﺘﻌﻠﻘﺔ ﺑﺎﻟﺴﻠﻄﺔ واﻟﻬﻴﻤﻨﺔ‪ ،‬ﻛﺎﻟﺤﺎل ﻋﲆ ﺳﺒﻴﻞ‬
‫املﺜﺎل ﻋﻨﺪﻣﺎ ﻳﻘﺘﴫ ﺗﺸﻜﻴﻞ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻋﲆ ﻋﺪدٍ ﻗﻠﻴﻞ ﻣﻦ اﻟﴩﻛﺎت اﻟﻀﺨﻤﺔ )‪Nemitz‬‬
‫‪ .(2018‬ﻓﻤَ ﻦ اﻟﺬي ﻳُﺸﻜﻞ ﻣُﺴﺘﻘﺒﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟‬
‫ﻳُﻠﻘﻲ ﻫﺬا اﻟﺴﺆال اﻟﻀﻮء ﻋﲆ اﻷﻫﻤﻴﺔ اﻻﺟﺘﻤﺎﻋﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﺗﺘﻌ ﱠﻠﻖ‬
‫ﱡ‬
‫ﺑﺎﻟﺘﻐري اﻟﺘﻜﻨﻮﻟﻮﺟﻲ وﺗﺄﺛريه ﻋﲆ ﺣﻴﺎة اﻷﻓﺮاد‪ ،‬وﻟﻜﻨﻬﺎ ﺗﺘﻌﻠﻖ‬
‫أﺧﻼﻗﻴﱠﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫أﻳﻀﺎ ﺑﺎﻟﺘﺤﻮﻻت اﻟﺘﻲ ﺗﺤﺪُث ﰲ املﺠﺘﻤﻊ وﰲ اﻻﻗﺘﺼﺎد‪ .‬وﺗﺪ ﱡل ﻗﻀﺎﻳﺎ اﻟﺘﺤﻴﱡﺰ واﻟﺘﻤﻴﻴﺰ ﺑﺎﻟﻔﻌﻞ‬
‫ُﻐري ً‬
‫أﻳﻀﺎ اﻻﻗﺘﺼﺎد‪ ،‬وﺑﺎﻟﺘﺎﱄ رﺑﻤﺎ ﻳ ﱢ‬
‫ﻋﲆ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣُﺮﺗ ِﺒﻂ ﺑﺎملﺠﺘﻤﻊ‪ .‬وﻟﻜﻨﻪ ﻳ ﱢ‬
‫ُﻐري‬
‫ً‬
‫ووﻓﻘﺎ ملﻜﺎﰲ وﺑﺮﻳﻨﺠﻮﻟﻔﺴﻮن )‪ ،(٢٠١٤‬ﻓﻘﺪ دﺧﻠﻨﺎ ﻋﴫ اﻵﻟﺔ‬
‫اﻟﻬﻴﻜﻞ اﻻﺟﺘﻤﺎﻋﻲ ملﺠﺘﻤﻌﺎﺗﻨﺎ‪.‬‬
‫اﻟﺜﺎﻧﻲ‪ ،‬اﻟﺬي ﻻ ﺗﻜﻮن ﻓﻴﻪ اﻵﻻت ﻣُﻜﻤﻠﺔ ﻟﻠﺒﴩ ﻓﺤﺴﺐ‪ ،‬ﻛﻤﺎ ﰲ اﻟﺜﻮرة اﻟﺼﻨﺎﻋﻴﺔ‪ ،‬وﻟﻜﻨﻬﺎ أﻳﻀﺎً‬
‫ﺑﺪاﺋﻞ ﻟﻠﺒﴩ‪ .‬وﻧﻈ ًﺮا إﱃ أن املِ ﻬﻦ واﻷﻋﻤﺎل ﻣﻦ ﺟﻤﻴﻊ اﻷﻧﻮاع ﺳﺘﺘﺄﺛﺮ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫ﱠ‬
‫ﻳﺘﻐري ﻣﺠﺘﻤﻌﻨﺎ ﱡ‬
‫ﱠ‬
‫ﺗﻐريًا ﺟﺬرﻳٍّﺎ ﻣﻊ دﺧﻮل اﻟﺘﻘﻨﻴﺎت اﻟﺘﻲ وَﺻﻔﺖ ﰲ ﻳﻮ ٍم‬
‫املﺘﻮﻗﻊ أن‬
‫ﻓﻤﻦ‬
‫ﻣﻦ اﻷﻳﺎم ﰲ رواﻳﺎت اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ ﺣﻴﱠﺰ اﻟﻌﺎﻟﻢ اﻟﺤﻘﻴﻘﻲ )‪McAfee and Brynjolfsson‬‬
‫‪ .(2017‬ﻓﻤﺎ ﻫﻮ ﻣﺴﺘﻘﺒﻞ اﻟﻌﻤﻞ؟ وﻣﺎ ﻧﻮع اﻟﺤﻴﺎة اﻟﺘﻲ ﺳﻨﻌﻴﺸﻬﺎ ﻧﺤﻦ ﻋﻨﺪﻣﺎ ﻳﺘﻮﱃ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﻴﺎم ﺑﺎﻟﻮﻇﺎﺋﻒ؟ وﻣَ ﻦ »ﻧﺤﻦ«؟ وﻣَ ﻦ اﻟﺬي ﺳﻴﺴﺘﻔﻴﺪ ﻣﻦ ﻫﺬا اﻟﺘﺤﻮﱡل وﻣﻦ‬
‫ﺳﻴﺨﴪ؟‬
‫ﻫﺬا اﻟﻜﺘﺎب‬
‫اﺳﺘﻨﺎدًا إﱃ اﻹﻧﺠﺎزات ا ُملﺬﻫﻠﺔ اﻟﺘﻲ ﺗﻢ ﺗﺤﻘﻴﻘﻬﺎ‪ ،‬ﻓﻬﻨﺎك اﻟﻜﺜري ﻣﻦ اﻟﻀﺠﺔ ا ُملﺜﺎرة ﺣﻮل اﻟﺬﻛﺎء‬
‫ٍ‬
‫ﻣﺠﻤﻮﻋﺔ واﺳﻌﺔ ﻣﻦ ﻣﺠﺎﻻت املﻌﺮﻓﺔ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻳُﺴﺘﺨﺪَم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻟﻔﻌﻞ ﰲ‬
‫واملﻤﺎرﺳﺎت اﻟﺒﴩﻳﺔ‪ .‬وﻗﺪ أﺛﺎرت اﻷوﱃ ﺗﻜﻬﱡ ٍ‬
‫ﻨﺎت ﺟﺎﻣﺤﺔ ﺣﻮل ﻣﺴﺘﻘﺒﻞ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬ﻛﻤﺎ‬
‫ً‬
‫ٍ‬
‫ً‬
‫إﺣﺴﺎﺳﺎ‬
‫ﻓﻠﺴﻔﻴﺔ ﻣﻬﻤﱠ ﺔ ﺣﻮل ﻣﻌﻨﻰ أن ﺗﻜﻮن إﻧﺴﺎﻧًﺎ‪ .‬ﺑﻴﻨﻤﺎ ﺧﻠﻘﺖ اﻟﺜﺎﻧﻴﺔ‬
‫ﻣﻨﺎﻗﺸﺎت‬
‫أﺛﺎرت‬
‫ﺑﺎﻹﻟﺤﺎح ﻣﻦ ﺟﺎﻧﺐ اﻷﺧﻼﻗﻴني وﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت ﻟﻀﻤﺎن أن ﺗُﻔﻴﺪﻧﺎ ﻫﺬه اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ً‬
‫ﺑﺪﻻ‬
‫ﻣﻦ أن ﺗﺨﻠﻖ أﻣﺎم اﻷﻓﺮاد واملﺠﺘﻤﻌﺎت ﺗﺤﺪﻳﺎت ﻻ ﻳُﻤﻜﻨﻬﻢ اﻟﺘﻐ ﱡﻠﺐ ﻋﻠﻴﻬﺎ‪ .‬وﺗُﻌﺪ ﻫﺬه املﺨﺎوف‬
‫ً‬
‫ﻋﻤﻠﻴﺔ وإﻟﺤﺎﺣً ﺎ‪.‬‬
‫اﻷﺧرية أﻛﺜﺮ‬

‫‪17‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﱡ‬
‫ريه ﻋﲆ ﺣﻴﺎة اﻷﻓﺮاد‪ ،‬وﻟﻜﻨﻬﺎ ﺗﺘﻌﻠﻖ‬
‫ﺗﺘﻌﻠﻖ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺑﺎﻟﺘﻐري اﻟﺘﻜﻨﻮﻟﻮﺟﻲ وﺗﺄﺛ ِ‬
‫ً‬
‫أﻳﻀﺎ ﺑﺎﻟﺘﺤﻮﱡﻻت اﻟﺘﻲ ﺗﺤﺪث ﰲ املﺠﺘﻤﻊ وﰲ اﻻﻗﺘﺼﺎد‪.‬‬

‫ﻳﺘﻨﺎول ﻫﺬا اﻟﻜﺘﺎب‪ ،‬اﻟﺬي ﻛﺘﺒَﻪ ﻓﻴﻠﺴﻮف أﻛﺎدﻳﻤﻲ ﻟﺪَﻳﻪ ً‬
‫أﻳﻀﺎ ﺧﱪة ﰲ ﺗﻘﺪﻳﻢ املﺸﻮرة‬
‫ﻣﻦ أﺟﻞ وﺿﻊ اﻟﺴﻴﺎﺳﺎت‪ ،‬ﻛِﻼ اﻟﺠﺎﻧﺒَني؛ ﻓﻬﻮ ﻳﺘﻌﺎﻣﻞ ﻣﻊ اﻷﺧﻼﻗﻴﺎت ﻋﲆ ﻫﺬه املﺴﺘﻮﻳﺎت‬
‫ﻛﺎﻓﺔ‪ .‬وﻳﻬﺪف إﱃ إﻋﻄﺎء اﻟﻘﺎرئ ﻧﻈﺮ ًة ﻋﺎﻣﺔ ﺟﻴﺪة ﻋﲆ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ ﻳُﺜريﻫﺎ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﺪءًا ﻣﻦ اﻟﴪدﻳﺎت املﺆﺛﺮة ﺣﻮل ﻣﺴﺘﻘﺒﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻷﺳﺌﻠﺔ اﻟﻔﻠﺴﻔﻴﺔ‬
‫ً‬
‫واﻧﻄﻼﻗﺎ إﱃ اﻟﻘﻀﺎﻳﺎ اﻷﺧﻼﻗﻴﺔ ا ُملﺘﻌﻠﻘﺔ ﺑﺎملﺴﺌﻮﻟﻴﺔ واﻟﺘﺤﻴﱡﺰ‬
‫ﺣﻮل ﻃﺒﻴﻌﺔ اﻹﻧﺴﺎن وﻣُﺴﺘﻘﺒﻠﻪ‪،‬‬
‫وﻛﻴﻔﻴﺔ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ املﺴﺎﺋﻞ اﻟﻌﻤﻠﻴﺔ اﻟﻮاﻗﻌﻴﺔ اﻟﺘﻲ أﺛﺎرﺗﻬﺎ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻋﻦ ﻃﺮﻳﻖ وﺿﻊ‬
‫اﻟﺴﻴﺎﺳﺎت؛ ﻻ ﺳﻴﻤﺎ إذا ﻛﺎن ذﻟﻚ ﻗﺒﻞ ﻓﻮات اﻷوان‪.‬‬
‫ﻟﻜﻦ ﻣﺎذا ﺳﻴﺤﺪُث إذا »ﻓﺎت اﻷوان«؟ ﺑﻌﺾ اﻟﺴﻴﻨﺎرﻳﻮﻫﺎت ﻣﺘﺸﺎﺋﻤﺔ وﻣﺘﻔﺎﺋﻠﺔ ﰲ اﻟﻮﻗﺖ‬
‫ﻧﻔﺴﻪ‪ .‬اﺳﻤﺤﻮا ﱄ أن أﺑﺪأ ﺑﺒﻌﺾ اﻷﺣﻼم واﻟﻜﻮاﺑﻴﺲ ﺣﻮل ﻣﺴﺘﻘﺒﻞ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬واﻟﴪدﻳﺎت‬
‫املﺆﺛﺮة اﻟﺘﻲ ﺗﺒﺪو‪ ،‬وﻟﻮ ﻟﻠﻮﻫﻠﺔ اﻷوﱃ ﻋﲆ اﻷﻗﻞ‪ ،‬ذات ِﺻ ٍﻠﺔ ﺑﺘﻘﻴﻴﻢ اﻟﻔﻮاﺋﺪ واملﺨﺎﻃﺮ ا ُملﺤﺘﻤَ ﻠﺔ‬
‫ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬

‫‪18‬‬

‫اﻟﻔﺼﻞ اﻟﺜﺎﻧﻲ‬

‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻟﻮﺣﻮش وﳖﺎﻳﺔ اﻟﻌﺎﱂ‬
‫ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ وﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‬
‫أدﱠت اﻟﻀﺠﺔ ا ُملﺤﻴﻄﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ ﻇﻬﻮر ﺟﻤﻴﻊ أﻧﻮاع اﻟﺘﻜﻬﱡ ﻨﺎت ﺣﻮل ﻣﺴﺘﻘﺒﻞ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻣﺴﺘﻘﺒﻞ ﻣﺎ ﺳﻴﻜﻮن ﻋﻠﻴﻪ اﻹﻧﺴﺎن‪ .‬إن إﺣﺪى اﻷﻓﻜﺎر اﻟﺸﺎﺋﻌﺔ‪ ،‬واﻟﺘﻲ‬
‫ﺗﺘﻜ ﱠﺮر ﻛﺜريًا ﰲ وﺳﺎﺋﻞ اﻹﻋﻼم وﰲ اﻟﻨﻘﺎﺷﺎت اﻟﻌﺎﻣﺔ ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﻞ ﻳﻨﴩﻫﺎ‬
‫ً‬
‫أﻳﻀﺎ ﺧﱪاء اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ املﺆﺛﱢﺮون اﻟﺬﻳﻦ ﻳُﻄﻮﱢرون ﺗﻘﻨﻴﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺜﻞ إﻳﻠﻮن‬
‫وﺑﺸﻜﻞ أﻛﺜﺮ ﻋﻤﻮﻣﻴﺔ‪ ،‬ﻓﻜﺮة أن اﻵﻻت‬
‫ﻣﺎﺳﻚ وراي ﻛﻮرزواﻳﻞ‪ ،‬ﻫﻲ ﻓﻜﺮة اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ‪،‬‬
‫ٍ‬
‫ﺳﺘُﺴﻴﻄﺮ ﻋﻠﻴﻨﺎ‪ ،‬وﺗﺴﺘﻌ ِﺒﺪﻧﺎ وﻟﻴﺲ اﻟﻌﻜﺲ‪ .‬ﺑﺎﻟﻨﺴﺒﺔ إﱃ اﻟﺒﻌﺾ‪ ،‬ﻫﺬا ﺣﻠﻢ؛ وﺑﺎﻟﻨﺴﺒﺔ إﱃ‬
‫اﻟﻜﺜريﻳﻦ‪ ،‬ﻫﺬا ﻛﺎﺑﻮس‪ .‬وﻫﻨﺎك ﻣَ ﻦ ﻳ َﺮون أﻧﻪ ﺣﻠﻢ وﻛﺎﺑﻮس ﰲ اﻟﻮﻗﺖ ﻧﻔﺴﻪ‪.‬‬
‫ﻓﻜﺮة اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ ﻫﻲ أن اﻵﻻت ﺳﺘﺘﻔﻮﱠق ﻋﲆ اﻟﺬﻛﺎء اﻟﺒﴩي‪ .‬وﻫﻲ ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﺮﺗﺒﻂ‬
‫ً‬
‫ووﻓﻘﺎ ﻟﻨﻴﻚ ﺑﻮﺳﱰوم )‪،(٢٠١٤‬‬
‫ﺑﻔﻜﺮة اﻧﻔﺠﺎر اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺘﻔ ﱡﺮد اﻟﺘﻜﻨﻮﻟﻮﺟﻲ‪.‬‬
‫ﻣﺄزق ﻳُﻤﺎﺛﻞ ذﻟﻚ اﻟﺬي وﻗﻌﺖ ﻓﻴﻪ اﻟﻐﻮرﻳﻼ‪ ،‬اﻟﺘﻲ ﻳﻌﺘﻤﺪ ﻣﺼريﻫﺎ اﻟﻴﻮم ﻋﻠﻴﻨﺎ‬
‫ﺳﻨﻘﻊ ﰲ‬
‫ٍ‬
‫َ‬
‫ﻃﺮﻳﻘني ﻋﲆ اﻷﻗﻞ ﻟﺒﻠﻮغ اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ وﻣﺎ ﻳُﺴﻤﱠ ﻰ أﺣﻴﺎﻧًﺎ ﺑﺎﻧﻔﺠﺎر‬
‫ﺑﺸﻜﻞ ﻛﺎﻣﻞ‪ .‬إﻧﻪ ﻳﺮى‬
‫ٍ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬أﺣﺪﻫﻤﺎ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺳﻮف ﻳُﻄﻮﱢر ﺗﺤﺴﻴﻨًﺎ ذاﺗﻴٍّﺎ ﺗﻜﺮارﻳٍّﺎ؛‬
‫ٍ‬
‫ﻧﺴﺨﺔ ﻣ ﱠ‬
‫ُﺤﺴﻨﺔ ﻣﻦ ﻧﻔﺴﻪ‪ ،‬واﻟﺘﻲ ﺑﺪورﻫﺎ ﺗُﺼﻤﱢ ﻢ‬
‫إذ ﻳﺴﺘﻄﻴﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺼﻤﻴﻢ‬
‫ً‬
‫ﻧﺴﺨﺔ أﻛﺜﺮ ذﻛﺎءً ﻣﻦ ﻧﻔﺴﻬﺎ‪ ،‬وﻫﻜﺬا دواﻟﻴﻚ‪ .‬أﻣﺎ اﻟﻄﺮﻳﻖ َ‬
‫اﻵﺧﺮ ﻓﻬﻮ ﻣﺤﺎﻛﺎة اﻟﺪﻣﺎغ ﺑﺎﻟﻜﺎﻣﻞ‬
‫أو ﺗﺤﻤﻴﻠﻪ‪ :‬دﻣﺎغ ﺑﻴﻮﻟﻮﺟﻲ ﻳُﻤﻜِﻦ ﻣﺴﺤﻪ ﺿﻮﺋﻴٍّﺎ ُ‬
‫إﻧﺘﺎﺟﻪ ﰲ‬
‫وﺻﻨﻊ ﻧﻤﻮذج ﻟﻪ‪ ،‬ﺛﻢ إﻋﺎدة‬
‫ِ‬
‫ٍ‬
‫ﻣﻜﻮﻧﺎت ﺑﺮﻣﺠﻴﺔ ذﻛﻴﱠﺔ وﻣِﻦ ﺧﻼﻟﻬﺎ‪ .‬ﻳﺘﻢ ﺑﻌﺪ ذﻟﻚ ﺗﻮﺻﻴﻞ ﻫﺬه ا ُملﺤﺎﻛﺎة ﻟﻠﺪﻣﺎغ اﻟﺒﻴﻮﻟﻮﺟﻲ‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻧﻔﺠﺎر ﰲ اﻟﺬﻛﺎء ﻏري اﻟﺒﴩي‪ .‬ﺣﺘﻰ‬
‫ﺑﺠﺴﻢ إﻧﺴﺎن آﱄ‪ .‬وﺳﺘﺆدي ﻣﺜﻞ ﻫﺬه اﻟﺘﻄﻮﱡرات إﱃ‬
‫ٍ‬
‫ً‬
‫ﻓﺮﻳﻘﺎ ﻣﺎ ﻳُﻤﻜِﻨﻪ إﻧﺸﺎء ذﻛﺎء اﺻﻄﻨﺎﻋﻲ ﻳُﺼﺒﺢ‬
‫إن ﻣﺎﻛﺲ ﺗﺠﻤﺎرك )‪ (٢٠١٧‬ﻳﺘﺨﻴﻞ أن‬
‫ﰲ ﻣﻨﺘﻬﻰ اﻟﻘﻮة ﺑﺤﻴﺚ ﻳﺴﺘﻄﻴﻊ إدارة اﻟﻜﻮﻛﺐ‪ .‬وﻳﻜﺘﺐ ﻳﻮﻓﺎل ﻫﺮاري ﻋﻦ ﻋﺎ َﻟ ٍﻢ ﻟﻢ ﻳﻌُ ﺪ‬
‫ﻓﻴﻪ اﻟﺒﴩ ﻳﺴﻴﻄﺮون‪ ،‬وﻟﻜﻨﻬﻢ ﻳﻌﺒﺪون اﻟﺒﻴﺎﻧﺎت وﻳﺜﻘﻮن ﰲ ﻗﺪرة اﻟﺨﻮارزﻣﻴﺎت ﻋﲆ اﺗﺨﺎذ‬
‫ﻗﺮاراﺗﻬﻢ‪ .‬وﺑﻌﺪ اﻧﻬﻴﺎر ﻛ ﱢﻞ أوﻫﺎم اﻹﻧﺴﺎﻧﻴني واملﺆﺳﺴﺎت اﻟﻠﻴﱪاﻟﻴﺔ‪ ،‬ﻟﻦ ﻳﺒﻘﻰ ﻟﻠﺒﴩ إﻻ أن‬
‫ﻳﺤﻠﻤﻮا ﺑﺎﻻﻧﺪﻣﺎج ﰲ ﱡ‬
‫ﺗﺪﻓﻖ اﻟﺒﻴﺎﻧﺎت‪ .‬ﻳﺴري اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﻣﺴﺎره اﻟﺨﺎص‪» ،‬اﻟﺬﻫﺎب‬
‫إﻧﺴﺎن أن ﻳﺘﺒﻌﻪ« )‪Harari‬‬
‫إﻧﺴﺎن ﻣﻦ ﻗﺒﻞ؛ وإﱃ ﺣﻴﺚ ﻻ ﻳﻤﻜﻦ ﻷي‬
‫إﱃ ﺣﻴﺚ ﻟﻢ ﻳﺬﻫﺐ أي‬
‫ٍ‬
‫ٍ‬
‫‪.(2015, 393‬‬
‫ﻃﺎ ً‬
‫ﺗﺮﺗﺒﻂ ﻓﻜﺮة اﻧﻔﺠﺎر اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ارﺗﺒﺎ ً‬
‫وﺛﻴﻘﺎ ﺑﻔﻜﺮة »اﻟﺘﻔ ﱡﺮد اﻟﺘﻜﻨﻮﻟﻮﺟﻲ«‪:‬‬
‫ﻟﺤﻈﺔ ﰲ ﺗﺎرﻳﺦ اﻟﺒﴩﻳﺔ ﺳﻴُﺤﺪِث ﻓﻴﻬﺎ اﻟﺘﻘﺪﱡم اﻟﺘﻜﻨﻮﻟﻮﺟﻲ اﻟﻬﺎﺋﻞ ﺗﻐﻴريًا دراﻣﺎﺗﻴﻜﻴٍّﺎ ﺑﺤﻴﺚ‬
‫ﻻ ﻧﻌﻮد ﻧﺴﺘﻮﻋِﺐ ﻣﺎ ﻳﺤﺪث و»ﺗﻨﺘﻬﻲ اﻟﺸﺌﻮن اﻹﻧﺴﺎﻧﻴﺔ ﻛﻤﺎ ﻧﻔﻬﻤﻬﺎ اﻟﻴﻮم« )‪Shanahan‬‬
‫‪ .(2015, xv‬ﰲ ﻋﺎم ‪ ،١٩٦٥‬ﺗﻜﻬﱠ َﻦ ﻋﺎﻟﻢ اﻟﺮﻳﺎﺿﻴﺎت اﻟﱪﻳﻄﺎﻧﻲ إﻳﺮﻓﻴﻨﺞ ﺟﻮن ﺟﻮد‬
‫ﺑﺂﻟﺔ ﻓﺎﺋﻘﺔ اﻟﺬﻛﺎء ﺗُﺼﻤﱢ ﻢ ٍ‬
‫آﻻت أﻓﻀﻞ؛ وﰲ اﻟﺘﺴﻌﻴﻨﻴﺎت‪ ،‬رأى ﻣﺆﻟﻒ اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ وﻋﺎﻟﻢ‬
‫اﻟﻜﻤﺒﻴﻮﺗﺮ ﻓريﻧﻮر ﻓﻴﻨﺞ أن ﻫﺬا ﺳﻴﻌﻨﻲ ﻧﻬﺎﻳﺔ ﻋﴫ اﻹﻧﺴﺎن‪ .‬وﻗﺪ اﻗﱰح راﺋﺪ ﻋﻠﻢ اﻟﻜﻤﺒﻴﻮﺗﺮ‬
‫ﺟﻮن ﻓﻮن ﻧﻴﻮﻣﺎن ﺑﺎﻟﻔﻌﻞ اﻟﻔﻜﺮة ﰲ ﺧﻤﺴﻴﻨﻴﺎت اﻟﻘﺮن اﻟﻌﴩﻳﻦ‪ .‬وﺗﺒﻨﱠﻰ راي ﻛﻮرزواﻳﻞ‬
‫ﱠ‬
‫ﺟﻨﺐ ﻣﻊ أﺟﻬﺰة‬
‫وﺗﻮﻗﻊ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺟﻨﺒًﺎ إﱃ‬
‫)‪ (٢٠٠٥‬ﻣﺼﻄﻠﺢ »اﻟﺘﻔ ﱡﺮد«‬
‫ٍ‬
‫ٍ‬
‫ﻧﻘﻄﺔ ﻳﻜﻮن ﻓﻴﻬﺎ‬
‫اﻟﻜﻤﺒﻴﻮﺗﺮ وﻋﻠﻢ اﻟﻮراﺛﺔ وﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﻨﺎﻧﻮ وﻋﻠﻢ اﻟﺮوﺑﻮﺗﺎت‪ ،‬ﺳﻴﺆدي إﱃ‬
‫ذﻛﺎءُ اﻵﻟﺔ أﻗﻮى ﻣﻦ ﻛ ﱢﻞ اﻟﺬﻛﺎء اﻟﺒﴩي ﻣُﺠﺘﻤﻌً ﺎ‪ ،‬وﻳﻨﺪﻣﺞ ﻋﻨﺪﻫﺎ اﻟﺬﻛﺎء اﻟﺒﴩي وذﻛﺎء اﻵﻟﺔ‬
‫ﰲ اﻟﻨﻬﺎﻳﺔ‪ .‬وﺳﻮف ﻳﺘﺠﺎوز اﻟﺒﴩ ﺣﺪود أﺟﺴﺎﻣﻬﻢ اﻟﺒﻴﻮﻟﻮﺟﻴﺔ‪ .‬وﻛﻤﺎ ﺟﺎء ﰲ ﻋﻨﻮان ﻛﺘﺎﺑﻪ‪:‬‬
‫»اﻟﺘﻔ ﱡﺮد ﻗﺮﻳﺐ«‪ .‬وﻫﻮ ﻳﻌﺘﻘﺪ أن ﻫﺬا ﺳﻴﺤﺪث ﺣﻮاﱄ ﻋﺎم ‪.٢٠٤٥‬‬
‫ﻟﻴﺲ ﻟﻬﺬه اﻟﻘﺼﺔ ﺑﺎﻟﴬورة ﻧﻬﺎﻳﺔ ﺳﻌﻴﺪة‪ :‬ﻓﻔﻲ رأي ﺑﻮﺳﱰوم وﺗﺠﻤﺎرك وآﺧﺮﻳﻦ‪،‬‬
‫ﺛﻤﱠ ﺔ »ﻣﺨﺎﻃﺮ وﺟﻮدﻳﺔ« ﻣُﺮﺗﺒﻄﺔ ﺑﺎﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ‪ .‬وﻗﺪ ﺗﻜﻮن ﻧﺘﻴﺠﺔ ﻫﺬه اﻟﺘﻄﻮﱡرات أن اﻟﺬﻛﺎء‬
‫ﱠ‬
‫وﻳﺘﻮﱃ زﻣﺎم اﻷﻣﻮر وﻳُﻬﺪﱢد ﺣﻴﺎة اﻹﻧﺴﺎن اﻟﺬﻛﻴﺔ‪ .‬وﺳﻮاء‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻟﻔﺎﺋﻖ ﺳﻮف ﻳُﺴﻴﻄﺮ‬
‫أﻛﺎن ﻫﺬا اﻟﻜﻴﺎن واﻋﻴًﺎ أم ﻻ‪ ،‬وﺑﺼﻮرة أﻋﻢ ﻣﻬﻤﺎ ﻛﺎﻧﺖ ﺣﺎﻟﺘﻪ أو ﻛﻴﻔﻴﺔ ﻧﺸﻮﺋﻪ‪ ،‬ﻓﺈن اﻟﻘﻠﻖ‬
‫ﻫﻨﺎ ﻳﺘﻌ ﱠﻠﻖ ﺑﻤﺎ ﺳﻴﻔﻌﻠﻪ ﻫﺬا اﻟﻜﻴﺎن )أو ﻣﺎ ﻻ ﻳﻔﻌﻠﻪ(‪ .‬ﻗﺪ ﻻ ﻳﻬﺘ ﱡﻢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺄﻫﺪاﻓﻨﺎ‬
‫اﻟﺒﴩﻳﺔ‪ .‬وﻧﻈ ًﺮا ﻟﻌﺪم اﻣﺘﻼﻛِﻪ ﺟﺴﺪًا ﺑﻴﻮﻟﻮﺟﻴٍّﺎ‪ ،‬ﻓﺈﻧﻪ ﻟﻦ ﻳﻔﻬﻢ ﺣﺘﻰ املﻌﺎﻧﺎة اﻟﺒﴩﻳﺔ‪ .‬وﻳُﻘﺪم‬
‫ً‬
‫ﺗﺠﺮﺑﺔ ﻓﻜﺮﻳﺔ ﻟﺬﻛﺎءٍ اﺻﻄﻨﺎﻋﻲ ﻳُﺤﺪﱠد ﻟﻪ ﻫﺪف ﻣ ﱠ‬
‫ُﻌني وﻫﻮ ﺗﺼﻨﻴﻊ ﻣﺸﺎﺑﻚ اﻟﻮرق‬
‫ﺑﻮﺳﱰوم‬
‫ﺑﺄﻛﱪ ﻛ ﱟﻢ ﻣُﻤﻜِﻦ‪ ،‬ﻓﻤﺎ ﻛﺎن ﻣﻨﻪ إﻻ أن ﺣﻮﱠل ﻛﻮﻛﺐ اﻷرض واﻟﺒﴩ اﻟﺬﻳﻦ ﻳﻌﻴﺸﻮن ﻋﻠﻴﻪ إﱃ‬
‫‪20‬‬

‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻟﻮﺣﻮش وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻣﻮارد ﻹﻧﺘﺎج ﻣﺸﺎﺑﻚ اﻟﻮرق‪ .‬إذَن اﻟﺘﺤﺪﱢي اﻟﺬي ﻳُﻮاﺟﻬﻨﺎ اﻟﻴﻮم ﻫﻮ اﻟﺘﺄ ﱡﻛﺪ ﻣﻦ أﻧﻨﺎ ﻧﺒﻨﻲ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻣﺎ ﻣﺸﻜﻠﺔ اﻟﺴﻴﻄﺮة ﻫﺬه؛ ﺑﻤﻌﻨﻰ أﻧﻪ ﻳﻔﻌﻞ ﻣﺎ ﻧﺮﻳﺪ وﻳﺄﺧﺬ‬
‫ذﻛﺎءً اﺻﻄﻨﺎﻋﻴٍّﺎ ﻻ ﻳُﺜري‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻣﺎ ﻣﻦ ﻗﺪرات اﻟﺬﻛﺎء‬
‫ﺣﻘﻮﻗﻨﺎ ﰲ اﻻﻋﺘﺒﺎر‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻫﻞ ﻳﺠﺐ أن ﻧﺤ ﱠﺪ‬
‫‪1‬‬
‫اﻻﺻﻄﻨﺎﻋﻲ؟ وﻛﻴﻒ ﻳُﻤﻜﻨﻨﺎ اﺣﺘﻮاء اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟‬
‫ﺛﻤﱠ ﺔ أﻓﻜﺎر أﺧﺮى ﻣﱰاﺑﻄﺔ وذات ﺻﻠﺔ؛ أﻻ وﻫﻲ اﻷﻓﻜﺎر املﺘﻌ ﱢﻠﻘﺔ ﺑﺘﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪.‬‬
‫ﰲ ﺿﻮء اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻹﺣﺒﺎط ﻣﻦ اﻟﻀﻌﻒ اﻟﺒﴩي و»اﻷﺧﻄﺎء«‪ ،‬ﻳﺠﺎدل أﻧﺼﺎر ﺗﺠﺎوز‬
‫ﺑﺤﺎﺟﺔ إﱃ ﺗﻌﺰﻳﺰ اﻹﻧﺴﺎن‪ :‬ﺟﻌﻠﻪ أﻛﺜﺮ ذﻛﺎءً‪ ،‬وأﻗﻞ ﻋُ ً‬
‫ٍ‬
‫ﺮﺿﺔ‬
‫اﻹﻧﺴﺎﻧﻴﺔ ﻣﺜﻞ ﺑﻮﺳﱰوم ﺑﺄﻧﻨﺎ‬
‫ﻟﻠﻤﺮض‪ ،‬وأﻃﻮَل ﻋﻤ ًﺮا‪ ،‬ورﺑﻤﺎ ﺣﺘﻰ ﺧﺎﻟﺪًا‪ ،‬ﻣﻤﺎ ﻳﺆدي إﱃ ﻣﺎ ﻳُﺴﻤﱢ ﻴﻪ ﻫﺎراري »اﻹﻧﺴﺎن اﻹﻟﻪ«‪:‬‬
‫ﺗﺮﻗﻴﺔ اﻟﺒﴩ إﱃ آﻟﻬﺔ‪ .‬وﻛﻤﺎ ﻗﺎل ﻓﺮاﻧﺴﻴﺲ ﺑﻴﻜﻮن ﰲ »دﺣﺾ اﻟﻔﻠﺴﻔﺎت«‪ :‬اﻟﺒﴩ »آﻟﻬﺔ‬
‫ﻓﺎﻧﻴﺔ« )‪ .(Bacon 1964, 106‬ملﺎذا ﻻ ﻧُﺤﺎول ﺗﺤﻘﻴﻖ اﻟﺨﻠﻮد؟ وﻟﻜﻦ ﺣﺘﻰ ﻟﻮ ﻟﻢ ﻧﺴﺘﻄﻊ‬
‫ﺗﺤﻘﻴﻖ ذﻟﻚ‪ ،‬ﻓﺈن اﻵﻟﺔ اﻟﺒﴩﻳﺔ‪ً ،‬‬
‫ٍ‬
‫ﺑﺤﺎﺟﺔ إﱃ ﺗﺮﻗﻴﺔ‪ .‬ﻓﻨﺤﻦ‬
‫وﻓﻘﺎ ُملﻨﺎﴏي ﺗﺠﺎوُز اﻹﻧﺴﺎﻧﻴﺔ‪،‬‬
‫إذا ﻟﻢ ﻧﻔﻌﻞ ذﻟﻚ‪ ،‬ﻓﺴﻴُﺨﺎﻃﺮ اﻟﺒﴩ ﺑﺄن ﻳﻈﻠﻮا »اﻟﺠﺰء ا ُملﺘﺨﻠﻒ ﻏري اﻟﻜﻒء ﺑﺸﻜﻞ ﻣﺘﺰاﻳﺪ«‬
‫ٍ‬
‫ﺑﺤﺎﺟﺔ إﱃ إﻋﺎدة‬
‫ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ )‪ .(Armstrong 2014, 23‬إن اﻟﺒﻴﻮﻟﻮﺟﻴﺎ اﻟﺒﴩﻳﺔ‬
‫ﺗﺼﻤﻴﻢ‪ ،‬وﻟﺬا ﻳﺘﺴﺎءل ﺑﻌﺾ ﻣﺆﻳﺪي ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬ملﺎذا ﻻ ﻧﺘﺨ ﱠﻠﺺ ﺗﻤﺎﻣً ﺎ ﻣﻦ اﻷﺟﺰاء‬
‫ٍ‬
‫ﻛﺎﺋﻨﺎت ذﻛﻴﺔ ﻏري ﻋﻀﻮﻳﺔ؟‬
‫اﻟﺒﻴﻮﻟﻮﺟﻴﺔ وﻧُﺼﻤﱢ ﻢ‬
‫ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن ﻣﻌﻈﻢ اﻟﻔﻼﺳﻔﺔ واﻟﻌﻠﻤﺎء اﻟﺬﻳﻦ ﻳُﺮوﱢﺟﻮن ﻟﻬﺬه اﻷﻓﻜﺎر ﻳﺤﺮﺻﻮن‬
‫ﻋﲆ ﺗﻤﻴﻴﺰ آراﺋﻬﻢ ﻋﻦ اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ واﻟﺪﻳﻦ‪ ،‬ﻓﺈن اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﺒﺎﺣﺜني ﻳ ﱢ‬
‫ُﻔﴪون أﻓﻜﺎرﻫﻢ‬
‫ﺑﻬﺬه املﺼﻄﻠﺤﺎت ﺑﺎﻟﻀﺒﻂ‪ .‬ﺑﺎدئ ذي ﺑﺪء‪ ،‬ﻟﻴﺲ ﻣﻦ اﻟﻮاﺿﺢ ﻣﺪى ارﺗﺒﺎط أﻓﻜﺎرﻫﻢ‬
‫ﺑﺎﻟﺘﻄﻮﱡرات اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ اﻟﺤﺎﻟﻴﺔ وﻋﻠﻮم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻣﺎ إذا ﻛﺎن ﻫﻨﺎك ﻓﺮﺻﺔ‬
‫ﺣﻘﻴﻘﻴﺔ ﻟﻠﻮﺻﻮل إﱃ اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ ﰲ ا ُملﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ‪ ،‬ﻫﺬا إن أﻣﻜﻦ اﻟﻮﺻﻮل إﻟﻴﻪ ﻣﻦ‬
‫اﻷﺳﺎس‪ .‬إذ ﻳﺮﻓﺾ اﻟﺒﻌﺾ ﺗﻤﺎﻣً ﺎ إﻣﻜﺎﻧﻴﺔ اﻟﻮﺻﻮل إﻟﻴﻪ )اﻧﻈﺮ اﻟﻔﺼﻞ اﻟﺘﺎﱄ(‪ ،‬وﺣﺘﻰ ﻫﺆﻻء‬
‫اﻟﺬﻳﻦ ﻋﲆ اﺳﺘﻌﺪادٍ ﻟﻘﺒﻮل إﻣﻜﺎﻧﻴﺔ اﻟﻮﺻﻮل إﻟﻴﻪ ﻣﻦ ﺣﻴﺚ املﺒﺪأ‪ ،‬ﻣﺜﻞ اﻟﻌﺎﻟِﻤﺔ ﻣﺎرﺟﺮﻳﺖ‬
‫ﺑﻮدن‪ ،‬ﻓﺈﻧﻬﻢ ﻻ ﻳﻌﺘﻘﺪون أﻧﻪ ﻣﻦ ا ُملﺮﺟﱠ ﺢ اﻟﻮﺻﻮل إﻟﻴﻪ ﻋﻤﻠﻴٍّﺎ‪ .‬إن ﻓﻜﺮة اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ‬
‫ﺗﻔﱰض أﻧﻨﺎ ﺳﻨُﻄﻮﱢر »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم«‪ ،‬أو اﻟﺬﻛﺎء اﻟﺬي ﻳﻜﺎﻓﺊ اﻟﺬﻛﺎء اﻟﺒﴩي‬
‫ِ‬
‫أو ﻳﺘﻔﻮﱠق ﻋﻠﻴﻪ‪ ،‬وﻫﻨﺎك اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﻌﻘﺒﺎت اﻟﺘﻲ ﻳﺠﺐ اﻟﺘﻐ ﱡﻠﺐ ﻋﻠﻴﻬﺎ ﻗﺒﻞ ﺗﺤﻘﻴﻖ ذﻟﻚ‪.‬‬
‫وﺗﺮى ﺑﻮدن )‪ (٢٠١٦‬أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺲ واﻋﺪًا ﻛﻤﺎ ﱠ‬
‫ﺗﻘﺮﻳﺮ‬
‫ﻳﺘﻮﻗﻊ اﻟﻜﺜريون‪ .‬وﰲ‬
‫ٍ‬
‫ﺻﺎدر ﻋﻦ اﻟﺒﻴﺖ اﻷﺑﻴﺾ ﻋﺎم ‪ ،٢٠١٦‬ﺗﻢ اﻟﺘﺄﻛﻴﺪ ﻋﲆ اﺗﻔﺎق ﺧﱪاء اﻟﻘﻄﺎع اﻟﺨﺎص ﻋﲆ‬
‫ﱠ‬
‫ﻳﺘﺤﻘﻖ ﻋﲆ اﻷﻗﻞ ﻗﺒﻞ ﻋﻘﻮد‪ .‬ﻛﻤﺎ ﻳﺮﻓﺾ اﻟﻌﺪﻳﺪ ﻣﻦ‬
‫أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم ﻟﻦ‬
‫‪21‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﺒﺎﺣﺜني ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺮؤى ا ُملﻈﻠﻤﺔ املﺘﺸﺎﺋﻤﺔ اﻟﺘﻲ ﻳُﺮوﱢج ﻟﻬﺎ ﺑﻮﺳﱰوم‬
‫ﱡ‬
‫ﺑﺸﻜﻞ إﻳﺠﺎﺑﻲ‪ ،‬ﻛﻤﺴﺎﻋ ٍﺪ أو زﻣﻴﻞ‪.‬‬
‫وﻳﺤﻀﻮن ﻋﲆ اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫وآﺧﺮون‪،‬‬
‫ٍ‬
‫وﻟﻜﻦ املﺴﺄﻟﺔ ﻻ ﺗﺘﻌﻠﻖ ﺑﻤﺎ ﺳﻴﺤﺪث ﻓﻌﻠﻴٍّﺎ ﰲ املﺴﺘﻘﺒﻞ‪ .‬ﺑﻞ ﻳﻮﺟَ ﺪ ﳾء َ‬
‫آﺧﺮ ﻳُﺜري اﻟﻘﻠﻖ وﻫﻮ‬
‫أن ﻫﺬه املﻨﺎﻗﺸﺔ ﺣﻮل ﺗﺄﺛريات اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ املﺴﺘﻘﺒﻞ )اﻟﺒﻌﻴﺪ( ﺗُﺸﺘﱢﺖ اﻻﻧﺘﺒﺎه ﻋﻦ‬
‫املﺨﺎﻃﺮ اﻟﺤﻘﻴﻘﻴﺔ واملﻮﺟﻮدة ﺣﺎﻟﻴٍّﺎ ﻟﻸﻧﻈﻤﺔ اﻟﺘﻲ ﺗﻢ ﻧﴩﻫﺎ ﻓﻌﻠﻴٍّﺎ )‪Crawford and Calo‬‬
‫ً‬
‫ذﻛﻴﺔ‬
‫‪ .(2016‬ﻳﺒﺪو أن ﻫﻨﺎك ﺧﻄ ًﺮا ﺣﻘﻴﻘﻴٍّﺎ أﻧﻪ ﰲ املﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ‪ ،‬ﻟﻦ ﺗﻜﻮن اﻷﻧﻈﻤﺔ‬
‫ﺑﺸﻜﻞ ﻏري ٍ‬
‫ﻛﺎف‪ ،‬وﻣﻊ ذﻟﻚ‬
‫ﺑﻤﺎ ﻓﻴﻪ اﻟﻜﻔﺎﻳﺔ وأﻧﻨﺎ ﺳﻨﻔﻬﻢ آﺛﺎرﻫﺎ اﻷﺧﻼﻗﻴﺔ واﻻﺟﺘﻤﺎﻋﻴﺔ‬
‫ٍ‬
‫ً‬
‫ﻔﺮط ﻋﲆ اﻟﺬﻛﺎء‪ ،‬ﺑﻮﺻﻔﻪ ِﺳﻤﺔ رﺋﻴﺴﻴﺔ‬
‫ﺳﻨﺴﺘﺨﺪِﻣﻬﺎ ﻋﲆ‬
‫ﻧﻄﺎق واﺳﻊ‪ .‬ﻛﻤﺎ أن اﻟﱰﻛﻴﺰ ا ُمل ِ‬
‫ٍ‬
‫ً‬
‫وﻫﺪﻓﺎ ﻧﻬﺎﺋﻴٍّﺎ وﺣﻴﺪًا‪ ،‬ﻫﻮ ً‬
‫أﻳﻀﺎ أﻣﺮ ﻣﺸﻜﻮك ﻓﻴﻪ )‪.(Boddington 2017‬‬
‫ﻟﻺﻧﺴﺎﻧﻴﺔ‪،‬‬
‫ﻣﻊ ذﻟﻚ‪ ،‬ﺗﺴﺘﻤﺮ اﻷﻓﻜﺎر ﻣﺜﻞ اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ ﰲ اﻟﺘﺄﺛري ﻋﲆ املﻨﺎﻗﺸﺔ اﻟﻌﺎﻣﺔ‪ .‬وﻣﻦ‬
‫ا ُملﺤﺘﻤﻞ أن ﺗﺆﺛﱢﺮ ً‬
‫أﻳﻀﺎ ﻋﲆ ﺗﻄﻮﱡر اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻻ ﻳُﻌﺘﱪ راي ﻛﻮرزواﻳﻞ‬
‫ﻣﻦ دُﻋﺎة املﺴﺘﻘﺒﻠﻴﺔ ﻓﺤﺴﺐ‪ .‬ﺑﻞ إﻧﻪ ﻳﺸﻐﻞ ﻣﻨﺼﺐ ﻣﺪﻳﺮ اﻟﻬﻨﺪﺳﺔ ﰲ ﴍﻛﺔ ﺟﻮﺟﻞ ﻣﻨﺬ ﻋﺎم‬
‫‪ .٢٠١٢‬ﻛﻤﺎ ﻳﺒﺪو أن إﻳﻠﻮن ﻣﺎﺳﻚ‪ ،‬اﻟﺮﺋﻴﺲ اﻟﺘﻨﻔﻴﺬي ﻟﴩﻛﺔ ﺗﻴﺴﻼ وﴍﻛﺔ ﺳﺒﻴﺲ إﻛﺲ‪،‬‬
‫وﻫﻮ ﺷﺨﺼﻴﺔ ﻋﺎﻣﺔ ﻣﻌﺮوﻓﺔ ﺟﺪٍّا‪ ،‬ﻳﺆﻳﺪ ﺳﻴﻨﺎرﻳﻮﻫﺎت اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واملﺨﺎﻃﺮ اﻟﻮﺟﻮدﻳﺔ‬
‫)ﺳﻴﻨﺎرﻳﻮﻫﺎت اﻟﻬﻼك؟( اﻟﺘﻲ وﺿﻌﻬﺎ ﺑﻮﺳﱰوم وﻛﻮرزواﻳﻞ‪ .‬وﻗﺪ ﺣﺬﱠر ﻣﺮا ًرا ﻣﻦ ﺧﻄﻮرة‬
‫َ‬
‫واﻋﺘﱪَه ﺗﻬﺪﻳﺪًا وﺟﻮدﻳٍّﺎ وزﻋﻢ أﻧﻨﺎ ﻻ ﻳُﻤﻜﻨﻨﺎ اﻟﺘﺤ ﱡﻜﻢ ﰲ اﻟﺸﻴﻄﺎن‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫)‪ .(Dowd 2017‬وﻳﻌﺘﻘﺪ ﻣﺎﺳﻚ أن اﻟﺒﴩ ﺳﻴﻨﻘﺮﺿﻮن ﻋﲆ اﻷرﺟﺢ‪ ،‬ﻣﺎ ﻟﻢ ﻳُﺪﻣَ ﺞ اﻟﺬﻛﺎء‬
‫اﻟﺒﴩي واﻟﺬﻛﺎء اﻵﱄ أو ﻧﺘﻤ ﱠﻜﻦ ﻣﻦ اﻟﻬﺮوب إﱃ املﺮﻳﺦ‪.‬‬
‫ً‬
‫ً‬
‫رﺑﻤﺎ ﺗﻜﻮن ﻫﺬه اﻷﻓﻜﺎر ﻣﺆﺛﺮة ﻟﻠﻐﺎﻳﺔ ﻷﻧﻬﺎ ﱡ‬
‫ﻋﻤﻴﻘﺔ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻟﺒﴩ‬
‫وآﻣﺎﻻ‬
‫ﺗﻤﺲ ﻣﺨﺎوف‬
‫َ‬
‫ُ‬
‫ْ‬
‫واﻵﻻت داﺧﻞ وﻋ ِﻴﻨﺎ اﻟﺠﻤﻌﻲ‪ .‬وﺳﻮاء َﻗ ِﺒﻠﻨﺎ ﻫﺬه اﻷﻓﻜﺎر املﺤﺪﱠدة أو رﻓﻀﻨﺎﻫﺎ‪ ،‬ﻓﺈن ﻫﻨﺎك‬
‫ِﺻ ٍ‬
‫ﻼت واﺿﺤﺔ ﺑﺎﻟﴪدﻳﺎت اﻟﺨﻴﺎﻟﻴﺔ ﰲ اﻟﺜﻘﺎﻓﺔ اﻟﺒﴩﻳﺔ واﻟﺘﺎرﻳﺦ اﻟﺘﻲ ﺗُﺤﺎول أن ﺗﻔﻬﻢ‬
‫اﻹﻧﺴﺎن وﻋﻼﻗﺘﻪ ﺑﺎﻵﻻت‪ .‬وﻳﺠﺪُر ﺑﻨﺎ أن ﻧ ُ ﱢ‬
‫ﻮﺿﺢ ﻫﺬه اﻟﴪدﻳﺎت ﻟﻜﻲ ﻧﻔﻬﻢ ﺑﻌﺾ ﻫﺬه‬
‫وﺑﺸﻜﻞ ﻋﺎم‪ ،‬ﻓﺈﻧﻪ ﻣﻦ ا ُملﻬﻢ أن‬
‫ﻧﺤﻮ أﻓﻀﻞ وﻧﻀﻌﻬﺎ ﰲ ﺳﻴﺎﻗﻬﺎ اﻟﺼﺤﻴﺢ‪.‬‬
‫ٍ‬
‫اﻷﻓﻜﺎر ﻋﲆ ٍ‬
‫ﻧﺪﻣﺞ ﺑﺤﺚ اﻟﴪدﻳﺎت ﰲ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻟﻜﻲ ﻧﻔﻬﻢ‬
‫اﻷﺳﺒﺎب اﻟﺘﻲ ﺗﺠﻌﻞ ﺑﻌﺾ اﻟﴪدﻳﺎت ﻣُﻨﺘﴩة‪ ،‬وﻣَ ﻦ أﻧﺸﺄﻫﺎ‪ ،‬وﻣَ ﻦ اﻟﺬي ﻳﺴﺘﻔﻴﺪ ﻣﻨﻬﺎ‬
‫)‪ .(Royal Society 2018‬ﻛﻤﺎ ﻳﻤﻜﻦ أن ﻳُﺴﺎﻋﺪﻧﺎ ﰲ إﻧﺸﺎء ﴎدﻳﱠﺎت ﺟﺪﻳﺪة ﺣﻮل ﻣﺴﺘﻘﺒﻞ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫‪22‬‬

‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻟﻮﺣﻮش وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫وﺣﺶ ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ اﻟﺠﺪﻳﺪ‬
‫ِﻣﻦ اﻟﺴﺒﻞ اﻟﺘﻲ ﻳُﻤﻜﻨﻨﺎ اﺗﺨﺎذﻫﺎ ﻟﺘﺠﺎوُز اﻟﻀﺠﺔ املﺜﺎرة أن ﻧﻔ ﱢﻜﺮ ﰲ ﺑﻌﺾ اﻟﴪدﻳﺎت‬
‫ذات اﻟﺼﻠﺔ ﻣﻦ ﺗﺎرﻳﺦ اﻟﺜﻘﺎﻓﺔ اﻟﺒﴩﻳﺔ اﻟﺘﻲ ﺗُﺸﻜﻞ املﻨﺎﻗﺸﺔ اﻟﻌﺎﻣﺔ اﻟﺤﺎﻟﻴﺔ ﺣﻮل اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻓﻠﻴﺴﺖ ﻫﺬه ﻫﻲ املﺮة اﻷوﱃ اﻟﺘﻲ ﻳﺘﺴﺎءل ﻓﻴﻬﺎ اﻟﻨﺎس ﻋﻦ ﻣُﺴﺘﻘﺒﻞ اﻟﺒﴩﻳﺔ‬
‫وﻣُﺴﺘﻘﺒﻞ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬وﻣﻬﻤﺎ ﻛﺎﻧﺖ ﺑﻌﺾ اﻷﻓﻜﺎر املﺘﻌﻠﻘﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺒﺪو‬
‫ﻏﺮﻳﺒﺔ‪ ،‬ﻓﺈﻧﻨﺎ ﻳُﻤﻜﻨﻨﺎ اﺳﺘﻜﺸﺎف ِﺻﻠﺘﻬﺎ ﺑﺄﻓﻜﺎر وﴎدﻳﺎت أﻛﺜﺮ ﺷﻬﺮة ﺗﻮﺟَ ﺪ ﰲ وﻋﻴﻨﺎ‬
‫ﺑﺸﻜﻞ أدق‪ ،‬ﰲ اﻟﻮﻋﻲ اﻟﺠﻤﺎﻋﻲ ﻟﻠﻐﺮب‪.‬‬
‫اﻟﺠﻤﻌﻲ‪ ،‬أو‬
‫ٍ‬
‫ً‬
‫أوﻻ‪ ،‬ﻫﻨﺎك ﺗﺎرﻳﺦ ﻃﻮﻳﻞ ﻟﻠﺘﻔﻜري ﰲ اﻟﺒﴩ واﻵﻻت أو املﺨﻠﻮﻗﺎت اﻻﺻﻄﻨﺎﻋﻴﺔ ﰲ‬
‫اﻟﺜﻘﺎﻓﺎت اﻟﻐﺮﺑﻴﺔ وﻏري اﻟﻐﺮﺑﻴﺔ ﻋﲆ ﺣ ﱟﺪ ﺳﻮاء‪ .‬ﻳُﻤﻜﻦ اﻟﻌﺜﻮر ﻋﲆ ﻓﻜﺮة إﻧﺸﺎء ﻛﺎﺋﻨﺎت‬
‫ﺣﻴﺔ ﻣﻦ ﻣﺎدة ﻏري ﺣﻴﺔ ﰲ ﻗﺼﺺ اﻟﺨﻠﻖ ﰲ اﻟﺜﻘﺎﻓﺎت اﻟﺴﻮﻣﺮﻳﺔ واﻟﺼﻴﻨﻴﺔ واﻟﻴﻬﻮدﻳﺔ‬
‫واملﺴﻴﺤﻴﺔ واﻹﺳﻼﻣﻴﺔ‪ .‬ﻓﻘﺪ ﻛﺎﻧﺖ ﻟﺪى اﻹﻏﺮﻳﻖ ﻓﻜﺮة إﻧﺸﺎء َ‬
‫ﺑﴩ اﺻﻄﻨﺎﻋﻴني‪ ،‬وﺧﺎﺻﺔ‬
‫اﻟﻨﺴﺎء اﻻﺻﻄﻨﺎﻋﻴﺎت‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ اﻹﻟﻴﺎذة‪ ،‬ﻳُﻘﺎل إن ﻫﻴﻔﺎﻳﺴﺘﻮس ﻳﻘﻮم ﻋﲆ‬
‫ﺧﺪﻣﺘﻪ َﺧﺪَم ﻣﺼﻨﻮﻋﻮن ﻣﻦ اﻟﺬﻫﺐ ﻳُﺸﺒﻬﻮن اﻟﻨﺴﺎء‪ .‬وﰲ أﺳﻄﻮرة ﺑﻴﺠﻤﺎﻟﻴﻮن اﻟﺸﻬرية‪،‬‬
‫ﻳﻘﻊ اﻟﻨﺤﱠ ﺎت ﰲ ﺣُ ﺐ ﺗﻤﺜﺎل اﻣﺮأة ﺻﻨَﻌَ ﻪ ﻣﻦ اﻟﻌﺎج‪ .‬وﻳﺘﻤﻨﱠﻰ أن ﺗﺪبﱠ ﻓﻴﻪ اﻟﺮوح وﻳُﺼﺒﺢ‬
‫اﻣﺮأة ﺣﻘﻴﻘﻴﺔ‪ ،‬ﻓﺘُ ﱢ‬
‫ﺤﻘﻖ ﻟﻪ اﻹﻟﻬﺔ أﻓﺮودﻳﺖ أُﻣﻨﻴﺘﻪ‪ :‬ﻓﺘﺼﺒﺢ ﺷﻔﺘﺎﻫﺎ داﻓﺌﺘَني وﺟﺴﺪُﻫﺎ ﻧﺎﻋﻤً ﺎ‪.‬‬
‫وﻳُﻤﻜﻨﻨﺎ ﺑﺴﻬﻮﻟﺔ ﻫﻨﺎ ﻣﻼﺣﻈﺔ ﱢ‬
‫اﻟﺼﻠﺔ ﺑني ذﻟﻚ وﺑني اﻟﺮوﺑﻮﺗﺎت اﻟﺠﻨﺴﻴﺔ املﻌﺎﴏة‪.‬‬
‫ﻫﺬه اﻟﴪدﻳﱠﺎت ﻻ ﺗﺄﺗﻲ ﻓﻘﻂ ﻣﻦ اﻷﺳﺎﻃري‪ :‬ﻓﻔﻲ ﻛﺘﺎﺑﻪ »اﻷوﺗﻮﻣﺎﺗﺎ«‪ ،‬ﻗﺪﱠم ﻋﺎﻟِﻢ‬
‫َ‬
‫اﻛﺘﺸﻔﺖ ﰲ اﻟﺒﺤﺮ‪،‬‬
‫اﻟﺮﻳﺎﺿﻴﺎت واملﻬﻨﺪس اﻹﻏﺮﻳﻘﻲ ﻫريون اﻟﺴﻜﻨﺪري )وﻟﺪ ﻋﺎم ‪ (١٠‬أداة‬
‫ﻇﺮي إﻏﺮﻳﻘﻲ ﻳﻌﺘﻤﺪ ﻋﲆ آﻟﻴﺔ ﻣ ﱠ‬
‫وﻫﻲ آﻟﻴﺔ »أﻧﺘﻴﻜﻴﺜريا«‪ ،‬اﻟﺘﻲ ﺗُﺤﺪد أﻧﻬﺎ ﻛﻤﺒﻴﻮﺗﺮ ﺗﻨﺎ ُ‬
‫ُﻌﻘﺪة‬
‫ﻣﻦ اﻟﱰوس وا ُملﺴﻨﱠﻨﺎت‪ .‬وﻟﻜﻦ اﻟﻘﺼﺺ اﻟﺨﻴﺎﻟﻴﺔ اﻟﺘﻲ ﺗﺠﻌﻞ اﻵﻻت ﺗُﺸ ِﺒﻪ اﻟﺒﴩ ﺗﺴﻠُﺐ‬
‫ﺑﺸﻜﻞ ﺧﺎص‪ .‬ﻓﻠﻨﺄﺧﺬ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬أﺳﻄﻮرة اﻟﺠﻮﻟﻴﻢ‪ :‬وﺣﺶ ﻣﺼﻨﻮع ﻣﻦ‬
‫أﻟﺒﺎﺑﻨﺎ‬
‫ٍ‬
‫َ‬
‫َ‬
‫ﻧﻮاﺟ ُﻪ ﻧﺴﺨﺔ‬
‫اﻟﻄني ﺻﻨﻌَ ﻪ ﺣﺎﺧﺎم ﰲ اﻟﻘﺮن اﻟﺴﺎدس ﻋﴩ‪ ،‬ﺛﻢ ﻓﻘ َﺪ اﻟﺴﻴﻄﺮة ﻋﻠﻴﻪ‪ .‬ﻫﻨﺎ‬
‫ِ‬
‫ﻣُﺒ ﱢﻜﺮة ﻣﻦ ﻣﺸﻜﻠﺔ اﻟﺘﺤ ﱡﻜﻢ‪ .‬وﻳﻤﻜﻦ ﺗﻔﺴري أﺳﻄﻮرة ﺑﺮوﻣﻴﺜﻴﻮس ﺑﻬﺬه اﻟﻄﺮﻳﻘﺔ ً‬
‫أﻳﻀﺎ؛ إذ‬
‫ﻳﴪق اﻟﻨﺎر ﻣﻦ اﻵﻟﻬﺔ وﻳُﻌﻄﻴﻬﺎ إﱃ اﻟﺒﴩ‪ ،‬ﻟﻜﻨﻪ ﻳ َ‬
‫ُﻌﺎﻗﺐ ﺑﻌﺪ ذﻟﻚ‪ .‬وﻋﻘﻮﺑﺘﻪ اﻷﺑﺪﻳﺔ ﻫﻲ أن‬
‫ﻳُﺮﺑﻂ ﺑﺼﺨﺮ ٍة ﺑﻴﻨﻤﺎ ﻳﺄﻛﻞ اﻟﻨﴪ ﻛ ِﺒﺪَه ﻛ ﱠﻞ ﻳﻮم‪ .‬وﻗﺪ ﻛﺎن اﻟﺪرس اﻟﻘﺪﻳﻢ ﻣﻦ ﻫﺬه اﻷﺳﻄﻮرة‬
‫ﻫﻮ اﻟﺘﺤﺬﻳﺮ ﻣﻦ اﻟﻐﻄﺮﺳﺔ‪ :‬ﻓﻬﺬه اﻟﻘﺪرات ﻟﻴﺴﺖ ﻣُﻘﺪﱠرة ﻟﻠﺒﴩ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﰲ رواﻳﺔ ﻣﺎري ﺷﻴﲇ »ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ« — اﻟﺘﻲ ﺗﺤﻤﻞ اﻟﻌﻨﻮان اﻟﻔﺮﻋﻲ‬
‫اﻟﺪال »ﺑﺮوﻣﻴﺜﻴﻮس اﻟﺤﺪﻳﺚ« — ﻳُﺼﺒﺢ إﻧﺸﺎء ﺣﻴﺎة ذﻛﻴﺔ ﻣﻦ ﻣﺎدة ﻏري ﺣﻴﱠﺔ ﻣﴩوﻋً ﺎ‬
‫‪23‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻋﻠﻤﻴٍّﺎ ﺣﺪﻳﺜًﺎ‪ .‬ﺣﻴﺚ ﻳﻨﺸﺊ اﻟﻌﺎﻟﻢ ﻓﻴﻜﺘﻮر ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ ﻛﺎﺋﻨًﺎ ﺷﺒﻴﻬً ﺎ ﺑﺎﻹﻧﺴﺎن ﻣﻦ أﺟﺰاء‬
‫اﻟﺠﺜﺚ‪ ،‬ﻟﻜﻨﻪ ﻳﻔﻘﺪ اﻟﺴﻴﻄﺮة ﻋﻠﻴﻪ‪ .‬وﻣﻊ أن اﻟﺤﺎﺧﺎم اﺳﺘﻄﺎع أن ﻳُﺴﻴﻄﺮ ﻋﲆ اﻟﺠﻮﻟﻴﻢ ﰲ‬
‫اﻟﻨﻬﺎﻳﺔ‪ ،‬ﻓﺈن اﻷﻣﺮ ﻟﻴﺲ ﻛﺬﻟﻚ ﰲ ﻫﺬه اﻟﺤﺎﻟﺔ‪ .‬وﻳﻤﻜﻦ اﻋﺘﺒﺎر ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ رواﻳﺔ روﻣﺎﻧﺴﻴﺔ‬
‫ﺗُﺤﺬﱢر ﻣﻦ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺤﺪﻳﺜﺔ‪ ،‬وﻟﻜﻨﻬﺎ ﺗﺴﺘﻨﺪ إﱃ اﻟﻌﻠﻢ ﰲ زﻣﻨِﻬﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻠﻌﺐ‬
‫اﺳﺘﺨﺪام اﻟﻜﻬﺮﺑﺎء — وﻫﻲ ﺗﻘﻨﻴﺔ ﺟﺪﻳﺪة ﺟﺪٍّا ﰲ ذﻟﻚ اﻟﻮﻗﺖ — دو ًرا ﻣﻬﻤٍّ ﺎ؛ إذ ﺗُﺴﺘﺨﺪَم‬
‫ﻹﺣﻴﺎء اﻟﺠﺜﺔ‪ .‬ﻛﻤﺎ أﻧﻬﺎ ﺗُﺸري إﱃ املﻐﻨﺎﻃﻴﺴﻴﺔ وﻋِﻠﻢ اﻟﺘﴩﻳﺢ‪ .‬ﰲ ذﻟﻚ اﻟﻮﻗﺖ‪ ،‬ﻛﺎن املﻔ ﱢﻜﺮون‬
‫واﻟﻜﺘﱠﺎب ﻳﻨﺎﻗﺸﻮن ﻃﺒﻴﻌﺔ اﻟﺤﻴﺎة وأﺻﻠﻬﺎ‪ .‬ﻣﺎ ﻗﻮة اﻟﺤﻴﺎة؟ ﻟﻘﺪ ﺗﺄﺛﺮت ﻣﺎري ﺷﻴﲇ ﺑﻌﻠﻮم‬
‫‪2‬‬
‫ري‬
‫ﻋﴫﻫﺎ‪ .‬وﺗُﻈﻬﺮ اﻟﻘﺼﺔ ﻛﻴﻒ ﻛﺎن اﻟﺮوﻣﺎﻧﺴﻴﻮن ﰲ اﻟﻘﺮن اﻟﺘﺎﺳﻊ ﻋﴩ ﻣﻔﺘﻮﻧني ﰲ ﻛﺜ ٍ‬
‫ﻇ ً‬
‫ﻓﻀﻼ ﻋﻦ أﻣﻠﻬﻢ ﰲ أن ﻳُﺤ ﱢﺮرﻧﺎ ﱢ‬
‫اﻟﺸﻌﺮ واﻷدب ﻣﻦ اﻟﺠﻮاﻧﺐ اﻷﻛﺜﺮ ُ‬
‫ً‬
‫ﻠﻤﺔ‬
‫ﻣﻦ اﻷﺣﻴﺎن ﺑﺎﻟﻌﻠﻢ‪،‬‬
‫ﰲ اﻟﺤﺪاﺛﺔ )‪ .(Coeckelbergh 2017‬ﻳﺠﺐ ﱠأﻻ ﻧﻌﺘﱪ ﻫﺬه اﻟﺮواﻳﺔ ﺑﺎﻟﴬورة ﺿﺪ اﻟﻌﻠﻢ‬
‫واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ؛ إذ ﻳﺒﺪو أن اﻟﺮﺳﺎﻟﺔ اﻟﺮﺋﻴﺴﻴﺔ اﻟﺘﻲ ﺗﺤﺮص ﻋﲆ ﺗﻮﺻﻴﻠﻬﺎ ﻫﻲ أن اﻟﻌﻠﻤﺎء‬
‫ﻳﻨﺒﻐﻲ أن ﻳﺘﺤﻤﻠﻮا ﻣﺴﺌﻮﻟﻴﺔ اﺧﱰاﻋﺎﺗﻬﻢ‪ .‬ﻳﻬﺮب اﻟﻮﺣﺶ‪ ،‬وﻟﻜﻨﻪ ﻳﻔﻌﻞ ذﻟﻚ ﻷن ﺻﺎﻧﻌﻪ‬
‫ﻳﺮﻓﻀﻪ‪ .‬ﻳﺠﺐ أن ﻧﺘﺬ ﱠﻛﺮ ﻫﺬا اﻟﺪرس ﻓﻴﻤﺎ ﻳﺘﻌ ﱠﻠﻖ ﺑﺄﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻣﻊ ذﻟﻚ‪،‬‬
‫ﺗﺆ ﱢﻛﺪ اﻟﺮواﻳﺔ ﺑﻮﺿﻮح ﺧﻄﺮ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺘﻲ ﺗﺨﺮج ﻋﻦ اﻟﺴﻴﻄﺮة‪ ،‬وﻋﲆ وﺟﻪ اﻟﺨﺼﻮص‬
‫ﺧﻄﺮ اﻟﺒﴩ اﻻﺻﻄﻨﺎﻋﻴني اﻟﺬﻳﻦ ﻳُﺼﻴﺒﻬﻢ اﻟﺠﻨﻮن‪ .‬ﺗﻌﻮد ﻫﺬه املﺨﺎوف ﻟﻠﻈﻬﻮر ﻋﲆ اﻟﺴﻄﺢ‬
‫ﰲ اﻟﻘﻠﻖ ا ُملﻌﺎﴏ ﻣﻦ أن ﻳﺨﺮج اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﻦ اﻟﺴﻴﻄﺮة‪.‬‬
‫ﰲ رواﻳﺔ ﻣﺎري ﺷﻴﲇ »ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ« — اﻟﺘﻲ ﺗﺤﻤﻞ اﻟﻌﻨﻮان اﻟﻔﺮﻋﻲ اﻟﺪال »ﺑﺮوﻣﻴﺜﻴﻮس اﻟﺤﺪﻳﺚ« —‬
‫ﻳُﺼﺒﺢ إﻧﺸﺎء ﺣﻴﺎة ذﻛﻴﺔ ﻣﻦ ﻣﺎدة ﻏري ﺣﻴﺔ ﻣﴩوﻋً ﺎ ﻋﻠﻤﻴٍّﺎ ﺣﺪﻳﺜًﺎ‪.‬‬

‫وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻛﻤﺎ ﻫﻮ اﻟﺤﺎل ﰲ رواﻳﺔ »ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ« وأﺳﻄﻮرة »اﻟﺠﻮﻟﻴﻢ«‪ ،‬ﺗﻈﻬﺮ‬
‫ﴎدﻳﺔ املﻨﺎﻓﺴﺔ‪ :‬ﻓﺎملﺨﻠﻮﻗﺎت اﻻﺻﻄﻨﺎﻋﻴﺔ ﺗﺘﻨﺎﻓﺲ ﻣﻊ اﻹﻧﺴﺎن‪ .‬وﺗﺴﺘﻤ ﱡﺮ ﻫﺬه اﻟﴪدﻳﺔ ﰲ‬
‫ﺗﺸﻜﻴﻞ ﺧﻴﺎﻟﻨﺎ اﻟﻌﻠﻤﻲ ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻟﻜﻨﻬﺎ ً‬
‫أﻳﻀﺎ ﺗﺆﺛﱢﺮ ﻋﲆ ﺗﻔﻜريﻧﺎ ا ُملﻌﺎﴏ‬
‫ُ‬
‫ﻓﻠﻨﺄﺧﺬ ﻣﴪﺣﻴﺔ »روﺑﻮﺗﺎت روﺳﻮم‬
‫ﰲ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻣﺜﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺮوﺑﻮﺗﺎت‪.‬‬
‫اﻟﻌﺎملﻴﺔ« اﻟﺘﻲ ﻛﺘﺒﺖ ﻋﺎم ‪ً ١٩٢٠‬‬
‫ﻣﺜﺎﻻ‪ ،‬وﻫﻲ ﺗﺘﻨﺎول ﻗﺼﺔ اﻟﺮوﺑﻮﺗﺎت اﻟﻌﺒﻴﺪ اﻟﺘﻲ ﺗﺘﻤ ﱠﺮد‬
‫ﻋﲆ ﺳﻴﺪﻫﺎ وﺗﺜﻮر ﻋﻠﻴﻪ‪ ،‬أو ﻓﻴﻠﻢ »‪ :٢٠٠١‬ﺳﺒﻴﺲ أودﻳﴘ« )‪ :٢٠٠١‬أودﻳﺴﺔ اﻟﻔﻀﺎء(‬
‫اﻟﺬي أﻧﺘﺞ ﻋﺎم ‪ ١٩٦٨‬واﻟﺬي ذﻛﺮﻧﺎه ﻣﻦ ﻗﺒ ُﻞ‪ ،‬وﻳﺘﺤﺪﱠث ﻋﻦ ذﻛﺎء اﺻﻄﻨﺎﻋﻲ ﻳﺒﺪأ ﰲ ﻗﺘﻞ‬
‫ﻃﺎﻗﻢ املﺮﻛﺒﺔ اﻟﻔﻀﺎﺋﻴﺔ ﻟﺘﺤﻘﻴﻖ ﻣﻬﻤﱠ ﺘﻪ‪ ،‬أو ﻓﻴﻠﻢ »إﻛﺲ ﻣﺎﻛﻴﻨﺎ« اﻟﺬي أﻧﺘﺞ ﻋﺎم ‪٢٠١٥‬‬
‫‪24‬‬

‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻟﻮﺣﻮش وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫وﻳﺮوي ﻗﺼﺔ روﺑﻮت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ »أﻓﺎ« اﻟﺘﻲ ﺗﻨﻘﻠﺐ ﻋﲆ ﺻﺎﻧﻌﻬﺎ‪ .‬ﻛﻤﺎ ﻳﻨﺪرج ﺗﺤﺖ‬
‫ﴎدﻳﺔ اﻵﻻت اﻟﺘﻲ ﺗﺘﻤ ﱠﺮد ﻋﻠﻴﻨﺎ ﻣﺠﻤﻮﻋﺔ أﻓﻼم »ا ُملﺪﻣﱢ ﺮ« )ﺗﺮﻣﻴﻨﻴﺘﻮر(‪ .‬وﻗﺪ وﺻﻒ ﻛﺎﺗﺐ‬
‫اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ أﻳﺰاك أﺳﻴﻤﻮف ﻫﺬا اﻟﺨﻮف ﺑ »ﻋﻘﺪة ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ«‪ :‬اﻟﺨﻮف ﻣﻦ اﻟﺮوﺑﻮﺗﺎت‪.‬‬
‫وﻳﺮﺗﺒﻂ ﻫﺬا ً‬
‫ﱠ‬
‫ﻳﺘﻌني ﻋﲆ اﻟﻌﻠﻤﺎء وا ُملﺴﺘﺜﻤﺮﻳﻦ‬
‫أﻳﻀﺎ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻴﻮم‪ .‬وﻫﻮ أﻣﺮ‬
‫اﻟﺘﻌﺎﻣُﻞ ﻣﻌﻪ‪ .‬ﻓﺒﻌﻀﻬﻢ ﻳُﺤﺎرﺑﻮن ﻫﺬا اﻟﺨﻮف؛ وﺑﻌﻀﻬﻢ ﻳﺴﺎﻋﺪ ﰲ ﺧﻠﻘﻪ واﻟﺤﻔﺎظ ﻋﻠﻴﻪ‪.‬‬
‫أﴍت ﺑﺎﻟﻔﻌﻞ إﱃ ﻣﺜﺎل »ﻣﺎﺳﻚ«‪ .‬وﺛﻤﱠ ﺔ ﻣﺜﺎل َ‬
‫ُ‬
‫آﺧﺮ ﻋﲆ ﺷﺨﺼﻴﺔ ﻣﺆﺛﺮة ﺳﺎﻫﻤﺖ ﰲ‬
‫وﻗﺪ‬
‫ﻧﴩ اﻟﺨﻮف ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻫﻮ ﻋﺎﻟﻢ اﻟﻔﻴﺰﻳﺎء ﺳﺘﻴﻔﻦ ﻫﻮﻛﻴﻨﺞ‪ ،‬اﻟﺬي ﴏﱠ ح ﰲ‬
‫ﻋﺎم ‪ ٢٠١٧‬ﺑﺄن ﺧﻠﻖ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻤﻜﻦ أن ﻳﻜﻮن أﺳﻮأ ﺣﺪ ٍَث ﰲ ﺗﺎرﻳﺦ ﺣﻀﺎرﺗﻨﺎ‬
‫)‪ .(Kharpal 2017‬إن »ﻋﻘﺪة ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ« ﻣﻨﺘﴩة وﻋﻤﻴﻘﺔ اﻟﺠﺬور ﰲ اﻟﺜﻘﺎﻓﺔ واﻟﺤﻀﺎرة‬
‫اﻟﻐﺮﺑﻴﺔ‪.‬‬
‫اﻟﺘﺴﺎﻣﻲ وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﺑﺴﺒﺐ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺛﻤﺔ ﻣﻘﺪﻣﺎت ﻷﻓﻜﺎر ﻣﺜﻞ »ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ« و»اﻟﺘﻔ ﱡﺮد اﻟﺘﻜﻨﻮﻟﻮﺟﻲ« ﰲ ﺗﺎرﻳﺦ اﻟﺘﻔﻜري‬
‫اﻟﺪﻳﻨﻲ واﻟﻔﻠﺴﻔﻲ اﻟﻐﺮﺑﻲ أو ﻋﲆ اﻷﻗﻞ ﺗﻮﺟَ ﺪ أﻓﻜﺎر ﻣﺸﺎﺑﻬﺔ ﻟﻬﺎ‪ ،‬وﻻ ﺳﻴﻤﺎ ﰲ اﻟﺘﻘﺎﻟﻴﺪ‬
‫اﻟﻴﻬﻮدﻳﺔ املﺴﻴﺤﻴﺔ وﰲ اﻟﻔﻜﺮ اﻷﻓﻼﻃﻮﻧﻲ‪ .‬وﻋﲆ ﻋﻜﺲ ﻣﺎ ﻳﻌﺘﻘﺪه اﻟﻜﺜريون‪ ،‬ﻓﺈن اﻟﺪﻳﻦ‬
‫واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻛﺎﻧﺎ داﺋﻤً ﺎ ﻣُﱰاﺑ َ‬
‫ﻄني ﰲ ﺗﺎرﻳﺦ اﻟﺜﻘﺎﻓﺔ اﻟﻐﺮﺑﻴﺔ‪ .‬ودﻋﻮﻧﻲ أﺣﴫ ﻧﻘﺎﳾ ﻫﻨﺎ ﰲ‬
‫اﻟﺘﺴﺎﻣﻲ وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ‪.‬‬
‫ﰲ اﻟﺪﻳﻦ اﻟﻼﻫﻮﺗﻲ‪ ،‬ﻳﻘﺼﺪ ﺑﺎﻟﺘﱠﺴﺎﻣﻲ أن اﻹﻟﻪ »ﻓﻮق« اﻟﻌﺎﻟﻢ املﺎدي واﻟﺠﺴﺪي وﻣُﺴﺘﻘﻞ‬
‫ﻋﻨﻪ‪ ،‬وﻫﻲ ﻓﻜﺮة ﻣُﻨﺎﻗﻀﺔ ﻟﻔﻜﺮة أﻧﻪ ﻣﻮﺟﻮد ﰲ اﻟﻌﺎ َﻟﻢ وأﻧﻪ ﺟﺰء ﻣﻨﻪ )اﻟﺤﻠﻮﻟﻴﺔ(‪ .‬ﰲ اﻟﺘﻘﻠﻴﺪ‬
‫اﻟﻴﻬﻮدي املﺴﻴﺤﻲ اﻷﺣﺎدي اﻟﻼﻫﻮﺗﻲ‪ ،‬ﻳُﺮى ﷲ ﻋﲆ أﻧﻪ ﻳﺘﺴﺎﻣﻰ ﻓﻮق ﺧﻠﻘﻪ‪ .‬وﻳُﻤﻜﻦ ﰲ‬
‫اﻟﻮﻗﺖ ﻧﻔﺴﻪ ً‬
‫أﻳﻀﺎ أن ﻳُﺮى ﻋﲆ أﻧﻪ ﻣُﺘﻐﻠﻐﻞ ﰲ ﻛﻞ ﻣﺨﻠﻮﻗﺎﺗِﻪ وﰲ ﻛﻞ اﻟﻜﺎﺋﻨﺎت )أي إﻧﻪ‬
‫ﱠ‬
‫ﻳﺘﺠﲆ ﻣﻦ ﺧﻼل‬
‫ﻳﺤ ﱡﻞ ﻓﻴﻬﺎ(‪ ،‬وﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ اﻟﻼﻫﻮت اﻟﻜﺎﺛﻮﻟﻴﻜﻲ‪ ،‬ﻳُﻔﻬﻢ ﷲ ﻛﻤﺎ‬
‫اﺑﻨﻪ )املﺴﻴﺢ( واﻟﺮوح اﻟﻘﺪس‪ .‬وﻳﺒﺪو أن ﴎدﻳﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺘﻲ ﺗﺘﺠﲆ ﻓﻴﻬﺎ‬
‫ً‬
‫اﻧﻔﺼﺎﻻ أو ﻓﺠﻮة ﺑني اﻟﺨﺎﻟﻖ‬
‫»ﻋﻘﺪة ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ« ﺗﺆﻛﺪ ﻓﻜﺮة اﻟﺘﺴﺎﻣﻲ ﺑﻤﻌﻨﻰ أن ﻫﻨﺎك‬
‫واملﺨﻠﻮق )ﺑني اﻹﻧﺴﺎن اﻹﻟﻪ واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ(‪ ،‬دون إﻋﻄﺎء اﻟﻜﺜري ﻣﻦ اﻷﻣﻞ ﰲ إﻣﻜﺎﻧﻴﺔ‬
‫ﺗﺠﺎوز ﻫﺬه اﻟﻔﺠﻮة‪.‬‬

‫‪25‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻋﲆ ﻋﻜﺲ ﻣﺎ ﻳﻌﺘﻘﺪه اﻟﻜﺜريون‪ ،‬ﻓﺈن اﻟﺪﻳﻦ واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻛﺎﻧﺎ داﺋﻤً ﺎ ﻣُﱰاﺑ َ‬
‫ﻄني ﰲ ﺗﺎرﻳﺦ اﻟﺜﻘﺎﻓﺔ‬
‫اﻟﻐﺮﺑﻴﺔ‪.‬‬

‫أﻳﻀﺎ أن ﻳُﺸري إﱃ ﺗﺠﺎوز اﻟﺤﺪود‪ ،‬أو ﺗﺨ ﱢ‬
‫اﻟﺘﺴﺎﻣﻲ ﻳﻤﻜﻦ ً‬
‫ﻄﻲ ﳾءٍ ﻣﺎ‪ .‬ﰲ اﻟﺘﺎرﻳﺦ‬
‫ري ﻣﻦ اﻷﺣﻴﺎن ﺷﻜ َﻞ اﻟﺴﻤﻮ ﻓﻮق‬
‫اﻟﺪﻳﻨﻲ واﻟﻔﻠﺴﻔﻲ اﻟﻐﺮﺑﻲ‪ ،‬اﺗﺨﺬت ﻫﺬه اﻟﻔﻜﺮة ﰲ ﻛﺜ ٍ‬
‫ﱢ‬
‫املﺘﻮﺳﻂ ﰲ‬
‫اﻟﻌﺎﻟﻢ املﺎدي واﻟﺠﺴﺪي وﺗﺠﺎوُز ﺣﺪوده‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﻣﻨﻄﻘﺔ اﻟﺒﺤﺮ‬
‫اﻟﻘﺮن اﻟﺜﺎﻧﻲ املﻴﻼدي‪ ،‬ﻛﺎﻧﺖ اﻟﻐﻨﻮﺻﻴﺔ ﺗﻨﻈﺮ إﱃ املﺎدﱠة ﺟﻤﻴﻌﻬﺎ ﺑﺎﻋﺘﺒﺎرﻫﺎ ﴍٍّا‪ ،‬وﺗﻬﺪف إﱃ‬
‫ﺗﺤﺮﻳﺮ اﻟﺸﻌﻠﺔ اﻹﻟﻬﻴﺔ ﻣﻦ اﻟﺠﺴﺪ اﻟﺒﴩي‪ .‬وﰲ ٍ‬
‫وﻗﺖ أﺳﺒَﻖ‪ ،‬رأى أﻓﻼﻃﻮن اﻟﺠﺴﺪ ﺳﺠﻨًﺎ‬
‫ﻟﻠﺮوح‪ .‬وﻋﲆ ﻋﻜﺲ اﻟﺠﺴﺪ‪ ،‬ﻛﺎن ﻳﻨﻈﺮ إﱃ اﻟﺮوح ﻋﲆ أﻧﻬﺎ ﺧﺎﻟﺪة‪ .‬وﰲ املﻴﺘﺎﻓﻴﺰﻳﻘﺎ اﻟﺨﺎﺻﺔ‬
‫ﺑﻪ‪ ،‬ﻣﻴﱠﺰ أﻓﻼﻃﻮن ﺑني اﻷﺷﻜﺎل‪ ،‬اﻟﺘﻲ ﻫﻲ أﺑﺪﻳﺔ‪ ،‬واﻷﺷﻴﺎء املﻮﺟﻮدة ﰲ اﻟﻌﺎﻟﻢ‪ ،‬اﻟﺘﻲ ﺗﺘﻐري؛‬
‫ﻓﺎﻷوﱃ ﺗﺘﺴﺎﻣﻰ ﻓﻮق اﻷﺧرية وﺗﺘﺠﺎوزﻫﺎ‪ .‬وﻫﻨﺎك أﻓﻜﺎر ﰲ ﻣﺒﺪأ ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﺗُﺬ ﱢﻛﺮﻧﺎ‬
‫ﺑﻬﺬا‪ .‬ﻓﻬﻲ ﺗُﺤﺎﻓﻆ ﻋﲆ ﻫﺪف اﻟﺘﺴﺎﻣﻲ ﺑﻤﻌﻨﻰ ﺗﺠﺎوز اﻟﻘﻴﻮد اﻟﺒﴩﻳﺔ‪ ،‬وﻟﻴﺲ ﻫﺬا ﻓﺤﺴﺐ‪،‬‬
‫ﺑﻞ إن اﻟﻄﺮق اﻟﺨﺎﺻﺔ اﻟﺘﻲ ﻳُﻔﱰض أن ﻳﺤﺪث ﺑﻬﺎ ﻫﺬا اﻟﺘﺴﺎﻣﻲ ﺗﺴﺘﺤﴬ أﻓﻼﻃﻮن‬
‫واﻟﻐﻨﻮﺻﻴﺔ‪ :‬ﻟﺘﺤﻘﻴﻖ اﻟﺨﻠﻮد‪ ،‬ﻳﺠﺐ اﻟﺘﺴﺎﻣﻲ ﻓﻮق اﻟﺠﺴﺪ اﻟﺒﻴﻮﻟﻮﺟﻲ ﻋﻦ ﻃﺮﻳﻖ ﺗﺤﻤﻴﻞ‬
‫ٍ‬
‫ﺑﺸﻜﻞ أﻛﺜﺮ ﻋﻤﻮﻣﻴﺔ‪ ،‬ﻋﻨﺪﻣﺎ ﻳَﺴﺘﺨﺪِم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫أدوات اﺻﻄﻨﺎﻋﻴﺔ وﺗﻄﻮﻳﺮﻫﺎ‪.‬‬
‫ٍ‬
‫أﺷﻜﺎل أﻛﺜﺮ ﻧﻘﺎءً ﻣﻦ اﻟﻌﺎ َﻟﻢ‬
‫واﻟﻌﻠﻮم واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ذات اﻟﺼﻠﺔ اﻟﺮﻳﺎﺿﻴﺎت ﻻﺳﺘﺨﻼص‬
‫ٍ‬
‫ﱠ‬
‫ﻳﺘﺤﻘﻖ ﺑﻮاﺳﻄﺔ وﺳﺎﺋﻞ‬
‫املﺎدي اﻟﻔﻮﺿﻮي‪ ،‬ﻳﻤﻜﻦ ﺗﻔﺴري ذﻟﻚ ﻋﲆ أﻧﻪ ﺑﺮﻧﺎﻣﺞ أﻓﻼﻃﻮﻧﻲ‬
‫ﺗﻜﻨﻮﻟﻮﺟﻴﺔ‪ .‬وﻣﻦ ﻫﻨﺎ ﱠ‬
‫ﻳﺘﺒني أن ﺧﻮارزﻣﻴﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻲ آﻟﺔ أﻓﻼﻃﻮﻧﻴﺔ ﺗﺴﺘﺨﻠِﺺ‬
‫ً‬
‫ﺷﻜﻼ )أو ﻧﻤﻮذﺟً ﺎ( ﻣﻦ ﻋﺎ َﻟﻢ اﻟﻈﻮاﻫﺮ )اﻟﺒﻴﺎﻧﺎت(‪.‬‬
‫اﻟﺘﺴﺎﻣﻲ ﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ أن ﻳﻌﻨﻲ ﺗﺠﺎوز اﻟﺤﺎﻟﺔ اﻹﻧﺴﺎﻧﻴﺔ‪ .‬ﰲ اﻟﺘﻘﻠﻴﺪ املﺴﻴﺤﻲ‪ ،‬ﻳﻤﻜﻦ‬
‫أن ﻳﺄﺧﺬ ﻫﺬا ﺷﻜﻞ ﻣﺤﺎوﻟﺔ رأب اﻟﻔﺠﻮة ﺑني ﷲ واﻟﺒﴩ ﻣﻦ ﺧﻼل ﺗﺤﻮﻳﻞ اﻟﺒﴩ إﱃ آﻟﻬﺔ‪،‬‬
‫رﺑﻤﺎ ﻋﻦ ﻃﺮﻳﻖ اﺳﺘﻌﺎدة ﺗﺸﺎﺑُﻬﻬﻢ ﻣﻊ اﻵﻟﻬﺔ وﻛﻤﺎﻟﻬﻢ اﻷﺻﲇ )‪ .(Noble 1997‬وﻟﻜﻦ‬
‫َﺳﻌْ ﻲ ﻣﺆﻳﺪي ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﻟﻠﺨﻠﻮد ﻟﻴﺲ ﺟﺪﻳﺪًا‪ ،‬ﺑﻞ ﻳﻤﻜﻦ ﺗﺘﺒﱡﻌﻪ إﱃ اﻟﻌﺼﻮر اﻟﻘﺪﻳﻤﺔ‪.‬‬
‫إذ ﻳُﻤﻜﻨﻨﺎ أن ﻧﺠﺪه ﰲ املﻴﺜﻮﻟﻮﺟﻴﺎ املﻴﺰوﺑﻮﺗﺎﻣﻴﺔ )اﻷﺳﺎﻃري اﻟﺘﻲ ﺗﺄﺗﻲ ﻣﻦ ﻣﻨﻄﻘﺔ ﻣﺎ ﺑني‬
‫اﻟﻨﻬ َﺮﻳﻦ(‪ :‬ﺗﺤﻜﻲ ﻟﻨﺎ ﻗﺼﺔ »ﻣﻠﺤﻤﺔ ﺟﻠﺠﺎﻣﺶ«‪ ،‬وﻫﻲ واﺣﺪة ﻣﻦ أﻗﺪم اﻟﻘﺼﺺ املﻜﺘﻮﺑﺔ ﻋﻦ‬
‫اﻟﺒﴩﻳﺔ‪ ،‬ﻋﻦ ﻣﻠﻚ أوروك )ﺟﻠﺠﺎﻣﺶ(‪ ،‬اﻟﺬي ﻳﺒﺤﺚ ﻋﻦ اﻟﺨﻠﻮد ﺑﻌﺪ وﻓﺎة ﺻﺪﻳﻘﻪ إﻧﻜﻴﺪو‪.‬‬
‫وﻟﻜﻨﻪ ﻳﻔﺸﻞ ﰲ اﻟﻌﺜﻮر ﻋﻠﻴﻪ‪ :‬وﻣﻊ ذﻟﻚ‪ ،‬ﻳﻨﺠﺢ ﰲ اﻟﺤﺼﻮل ﻋﲆ ٍ‬
‫ﻧﺒﺘﺔ ﻳُﻘﺎل إﻧﻬﺎ ﺗُﻌﻴﺪ‬
‫اﻟﺸﺒﺎب‪ ،‬وﻟﻜﻦ ﺗﴪﻗﻬﺎ أﻓﻌﻰ‪ ،‬وﰲ اﻟﻨﻬﺎﻳﺔ‪ ،‬ﱠ‬
‫ﻳﺘﻌني ﻋﻠﻴﻪ أن ﻳﺘﻌ ﱠﻠﻢ اﻟﺪرس ﺑﺄن ﻋﻠﻴﻪ ﻣﻮاﺟﻬﺔ‬
‫‪26‬‬

‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻟﻮﺣﻮش وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺣﻘﻴﻘﺔ ﻣﻮﺗِﻪ ﻫﻮ ﺷﺨﺼﻴٍّﺎ؛ إذ إن ﺳﻌﻴﻪ إﱃ اﻟﺨﻠﻮد ﺑﻼ ﺟﺪوى‪ .‬ﻋﲆ ﻣ ﱢﺮ اﻟﺘﺎرﻳﺦ‪ ،‬ﻛﺎن اﻟﻨﺎس‬
‫ٍ‬
‫ﻋﻼﺟﺎت ﻣﻀﺎدﱠة ﻟﻠﺸﻴﺨﻮﺧﺔ‪ .‬وﻣِﻦ‬
‫ﻳﺒﺤﺜﻮن ﻋﻦ إﻛﺴري اﻟﺤﻴﺎة‪ .‬واﻟﻴﻮم‪ ،‬ﺗﺒﺤﺚ اﻟﻌﻠﻮم ﻋﻦ‬
‫ﻫﺬا املﻨﻄﻠﻖ‪ ،‬ﻓﺈن ﺳﻌﻲ ﻣﺆﻳﺪي ﻣﺒﺪأ ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ إﱃ اﻟﺨﻠﻮد أو إﱃ إﻃﺎﻟﺔ اﻟﻌﻤﺮ ﻟﻴﺲ‬
‫ﺟﺪﻳﺪًا أو ﻏﺮﻳﺒًﺎ؛ ﺑﻞ ﻫﻮ واﺣﺪ ﻣﻦ أﻗﺪم أﺣﻼم اﻟﺒﴩﻳﺔ وأﻫﺪاف اﻟﻌﻠﻢ ا ُملﻌﺎﴏ‪ .‬وﰲ أﻳﺪي‬
‫ﻣﺆﻳﺪي ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬ﻳُﺼﺒﺢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻮ أداة اﻟﺘﺠﺎوز اﻟﺘﻲ ﺗَﻌِ ﺪﻧﺎ ﺑﺎﻟﺨﻠﻮد‪.‬‬
‫ﻣﻦ املﻔﺎﻫﻴﻢ اﻟﻘﺪﻳﻤﺔ اﻷﺧﺮى اﻟﺘﻲ ﺗﺴﺎﻋﺪﻧﺎ ﻋﲆ وﺿﻊ أﻓﻜﺎر ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﰲ‬
‫ﺳﻴﺎﻗﻬﺎ‪ ،‬وﻻ ﺳﻴﻤﺎ ﻓﻜﺮة اﻟﺘﻔ ﱡﺮد اﻟﺘﻜﻨﻮﻟﻮﺟﻲ‪ ،‬ﻣﻔﻬﻮم ﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ )أﺑﻮﻛﺎﻟﻴﺒﺲ( واﻷﺧﺮوﻳﺔ‪.‬‬
‫وﻣﺼﻄﻠﺢ »أﺑﻮﻛﺎﻟﻴﺒﺲ« ﻋﻨﺪ اﻹﻏﺮﻳﻖ اﻟﻘﺪﻣﺎء‪ ،‬اﻟﺬي ﻳﻠﻌﺐ ً‬
‫أﻳﻀﺎ دو ًرا ﰲ اﻟﻔﻜﺮ اﻟﻴﻬﻮدي‬
‫واملﺴﻴﺤﻲ‪ ،‬ﻳُﺸري إﱃ ﻛﺸﻒ اﻟﺤﺠﺎب‪ .‬وﰲ اﻟﻮﻗﺖ اﻟﺤﺎﴐ‪ ،‬ﻳُﺸري ﻫﺬا املﺼﻄﻠﺢ ﻏﺎﻟﺒًﺎ إﱃ‬
‫ﻧﻮع ﱠ‬
‫ﻣﻌني ﻣﻦ اﻟﻜﺸﻒ‪ :‬وﻫﻮ ﻛﺸﻒ ﺳﻴﻨﺎرﻳﻮ ﻧﻬﺎﻳﺔ اﻟﺰﻣﺎن أو ﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ‪ .‬وﰲ اﻟﺴﻴﺎﻗﺎت‬
‫اﻟﺪﻳﻨﻴﺔ‪ ،‬ﻧﺠﺪ ﻣﺼﻄﻠﺢ »اﻷﺧﺮوﻳﺔ«‪ :‬وﻫﻮ ﺟﺰء ﻣﻦ ﻋﻠﻢ اﻟﻼﻫﻮت ﻳﺘﻌ ﱠﻠﻖ ﺑﺎﻷﺣﺪاث اﻟﻨﻬﺎﺋﻴﺔ‬
‫ﻟﻠﺘﺎرﻳﺦ واملﺼري اﻟﻨﻬﺎﺋﻲ ﻟﻠﺒﴩﻳﺔ‪ .‬وﺗﻨﻄﻮي ﻣﻌﻈﻢ اﻷﻓﻜﺎر اﻷﺧﺮوﻳﺔ وﺗﻠﻚ اﻟﺘﻲ ﺗﺘﻌ ﱠﻠﻖ‬
‫ﺑﻨﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﻋﲆ ﺗﺨﺮﻳﺐ أو ﺗﺪﻣري ِﺟﺬري وﻏﺎﻟﺒًﺎ ﻋﻨﻴﻒ ﻟﻠﻌﺎﻟﻢ‪ ،‬واﻻﺗﺠﺎه ﻧﺤﻮ ﻣﺴﺘﻮى‬
‫أﻋﲆ ﻣﻦ اﻟﻮاﻗﻊ واﻟﻜﻴﻨﻮﻧﺔ واﻟﻮﻋﻲ‪ .‬وﻳُﺬﻛﺮﻧﺎ ذﻟﻚ ً‬
‫أﻳﻀﺎ ﺑﺎﻟﻄﻮاﺋﻒ واﻟﺠﻤﺎﻋﺎت املﺘﻄﺮﻓﺔ‬
‫املﺘﺸﺎﺋﻤﺔ اﻟﺘﻲ ﻛﺎﻧﺖ وﻣﺎ ﺗﺰال ﺗﺘﻨﺒﱠﺄ ﺑﺎﻟﻜﻮارث وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ‪ .‬ورﻏﻢ أن ﻣﺆﻳﺪي ﺗﺠﺎوز‬
‫اﻹﻧﺴﺎﻧﻴﺔ ﰲ اﻟﻌﺎدة ﻟﻴﺲ ﻟﻬﻢ ﻋﻼﻗﺔ ﺑﻤﺜﻞ ﻫﺬه اﻟﻄﻮاﺋﻒ واملﻤﺎرﺳﺎت اﻟﺪﻳﻨﻴﺔ‪ ،‬ﻓﺈن ﻓﻜﺮة‬
‫اﻟﺘﻔﺮد اﻟﺘﻜﻨﻮﻟﻮﺟﻲ ﺗُﺸﺒﻪ إﱃ ﺣ ﱟﺪ ﻣﺎ ﴎدﻳﺎت ﻧﻬﺎﻳﺔ اﻟﻌﺎ َﻟﻢ واﻷﺧﺮوﻳﺔ واﻟﺘﻨﺒﺆ ﺑﺎﻟﻜﻮارث‪،‬‬
‫وﻫﺬا أﻣﺮ واﺿﺢ‪.‬‬
‫ﺑﺎﻟﺘﺎﱄ‪ ،‬ﺑﻴﻨﻤﺎ ﻳﺴﺘﻨﺪ ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ ﻋﻠ ٍﻢ ﻣﻦ ا ُملﻔﱰَض أﻧﻪ ﻻ ﺧﻴﺎﱄ وﻻ‬
‫اﻗﱰاح‬
‫دﻳﻨﻲ‪ ،‬وﺑﻴﻨﻤﺎ ﻳﻨﺄى ﻣﺆﻳﺪو ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﺑﺄﻧﻔﺴﻬﻢ ﻋﺎد ًة ﻋﻦ اﻟﺪﻳﻦ وﻳﺮﻓﻀﻮن أيﱠ‬
‫ٍ‬
‫ﺑﺄن أﻋﻤﺎﻟﻬﻢ ﺗﺴﺘﻨِﺪ إﱃ اﻟﺨﻴﺎل‪ ،‬إﻻ أن اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ واﻷﻓﻜﺎر اﻟﺪﻳﻨﻴﺔ واﻟﻔﻠﺴﻔﻴﺔ اﻟﻘﺪﻳﻤﺔ‬
‫ﺗﻠﻌﺐ ﺑﺎﻟﴬورة دو ًرا ﻣُﻬﻤٍّ ﺎ ﻋﻨﺪﻣﺎ ﻧﻨﺎﻗﺶ ﻣُﺴﺘﻘﺒﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ ﻫﺬا املﻨﻄﻠﻖ‪.‬‬
‫ﻛﻴﻔﻴﺔ ﺗﺠﺎوز ﴎدﻳﺎت املﻨﺎﻓﺴﺔ وﺗﺠﺎوز اﻟﻀﺠﱠ ﺔ ا ُملﺜﺎرة ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻳﻤﻜﻦ ﻟﻠﻤﺮء أن ﻳﺘﺴﺎءل اﻵن‪ :‬ﻫﻞ ﻫﻨﺎك ﺳﺒُﻞ ﻟﻠﻨﺠﺎة؟ ﻫﻞ ﻳُﻤﻜﻨﻨﺎ ﺗﺠﺎوز ﴎدﻳﺎت املﻨﺎﻓﺴﺔ‬
‫ً‬
‫رﺳﻮﺧﺎ ﻟﻔﻬﻢ ﻣﺴﺘﻘﺒﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ا ُملﻤﺎﺛﻠﺔ؟ أم‬
‫ﻃﺮق أﻛﺜﺮ‬
‫وإﻳﺠﺎد‬
‫ٍ‬
‫إن اﻟﺘﻔﻜري اﻟﻐﺮﺑﻲ ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺤﻜﻮم ﻋﻠﻴﻪ ﺑﺎﻟﺒﻘﺎء ﰲ ﺳﺠﻦ ﻫﺬه املﺨﺎوف‬
‫‪27‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﻌﴫﻳﺔ وﺟﺬورﻫﺎ اﻟﻘﺪﻳﻤﺔ؟ ﻫﻞ ﻳُﻤﻜﻨﻨﺎ ﺗﺠﺎوز اﻟﻀﺠﺔ املﺜﺎرة ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟‬
‫ُﻨﺼﺒﱠﺔ ﻋﲆ اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ؟ أﻋﺘﻘﺪ أن ﻟﺪَﻳﻨﺎ ً‬
‫أم ﺳﺘﻈ ﱡﻞ املﻨﺎﻗﺸﺔ ﻣ َ‬
‫ﺳﺒﻼ ﻟﻠﻨﺠﺎة‪.‬‬
‫رﻏﻢ أن ﻣﺆﻳﺪي ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﰲ اﻟﻌﺎدة ﻟﻴﺲ ﻟﻬﻢ ﻋﻼﻗﺔ ﺑﻤﺜﻞ ﻫﺬه اﻟﻄﻮاﺋﻒ وا ُملﻤﺎرﺳﺎت اﻟﺪﻳﻨﻴﺔ‪،‬‬
‫ﻓﺈن ﻓﻜﺮة اﻟﺘﻔ ﱡﺮد اﻟﺘﻜﻨﻮﻟﻮﺟﻲ ﺗُﺸﺒﻪ إﱃ ﺣ ﱟﺪ ﻣﺎ ﴎدﻳﺎت ﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ واﻷﺧﺮوﻳﺔ واﻟﺘﻨﺒﺆ ﺑﺎﻟﻜﻮارث‪.‬‬

‫ً‬
‫أوﻻ‪ ،‬ﻳﻤﻜﻨﻨﺎ ﺗﺠﺎوز اﻟﺜﻘﺎﻓﺔ اﻟﻐﺮﺑﻴﺔ ﻟﻠﻌﺜﻮر ﻋﲆ أﻧﻮاع ﻣﺨﺘﻠﻔﺔ ﻣﻦ اﻟﴪدﻳﱠﺎت ﻏري‬
‫ا َملﺒﻨﻴﺔ ﻋﲆ »ﻋﻘﺪة ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ« ﻓﻴﻤﺎ ﻳﺨﺺ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﻃﺮق اﻟﺘﻔﻜري ﻏري اﻷﻓﻼﻃﻮﻧﻴﺔ‪.‬‬
‫ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ اﻟﻴﺎﺑﺎن ﺣﻴﺚ ﺗﺘﺄﺛﺮ ﺛﻘﺎﻓﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﺑﺪﻳﺎﻧﺎت اﻟﻄﺒﻴﻌﺔ أﻛﺜﺮ ﻣﻦ‬
‫اﻟﻐﺮب‪ ،‬وﺗﺤﺪﻳﺪًا ﺑﺪﻳﺎﻧﺔ اﻟﺸﻨﺘﻮ‪ ،‬وﺣﻴﺚ ﺻﻮﱠرت اﻟﺜﻘﺎﻓﺔ اﻟﺸﻌﺒﻴﺔ اﻵﻻت ﻛﻤُﺴﺎﻋﺪﻳﻦ‪ ،‬ﻧﺠﺪ‬
‫ً‬
‫ﻣﻮﻗﻔﺎ أﻛﺜﺮ ودٍّا ﺗﺠﺎه اﻟﺮوﺑﻮﺗﺎت واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻫﻨﺎ‪ ،‬ﻻ ﻧﺠﺪ ﻋﻘﺪة ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ‪.‬‬
‫وﺗﻨﻄﻮي ﻃﺮﻳﻘﺔ اﻟﺘﻔﻜري اﻟﺘﻲ ﻳُﻄﻠﻖ ﻋﻠﻴﻬﺎ أﺣﻴﺎﻧًﺎ »اﻷرواﺣﻴﺔ« ﻋﲆ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻳﻤﻜﻦ ً‬
‫ﻧﻔﺴﺎ‪ ،‬وﻳﻤﻜﻦ أن ﻳ َ‬
‫ً‬
‫أﻳﻀﺎ ﻣﻦ ﺣﻴﺚ املﺒﺪأ أن ﻳﻤﺘﻠﻚ روﺣً ﺎ أو ً‬
‫ﻣﻘﺪﺳﺎ‪ .‬وﻫﺬا ﻳﻌﻨﻲ‬
‫ُﻌﺘﱪ‬
‫ﻋﺪم وﺟﻮد ﴎدﻳﺔ ُ‬
‫ﺗﻨﺎﻓﺴﻴﺔ؛ وﻋﺪم وﺟﻮد رﻏﺒﺔ أﻓﻼﻃﻮﻧﻴﺔ ﰲ ﺗﺠﺎوز املﺎدﻳﺔ واﻟﺪﻓﺎع ا ُملﺴﺘﻤﺮ‬
‫ً‬
‫اﺧﺘﻼﻓﺎ ﺟﻮﻫﺮﻳٍّﺎ‪.‬‬
‫ﻋﻦ اﻹﻧﺴﺎن ﺑﻮﺻﻔﻪ ﻛﺎﺋﻨًﺎ ﻳﺴﻤﻮ ﻓﻮق اﻵﻟﺔ وﻳﺘﺠﺎوزﻫﺎ‪ ،‬أو ﻳﺨﺘﻠﻒ ﻋﻨﻬﺎ‬
‫ِ‬
‫ﺗﺸﺘﻤﻞ اﻟﺜﻘﺎﻓﺔ اﻟﴩﻗﻴﺔ ﻋﲆ أﻓﻜﺎر ﺣﻮل ﻧﻬﺎﻳﺔ اﻟﺰﻣﺎن‪ .‬وﻋﲆ ﻋﻜﺲ‬
‫ﰲ ﺣﺪود ﻣﻌﺮﻓﺘﻲ‪ ،‬ﻻ‬
‫اﻟﺪﻳﺎﻧﺎت اﻟﺘﻮﺣﻴﺪﻳﺔ‪ ،‬ﺗﺤﻤﻞ دﻳﺎﻧﺎت اﻟﻄﺒﻴﻌﺔ ﻓﻬﻤً ﺎ دورﻳٍّﺎ ﻟﻠﺰﻣﻦ‪ .‬وﺑﺎﻟﺘﺎﱄ‪ ،‬ﻳﻤﻜﻦ أن ﻳﺴﺎﻋﺪ‬
‫اﻟﻨﻈﺮ إﱃ ﻣﺎ ﻫﻮ أﺑﻌﺪ ﻣﻦ اﻟﺜﻘﺎﻓﺔ اﻟﻐﺮﺑﻴﺔ )أو ﰲ واﻗﻊ اﻷﻣﺮ إﱃ املﺎﴈ اﻟﻘﺪﻳﻢ ﻟﻠﻐﺮب‪ ،‬ﺣﻴﺚ‬
‫ﻧﺠﺪ ً‬
‫أﻳﻀﺎ دﻳﺎﻧﺎت ﻃﺒﻴﻌﺔ( ﰲ اﻟﺘﻘﻴﻴﻢ اﻟﻨﻘﺪي ﻟﻠﴪدﻳﺎت اﻟﺴﺎﺋﺪة ﺣﻮل ﻣﺴﺘﻘﺒﻞ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﺛﺎﻧﻴًﺎ‪ :‬ﻟﺘﺠﺎوز اﻟﻀﺠﺔ ا ُملﺜﺎرة ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺗﺠﻨﱡﺐ ﺣﴫ ﻣﻨﺎﻗﺸﺔ أﺧﻼﻗﻴﺎت‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ أﺣﻼم املﺴﺘﻘﺒﻞ اﻟﺒﻌﻴﺪ وﻛﻮاﺑﻴﺴﻪ‪ ،‬ﻳُﻤﻜﻨﻨﺎ )‪ (١‬اﺳﺘﺨﺪام اﻟﻔﻠﺴﻔﺔ‬
‫واﻟﻌﻠﻢ ﻟﻔﺤﺺ وﻣﻨﺎﻗﺸﺔ اﻻﻓﱰاﺿﺎت املﺘﻌ ﱢﻠﻘﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻹﻧﺴﺎن اﻟﺬي ﻳﻠﻌﺐ‬
‫دو ًرا ﰲ ﻫﺬه اﻟﺴﻴﻨﺎرﻳﻮﻫﺎت واملﻨﺎﻗﺸﺎت )ﻣﺜﻞ‪ :‬ﻫﻞ اﻟﺬﻛﺎء اﻟﻌﺎم ﻣُﻤﻜﻦ؟ ﻣﺎ اﻟﻔﺎرق‬
‫ﺑني اﻹﻧﺴﺎن واﻵﻟﺔ؟ ﻣﺎ اﻟﻌﻼﻗﺔ ﺑني اﻹﻧﺴﺎن واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ؟ ﻣﺎ اﻟﻮﺿﻊ اﻷﺧﻼﻗﻲ ﻟﻠﺬﻛﺎء‬
‫ﺑﺘﻔﺼﻴﻞ أﻛﺜﺮ إﱃ ﻣﺎﻫﻴﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ املﻮﺟﻮد وﻣﺎ‬
‫اﻻﺻﻄﻨﺎﻋﻲ؟(؛ و)‪ (٢‬اﻟﻨﻈﺮ‬
‫ٍ‬
‫ﻳﻔﻌﻠﻪ اﻟﻴﻮم ﰲ اﻟﺘﻄﺒﻴﻘﺎت املﺨﺘﻠﻔﺔ؛ و)‪ (٣‬ﻣﻨﺎﻗﺸﺔ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ واﻻﺟﺘﻤﺎﻋﻴﺔ اﻷﻛﺜﺮ‬
‫ً‬
‫واﻗﻌﻴﺔ وإﻟﺤﺎﺣً ﺎ اﻟﺘﻲ ﻳُﺜريﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻛﻤﺎ ﻳُﻄﺒﻖ اﻟﻴﻮم؛ و)‪ (٤‬اﻟﺘﻔﻜري ﰲ ﺳﻴﺎﺳﺔ‬
‫‪28‬‬

‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻟﻮﺣﻮش وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻠﻤﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ؛ و)‪ (٥‬ﻃﺮح ﺗﺴﺎؤل ﻋﻤﺎ إذا ﻛﺎن اﻟﱰﻛﻴﺰ ﻋﲆ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻟﺨﻄﺎب اﻟﺠﻤﺎﻫريي اﻟﺤﺎﱄ ﻣُﻔﻴﺪًا ﰲ ﺿﻮء املﺸﻜﻼت اﻷﺧﺮى اﻟﺘﻲ ﺗُﻮاﺟﻬﻨﺎ‪،‬‬
‫وﻣﺎ إذا ﻛﺎن ﺗﺮﻛﻴﺰﻧﺎ ﻳﻨﺒﻐﻲ أن ﻳﻨﺼﺐﱠ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺣﺪَه‪ .‬وﺳﻮف ﻧﺘﺒﻊ ﻫﺬه‬
‫املﺴﺎرات ﰲ اﻟﻔﺼﻮل اﻟﻘﺎدﻣﺔ ﻣﻦ اﻟﻜﺘﺎب‪.‬‬

‫‪29‬‬

‫اﻟﻔﺼﻞ اﻟﺜﺎﻟﺚ‬

‫ﻛﻞ ﻣﺎ ﻟﻪ ﻋﻼﻗﺔ ﺑﺎﻟﺒﴩ‬

‫ﻫﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم ﻣُﻤﻜﻦ؟‬
‫ﻫﻞ ﻫﻨﺎك ﻓﺮوق ﺟﻮﻫﺮﻳﺔ ﺑني اﻹﻧﺴﺎن واﻵﻟﺔ؟‬
‫ﺗﻔﱰض رؤﻳﺔ أﻧﺼﺎر ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﻟﻠﻤُﺴﺘﻘﺒﻞ اﻟﺘﻜﻨﻮﻟﻮﺟﻲ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم‬
‫)أو اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﻮي( ﻣﻤﻜﻦ‪ ،‬وﻟﻜﻦ ﻫﻞ ﻫﻮ ﻛﺬﻟﻚ؟ ﺑﻌﺒﺎر ٍة أﺧﺮى‪ ،‬ﻫﻞ ﻳُﻤﻜﻨﻨﺎ‬
‫إﻧﺸﺎء آﻻت ﺗﺘﻤﺘﱠﻊ ﺑﻘﺪرات ﻣﻌﺮﻓﻴﺔ ﺗُﺸﺒﻪ ﺗﻠﻚ اﻟﺨﺎﺻﺔ ﺑﺎﻟﺒﴩ؟ إذا ﻛﺎﻧﺖ اﻹﺟﺎﺑﺔ ﻻ‪ ،‬ﻓﺈن‬
‫رؤﻳﺔ اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ ﺑﺎﻟﻜﺎﻣﻞ ﺗُﺼﺒﺢ ﻏري ذات ِﺻﻠﺔ ﺑﺄﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻓﺈذا‬
‫ﻛﺎن ِﻣﻦ ا ُملﺴﺘﺤﻴﻞ أن ﺗﺘﻤﺘﱠﻊ اﻵﻻت ﺑﺎﻟﺬﻛﺎء اﻟﺒﴩي اﻟﻌﺎم‪ ،‬ﻓﺈﻧﻨﺎ ﻏري ﻣُﻀﻄﺮﻳﻦ إﱃ أن ﻧﻘﻠﻖ‬
‫ِ‬
‫ﻳﻌﺘﻤﺪ ﻋﲆ ﻓﻬﻤﻨﺎ‬
‫ﺑﺸﻜﻞ ﻋﺎم‪ ،‬ﻳﺒﺪو أن ﺗﻘﻴﻴﻤﻨﺎ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺑﺸﺄن اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ‪.‬‬
‫ٍ‬
‫ملﺎﻫﻴﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ وﻣﺎ ﻳُﻤﻜﻦ أن ﻳﺼﺒﺢ ﻋﻠﻴﻪ ﰲ املﺴﺘﻘﺒﻞ‪ ،‬ﻛﻤﺎ ﻳﻌﺘﻤﺪ‬
‫ﻋﲆ رؤﻳﺘﻨﺎ ﻟﻠﻔﺮوق ﺑني اﻹﻧﺴﺎن واﻵﻟﺔ‪ .‬ﻋﲆ اﻷﻗﻞ ﻣﻨﺬ ﻣﻨﺘﺼﻒ اﻟﻘﺮن اﻟﻌﴩﻳﻦ‪ ،‬ﻧﺎﻗﺶ‬
‫اﻟﻔﻼﺳﻔﺔ واﻟﻌﻠﻤﺎء ﻣﺎ ﺗﺴﺘﻄﻴﻊ أﺟﻬﺰة اﻟﻜﻤﺒﻴﻮﺗﺮ أن ﺗﻘﻮم ﺑﻪ وﻣﺎ ﻳُﻤﻜﻦ أن ﺗُﺼﺒﺢ ﻋﻠﻴﻪ‪،‬‬
‫واﻟﻔﺮوق ﺑني اﻹﻧﺴﺎن واﻵﻟﺔ اﻟﺬﻛﻴﺔ‪ .‬دﻋﻮﻧﺎ ﻧُﻠﻘﻲ ﻧﻈﺮ ًة ﻋﲆ ﺑﻌﺾ ﻫﺬه اﻟﻨﻘﺎﺷﺎت‪ ،‬اﻟﺘﻲ‬
‫ﺗﺘﻨﺎول ﻣﺎﻫﻴﺔ اﻹﻧﺴﺎن وﻣﺎ ﻳﺠﺐ أن ﻳﻜﻮن ﻋﻠﻴﻪ‪ ،‬ﺑﻘﺪْر ﻣﺎ ﺗﺘﻨﺎول ﻣﺎﻫﻴﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫وﻣﺎ ﻳﺠﺐ أن ﻳﻜﻮن ﻋﻠﻴﻪ‪.‬‬
‫ﻫﻞ ﻳﻤﻜﻦ ﻷﺟﻬﺰة اﻟﻜﻤﺒﻴﻮﺗﺮ أن ﺗﺘﻤﺘﱠﻊ ﺑﺎﻟﺬﻛﺎء واﻟﻮﻋﻲ واﻹﺑﺪاع؟ ﻫﻞ ﻳُﻤﻜﻨﻬﺎ ﻓﻬﻢ‬
‫اﻷﺷﻴﺎء وإدراك املﻌﺎﻧﻲ؟ ﻫﻨﺎك ﺗﺎرﻳﺦ ﻣﻦ اﻟﻨﻘﺪ واﻟﺸﻚ ﰲ إﻣﻜﺎﻧﻴﺔ وﺟﻮد ذﻛﺎء اﺻﻄﻨﺎﻋﻲ‬
‫ﻣُﺸﺎ ِﺑ ٍﻪ ﻟﺬﻛﺎء اﻹﻧﺴﺎن‪ .‬ﰲ ﻋﺎم ‪ ،١٩٧٢‬ﻧﴩ ﻫﻴﻮﺑﺮت درﻳﻔﻮس‪ ،‬ﻓﻴﻠﺴﻮف ذو ﺧﻠﻔﻴﺔ ﰲ ﻋﻠﻢ‬
‫اﻟﻈﻮاﻫﺮ‪ ،‬ﻛﺘﺎﺑًﺎ ﺑﻌﻨﻮان »ﻣﺎ ﻻ ﺗﺴﺘﻄﻴﻊ أﺟﻬﺰة اﻟﻜﻤﺒﻴﻮﺗﺮ ﻓﻌﻠﻪ«‪ 1 .‬ﻣﻨﺬ اﻟﺴﺘﻴﻨﻴﱠﺎت‪ ،‬ﻛﺎن‬
‫درﻳﻔﻮس ﻳُﻈﻬﺮ اﻧﺘﻘﺎدًا ﺷﺪﻳﺪًا ﻟﻸﺳﺎس اﻟﻔﻠﺴﻔﻲ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺷ ﱠﻜﻚ ﰲ وﻋﻮده‪:‬‬
‫وﻗﺎل إن ﺑﺮﻧﺎﻣﺞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺒﺤﺜﻲ ﻣﺤﻜﻮم ﻋﻠﻴﻪ ﺑﺎﻟﻔﺸﻞ‪ .‬وﻗﺒﻞ أن ﻳﻨﺘﻘﻞ إﱃ‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺑريﻛﲇ‪ ،‬ﻛﺎن ﻳﻌﻤﻞ ﰲ ﻣﻌﻬﺪ ﻣﺎﺳﺎﺗﺸﻮﺳﺘﺲ ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬وﻫﻮ ﻣﻜﺎن ﻣُﻬﻢ ﻟﺘﻄﻮﻳﺮ اﻟﺬﻛﺎء‬
‫ً‬
‫أﺳﺎﺳﺎ ﰲ ذﻟﻚ اﻟﻮﻗﺖ ﻋﲆ ا ُملﻌﺎﻟﺠﺔ اﻟﺮﻣﺰﻳﺔ‪ .‬رأى درﻳﻔﻮس‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬واﻟﺬي ﻛﺎن ﻳﻌﺘﻤﺪ‬
‫أن اﻟﺪﻣﺎغ ﻟﻴﺲ ﺟﻬﺎز ﻛﻤﺒﻴﻮﺗﺮ وأن اﻟﻌﻘﻞ ﻻ ﻳﻌﻤﻞ ﻋﻦ ﻃﺮﻳﻖ ا ُملﻌﺎﻟﺠﺔ اﻟﺮﻣﺰﻳﺔ‪ .‬إن ﻟﺪَﻳﻨﺎ‬
‫ﺧﻠﻔﻴﺔ ﻻ واﻋﻴﺔ ﻣﻦ املﻌﺮﻓﺔ املﺸﱰﻛﺔ اﻟﻘﺎﺋﻤﺔ ﻋﲆ اﻟﺨﱪة وﻣﺎ ﻳﻤﻜﻦ أن ﻳُﻄﻠِﻖ ﻋﻠﻴﻪ ﻫﺎﻳﺪﺟﺮ‬
‫»ﻛﻴﻨﻮﻧﺘﻨﺎ ﰲ اﻟﻌﺎﻟﻢ«‪ ،‬وﻫﺬه املﻌﺮﻓﺔ ِﺿﻤﻨﻴﺔ وﻻ ﻳﻤﻜﻦ ﺗﺸﻜﻴﻠﻬﺎ‪ .‬وﺗﻌﺘﻤﺪ ﺧﱪة اﻹﻧﺴﺎن‪،‬‬
‫ﺣﺴﺐ رأي درﻳﻔﻮس‪ ،‬ﻋﲆ ا ُملﻤﺎرﺳﺔ ً‬
‫ﺑﺪﻻ ﻣﻦ املﻌﺮﻓﺔ‪ .‬وﻻ ﻳﺴﺘﻄﻴﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ا ْﻟﺘِﻘﺎط ﻫﺬا املﻌﻨﻰ واملﻌﺮﻓﺔ اﻟﻀﻤﻨﻴﺔ؛ وإذا ﻛﺎن ﻫﺬا ﻫﻮ ﻫﺪف اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻓﻬﺬا‬
‫ُ‬
‫ﻣﺤﺾ أﺳﺎﻃري‪ .‬ﻓﺎﻟﺒﴩ وﺣﺪَﻫﻢ ﻗﺎدرون ﻋﲆ رؤﻳﺔ ﻣﺎ ﻫﻮ ذو ﺻﻠﺔ ﻷﻧﻬﻢ‪ ،‬ﺑﻮﺻﻔِ ﻬﻢ ﻛﺎﺋﻨﺎت‬
‫ﻣ ﱢ‬
‫ُﺘﺠﺴﺪة ووﺟﻮدﻳﺔ‪ ،‬ﻳﺸﺎرﻛﻮن ﰲ اﻟﻌﺎﻟﻢ وﻗﺎدرون ﻋﲆ اﻻﺳﺘﺠﺎﺑﺔ ملﺘﻄﻠﺒﺎت اﻟﻮﺿﻊ‪.‬‬
‫ﻫﻨﺎك ﺗﺎرﻳﺦ ﻣﻦ اﻟﻨﻘﺪ واﻟﺸﻚ ﰲ إﻣﻜﺎﻧﻴﺔ وﺟﻮد ذﻛﺎء اﺻﻄﻨﺎﻋﻲ ﻣُﺸﺎ ِﺑ ٍﻪ ﻟﺬﻛﺎء اﻹﻧﺴﺎن‪.‬‬

‫ﰲ ذﻟﻚ اﻟﻮﻗﺖ‪ ،‬واﺟ َﻪ درﻳﻔﻮس اﻟﻜﺜري ﻣﻦ املﻌﺎرﺿﺔ‪ ،‬وﻟﻜﻦ ﰲ ٍ‬
‫وﻗﺖ ﻻﺣﻖ‪ ،‬ﻟﻢ ﻳﻌُ ﺪ‬
‫اﻟﻜﺜريون ﻣﻦ ﺑﺎﺣﺜﻲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻌِ ﺪُون ﺑﺘﺤﻘﻴﻖ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم أو‬
‫ﱠ‬
‫ﻳﺘﻮﻗﻌﻮن ﺗﺤﻘﻴﻘﻪ‪ .‬واﻧﺘﻘﻠﺖ أﺑﺤﺎث اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ اﻻﻋﺘﻤﺎد ﻋﲆ ﻣُﻌﺎﻟﺠﺔ اﻟﺮﻣﻮز إﱃ‬
‫ﻧﻤﺎذج ﺟﺪﻳﺪة‪ ،‬وﻣﻨﻬﺎ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ اﻟﻘﺎﺋﻢ ﻋﲆ اﻹﺣﺼﺎء‪ .‬وﰲ ﺣني ﻛﺎﻧﺖ ﻫﻨﺎك ﻓﺠﻮة ﻫﺎﺋﻠﺔ‬
‫ﰲ وﻗﺖ درﻳﻔﻮس ﺑني ﻋِ ﻠﻢ اﻟﻈﻮاﻫﺮ واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻓﺈن اﻟﻌﺪﻳﺪ ﻣﻦ ﺑﺎﺣﺜﻲ اﻟﺬﻛﺎء‬
‫ﱢ‬
‫املﺘﺠﺴﺪة واملﻮﺟﻮدة‪ ،‬اﻟﺘﻲ ﺗﺪﱠﻋﻲ أﻧﻬﺎ‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻟﻴﻮم ﻳﻌﺘﻨِﻘﻮن ﻣﻨﺎﻫﺞ اﻟﻌﻠﻮم املﻌﺮﻓﻴﺔ‬
‫أﻗﺮب إﱃ ﻋﻠﻢ اﻟﻈﻮاﻫﺮ‪.‬‬
‫ً‬
‫ﺻﺎﺋﺒﺔ وﺗُﻈﻬﺮ ﻛﻴﻒ ﻳﻤﻜﻦ أن ﺗﺘﻌﺎ َرض‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻓﺈن اﻋﱰاﺿﺎت درﻳﻔﻮس ﻻ ﺗﺰال‬
‫وﺟﻬﺎت ﻧﻈﺮ اﻹﻧﺴﺎن ﻏﺎﻟﺒًﺎ ﻣﻊ اﻵراء اﻟﻌﻠﻤﻴﺔ‪ ،‬ﺧﺎﺻﺔ — وﻟﻜﻦ ﻟﻴﺲ ﺣﴫﻳٍّﺎ — ﻓﻴﻤﺎ ﻳُﺴﻤﱠ ﻰ‬
‫ﺑﺎﻟﻔﻠﺴﻔﺔ اﻟﻘﺎرﻳﺔ‪ .‬ﻳُﺸﺪﱢد اﻟﻔﻼﺳﻔﺔ اﻟﻘﺎرﻳﻮن ﻋﺎد ًة ﻋﲆ أن اﻟﺒﴩ واﻟﻌﻘﻮل اﻟﺒﴩﻳﺔ ﻣﺨﺘﻠﻔﺔ‬
‫ً‬
‫اﺧﺘﻼﻓﺎ ﺟﻮﻫﺮﻳٍّﺎ ﻋﻦ اﻵﻻت‪ ،‬وﻳُﺮ ﱢﻛﺰون ﻋﲆ اﻟﺘﺠﺮﺑﺔ اﻹﻧﺴﺎﻧﻴﺔ اﻟﻮاﻋﻴﺔ واﻟﻮﺟﻮد اﻹﻧﺴﺎﻧﻲ‪،‬‬
‫اﻟﺬي ﻻ ﻳﻤﻜﻦ وﻻ ﻳﻨﺒﻐﻲ اﺧﺘﺰاﻟﻪ ﰲ أوﺻﺎف ﺷﻜﻠﻴﺔ أو ﺗﻔﺴريات ﻋﻠﻤﻴﺔ‪ .‬ﻣﻦ ﺟﻬﺔ أﺧﺮى‪،‬‬
‫ﻳﺆﻳﺪ ﺑﻌﺾ اﻟﻔﻼﺳﻔﺔ — ﻏﺎﻟﺒًﺎ ﻣﻦ ﻣﻨﻄﻠﻖ اﻟﺘﻘﻠﻴﺪ اﻟﺘﺤﻠﻴﲇ ﻟﻠﻔﻠﺴﻔﺔ — رؤﻳﺔ ﻟﻺﻧﺴﺎن‬
‫ﺗﺪﻋﻢ اﻟﺒﺎﺣﺜني ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬﻳﻦ ﻳﻌﺘﻘﺪون أن اﻟﺪﻣﺎغ واﻟﻌﻘﻞ اﻟﺒﴩي‬
‫ﻳُﺸﺒﻬﺎن وﻳﻌﻤﻼن ٍّ‬
‫ﺣﻘﺎ ﻣﺜﻞ ﻧﻤﺎذج اﻟﻜﻤﺒﻴﻮﺗﺮ اﻟﺨﺎﺻﺔ ﺑﻬﻢ‪ .‬وﻣﻦ أﻣﺜﻠﺔ ﻫﺆﻻء اﻟﻔﻼﺳﻔﺔ‬
‫‪32‬‬

‫ﻛﻞ ﻣﺎ ﻟﻪ ﻋﻼﻗﺔ ﺑﺎﻟﺒﴩ‬

‫ﺑﻮل ﺗﺸريﺷﻼﻧﺪ وداﻧﻴﻴﻞ دﻧﻴﺖ‪ .‬ﻳﻌﺘﻘﺪ ﺗﺸريﺷﻼﻧﺪ أن اﻟﻌﻠﻢ‪ ،‬وﺧﺎﺻﺔ ﻋِ ﻠﻢ اﻷﺣﻴﺎء اﻟﺘﻄﻮﱡري‬
‫ً‬
‫ﻛﺎﻣﻼ‪ .‬وﻳﻌﺘﻘﺪ‬
‫وﻋﻠﻢ اﻷﻋﺼﺎب‪ ،‬واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳُﻤﻜﻨﻬﻤﺎ ﺗﻔﺴري اﻟﻮﻋﻲ اﻟﺒﴩي ﺗﻔﺴريًا‬
‫ٍ‬
‫ﺷﺒﻜﺔ ﻋﺼﺒﻴﺔ ﻣُﺘﻜ ﱢﺮرة‪ .‬وﻳﻨﻜﺮ وﺟﻮد أﻓﻜﺎر أو ﺗﺠﺎرب ﻏري ﻣﺎدﱢﻳﺔ ﻓﻴﻤﺎ‬
‫أن اﻟﺪﻣﺎغ ﻋﺒﺎرة ﻋﻦ‬
‫ُ‬
‫ﻳُﻄ َﻠﻖ ﻋﻠﻴﻪ املﺎدﻳﺔ اﻹﻗﺼﺎﺋﻴﺔ‪ .‬ﻓﻤﺎ ﻧﺴﻤﱢ ﻴﻪ أﻓﻜﺎ ًرا وﺗﺠﺎرب ﻣﺎ ﻫﻮ إﻻ ﺣﺎﻻت ﻟﻠﺪﻣﺎغ‪ .‬وﻳﻨﻜﺮ‬
‫دﻧﻴﺖ ً‬
‫أﻳﻀﺎ وﺟﻮد أي ﳾءٍ ﺑﺨﻼف ﻣﺎ ﻳﺤﺪُث ﰲ اﻟﺠﺴﻢ‪ :‬وﻳﺮى أﻧﻨﺎ »ﻧﺤﻦ أﻧﻔﺴﻨﺎ ﻧﻮع ﻣﻦ‬
‫اﻟﺮوﺑﻮﺗﺎت« )‪ .(Dennett 1997‬وإذا ﻛﺎن اﻹﻧﺴﺎن ﰲ اﻷﺳﺎس آﻟﺔ واﻋﻴﺔ‪ ،‬ﻓﺈن ﻣﺜﻞ ﻫﺬه‬
‫اﻵﻻت ﻣُﻤﻜﻨﺔ‪ ،‬وﻟﻴﺲ ﻓﻘﻂ ﻣﻦ ﺣﻴﺚ املﺒﺪأ وﻟﻜﻦ ﰲ اﻟﻮاﻗﻊ‪ .‬ﻳُﻤﻜﻨﻨﺎ أن ﻧﺤﺎول ﺻﻨﻌﻬﺎ‪ .‬وﻣﻦ‬
‫ﺑﻤﻜﺎن أن ٍّ‬
‫ﻛﻼ ﻣﻦ اﻟﻔﻼﺳﻔﺔ اﻟﻘﺎرﻳني واﻟﺘﺤﻠﻴﻠﻴني ﻳُﻌﺎرﺿﺎن اﻟﺜﻨﺎﺋﻴﺔ اﻟﺪﻳﻜﺎرﺗﻴﺔ‬
‫اﻷﻫﻤﻴﺔ‬
‫ٍ‬
‫ﻷﺳﺒﺎب ﻣﺨﺘﻠﻔﺔ‪ :‬ﻓﺎﻟﻔﻼﺳﻔﺔ اﻟﻘﺎرﻳﻮن ﻳﻌﺘﻘﺪون أن‬
‫اﻟﺘﻲ ﺗﻔﺼﻞ ﺑني اﻟﻌﻘﻞ واﻟﺠﺴﻢ‪ ،‬وﻟﻜﻦ‬
‫ٍ‬
‫وﺟﻮد اﻹﻧﺴﺎن ﻳﺘﻌ ﱠﻠﻖ ﺑﻜﻮﻧﻪ ﰲ اﻟﻌﺎ َﻟﻢ اﻟﺬي ﻻ ﻳُﻔﺼﻞ ﻓﻴﻪ اﻟﻌﻘﻞ ﻋﻦ اﻟﺠﺴﻢ‪ ،‬أﻣﺎ اﻟﻔﻼﺳﻔﺔ‬
‫ٍّ‬
‫ُﺴﺘﻘﻼ ﻋﻦ اﻟﺠﺴﻢ‪.‬‬
‫ﻷﺳﺒﺎب ﻣﺎدﻳﺔ أن اﻟﻌﻘﻞ ﻟﻴﺲ ﺷﻴﺌًﺎ ﻣ‬
‫اﻟﻘﺎرﻳﻮن ﻓﻴﻌﺘﻘﺪون‬
‫ٍ‬
‫وﻟﻜﻦ ﻟﻴﺲ ﺟﻤﻴﻊ اﻟﻔﻼﺳﻔﺔ اﻟﺘﺤﻠﻴﻠﻴﱢني ﻳ َﺮون أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم أو اﻟﻘﻮي‬
‫ﻣُﻤﻜﻦ‪ .‬ﻣﻦ وﺟﻬﺔ ﻧﻈﺮ اﻟﻔﻴﻠﺴﻮف ﻓﻴﺘﺠﻨﺸﺘﺎﻳﻦ )ﰲ ٍ‬
‫وﻗﺖ ﻻﺣﻖ(‪ ،‬ﻳﻤﻜﻦ ﻟﻠﺸﺨﺺ أن ﻳُﺠﺎدل‬
‫ٍ‬
‫ملﺠﻤﻮﻋﺔ ﻣﻦ اﻟﻘﻮاﻋﺪ أن ﺗﺼﻒ ﻇﺎﻫﺮ ًة ﻣﻌﺮﻓﻴﺔ‪ ،‬ﻓﺈن ذﻟﻚ ﻻ ﻳﻌﻨﻲ‬
‫ﺑﺄﻧﻪ ﰲ ﺣني ﻳﻤﻜﻦ‬
‫ﺑﺎﻟﴬورة أن ﻟﺪَﻳﻨﺎ ﻓﻌﻠﻴٍّﺎ ﻗﻮاﻋﺪ ﰲ رءوﺳﻨﺎ )‪ .(Arkoudas and Bringsjord 2014‬ﻛﻤﺎ‬
‫ﻟﻨﻮع واﺣﺪ ﻣﻦ أﻧﻮاع اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫ﻫﻮ اﻟﺤﺎل ﻣﻊ اﻧﺘﻘﺎد درﻳﻔﻮس‪ ،‬ﻳُﺜري ﻫﺬا ﻣﺸﻜﻠﺔ‬
‫ٍ‬
‫وﻫﻮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺮﻣﺰي‪ ،‬إذا اﻓﱰض أن ﻫﺬه ﻫﻲ اﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻳُﻔ ﱢﻜﺮ ﺑﻬﺎ اﻟﺒﴩ‪.‬‬
‫ﺛﻤﱠ ﺔ اﻧﺘﻘﺎد ﻓﻠﺴﻔﻲ َ‬
‫آﺧﺮ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺄﺗﻲ ﻣﻦ ﺟﻮن ﺳريل‪ ،‬اﻟﺬي ﻳُﻌﺎرض ﻓﻜﺮة أن‬
‫ﺑﺮاﻣﺞ اﻟﻜﻤﺒﻴﻮﺗﺮ ﻳﻤﻜﻦ أن ﺗﻜﻮن ﻟﺪﻳﻬﺎ ﺣﺎﻻت ﻣﻌﺮﻓﻴﺔ ﺣﻘﻴﻘﻴﺔ أو ﻓﻬﻢ ﻟﻠﻤﻌﻨﻰ )‪Searle‬‬
‫‪ .(1980‬وﻓﻴﻤﺎ ﻳﲇ اﻟﺘﺠﺮﺑﺔ اﻟﻔﻜﺮﻳﺔ اﻟﺘﻲ ﻳُﻘﺪﱢﻣﻬﺎ‪ ،‬واﻟﺘﻲ ﺗُﻌ َﺮف ﺑﺎﺳﻢ ﺣﺠﱠ ﺔ اﻟﻐﺮﻓﺔ‬
‫اﻟﺼﻴﻨﻴﺔ‪ :‬ﻳُﺤﺒَﺲ ﺳريل ﰲ ﻏﺮﻓﺔ وﻳُﻌﻄﻰ ﻛﺘﺎﺑﺎت ﺻﻴﻨﻴﺔ وﻟﻜﻨﻪ ﻻ ﻳﻌﺮف اﻟﺼﻴﻨﻴﺔ‪ .‬وﻣﻊ‬
‫ٌ‬
‫أﺷﺨﺎص ﺧﺎرج اﻟﻐﺮﻓﺔ ﻳﺘﺤﺪﺛﻮن ﺑﺎﻟﺼﻴﻨﻴﺔ‬
‫ذﻟﻚ‪ ،‬ﻳﺴﺘﻄﻴﻊ اﻟﺮد ﻋﲆ اﻷﺳﺌﻠﺔ اﻟﺘﻲ ﻳﻄﺮﺣﻬﺎ‬
‫ﻷﻧﻪ ﻳﺴﺘﺨﺪم ُﻛﺘﻴﱢﺐ اﻟﻘﻮاﻋﺪ اﻟﺬي ﻳُﻤ ﱢﻜﻨﻪ ﻣﻦ إﻧﺘﺎج اﻹﺟﺎﺑﺎت اﻟﺼﺤﻴﺤﺔ )ﻣُﺨﺮﺟﺎت( اﺳﺘﻨﺎدًا‬
‫إﱃ املﺴﺘﻨﺪات )املﺪﺧﻼت( اﻟﺘﻲ ﱠ‬
‫ﺑﻨﺠﺎح دون ﻓﻬﻢ اﻟﻠﻐﺔ‬
‫ﻳﺘﻠﻘﺎﻫﺎ‪ .‬وﻫﻮ ﻳﺴﺘﻄﻴﻊ اﻟﻘﻴﺎم ﺑﺬﻟﻚ‬
‫ٍ‬
‫اﻟﺼﻴﻨﻴﺔ‪ .‬وﺑﺎملﺜﻞ‪ ،‬ﻳُﺠﺎدل ﺳريل‪ ،‬ﻳُﻤﻜﻦ ﻟﱪاﻣﺞ اﻟﻜﻤﺒﻴﻮﺗﺮ إﻧﺘﺎج ﻣُﺨﺮﺟَ ﺎت اﺳﺘﻨﺎدًا إﱃ‬
‫ﻣﺪﺧﻼت ﺑﺎﻻﺳﺘﻌﺎﻧﺔ ﺑﺎﻟﻘﻮاﻋﺪ اﻟﺘﻲ ﺗُﺰوﱠد ﺑﻬﺎ‪ ،‬وﻟﻜﻨﻬﺎ ﻻ ﺗﻔﻬﻢ ﺷﻴﺌًﺎ‪ .‬ﺑﻤﺼﻄﻠﺤﺎت ﻓﻠﺴﻔﻴﺔ‬
‫ﱡ‬
‫ﺗﺨﺼ ً‬
‫ﺼﺎ‪ :‬ﻻ ﺗﻤﺘﻠﻚ ﺑﺮاﻣﺞ اﻟﻜﻤﺒﻴﻮﺗﺮ ﻗﺼﺪﻳﺔ‪ ،‬وﻻ ﻳﻤﻜﻦ ﺧﻠﻖ ﻓﻬﻢ ﺣﻘﻴﻘﻲ ﺑﻮاﺳﻄﺔ‬
‫أﻛﺜﺮ‬
‫اﻟﺤﻮﺳﺒﺔ اﻟﺸﻜﻠﻴﺔ‪ .‬أو ﻛﻤﺎ ﻳﻘﻮل ﺑﻮدن )‪ ،(٢٠١٦‬اﻟﻔﻜﺮة ﻫﻲ أن املﻌﻨﻰ ﻳﺄﺗﻲ ﻣﻦ اﻟﺒﴩ‪.‬‬
‫‪33‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن ﺑﺮاﻣﺞ اﻟﻜﻤﺒﻴﻮﺗﺮ اﻟﺤﺎﻟﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﺨﺘﻠﻒ ﻋﻦ‬
‫ﺗﻠﻚ اﻟﺘﻲ اﻧﺘﻘﺪﻫﺎ درﻳﻔﻮس وﺳريل‪ ،‬ﻓﺈن اﻟﻨﻘﺎش ﻻ ﻳﺰال ﻣُﺴﺘﻤ ٍّﺮا‪ .‬ﻳﻌﺘﻘﺪ اﻟﻌﺪﻳﺪ ﻣﻦ‬
‫ً‬
‫ﻓﺮوﻗﺎ ﺣﺎﺳﻤﺔ ﺑني ﻃﺮﻳﻘﺔ ﺗﻔﻜري اﻟﺒﴩ وأﺟﻬﺰة اﻟﻜﻤﺒﻴﻮﺗﺮ‪ .‬ﻋﲆ ﺳﺒﻴﻞ‬
‫اﻟﻔﻼﺳﻔﺔ أن ﻫﻨﺎك‬
‫َ‬
‫ْ‬
‫املﺜﺎل‪ ،‬ﻳﻤﻜﻦ ﻟﻠﻤﺮء اﻟﻴﻮم أن ﻳُﺠﺎدل ﺑﺄﻧﻨﺎ ﻛﺎﺋﻨﺎت ﻗﺎدرة ﻋﲆ ﺧﻠﻖ املﻌﻨﻰ‪ ،‬وواﻋﻴﺔ وﻣ ﱢ‬
‫ُﺘﺠﺴﺪة‬
‫وﺣﻴﺔ‪ ،‬وﻻ ﻳﻤﻜﻦ ﺗﻔﺴري ﻃﺒﻴﻌﺘﻨﺎ وﻋﻘﻮﻟﻨﺎ وﻣﻌﺮﻓﺘﻨﺎ ﺑﺎملﻘﺎرﻧﺔ ﺑﺎﻵﻻت‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻋﻠﻴﻚ أن‬
‫ﺗﻼﺣﻆ أﻧﻪ ﺣﺘﻰ اﻟﻌﻠﻤﺎء واﻟﻔﻼﺳﻔﺔ اﻟﺬﻳﻦ ﻳﻌﺘﻘﺪون أن ﻫﻨﺎك اﻟﻜﺜري ﻣﻦ اﻟﺘﺸﺎﺑُﻪ ﺑني اﻟﺒﴩ‬
‫ري ﻣﻦ‬
‫واﻵﻻت ﻣﻦ ﺣﻴﺚ املﺒﺪأ‪ ،‬وأن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم ﻣُﻤﻜﻦ ﻧﻈﺮﻳٍّﺎ‪ ،‬ﻳﺮﻓﻀﻮن ﰲ ﻛﺜ ٍ‬
‫ﻌﺘﱪ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺸﺎﺑﻪ‬
‫اﻷﺣﻴﺎن رؤﻳﺔ ﺑﻮﺳﱰوم ﻟﻠﺬﻛﺎء اﻟﻔﺎﺋﻖ وأﻓﻜﺎر ﻣُﻤﺎﺛﻠﺔ ﺗَ ِ‬
‫ﱡ‬
‫اﻟﺘﺤﻘﻖ‪ .‬ﻓﺒﻮدن ودﻧﻴﺖ ﻛﻼﻫﻤﺎ ﻳﻌﺘﻘﺪان‬
‫ﻟﺬﻛﺎء اﻹﻧﺴﺎن ﻗﺪ أﺻﺒﺢ ﻗﺎب ﻗﻮﺳني أو أدﻧﻰ ﻣﻦ‬
‫أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم ﺻﻌﺐ ﺟﺪٍّا ﺗﺤﻘﻴﻘﻪ ﻋﻤﻠﻴٍّﺎ‪ ،‬وﺑﺎﻟﺘﺎﱄ ﻟﻴﺲ ﺷﻴﺌًﺎ ﻳﺠﺐ اﻟﻘﻠﻖ‬
‫ﺑﺸﺄﻧﻪ ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ‪.‬‬
‫ﻧﺤﻦ ﻛﺎﺋﻨﺎت ﻗﺎدرة ﻋﲆ َﺧ ْﻠﻖ املﻌﻨﻰ‪ ،‬وواﻋﻴﺔ وﻣﺘﺠﺴﺪة وﺣﻴﺔ‪ ،‬وﻻ ﻳﻤﻜﻦ ﺗﻔﺴري ﻃﺒﻴﻌﺘﻨﺎ وﻋﻘﻮﻟﻨﺎ‬
‫وﻣﻌﺮﻓﺘﻨﺎ ﺑﺎملﻘﺎرﻧﺔ ﺑﺎﻵﻻت‪.‬‬

‫وﺑﻨﺎءً ﻋﻠﻴﻪ ﻳﻤﻜﻨﻨﺎ اﻟﻘﻮل إن ﻫﻨﺎك‪ ،‬ﰲ ﺧﻠﻔﻴﺔ اﻟﻨﻘﺎش ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺗﺒﺎﻳُﻦ‬
‫ﻋﻤﻴﻖ ﰲ اﻵراء ﺣﻮل ﻃﺒﻴﻌﺔ اﻹﻧﺴﺎن واﻟﺬﻛﺎء اﻟﺒﴩي واﻟﻌﻘﻞ واﻟﻔﻬﻢ واﻟﻮﻋﻲ واﻹﺑﺪاع‬
‫واملﻌﻨﻰ واملﻌﺮﻓﺔ اﻟﺒﴩﻳﺔ واﻟﻌﻠﻮم‪ ،‬وﻫﻜﺬا‪ .‬ﻓﺈذا ﻛﺎن ﺛﻤﺔ »ﻣﻌﺮﻛﺔ« ﻣﻦ اﻷﺳﺎس‪ ،‬ﻓﻬﻲ‬
‫ﻣﻌﺮﻛﺔ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻹﻧﺴﺎن ﺑﻘﺪْر ﻣﺎ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫اﻟﺤﺪاﺛﺔ و)ﻣﺎ ﺑﻌﺪ( اﻹﻧﺴﺎﻧﻴﺔ وﻣﺎ ﺑﻌﺪ اﻟﻈﺎﻫﺮﻳﺔ‬
‫ﻧﻈﺮ َ‬
‫أوﺳﻊ ﰲ اﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬ﻣﻦ املﻬﻢ أن ﻧﻀﻊ ﻫﺬه اﻟﻨﻘﺎﺷﺎت ﺣﻮل‬
‫ﻣﻦ وﺟﻬﺔ‬
‫ٍ‬
‫ﺳﻴﺎق َ‬
‫أوﺳﻊ ﻟﻠﻮﻗﻮف ﻋﲆ ﻣﺎﻫﻴﺘﻬﺎ وﻣﺎ ﺗﻨﻄﻮي ﻋﻠﻴﻪ‪ .‬ﻓﻬﺬه‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻹﻧﺴﺎن ﰲ‬
‫ٍ‬
‫ٍ‬
‫اﻧﻘﺴﺎﻣﺎت ﻋﻤﻴﻘﺔ ﰲ‬
‫اﻟﻨﻘﺎﺷﺎت ﻻ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ واﻹﻧﺴﺎن ﻓﺤﺴﺐ‪ ،‬وﻟﻜﻨﻬﺎ ﺗﻌﻜﺲ‬
‫ﺑﺸﻜﻞ ﻏري ﻣﺒﺎﴍ ﰲ‬
‫اﻟﺤﺪاﺛﺔ‪ .‬دﻋﻮﻧﻲ أﻣ ﱡﺮ ﻣﺮور اﻟﻜﺮام ﻋﲆ ﺛﻼﺛﺔ اﻧﻘﺴﺎﻣﺎت ﺗُﺴﺎﻫﻢ‬
‫ٍ‬
‫ﺗﺸﻜﻴﻞ املﻨﺎﻗﺸﺎت اﻷﺧﻼﻗﻴﺔ ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬اﻻﻧﻘﺴﺎم اﻷول ﻫﻮ اﻧﻘﺴﺎم ﻇﻬَ َﺮ ﰲ‬
‫ﻣُﺴﺘﻬَ ﱢﻞ ﻋﴫ اﻟﺤﺪاﺛﺔ ﺑني ﺣﺮﻛﺘَﻲ اﻟﺘﻨﻮﻳﺮ واﻟﺮوﻣﺎﻧﺴﻴﺔ‪ .‬أﻣﺎ َ‬
‫اﻵﺧﺮان ﻓﻬﻤﺎ ﺗﻄﻮﱡرات ﺣﺪﻳﺜﺔ‬
‫‪34‬‬

‫ﻛﻞ ﻣﺎ ﻟﻪ ﻋﻼﻗﺔ ﺑﺎﻟﺒﴩ‬

‫ﻧﺴﺒﻴٍّﺎ‪ :‬اﻷول ﺑني اﻹﻧﺴﺎﻧﻴﺔ وﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬وﻳﺒﻘﻰ ﺣﺒﻴﺲ ﺗﻮﺗﺮات اﻟﺤﺪاﺛﺔ‪ ،‬واﻟﺜﺎﻧﻲ ﺑني‬
‫اﻹﻧﺴﺎﻧﻴﺔ وﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬واﻟﺬي ﻳُﺤﺎول ﺗﺨ ﱢ‬
‫ﻄﻲ اﻟﺤﺪاﺛﺔ‪.‬‬
‫إﺣﺪى وﺳﺎﺋﻞ ﻓﻬﻢ اﻟﻨﻘﺎش ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻹﻧﺴﺎن ﻫﻲ أن ﻧﻀﻊ ﰲ اﻻﻋﺘﺒﺎر‬
‫اﻟﺘﻮﺗﱡﺮ اﻟﻘﺎﺋﻢ ﺑني اﻟﺘﻨﻮﻳﺮ واﻟﺮوﻣﺎﻧﺴﻴﺔ ﰲ اﻟﺤﺪاﺛﺔ‪ .‬ﰲ اﻟﻘﺮﻧَني اﻟﺜﺎﻣﻦ ﻋﴩ واﻟﺘﺎﺳﻊ ﻋﴩ‪،‬‬
‫ﺗﺤﺪى اﻟﻌﻠﻤﺎء وا ُملﻔﻜﺮون اﻟﺘﻨﻮﻳﺮﻳﻮن اﻵراء اﻟﺪﻳﻨﻴﺔ اﻟﺘﻘﻠﻴﺪﻳﺔ وزﻋﻤﻮا أن اﻟﻌﻘﻞ واﻟﺸﻚ‬
‫ﻈﻬﺮ ﻟﻨﺎ ﻣﺎﻫﻴﺔ اﻹﻧﺴﺎن واﻟﻌﺎﻟﻢ اﻟﺤﻘﻴﻘﻴﺔ‪ ،‬ﻋﲆ ﻋﻜﺲ ا ُملﻌﺘﻘﺪات ا ُملﺴ ﱠﻠﻢ ﺑﻬﺎ ﻏري‬
‫واﻟﻌﻠﻢ ﺗُ ِ‬
‫ا ُملﱪرة ﺑﺎﻟﺤﺠﺞ أو ﻏري املﺪﻋﻮﻣﺔ ﺑﺎﻷدﻟﺔ‪ .‬وﻛﺎﻧﻮا ﻣﺘﻔﺎﺋﻠني ﺣﻴﺎل ﻣﺎ ﻳﻤﻜﻦ أن ﻳﻘﺪﱢﻣﻪ اﻟﻌﻠﻢ‬
‫ﻟﺼﺎﻟﺢ اﻹﻧﺴﺎﻧﻴﺔ‪ .‬ردٍّا ﻋﲆ ذﻟﻚ‪ ،‬ﻗﺎل اﻟﺮوﻣﺎﻧﺴﻴﻮن إن اﻟﻌﻘﻞ املﺠ ﱠﺮد واﻟﻌﻠﻢ اﻟﺤﺪﻳﺚ ﻗﺪ‬
‫أﻓﻘﺪا اﻟﻌﺎ َﻟﻢ ِﺳﺤﺮه وأﻧﻨﺎ ﰲ ﺣﺎﺟﺔ إﱃ إﻋﺎدة اﻟﻐﻤﻮض واﻟﺴﺤﺮ اﻟﻠﺬَﻳﻦ ﻳُﺮﻳﺪ اﻟﻌﻠﻢ اﻟﻘﻀﺎء‬
‫ﻋﻠﻴﻬﻤﺎ‪ .‬ﻋﻨﺪ اﻟﻨﻈﺮ إﱃ اﻟﻨﻘﺎش ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻳﺒﺪو ﻟﻨﺎ أﻧﻨﺎ ﻟﻢ ﻧﺒﺘﻌِ ﺪ ﻛﺜريًا‬
‫ﻋﻦ ذﻟﻚ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﺴﺘﻬﺪف ﻋﻤﻞ دﻧﻴﻴﺖ ﺣﻮل اﻟﻮﻋﻲ وﻋﻤﻞ ﺑﻮدﻳﻦ ﺣﻮل اﻹﺑﺪاع‬
‫ٍ‬
‫ﺗﻔﺴريات ﻟﻜﻞ ﳾء‪ ،‬أو ﻛﻤﺎ ﻳﻘﻮل دﻧﻴﻴﺖ »ﻓﻚ اﻟﺴﺤﺮ«‪ .‬ﻓﻬﺬان اﻟﻔﻴﻠﺴﻮﻓﺎن ﻣُﺘﻔﺎﺋﻼن‬
‫ﺗﻘﺪﻳﻢ‬
‫ﱠ‬
‫ﺑﺄن اﻟﻌﻠﻢ ﻳُﻤﻜﻨﻪ ﻛﺸﻒ ﻏﻤﻮض اﻟﻮﻋﻲ واﻹﺑﺪاع وﻏريﻫﻤﺎ‪ .‬إﻧﻬﻤﺎ ﻳُﻌﺎرﺿﺎن ﻛﻞ ﻣَ ﻦ ﻳﻘﺎوم‬
‫ﺟﻬﻮد ﻓﻚ ﺳﺤﺮ اﻹﻧﺴﺎن‪ ،‬ﻣﺜﻞ اﻟﻔﻼﺳﻔﺔ اﻟﻘﺎرﻳني اﻟﺬﻳﻦ ﻳﺴريون ﰲ رﻛﺐ ﻣﺎ ﺑﻌﺪ اﻟﺤﺪاﺛﺔ‬
‫وﻳُﺸﺪﱢدون ﻋﲆ ﻏﻤﻮض ﻣﻌﻨﻰ أن ﺗﻜﻮن إﻧﺴﺎﻧًﺎ؛ ﺑﻌﺒﺎرة أﺧﺮى‪ :‬اﻟﺮوﻣﺎﻧﺴﻴني اﻟﺠُ ﺪد‪ .‬ﻳﺒﺪو‬
‫أن ﺳﺆال »ﻫﻞ ﻧﻔ ﱡﻚ اﻟﺴﺤﺮ أم ﻧﺤﺘﻔﻆ ﺑﻐﻤﻮض اﻹﻧﺴﺎن؟« ﻫﻮ اﻟﺴﺆال اﻟﺮﺋﻴﴘ ﰲ املﻨﺎﻗﺸﺎت‬
‫اﻟﺘﻲ ﺗﺘﻨﺎول اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم وﻣُﺴﺘﻘﺒﻠﻪ‪.‬‬
‫أﻣﺎ اﻟﺘﻮﺗﺮ اﻟﺜﺎﻧﻲ ﻓﻬﻮ ﺑني ﻣﺆﻳﺪي اﻹﻧﺴﺎﻧﻴﺔ وﻣﺆﻳﺪي ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ .‬ﻣﺎ ﻫﻮ‬
‫ﱠ‬
‫ﻳﺘﻌني‬
‫»اﻹﻧﺴﺎن«‪ ،‬وﻣﺎذا ﻳﺠﺐ أن ﻳﻜﻮن؟ ﻫﻞ ﻣﻦ ا ُملﻬﻢ اﻟﺪﻓﺎع ﻋﻦ اﻹﻧﺴﺎن ﻛﻤﺎ ﻫﻮ‪ ،‬أم‬
‫ﻋﻠﻴﻨﺎ ﺗﻌﺪﻳﻞ ﺗﺼﻮﱡرﻧﺎ ﻟﻪ؟ ﻳﺤﺘﻔﻲ دُﻋﺎة اﻹﻧﺴﺎﻧﻴﺔ ﺑﺎﻹﻧﺴﺎن ﻛﻤﺎ ﻫﻮ‪ .‬وﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ‪،‬‬
‫ﻳُﺸﺪﱢدون ﻋﲆ اﻟﻘﻴﻤﺔ اﻟﺠﻮﻫﺮﻳﺔ واملﺘﻔﻮﱢﻗﺔ ﻟﻠﺒﴩ‪ .‬وﻳُﻤﻜﻨﻨﺎ اﻟﻌﺜﻮر ﻋﲆ أﻓﻜﺎر دﻋﺎة اﻹﻧﺴﺎﻧﻴﺔ‬
‫ﰲ اﻟﻨﻘﺎش اﻟﺪاﺋﺮ ﻋﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻟﺤﺠﺞ اﻟﺘﻲ ﺗُﺪاﻓﻊ ﻋﻦ ﺣﻘﻮق اﻹﻧﺴﺎن وﻛﺮاﻣﺘﻪ‬
‫ﻛﺄﺳﺎس ﻷﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬أو ﰲ اﻟﺤﺠﺔ املﺆﻳﺪة ﻷن ﻳﻜﻮن اﻟﺒﴩ و ِﻗﻴَﻤُﻬﻢ ﰲ‬
‫ٍ‬
‫ﻗﻠﺐ وﰲ ﻣﺮﻛﺰ ﻣﺴﺄﻟﺔ ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻣُﺴﺘﻘﺒﻠﻪ‪ .‬ﻫﻨﺎ ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﺘﱠﻔﻖ اﻹﻧﺴﺎﻧﻴﺔ‬
‫أﺷﻜﺎﻻ أﻛﺜﺮ ﺗﺤﻔ ً‬
‫ً‬
‫ﻣﻊ اﻟﺘﻔﻜري اﻟﺘﻨﻮﻳﺮي‪ .‬وﻟﻜﻦ ﻳُﻤﻜﻦ أن ﺗﺄﺧﺬ ً‬
‫ﻈﺎ أو روﻣﺎﻧﺴﻴﺔ‪ .‬ﻛﺬﻟﻚ‬
‫أﻳﻀﺎ‬
‫ﻳُﻤﻜﻨﻨﺎ أن ﻧﻌﺜﺮ ﻋﲆ اﻹﻧﺴﺎﻧﻴﺔ ﰲ ﻣﻘﺎوﻣﺔ ﻣﴩوع دُﻋﺎة ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ .‬ﻓﺒﻴﻨﻤﺎ ﻳﻌﺘﻘﺪ‬
‫دُﻋﺎة ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﱠ‬
‫أن ﻋﻠﻴﻨﺎ ا ُمل ُﴤ ﻗﺪﻣً ﺎ ﻧﺤﻮ ﻧﻮع ﺟﺪﻳ ٍﺪ ﻣﻦ اﻹﻧﺴﺎن ﻳﺘﻢ ﺗﺤﺴﻴﻨُﻪ‬
‫ﺑﻮاﺳﻄﺔ اﻟﻌﻠﻢ واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬ﻳﺪاﻓﻊ اﻹﻧﺴﺎﻧﻴﻮن ﻋﻦ اﻹﻧﺴﺎن ﻛﻤﺎ ﻫﻮ‪ ،‬وﻳﺸﺪدون ﻋﲆ ﻗﻴﻤﺘﻪ‬
‫وﻛﺮاﻣﺘﻪ‪ ،‬اﻟﺘﻲ ﻳُﻘﺎل إﻧﻬﺎ ﻣﻬﺪﱠدة ﻣﻦ ﻗِ ﺒﻞ ﻋﻠﻮم دﻋﺎة ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ وﻓﻠﺴﻔﺘﻬﻢ‪.‬‬
‫‪35‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ردود اﻟﻔﻌﻞ اﻟﺪﻓﺎﻋﻴﺔ ﺗﺠﺎه اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺠﺪﻳﺪة ﻟﻬﺎ ﺗﺎرﻳﺨﻬﺎ اﻟﺨﺎص‪ .‬ﻓﻔﻲ اﻟﻌﻠﻮم‬
‫اﻻﺟﺘﻤﺎﻋﻴﺔ واﻹﻧﺴﺎﻧﻴﺔ‪ ،‬ﻛﺜريًا ﻣﺎ ﺗُ َ‬
‫ﻨﺘﻘﺪ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﺑﺎﻋﺘﺒﺎرﻫﺎ ﺗﻬﺪﻳﺪًا ﻟﻺﻧﺴﺎﻧﻴﺔ واملﺠﺘﻤﻊ‪.‬‬
‫ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻛﺎن ﻛﺜريٌ ﻣﻦ ﻓﻼﺳﻔﺔ اﻟﻘﺮن اﻟﻌﴩﻳﻦ ﺷﺪﻳﺪي اﻟﺘﺸﺎؤم ﺣﻴﺎل اﻟﻌﻠﻢ‪،‬‬
‫وﺣﺬروا ﻣﻦ ﺳﻴﻄﺮة اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻋﲆ املﺠﺘﻤﻊ‪ .‬وﻟﻜﻦ اﻟﴫاع اﻵن ﻻ ﻳﺘﻌ ﱠﻠﻖ ﻓﻘﻂ ﺑﺤﻴﺎة‬
‫اﻹﻧﺴﺎن واملﺠﺘﻤﻊ‪ ،‬ﺑﻞ ﻳﺘﻌ ﱠﻠﻖ ﺑﺎﻹﻧﺴﺎن ﻧﻔﺴﻪ‪ :‬ﻫﻞ ﻧﺤﻦ ﺑﺼﺪد ﺗﺤﺴﻴﻨﻪ وﺗﻄﻮﻳﺮه أم ﻻ؟ ﻫﺬا‬
‫ﻫﻮ اﻟﺴﺆال‪ .‬ﻓﻤﻦ ﺟﻬﺔ‪ ،‬ﻳُﺼﺒﺢ اﻹﻧﺴﺎن ﻧﻔﺴﻪ ﻣﴩوﻋً ﺎ ﻋﻠﻤﻴٍّﺎ ﺗﻜﻨﻮﻟﻮﺟﻴٍّﺎ‪ً ،‬‬
‫ﻗﺎﺑﻼ ﻟﻠﺘﺤﺴني‬
‫واﻟﺘﻄﻮﻳﺮ‪ .‬وﺑﻤﺠ ﱠﺮد أن ﻳ َُﻔﻚ ﺳﺤﺮ اﻹﻧﺴﺎن — ﻣﻦ ﺧﻼل داروﻳﻦ وﻋﻠﻢ اﻷﻋﺼﺎب واﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ — ﻳُﻤﻜﻨﻨﺎ أن ﻧﺒﺪأ ﰲ ﺗﺤﺴﻴﻨﻪ‪ .‬وﻳﻤﻜﻦ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن ﻳُﺴﺎﻋﺪﻧﺎ ﰲ‬
‫ِ‬
‫ﻧﺤﺘﻀﻦ اﻹﻧﺴﺎن ﻛﻤﺎ ﻫﻮ‪ .‬ورﺑﻤﺎ ﻳﻘﻮل‬
‫ﺗﺤﺴني اﻹﻧﺴﺎن‪ .‬وﻣﻦ ﺟﻬﺔ أﺧﺮى‪ ،‬ﻳﺠﺐ ﻋﻠﻴﻨﺎ أن‬
‫اﻟﺒﻌﺾ‪ :‬داﺋﻤً ﺎ ﻣﺎ ﻳﻔﻮﺗﻨﺎ أن ﻧُﺪرك ﻣﺎﻫﻴﺔ اﻹﻧﺴﺎن‪ .‬ﻓﻨﺤﻦ ﻻ ﻧﺴﺘﻄﻴﻊ أن ﻧﻔﻬﻤﻪ ﻓﻬﻤً ﺎ ﺗﺎﻣٍّ ﺎ‬
‫ﺑﻮاﺳﻄﺔ اﻟﻌﻠﻢ‪.‬‬
‫ﺗﺴﺘﻤﺮ ﻫﺬه اﻟﺘﻮﺗﱡﺮات ﰲ ﺗﻘﺴﻴﻢ اﻟﻌﻘﻮل واﻟﻘﻠﻮب ﰲ ﻫﺬا اﻟﻨﻘﺎش‪ .‬ﻓﻬﻞ ﻳُﻤﻜﻨﻨﺎ ﺗﺨ ﱢ‬
‫ﻄﻴﻬﺎ؟‬
‫ﱠ‬
‫ﻳﺘﺨﲆ ﻋﻦ ﻫﺪف إﻧﺸﺎء ذﻛﺎءٍ اﺻﻄﻨﺎﻋﻲ ﺷﺒﻴﻪ ﺑﺎﻹﻧﺴﺎن‪ .‬وﻟﻜﻦ ﺣﺘﻰ‬
‫ﻋﻤﻠﻴٍّﺎ‪ ،‬ﻳﻤﻜﻦ ﻟﻠﻤﺮء أن‬
‫ﰲ ﻫﺬه اﻟﺤﺎﻟﺔ‪ ،‬ﺗﻈ ﱡﻞ ﻫﻨﺎك ﺧﻼﻓﺎت ﺑﺸﺄن وﺿﻊ »آﻻت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻛﻨﻤﺎذج ﻟﻠﺒﴩ«‬
‫ا ُملﺴﺘﺨﺪَم ﰲ ﻋﻠﻢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻫﻞ ﺗُﻌ ﱢﻠﻤﻨﺎ ٍّ‬
‫ﺣﻘﺎ ﺷﻴﺌًﺎ ﻋﻦ ﻛﻴﻔﻴﺔ ﺗﻔﻜري اﻟﺒﴩ؟ أم‬
‫ﻧﻮع ﱠ‬
‫ﻣﻌني ﻣﻦ اﻟﺘﻔﻜري‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﺗﻔﻜري ﻳﻤﻜﻦ ﺻﻴﺎﻏﺘﻪ‬
‫إﻧﻬﺎ ﺗُﻌ ﱢﻠﻤﻨﺎ ﻓﻘﻂ ﺷﻴﺌًﺎ ﻋﻦ ٍ‬
‫ﺑﻮاﺳﻄﺔ اﻟﺮﻳﺎﺿﻴﺎت‪ ،‬أو ﺗﻔﻜري ﻳﻬﺪف إﱃ اﻟﺴﻴﻄﺮة واﻟﺘﻼﻋُ ﺐ؟ إﱃ أي ﻣﺪًى ﻳُﻤﻜﻨﻨﺎ ٍّ‬
‫ﺣﻘﺎ‬
‫اﻟﺘﻌ ﱡﻠﻢ ﻣﻦ ﻫﺬه اﻟﺘﻘﻨﻴﺎت ﻋﻦ اﻹﻧﺴﺎن؟ ﻫﻞ اﻟﺒﴩﻳﺔ أﻛﱪ ﻣﻤﺎ ﻳﺴﺘﻄﻴﻊ اﻟﻌﻠﻢ أن ﻳُﺪرك؟‬
‫ً‬
‫اﻋﺘﺪاﻻ‪ ،‬ﺗﻈﻬﺮ اﻟﴫاﻋﺎت ﺑﺸﺄن اﻟﺤﺪاﺛﺔ‪.‬‬
‫ﺣﺘﻰ ﰲ املﻨﺎﻗﺸﺎت اﻷﻛﺜﺮ‬
‫ﻧﻬﺞ دارﳼ اﻟﻌﻠﻮم اﻻﺟﺘﻤﺎﻋﻴﺔ واﻹﻧﺴﺎﻧﻴﺔ‬
‫ﻟﻠﺨﺮوج ﻣﻦ ﻫﺬا املﺄزق‪ ،‬ﻳُﻤﻜﻦ ﻟﻠﻤﺮء اﺗﺒﺎع ِ‬
‫ً‬
‫ﻃﺮﻗﺎ »ﻏري ﺣﺪﻳﺜﺔ« ﻟﻠﺘﻔﻜري ﺧﻼل اﻟﺨﻤﺴني ﻋﺎﻣً ﺎ املﺎﺿﻴﺔ‪ .‬أوﺿﺢ ﻛﺘﱠﺎبٌ‬
‫اﻟﺬﻳﻦ اﺳﺘﻜﺸﻔﻮا‬
‫ً‬
‫أﻣﺜﺎل ﺑﺮوﻧﻮ ﻻﺗﻮر وﺗﻴﻢ إﻧﺠﻮﻟﺪ أﻧﻪ ﻳﻤﻜﻨﻨﺎ اﻟﻌﺜﻮر ﻋﲆ ﻃﺮق أﻗﻞ ﻣﻴﻼ ﻟﻠﻤُﻘﺎرﻧﺔ ﺑني ﺛﻨﺎﺋﻴﺎت‬
‫وأﻛﺜﺮ ً‬
‫ﻣﻴﻼ ﻟ ﱡﻠﺠﻮء إﱃ اﻟﻼﺣﺪاﺛﺔ ﻋﻨﺪ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﻌﺎ َﻟﻢ ﻣﻦ أﺟﻞ ﺗﺠﺎوز اﻟﺨﻼف ﻣﺎ ﺑني‬
‫اﻟﺘﻨﻮﻳﺮ واﻟﺮوﻣﺎﻧﺴﻴﺔ‪ .‬ﻳُﻤﻜﻨﻨﺎ ﻋﻨﺪﺋ ٍﺬ أن ﻧُﺤﺎول اﺟﺘﻴﺎز اﻟﻔﺠﻮة اﻟﺤﺪﻳﺜﺔ ﺑني اﻟﺒﴩ وﻏري‬
‫اﻟﺒﴩ ﻟﻴﺲ ﻣﻦ ﺧﻼل اﻟﻌِ ﻠﻢ اﻟﺤﺪﻳﺚ أو ﻣﻦ ﺧﻼل ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬اﻟﺘﻲ ﺗﺮى ﻣﻦ وﺟﻬﺔ‬
‫ﴏاع أﺳﺎﳼ‪ ،‬وﻟﻜﻦ ﻣﻦ ﺧﻼل اﻟﻔﻜﺮ ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻲ‬
‫ﻧﻈﺮﻫﺎ أن اﻟﺒﴩ واﻵﻻت ﻟﻴﺴﺎ ﰲ‬
‫ٍ‬
‫ﻣﻦ وﺟﻬﺔ اﻟﻨﻈﺮ )ﻣﺎ ﺑﻌﺪ( اﻹﻧﺴﺎﻧﻴﺔ‪ .‬وﻫﺬا ﻳﺆدي إﱃ اﻟﺘﻮﺗﺮ اﻟﺜﺎﻟﺚ‪ :‬ﺑني اﻹﻧﺴﺎﻧﻴﺔ وﻣﺎ ﺑﻌﺪ‬
‫اﻹﻧﺴﺎﻧﻴﺔ‪ .‬ﻳُﺸ ﱢﻜﻚ ﻣﺆﻳﺪو ﻣﺎ ﺑﻌ َﺪ اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬اﻟﺬﻳﻦ ﻳُﻌﺎرﺿﻮن اﻹﻧﺴﺎﻧﻴني ا ُملﺘﻬﻤني ﺑﺎﻟﻌُ ﻨﻒ‬
‫‪36‬‬

‫ﻛﻞ ﻣﺎ ﻟﻪ ﻋﻼﻗﺔ ﺑﺎﻟﺒﴩ‬

‫ﻣﻊ ﻏري اﻟﺒﴩ‪ ،‬ﻣﺜﻞ اﻟﺤﻴﻮاﻧﺎت‪ ،‬ﺗﺤﺖ ﻣُﺴﻤﱠ ﻰ اﻟﻘﻴﻤﺔ اﻟﻔﺎﺋﻘﺔ ﻟﻺﻧﺴﺎن‪ ،‬ﻳُﺸ ﱢﻜﻜﻮن ﰲ ﻣﺮﻛﺰﻳﺔ‬
‫اﻹﻧﺴﺎن ﰲ اﻷﻧﻈﻤﺔ اﻷﻧﻄﻮﻟﻮﺟﻴﺔ واﻷﺧﻼﻗﻴﺔ اﻟﺤﺪﻳﺜﺔ‪ .‬ﻓﻬﻢ ﻳ َﺮون أن ﻏري اﻟﺒﴩ ﻣُﻬﻤﱡ ﻮن‬
‫ً‬
‫أﻳﻀﺎ‪ ،‬وأﻧﻨﺎ ﻳﺠﺐ أﻻ ﻧﺨﺎف ﻣﻦ ﻋﺒﻮر اﻟﺤﺪود ﺑني اﻟﺒﴩ وﻏري اﻟﺒﴩ‪ .‬وﻫﺬا اﺗﺠﺎه ﻣُﺜري‬
‫ﻟﻼﺳﺘﻜﺸﺎف ﻷﻧﻪ ُ‬
‫ﻳﺄﺧﺬﻧﺎ ﺧﺎرج ﴎدﻳﺔ املﻨﺎﻓﺴﺔ ﺑني اﻟﺒﴩ واﻵﻻت‪.‬‬
‫ً‬
‫رؤﻳﺔ ﺗﺼﻮﱢر أن اﻟﻌﻴﺶ‬
‫ﻳُﻘﺪم ﻣﻨﺎﴏو ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬ﻣﻦ أﻣﺜﺎل دوﻧﺎ ﻫﺎراواي‪،‬‬
‫ﻣﻊ اﻵﻻت‪ ،‬ﺑﻞ رﺑﻤﺎ اﻻﻧﺪﻣﺎج ﻣﻌﻬﺎ‪ ،‬ﻟﻢ ﻳﻌُ ﺪ ﻳُﺮى ﻛﺘﻬﺪﻳ ٍﺪ أو ﻛﻜﺎﺑﻮس‪ ،‬ﻛﻤﺎ ﻛﺎن ﻳﺮى ﻣﻦ‬
‫ﱠ‬
‫ﻳﺘﺤﻘﻖ ملﻨﺎﴏي ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬وﻟﻜﻨﻪ وﺳﻴﻠﺔ ﻳُﻤﻜﻦ ﻣﻦ‬
‫ﻗﺒﻞ دﻋﺎة اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬أو ﻛﺤﻠ ٍﻢ‬
‫ﺧﻼﻟﻬﺎ ﻋﺒﻮر اﻟﺤﺪود اﻷﻧﻄﻮﻟﻮﺟﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ ﺑني اﻟﺒﴩ وﻏري اﻟﺒﴩ‪ .‬وﻣﻦ ﺛَﻢ ﻳﻤﻜﻦ أن‬
‫ﻳﻜﻮن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺟﺰءًا ﻟﻴﺲ ﻣﻦ ﻣﴩوع دُﻋﺎة ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬وﻟﻜﻦ ﻣﻦ ﻣﴩوع‬
‫دُﻋﺎة ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ ا ُملﻬﻢ‪ ،‬اﻟﺬي ﻳﺪﺧﻞ ﻣﻦ ﺟﺎﻧﺐ اﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ واﻟﻔﻨﻮن ً‬
‫ﺑﺪﻻ ﻣﻦ‬
‫اﻟﻌﻠﻢ‪ .‬ﻳﺘﻢ ﻋﺒﻮر اﻟﺤﺪود ﻟﻴﺲ ﺑﺎﺳﻢ اﻟﻌﻠﻢ واﻟﺘﻘﺪﱡم اﻟﻌﺎملﻲ‪ ،‬ﻛﻤﺎ ﻗﺪ ﻳﺮﻏﺐ ﺑﻌﺾ ﻣﻨﺎﴏي‬
‫ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ اﻟﺘﻨﻮﻳﺮﻳني ﰲ اﻟﻘﻮل‪ ،‬وﻟﻜﻦ ﺑﺎﺳﻢ ﺳﻴﺎﺳﺔ ﻣﻨﺎﴏي ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ‬
‫أﻳﻀﺎ أن ﺗُﻘﺪﱢم ﺷﻴﺌًﺎ َ‬
‫وأﻳﺪﻳﻮﻟﻮﺟﻴﺔ ﻋﺒﻮر اﻟﺤﺪود‪ .‬وﻳﻤﻜﻦ ملﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ ً‬
‫آﺧﺮ ﻳﺘﻌ ﱠﻠﻖ‬
‫ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ :‬ﻳﻤﻜﻨﻬﺎ أن ﺗﺤُ ﺜﱠﻨﺎ ﻋﲆ اﻻﻋﱰاف ﺑﺄﻧﻪ »ﻟﻴﺲ ﺛﻤﺔ ﺣﺎﺟﺔ ﻷن ﻳﻜﻮن ﻏري‬
‫اﻟﺒﴩ ﻣُﻤﺎﺛِﻠني ﻟﻨﺎ وﻳﺠﺐ ﻋﺪم ﺟﻌﻠﻬﻢ ﻣُﻤﺎﺛِﻠني ﻟﻨﺎ«‪ .‬ﻳﺒﺪو أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻤﻜﻨﻪ‪،‬‬
‫ﺑﺎﻻﺳﺘﻨﺎد إﱃ آراء ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬أن ﻳُﺤ ﱢﺮر ﻧﻔﺴﻪ ﻣﻦ ﻋﺐء ﺗﻘﻠﻴﺪ اﻹﻧﺴﺎن أو إﻋﺎدة‬
‫أﺷﻜﺎل ﻣﺨﺘﻠﻔﺔ ﻣﻦ اﻟﻮﺟﻮد واﻟﺬﻛﺎء واﻹﺑﺪاع‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪ .‬ﻟﻴﺲ‬
‫ﺑﻨﺎﺋﻪ وﻳﻤﻜﻨﻪ اﺳﺘﻜﺸﺎف‬
‫ٍ‬
‫ﻫﻨﺎك ﺣﺎﺟﺔ ﻷن ﻳُﺼﻨﱠﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ ﺻﻮرﺗﻨﺎ‪ .‬ﻓﺎﻟﺘﻘﺪﱡم ﻫﻨﺎ ﻳﻌﻨﻲ ﺗﺠﺎوز اﻹﻧﺴﺎن‬
‫وﻗﺒﻮل ﻏري اﻟﺒﴩ ﻟﻜﻲ ﻧﺘﻌ ﱠﻠﻢ ﻣﻨﻬﻢ‪ .‬وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻳﻤﻜﻦ أن ﻳﺘﻔﻖ ﻛ ﱞﻞ ﻣﻦ دﻋﺎة ﺗﺠﺎوز‬
‫ُ‬
‫اﻹﻧﺴﺎﻧﻴﺔ وﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ ﻋﲆ أﻧﻪ ً‬
‫اﻟﺘﻨﺎﻓﺲ ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻷداء ﻣﻬﻤﺔ‬
‫ﺑﺪﻻ ﻣﻦ‬
‫ﻣﻌﻴﱠﻨﺔ‪ ،‬ﻳُﻤﻜﻨﻨﺎ ً‬
‫ٍ‬
‫ﱡ‬
‫اﻟﺘﻮﺻﻞ إﻟﻴﻪ ﻣﻦ ﺧﻼل اﻟﺘﻌﺎون وﺣﺸﺪ‬
‫ﻫﺪف ﻣﺸﱰك‪ ،‬ﻳﺘﻢ‬
‫أﻳﻀﺎ ﺗﺤﺪﻳﺪ‬
‫أﻓﻀﻞ ﻣﺎ ﻳﻤﻜﻦ أن ﻳﻘﺪﱢﻣﻪ اﻟﺒﴩ واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ أﺟﻞ اﻟﺘﻮﺟﱡ ﻪ ﻧﺤﻮ ﺗﺤﻘﻴﻖ ذﻟﻚ‬
‫اﻟﻬﺪف املﺸﱰك‪.‬‬
‫وﺳﻴﻠﺔ أﺧﺮى ﻟﺘﺠﺎوز ﴎدﻳﺔ املﻨﺎﻓﺴﺔ — وﻫﻲ وﺳﻴﻠﺔ ﺗﻘﱰب ﰲ ﺑﻌﺾ اﻷﺣﻴﺎن ﻣﻦ‬
‫ﻣﻔﺎﻫﻴﻢ ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ — ﻫﻲ ﻧﻬﺞ ﰲ ﻓﻠﺴﻔﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻳُﺴﻤﱠ ﻰ ﻣﺎ ﺑﻌﺪ اﻟﻈﺎﻫﺮﻳﺔ‪.‬‬
‫ﻳﺴﺘﻨِﺪ درﻳﻔﻮس إﱃ ﻋﻠﻢ اﻟﻈﻮاﻫﺮ أو اﻟﻈﺎﻫﺮﻳﺔ‪ ،‬وﻻ ﺳﻴﻤﺎ أﻋﻤﺎل ﻫﺎﻳﺪﺟﺮ‪ .‬وﻟﻜﻦ اﻷﻓﻜﺎر‬
‫ﻣﺎ ﺑﻌﺪ اﻟﻈﺎﻫﺮﻳﺔ‪ ،‬اﻟﺘﻲ ﺑﺪأﻫﺎ اﻟﻔﻴﻠﺴﻮف دون إﻳﺪه‪ ،‬ﺗﺘﺠﺎوز ﻓﻠﺴﻔﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﻈﺎﻫﺮﻳﺔ‬
‫اﻟﺘﻲ اﺑﺘﻜﺮﻫﺎ ﻫﺎﻳﺪﺟﺮ ﺑﺎﻟﱰﻛﻴﺰ ﻋﲆ ﻛﻴﻔﻴﺔ ﺗﻔﺎﻋﻞ اﻟﺒﴩ ﻣﻊ ﺗﻘﻨﻴﺎت ِﺑﻌَ ﻴﻨﻬﺎ وﻻ ﺳﻴﻤﺎ‬
‫‪37‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ري ﻣﻦ اﻷﺣﻴﺎن ﻣﻊ دراﺳﺎت اﻟﻌﻠﻮم‬
‫املﺼﻨﻮﻋﺎت املﺎدﻳﺔ‪ .‬ﻳُﺮ ﱢﻛﺰ ﻫﺬا اﻟﻨﻬﺞ‪ ،‬اﻟﺬي ﻳﺘﻌﺎون ﰲ ﻛﺜ ٍ‬
‫واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬ﻋﲆ اﻟﺒُﻌﺪ املﺎدي ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻗﺪ ﻳُﻨ َ‬
‫ﻈﺮ إﱃ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ‬
‫ٍ‬
‫ﺑﻤﺼﻨﻮﻋﺎت ﻣﺎدﻳﺔ و ِﺑﻨﻴﺎت‬
‫ﻃﺎﺑﻊ ﻣُﺠ ﱠﺮد أو ﺷﻜﲇ‪ ،‬ﻏري ﻣُﺘﺼﻞ‬
‫ﺑﻌﺾ اﻷﺣﻴﺎن ﻋﲆ أﻧﻪ ذو‬
‫ٍ‬
‫ً‬
‫ﺳﺎﺑﻘﺎ‬
‫أﺳﺎﺳﻴﺔ ﻣُﺤﺪﱠدة‪ .‬وﻟﻜﻦ ﺟﻤﻴﻊ اﻟﺸﻜﻠﻴﺎت واﻟﺘﺠﺮﻳﺪات واﻟﻌﻤﻠﻴﺎت اﻟﺮﻣﺰﻳﺔ املﺬﻛﻮرة‬
‫ٍ‬
‫ِ‬
‫أدوات ﻣﺎدﻳﺔ و ِﺑﻨﻴﺎت أﺳﺎﺳﻴﺔ ﻣﺎدﻳﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻛﻤﺎ ﺳﻨﺮى ﰲ اﻟﻔﺼﻞ‬
‫ﺗﻌﺘﻤﺪ ﻋﲆ‬
‫اﻟﺘﺎﱄ‪ِ ،‬‬
‫ﺑﺸﻜﻞ ﻛﺒري ﻋﲆ اﻟﺸﺒﻜﺎت وإﻧﺘﺎج ﻛﻤﻴﱠﺎت ﺿﺨﻤﺔ‬
‫ﻳﻌﺘﻤﺪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺤﺎﱄ‬
‫ٍ‬
‫ﻣﻦ اﻟﺒﻴﺎﻧﺎت ﺑﺎﺳﺘﺨﺪام اﻷﺟﻬﺰة اﻹﻟﻜﱰوﻧﻴﺔ‪ .‬ﺗﻠﻚ اﻟﺸﺒﻜﺎت واﻷﺟﻬﺰة ﻟﻴﺴﺖ ﻣﺠﺮد أﺷﻴﺎء‬
‫»اﻓﱰاﺿﻴﺔ« وﻟﻜﻦ ﱠ‬
‫ﺑﺸﻜﻞ ﻣﺎدي‪ .‬وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻳﺘﺤﺪﱠث ﻣﺎ ﺑﻌﺪ‬
‫ﻳﺘﻌني إﻧﺘﺎﺟﻬﺎ وﺻﻴﺎﻧﺘﻬﺎ‬
‫ٍ‬
‫اﻟﻈﺎﻫِ ﺮﻳﱢني‪ ،‬ﻣﺜﻞ ﺑﻴﱰ ﺑﻮل ﻓريﺑﻴﻚ‪ ،‬ﻋﻜﺲ اﻟﺘﻘﺴﻴﻢ اﻟﺤﺪﻳﺚ ﺑني املﻮﺿﻮع واملﺤﻤﻮل‪ ،‬ﻋﻦ‬
‫اﻟﺘﺸﻜﻴﻞ ا ُملﺘﺒﺎدل ﺑني اﻟﺒﴩ واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬أو ﻋﲆ اﻷﺣﺮى اﻟﺘﺸﻜﻴﻞ ا ُملﺘﺒﺎدل ﺑني املﻮﺿﻮع‬
‫ً‬
‫وﺑﺪﻻ ﻣﻦ رؤﻳﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻛﺘﻬﺪﻳﺪ‪ ،‬ﻳﺆ ﱢﻛﺪون أن اﻟﺒﴩ ﻣَ ﻴﱠﺎﻟﻮن إﱃ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫واملﺤﻤﻮل‪.‬‬
‫)ﺑﻤﻌﻨﻰ أﻧﻬﻢ ﻛﺎﻧﻮا داﺋﻤً ﺎ ﻳﺴﺘﺨﺪﻣﻮن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ؛ أي إﻧﻬﺎ ﺟﺰء ﻣﻦ وﺟﻮدﻧﺎ وﻟﻴﺴﺖ‬
‫ﺷﻴﺌًﺎ ﺧﺎرﺟﻴٍّﺎ ﻳُﻬﺪﱢد ﻫﺬا اﻟﻮﺟﻮد(‪ ،‬وأن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﺗُﺴﺎﻋﺪ اﻟﺒﴩ ﻋﲆ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﻌﺎﻟﻢ‪.‬‬
‫ﺑﺎﻟﻨﺴﺒﺔ إﱃ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻳﺒﺪو أن ﻫﺬه اﻟﺮؤﻳﺔ ﺗَﻌﻨﻲ أن املﻌﺮﻛﺔ اﻹﻧﺴﺎﻧﻴﺔ ﻟﻠﺪﻓﺎع‬
‫وﺑﺪﻻ ﻣﻦ ذﻟﻚ‪ً ،‬‬
‫ً‬
‫وﻓﻘﺎ ﻟﻬﺬا اﻟﻨﻬﺞ‪ ،‬ﻛﺎن‬
‫ﻋﻦ اﻹﻧﺴﺎن ﺿﺪ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻫﻲ ﻣﻌﺮﻛﺔ ﻣُﻀﻠ َﻠﺔ‪.‬‬
‫اﻹﻧﺴﺎن داﺋﻤً ﺎ ﻣﻴ ًﱠﺎﻻ إﱃ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬وﻟﻬﺬا ﻋﻠﻴﻨﺎ أن ﻧﺴﺄل ﻛﻴﻒ ﻳُﺴﺎﻋﺪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺑﺸﻜﻞ ﺗﻔﺎﻋُ ﲇ ﺑﻴﻨﻤﺎ ﻻ ﻳﺰال‬
‫اﻟﺒﴩ ﰲ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﻌﺎﻟﻢ وﻧﺤﺎول ﺗﺸﻜﻴﻞ ﻫﺬه ا ُملﺴﺎﻋﺪات‬
‫ٍ‬
‫ﺑﺈﻣﻜﺎﻧﻨﺎ‪ :‬إﻧﻨﺎ ﻧﺴﺘﻄﻴﻊ ﻣﻨﺎﻗﺸﺔ اﻷﺧﻼﻗﻴﺎت ﰲ ﻣﺮﺣﻠﺔ ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﻞ ﱠ‬
‫ﻳﺘﻌني‬
‫ﻋﻠﻴﻨﺎ ذﻟﻚ‪ً ،‬‬
‫ﺑﺪﻻ ﻣﻦ أن ﻧﺸﻜﻮ ﻓﻴﻤﺎ ﺑﻌ ُﺪ ﻣﻦ املﺸﻜﻼت اﻟﺘﻲ ﻳُﺴﺒﱢﺒﻬﺎ‪.‬‬
‫ﻳﺒﺪو أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳُﻤﻜﻨﻪ‪ ،‬ﺑﺎﻻﺳﺘﻨﺎد إﱃ آراء ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬أن ﻳُﺤ ﱢﺮر ﻧﻔﺴﻪ ﻣﻦ ﻋﺐء‬
‫أﺷﻜﺎل ﻣﺨﺘﻠﻔﺔ ﻣﻦ اﻟﻮﺟﻮد واﻟﺬﻛﺎء واﻹﺑﺪاع‪ ،‬وﻣﺎ‬
‫ﺗﻘﻠﻴﺪ اﻹﻧﺴﺎن أو إﻋﺎدة ﺑﻨﺎﺋﻪ وﻳُﻤﻜﻨﻪ اﺳﺘﻜﺸﺎف‬
‫ٍ‬
‫إﱃ ذﻟﻚ‪.‬‬

‫وﻣﻊ ذﻟﻚ‪ ،‬رﺑﻤﺎ ﻳﺸﻌُ ﺮ املﺮء ﺑﺎﻟﻘﻠﻖ ﻣﻦ ﱠ‬
‫أن ُرؤى ﻣُﻨﺎﴏي ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ وﻣﺎ ﺑﻌﺪ‬
‫اﻟﻈﺎﻫﺮﻳﺔ ﻟﻴﺴﺖ ﻧﺎﻗﺪ ًة ﺑﻤﺎ ﻓﻴﻪ اﻟﻜﻔﺎﻳﺔ؛ ﻷﻧﻬﺎ ﺷﺪﻳﺪة اﻟﺘﻔﺎؤل وﺷﺪﻳﺪة اﻟﺒُﻌﺪ ﻋﻦ املﻤﺎرﺳﺔ‬
‫اﻟﻌﻠﻤﻴﺔ واﻟﻬﻴﻜﻠﻴﺔ‪ ،‬وﺑﺎﻟﺘﺎﱄ ﻓﻬﻲ ﻟﻴﺴﺖ ﱠ‬
‫ﺣﺴﺎﺳﺔ ﺑﻤﺎ ﻓﻴﻪ اﻟﻜﻔﺎﻳﺔ ﺗﺠﺎه اﻷﺧﻄﺎر اﻟﺤﻘﻴﻘﻴﺔ‬
‫واﻟﻌﻮاﻗﺐ اﻷﺧﻼﻗﻴﺔ وا ُملﺠﺘﻤﻌﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬إن ﻋﺒﻮر اﻟﺤﺪود اﻟﺘﻲ ﻟﻢ ﻳﺴﺒﻖ ﻋﺒﻮرﻫﺎ‬
‫‪38‬‬

‫ﻛﻞ ﻣﺎ ﻟﻪ ﻋﻼﻗﺔ ﺑﺎﻟﺒﴩ‬

‫ﻻ ﻳﻜﻮن ﺑﺎﻟﴬورة ﻣﻦ دون ﻣﺸﻜﻼت‪ ،‬وﰲ املﻤﺎرﺳﺔ اﻟﻌﻤﻠﻴﺔ ﻗﺪ ﻻ ﺗﻔﻴﺪ أﻓﻜﺎر ﻣﺎ ﺑﻌﺪ‬
‫اﻹﻧﺴﺎﻧﻴﺔ وﻣﺎ ﺑﻌﺪ اﻟﻈﺎﻫﺮﻳﺔ ﰲ ﺣﻤﺎﻳﺘﻨﺎ ﻣﻦ اﻟﺘﺴ ﱡﻠﻂ واﻻﺳﺘﻐﻼل اﻟﺬي ﻗﺪ ﻧُﻌﺎﻧﻲ ﻣﻨﻪ ﺟ ﱠﺮاء‬
‫اﺳﺘﺨﺪام ﺗﻘﻨﻴﺎت ﻛﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻳُﻤﻜﻦ ﻟﻠﻤﺮء ً‬
‫أﻳﻀﺎ أن ﻳُﺪاﻓﻊ ﻋﻦ رؤﻳﺔ أﻛﺜﺮ ﺗﻘﻠﻴﺪﻳﺔ‬
‫ﺑﻨﻮع ﺟﺪﻳﺪ ﻣﻦ اﻹﻧﺴﺎﻧﻴﺔ‪ً ،‬‬
‫ﺑﺪﻻ ﻣﻦ أن ﻳﺪﻋﻢ ﻣﺎ ﺑﻌ َﺪ اﻹﻧﺴﺎﻧﻴﺔ‪ .‬وﻫﻜﺬا‬
‫ﻟﻺﻧﺴﺎن أو ﻳُﻄﺎﻟﺐ‬
‫ٍ‬
‫ﻳﺴﺘﻤﺮ اﻟﻨﻘﺎش‪.‬‬

‫‪39‬‬

‫اﻟﻔﺼﻞ اﻟﺮاﺑﻊ‬

‫أﻫﻲ ﺣﻘﺎ ﳎﺮد آﻻت؟‬

‫اﻟﺘﺸﻜﻴﻚ ﰲ املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪:‬‬
‫اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ واﻛﺘﺴﺎب املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ‬
‫إﺣﺪى اﻟﻘﻀﺎﻳﺎ اﻟﺘﻲ أُﺛريَت ﰲ اﻟﻔﺼﻞ اﻟﺴﺎﺑﻖ ﺗﺘﻌ ﱠﻠﻖ ﺑﻤﺎ إذا ﻛﺎن ﻏري اﻟﺒﴩ ﻣُﻬﻤﱢ ني ً‬
‫أﻳﻀﺎ‪.‬‬
‫ﻳﻌﺘﻘﺪ اﻟﻜﺜريون اﻟﻴﻮم أن اﻟﺤﻴﻮاﻧﺎت ﻣُﻬﻤﱠ ﺔ ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ‪ .‬وﻟﻜﻦ ﻟﻢ ﻳ ُﻜﻦ اﻷﻣﺮ ﻛﺬﻟﻚ‬
‫داﺋﻤً ﺎ‪ .‬ﻋﲆ ﻣﺎ ﻳﺒﺪو‪ ،‬ﻛﻨﱠﺎ ﻣُﺨﻄﺌني ﰲ املﺎﴈ ﺑﺸﺄن اﻟﺤﻴﻮاﻧﺎت‪ .‬ﻓﺈذا ﻛﺎن اﻟﻜﺜريون اﻟﻴﻮم‬
‫ﻳﻌﺘﻘﺪون أن اﻵﻻت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺠﺮد آﻻت‪ ،‬ﻓﻬﻞ ﻳﺮﺗﻜِﺒﻮن ﺧﻄﺄ ً ﻣ ً‬
‫ُﻤﺎﺛﻼ؟‬
‫ً‬
‫ﻣﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ؟ ﻫﻞ ﻳﻨﺒﻐﻲ‬
‫ﻫﻞ ﺗﺴﺘﺤِ ﱡﻖ اﻵﻻت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻔﺎﺋﻘﺔ اﻟﺬﻛﺎء‬
‫ً‬
‫ﺑﻤﻜﺎن أن ﻧُﻔ ﱢﻜﺮ ﺣﺘﻰ ﰲ ﻣﺴﺄﻟﺔ ﻣﺎ إذا ﻛﺎﻧﺖ اﻵﻻت‬
‫ﺣﻘﻮﻗﺎ؟ أم إﻧﻪ ﻣﻦ اﻟﺨﻄﻮرة‬
‫أن ﻧُﻌﻄﻴﻬﺎ‬
‫ٍ‬
‫ٍ‬
‫ﺑﻤﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ؟‬
‫ﻳُﻤﻜﻦ أن ﺗﺤﻈﻰ‬
‫إﺣﺪى اﻟﻄﺮق ملﻨﺎﻗﺸﺔ ﻣﺎ ﻫﻮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻣﺎ ﻳﻤﻜﻦ أن ﻳُﺼﺒﺢ ﻋﻠﻴﻪ ﻫﻲ‬
‫ٍ‬
‫أﺳﺌﻠﺔ ﻓﻠﺴﻔﻴﺔ‬
‫اﻟﺴﺆال ﻋﻦ املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻧﺤﻦ ﻫﻨﺎ ﻧﺘﻄ ﱠﺮق إﱃ‬
‫ﻣُﺘﻌ ﱢﻠﻘﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻟﻴﺲ ﻋﱪ املﻴﺘﺎﻓﻴﺰﻳﻘﺎ أو اﻹﺑﺴﺘﻤﻮﻟﻮﺟﻴﺎ أو ﺗﺎرﻳﺦ اﻷﻓﻜﺎر‪،‬‬
‫وﻟﻜﻦ ﻋﱪ ﻓﻠﺴﻔﺔ اﻷﺧﻼق‪ .‬ﻳﻤﻜﻦ أن ﻳُﺸري ﻣﺼﻄﻠﺢ »املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ« )وﻳُﺴﻤﻰ أﺣﻴﺎﻧًﺎ‬
‫»اﻷﻫﻤﻴﺔ اﻷﺧﻼﻗﻴﺔ«( إﱃ ﻧﻮﻋَ ني ﻣﻦ اﻷﺳﺌﻠﺔ‪ .‬اﻷول ﻳﺘﻌ ﱠﻠﻖ ﺑﻤﺎ ﻳُﻤﻜِﻦ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻟﻘﻴﺎم ﺑﻪ ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ؛ ﺑﻌﺒﺎر ٍة أﺧﺮى‪ ،‬ﻣﺎ إذا ﻛﺎن ﻳﻤﻜﻦ أن ﻳﺘﻤﺘﱠﻊ ﺑﻤﺎ ﻳُﻄﻠﻖ‬
‫ﻋﻠﻴﻪ اﻟﻔﻼﺳﻔﺔ »اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ«‪ ،‬وإذا ﻛﺎن اﻷﻣﺮ ﻛﺬﻟﻚ‪ ،‬ﻓﻬﻞ ﻳﺘﻤﺘﱠﻊ ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‬
‫ِ‬
‫اﻟﻜﺎﻣﻠﺔ؟ ﻣﺎذا ﻳﻌﻨﻲ ﻫﺬا؟ ﻳﺒﺪو أن أﻓﻌﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻴﻮم ﻟﻬﺎ ﺑﺎﻟﻔﻌﻞ ﻋﻮاﻗِ ﺐ‬
‫ً‬
‫ً‬
‫»ﺿﻌﻴﻔﺎ« ﻣﻦ أﺷﻜﺎل‬
‫ﺷﻜﻼ‬
‫أﺧﻼﻗﻴﺔ‪ .‬ﺳﻴﺘﱠﻔﻖ ﻣﻌﻈﻢ اﻟﻨﺎس ﻋﲆ أن ﻟﺪى اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ ﺑﻬﺬا املﻌﻨﻰ‪ ،‬واﻟﺬي ﻳُﺸ ِﺒﻪ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣُﻌﻈﻢ اﻟﺴﻴﺎرات اﻟﻴﻮم؛ إذ‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻳﻤﻜﻦ أن ﻳﻜﻮن ﻟﻸﺧرية ً‬
‫أﻳﻀﺎ ﻋﻮاﻗِ ﺐ أﺧﻼﻗﻴﺔ‪ .‬وﻟﻜﻦ إذا ﺳ ﱠﻠﻤﻨﺎ ﺑﺄن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫ﺑﺸﻜﻞ أﻗﻮى ﻣﻦ أﺷﻜﺎل اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ؟‬
‫واﺳﺘﻘﻼﻻ‪ ،‬ﻓﻬﻞ ﻳﻤﻜﻦ أن ﻳﺘﻤﺘﱠﻊ‬
‫ﻳﺰداد ذﻛﺎءً‬
‫ٍ‬
‫ﻳﺠﺐ أن ﻳﺘﻢ ﻣﻨﺤُ ﻪ أو ﺳﻴﺘﻄﻮﱠر ﻟﺪَﻳﻪ ﺑﻌﺾ اﻟﻘﺪرة ﻋﲆ اﻟﺘﻔﻜري اﻷﺧﻼﻗﻲ واﻟﻘﺪرة‬
‫ﻫﻞ ِ‬
‫ﻋﲆ إﺻﺪار اﻷﺣﻜﺎم واﺗﺨﺎذ اﻟﻘﺮارات؟ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ :‬ﻫﻞ ﻳُﻤﻜﻦ وﻫﻞ ﻳﺠﺐ أن ﻧﻌﺘﱪ‬
‫اﻟﺴﻴﺎرات اﻟﺬاﺗﻴﺔ اﻟﻘﻴﺎدة اﻟﺘﻲ ﺗﺴﺘﺨﺪِم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ذات وﻛﺎﻟﺔ أﺧﻼﻗﻴﺔ؟ ﻫﺬه‬
‫اﻷﺳﺌﻠﺔ ﺗﺘﻌ ﱠﻠﻖ ﺑﺄﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﻤﻌﻨﻰ أﻧﻬﺎ ﺗﺘﻄ ﱠﺮق إﱃ ﻣﺎﻫﻴﺔ اﻟﻘﺪرات‬
‫اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ ﻳﻤﻜﻦ أو ﻳﻨﺒﻐﻲ أن ﻳﺘﻤﺘﱠﻊ ﺑﻬﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ وﻟﻜﻦ اﻷﺳﺌﻠﺔ ا ُملﺘﻌﻠﻘﺔ ﺑ‬
‫»املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ« ﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ أن ﺗُﺸري إﱃ ﻛﻴﻒ ﻳﻨﺒﻐﻲ أن ﻧُﻌﺎﻣﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻫﻞ‬
‫ً‬
‫ﺷﻜﻼ ﻣﻦ أﺷﻜﺎل اﻻﺣﱰام اﻷﺧﻼﻗﻲ؟ ﻫﻞ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ »ﻣﺠﺮد آﻟﺔ«‪ ،‬أم أﻧﻪ ﻳﺴﺘﺤﻖ‬
‫ً‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻣﺨﺘﻠﻔﺔ ﻋﻦ اﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻧﺘﻌﺎﻣﻞ ﺑﻬﺎ ﻣﺜﻼ ﻣﻊ آﻟﺔ اﻟﺘﺤﻤﻴﺺ‬
‫ﻳﺠﺐ ﻋﻠﻴﻨﺎ ﻣُﻌﺎﻣﻠﺘﻪ‬
‫ً‬
‫ﻟﻜﻴﺎن ﺻﻨﺎﻋﻲ ذﻛﻲ ﻟﻠﻐﺎﻳﺔ‪ ،‬إذا ﺗﻢ ﺗﻄﻮﻳﺮ ﻣﺜﻞ‬
‫أو املﻐﺴﻠﺔ؟ ﻫﻞ ﻳﺠﺐ أن ﻧﻤﻨﺢ ﺣﻘﻮﻗﺎ‬
‫ٍ‬
‫ﻫﺬا اﻟﻜﻴﺎن ﻳﻮﻣً ﺎ ﻣﺎ‪ ،‬ﺣﺘﻰ ﻟﻮ ﻟﻢ ﻳ ُﻜﻦ ﺑﴩﻳٍّﺎ؟ ﻫﺬا ﻣﺎ ﻳُﻄﻠِﻖ ﻋﻠﻴﻪ اﻟﻔﻼﺳﻔﺔ اﻟﺴﺆال املﺘﻌﻠﻖ‬
‫ﺑ »اﻛﺘﺴﺎب املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ«‪ .‬ﻫﺬا اﻟﺴﺆال ﻳﺘﻌﻠﻖ ﺑﺄﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺬاﺗﻪ‪،‬‬
‫وﻟﻜﻨﻪ ﻳﺘﻌ ﱠﻠﻖ ﺑﺄﺧﻼﻗﻴﺎﺗﻨﺎ ﺗﺠﺎﻫﻪ‪ .‬ﻫﻨﺎ ﻳﻜﻮن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻮﺿ َﻊ اﻫﺘﻤﺎ ٍم ﻣﻦ اﻟﻨﺎﺣﻴﺔ‬
‫وﻛﻴﻼ أﺧﻼﻗﻴٍّﺎ ﻣ ً‬
‫ً‬
‫اﻷﺧﻼﻗﻴﺔ‪ً ،‬‬
‫ُﺤﺘﻤﻼ ﰲ ﺣ ﱢﺪ ذاﺗﻪ‪.‬‬
‫ﺑﺪﻻ ﻣﻦ ﻛﻮﻧﻪ‬
‫ﻫﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ »ﻣﺠﺮد آﻟﺔ«؟ ﻫﻞ ﻳﺠﺐ ﻋﻠﻴﻨﺎ ﻣﻌﺎﻣﻠﺘﻪ ﺑﻄﺮﻳﻘﺔٍ ﻣﺨﺘﻠﻔﺔ ﻋﻦ اﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ‬
‫ﻧﺘﻌﺎﻣﻞ ﺑﻬﺎ ً‬
‫ﻣﺜﻼ ﻣﻊ آﻟﺔ اﻟﺘﺤﻤﻴﺺ أو املﻐﺴﻠﺔ؟‬

‫اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‬
‫ﻟﻨﺒﺪأ ﺑﺎﻟﺘﺤﺪﱡث ﻋﻦ ﺳﺆال اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‪ .‬إذا ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳُﻤﻜﻦ أن ﻳُﺼﺒﺢ‬
‫أﻛﺜﺮ ذﻛﺎءً ﻣﻤﺎ ﻫﻮ ﻋﻠﻴﻪ اﻟﻴﻮم‪ ،‬ﻓﻴُﻤﻜﻨﻨﺎ أن ﻧﻔﱰض أﻧﻪ ﻳﺴﺘﻄﻴﻊ أن ﻳُﻄﻮﱢر ﻗﺪرﺗﻪ ﻋﲆ اﻟﺘﻔﻜري‬
‫اﻷﺧﻼﻗﻲ وأﻧﻪ ﻳﺴﺘﻄﻴﻊ أن ﻳﺘﻌ ﱠﻠﻢ ﻛﻴﻒ ﻳﺘﱠﺨِ ﺬ اﻟﺒﴩ اﻟﻘﺮارات ﺑﺸﺄن اﻟﻘﻀﺎﻳﺎ اﻷﺧﻼﻗﻴﺔ‪.‬‬
‫وﻟﻜﻦ ﻫﻞ ﺳﻴﻜﻮن ﻫﺬا ﻛﺎﻓﻴًﺎ ﻟﻜﻲ ﻳﺤﻈﻰ ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﻜﺎﻣﻠﺔ؛ أي اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‬
‫ً‬
‫ﺧﻴﺎﻻ ﻋﻠﻤﻴٍّﺎ ﺑﺎﻟﻜﺎﻣﻞ‪ .‬ﻓﺈذا ﻛﻨﺎ ِ‬
‫ﻧﻌﺘﻤﺪ اﻟﻴﻮم ﻋﲆ‬
‫اﻟﺘﻲ ﻳﺘﻤﺘﱠﻊ ﺑﻬﺎ اﻹﻧﺴﺎن؟ ﻫﺬا اﻟﺴﺆال ﻟﻴﺲ‬
‫اﻟﺨﻮارزﻣﻴﺎت ﰲ اﺗﺨﺎذ ﺑﻌﺾ ﻗﺮاراﺗﻨﺎ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﰲ اﻟﺴﻴﺎرات أو املﺤﺎﻛﻢ‪ ،‬ﻓﻴﺒﺪو‬
‫ً‬
‫ﺳﻠﻴﻤﺔ ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ‪ .‬وﻟﻜﻦ ﻟﻴﺲ‬
‫أﻧﻪ ﺳﻴﻜﻮن ﻣﻦ ا ُملﻬ ﱢﻢ أن ﺗﻜﻮن ﺗﻠﻚ اﻟﻘﺮارات‬
‫‪42‬‬

‫أﻫﻲ ٍّ‬
‫ﺣﻘﺎ ﻣﺠﺮد آﻻت؟‬

‫ﻣﻦ اﻟﻮاﺿﺢ ﻣﺎ إذا ﻛﺎﻧﺖ اﻵﻻت ﻳﻤﻜﻦ أن ﺗﺘﻤﺘﱠﻊ ﺑﻨﻔﺲ اﻟﻘﺪرات اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ ﻳﺘﻤﺘﱠﻊ ﺑﻬﺎ‬
‫ﺑﺄﻓﻌﺎل ﰲ اﻟﻌﺎ َﻟﻢ‪ ،‬وﻫﺬه اﻷﻓﻌﺎل ﻟﻬﺎ‬
‫اﻟﺒﴩ‪ .‬إﻧﻬﺎ ﺗﺘﻤﺘﱠﻊ ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ ﺑﻤﻌﻨﻰ أﻧﻬﺎ ﺗﻘﻮم‬
‫ٍ‬
‫ﻋﻮاﻗﺐ أﺧﻼﻗﻴﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﺗﺘﺴﺒﱠﺐ ﺳﻴﺎرة ذاﺗﻴﺔ اﻟﻘﻴﺎدة ﰲ ﺣﺎدث‪ ،‬أو ﻗﺪ ﻳﻮﴆ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺴﺠﻦ ﺷﺨﺺ ﱠ‬
‫ﻣﻌني‪ .‬ﻫﺬه اﻟﺴﻠﻮﻛﻴﺎت واﻟﺨﻴﺎرات ﻟﻴﺴﺖ ﺣﻴﺎدﻳﺔ ﻣﻦ‬
‫اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ؛ إذ إن ﻟﻬﺎ ﻋﻮاﻗﺐ أﺧﻼﻗﻴﺔ واﺿﺤﺔ ﻋﲆ اﻷﺷﺨﺎص ذوي اﻟﺼﻠﺔ‪ .‬وﻟﻜﻦ‬
‫ﻟﻠﺘﻌﺎﻣُﻞ ﻣﻊ ﻫﺬه املﺸﻜﻠﺔ‪ ،‬ﻫﻞ ﻳﺠﺐ ﻣﻨﺢ اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ وﻫﻞ ﻳﻤﻜﻦ‬
‫أن ﻳﺘﻤﺘﱠﻊ ﺑﻮﻛﺎﻟﺔ أﺧﻼﻗﻴﺔ ﻛﺎﻣﻠﺔ؟‬
‫ﻫﻨﺎك ﻣﻮاﻗﻒ ﻓﻠﺴﻔﻴﺔ ﻣُﺘﻨﻮﱢﻋﺔ ﺣﻴﺎل ﻫﺬه اﻷﺳﺌﻠﺔ‪ .‬ﻳﻘﻮل ﺑﻌﺾ اﻷﺷﺨﺎص إن اﻵﻻت ﻻ‬
‫ﻳﻤﻜﻦ أن ﺗﺘﻤﺘﱠﻊ أﺑﺪًا ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‪ .‬وﻳﺮى ﻫﺆﻻء أن اﻵﻻت ﻟﻴﺲ ﻟﺪَﻳﻬﺎ اﻟﻘﺪرات اﻟﻼزﻣﺔ‬
‫ﻟﻠﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‪ ،‬ﻣﺜﻞ اﻟﺤﺎﻻت اﻟﻌﻘﻠﻴﺔ أو اﻻﻧﻔﻌﺎﻻت أو اﻹرادة اﻟﺤﺮة‪ .‬وﻟﺬﻟﻚ ﻫﻨﺎك ﺧﻄﻮرة‬
‫ِ‬
‫ٍ‬
‫ﻧﻌﺘﻤﺪ ﻋﻠﻴﻬﺎ ﰲ اﺗﺨﺎذ‬
‫ﻗﺮارات ﺳﻠﻴﻤﺔ أﺧﻼﻗﻴٍّﺎ وأن‬
‫ﻧﻔﱰض أﻧﻬﺎ ﺗﺴﺘﻄﻴﻊ اﺗﺨﺎذ‬
‫ﰲ أن ِ‬
‫ً‬
‫ﻛﺎﻣﻼ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺎﻟﺖ دﻳﺒﻮرا ﺟﻮﻧﺴﻮن )‪ (٢٠٠٦‬إن‬
‫ﻣﺜﻞ ﻫﺬه اﻟﻘﺮارات اﻋﺘﻤﺎدًا‬
‫ٍ‬
‫ﺑﻮﻛﺎﻟﺔ أﺧﻼﻗﻴﺔ ﺧﺎﺻﺔ ﺑﻬﺎ‪ :‬إﻧﻬﺎ ﻣﻦ إﻧﺘﺎج اﻟﺒﴩ وﺗُﺴﺘﺨﺪَم ﻣﻦ‬
‫أﻧﻈﻤﺔ اﻟﻜﻤﺒﻴﻮﺗﺮ ﻻ ﺗﺘﻤﺘﱠﻊ‬
‫ﻗِ ﺒَﻠﻬﻢ‪ ،‬واﻟﺒﴩ وﺣﺪَﻫﻢ ﻟﺪﻳﻬﻢ اﻟﺤﺮﻳﺔ واﻟﻘﺪرة ﻋﲆ اﻟﺘﴫﱡف واﺗﺨﺎذ اﻟﻘﺮارات ﻣﻦ اﻟﻨﺎﺣﻴﺔ‬
‫اﻷﺧﻼﻗﻴﺔ‪ .‬وﺑﺎﻟﻄﺮﻳﻘﺔ ﻧﻔﺴﻬﺎ‪ ،‬ﻳﻤﻜﻦ ﻟﻠﻤﺮء أن ﻳﻘﻮل إن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ إﻧﺘﺎج اﻟﺒﴩ‪،‬‬
‫وﺑﺎﻟﺘﺎﱄ ﻳﺠﺐ أن ﻳﻜﻮن اﺗﺨﺎذ اﻟﻘﺮارات اﻷﺧﻼﻗﻴﺔ ﰲ املﻤﺎرﺳﺎت اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ ﻣﻦ اﺧﺘﺼﺎص‬
‫ٍ‬
‫ﺑﻮﻛﺎﻟﺔ‬
‫اﻟﺒﴩ‪ .‬ﻋﲆ اﻟﻨﻘﻴﺾ ﻣﻦ ذﻟﻚ‪ ،‬ﻫﻨﺎك أوﻟﺌﻚ اﻟﺬﻳﻦ ﻳﻌﺘﻘﺪون أن اﻵﻻت ﻳﻤﻜﻦ أن ﺗﺘﻤﺘﱠﻊ‬
‫أﺧﻼﻗﻴﺔ ﻛﺎﻣﻠﺔ ﺗﻤﺎﻣً ﺎ ﻣﺜﻞ اﻟﺒﴩ‪ .‬وﻳﺰﻋﻢ اﻟﺒﺎﺣﺜﻮن ﻣﺜﻞ ﻣﺎﻳﻜﻞ وﺳﻮزان أﻧﺪرﺳﻮن‪ ،‬ﻋﲆ‬
‫ﺳﺒﻴﻞ املﺜﺎل‪ ،‬أﻧﻪ ﻣﻦ ﺣﻴﺚ املﺒﺪأ ﻳﻤﻜﻦ‪ ،‬ﺑﻞ ﻳﺠﺐ‪ ،‬أن ﺗُﻤﻨﺢ اﻵﻻت ﻧﻮﻋً ﺎ ﻣﻦ اﻷﺧﻼق اﻟﺒﴩﻳﺔ‬
‫)‪ .(Anderson and Anderson 2011‬وﻳُﻤﻜﻨﻨﺎ ﺗﺰوﻳﺪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎملﺒﺎدئ‪ ،‬ورﺑﻤﺎ‬
‫ﺗﻜﻮن اﻵﻻت ﺣﺘﻰ أﻓﻀﻞ ﻣﻦ اﻟﺒﴩ ﰲ اﻟﻮﺻﻮل إﱃ اﻟﻘﺮارات اﻷﺧﻼﻗﻴﺔ ﻧﻈ ًﺮا ﻷﻧﻬﺎ أﻛﺜﺮ‬
‫ﻋﻘﻼﻧﻴﺔ وﻻ ﺗﻨﺠﺮف وراء ﻋﻮاﻃﻔﻬﺎ‪ .‬وﻗﺪ ﺟﺎدل اﻟﺒﻌﺾ‪ ،‬ﻟﺪﺣﺾ ﻫﺬه اﻟﻔﻜﺮة‪ ،‬ﺑﺄن اﻟﻘﻮاﻋﺪ‬
‫اﻷﺧﻼﻗﻴﺔ ﻛﺜريًا ﻣﺎ ﺗﺘﻀﺎرب )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﻧﻈﺮ إﱃ ﻗﺼﺺ اﻟﺮوﺑﻮﺗﺎت ﻷﺳﻴﻤﻮف‪،‬‬
‫ﺣﻴﺚ ﺗﺘﺴﺒﱠﺐ اﻟﻘﻮاﻧني اﻷﺧﻼﻗﻴﺔ ﻟﻠﺮوﺑﻮﺗﺎت داﺋﻤً ﺎ ﰲ ﻣﺸﻜﻼت ﻟﻠﺒﴩ واﻟﺮوﺑﻮﺗﺎت(‪ ،‬وأن‬
‫ٍ‬
‫اﻓﱰاﺿﺎت ﺧﺎﻃﺌﺔ‬
‫ﻣﴩوع إﻧﺸﺎء »آﻻت أﺧﻼﻗﻴﺔ« ﻣﻦ ﺧﻼل ﺗﻐ ِﺬﻳَﺘﻬﺎ ﺑﺎﻟﻘﻮاﻋﺪ ﻳﺴﺘﻨﺪ إﱃ‬
‫ﺑﺨﺼﻮص ﻃﺒﻴﻌﺔ اﻷﺧﻼق‪ .‬ﻓﺎﻷﺧﻼق ﻻ ﻳﻤﻜﻦ اﺧﺘﺰاﻟﻬﺎ ﰲ اﺗﱢﺒﺎع اﻟﻘﻮاﻋﺪ‪ ،‬ﻛﻤﺎ أﻧﻬﺎ ﻟﻴﺴﺖ‬
‫ﻣﺴﺄﻟﺔ ﻋﻮاﻃﻒ ﺑﴩﻳﺔ ﻓﺤﺴﺐ؛ وﻟﻜﻦ ﻫﺬه اﻟﻌﻮاﻃﻒ ﻗﺪ ﺗﻜﻮن ﴐورﻳﺔ ﻟﻠﻐﺎﻳﺔ ﻟﻠﺤُ ﻜﻢ‬
‫اﻷﺧﻼﻗﻲ‪ .‬ﻓﺈذا ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم ﻣُﻤﻜﻨًﺎ ﻋﲆ اﻹﻃﻼق‪ ،‬ﻓﺈﻧﻨﺎ ﻻ ﻧُﺮﻳﺪ ﻧﻮﻋً ﺎ ﻣﻦ‬
‫‪43‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫»اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ املﺮﻳﺾ ﻧﻔﺴﻴٍّﺎ« أي اﻟﺬي ﻳﺘﻤﺘﱠﻊ ﺑﺎﻟﻌﻘﻼﻧﻴﺔ اﻟﻜﺎﻣﻠﺔ وﻟﻜﻨﱠﻪ ﻻ ﻳﻬﺘ ﱡﻢ‬
‫ﺑﺎﻫﺘﻤﺎﻣﺎت اﻹﻧﺴﺎن ﻷﻧﻪ ﻳﻔﺘﻘﺮ إﱃ املﺸﺎﻋﺮ )‪.(Coeckelbergh 2010‬‬
‫ﻟﻬﺬه اﻷﺳﺒﺎب‪ ،‬ﻳﻤﻜﻦ أن ﻧﺮﻓﺾ ﻓﻜﺮة ﺗﻤﺘﱡﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﻮﻛﺎﻟﺔ أﺧﻼﻗﻴﺔ ﻛﺎﻣﻠﺔ‬
‫ً‬
‫ﻣﻮﻗﻔﺎ وﺳ ً‬
‫ً‬
‫ﻄﺎ‪ :‬ﻳﺠﺐ أن ﻧﻤﻨﺢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻧﻮﻋً ﺎ ﻣﻦ‬
‫رﻓﻀﺎ ﺗﺎﻣٍّ ﺎ‪ ،‬أو ﻳﻤﻜﻦ أن ﻧﺘﱠﺨِ ﺬ‬
‫اﻟﻘﻮاﻋﺪ اﻷﺧﻼﻗﻴﺔ‪ ،‬وﻟﻜﻦ ﻟﻴﺲ ﻛﻞ اﻟﻘﻮاﻋﺪ اﻷﺧﻼﻗﻴﺔ‪ .‬ﻳﺴﺘﺨﺪم وﻳﻨﺪﻳﻞ ﻓﺎﻻخ وﻛﻮﻟني أﻟني‬
‫ﻣُﺼﻄﻠﺢ »اﻟﻘﻮاﻋﺪ اﻷﺧﻼﻗﻴﺔ اﻟﻮﻇﻴﻔﻴﺔ« )‪ .(٣٩ ،٢٠٠٩‬ﺗﺤﺘﺎج أﻧﻈﻤﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫إﱃ ﺑﻌﺾ اﻟﻘﺪرة ﻋﲆ ﺗﻘﻴﻴﻢ اﻟﻌﻮاﻗﺐ اﻷﺧﻼﻗﻴﺔ ﻷﻓﻌﺎﻟِﻬﺎ‪ .‬واملﻨﻄﻖ وراء ﻫﺬا اﻟﻘﺮار واﺿﺢ‬
‫ﰲ ﺣﺎﻟﺔ اﻟﺴﻴﺎرات ذاﺗﻴﺔ اﻟﻘﻴﺎدة‪ :‬ﺳﺘﺘﻮ ﱠرط اﻟﺴﻴﺎرة ﻋﲆ اﻷرﺟﺢ ﰲ ﻣﻮاﻗﻒ ﺗﺘﻄ ﱠﻠﺐ اﺗﺨﺎذ‬
‫ﱡ‬
‫اﻟﺘﺪﺧﻞ‬
‫ﺧﻴﺎر أﺧﻼﻗﻲ وﻟﻜﻦ ﻻ ﻳﻮﺟَ ﺪ وﻗﺖ ﻟﻼﺳﺘﻌﺎﻧﺔ ﺑﺎﻟﺒﴩ ﻻﺗﺨﺎذ اﻟﻘﺮار أو اﻧﺘﻈﺎر‬
‫ٍ‬
‫اﻟﺒﴩي‪ .‬وﰲ ﺑﻌﺾ اﻷﺣﻴﺎن‪ ،‬ﺗﻜﻮن ﻫﺬه اﻟﺨﻴﺎرات ﻋﺒﺎرة ﻋﻦ ﻣﻌﻀﻠﺔ‪ .‬ﻳﺘﺤﺪﱠث اﻟﻔﻼﺳﻔﺔ ﻋﻦ‬
‫ﻣﻌﻀﻠﺔ ﻋﺮﺑﺔ اﻟﱰام‪ ،‬وﻫﻲ ﺗﺠﺮﺑﺔ ﻓﻜﺮﻳﺔ ﺗﺘﻌﻠﻖ َ‬
‫ﺑﺴري ﻋﺮﺑﺔ ﺗﺮام ﻋﲆ ﻣﺴﺎر ﺳﻜﻚ ﺣﺪﻳﺪﻳﺔ‬
‫وﻳﺠﺐ ﻋﻠﻴﻚ اﻻﺧﺘﻴﺎر ﺑني ﻋﺪم ﻓِ ﻌﻞ أي ﳾء‪ ،‬اﻷﻣﺮ اﻟﺬي ﺳﻴﺆدﱢي إﱃ ﻣَ ﻮت ﺧﻤﺴﺔ أﺷﺨﺎص‬
‫ﻣﺴﺎر َ‬
‫آﺧﺮ‪ ،‬ﺣﻴﺚ ﻳﻜﻮن ﻫﻨﺎك‬
‫ﻣُﻘﻴﱠﺪﻳﻦ ﺑﺎملﺴﺎر‪ ،‬أو ﺳﺤﺐ اﻟﺮاﻓﻌﺔ وإرﺳﺎل اﻟﻌﺮﺑﺔ إﱃ‬
‫ٍ‬
‫ٌ‬
‫ﺗﻌﺮﻓﻪ‪ .‬ﻣﺎ ﻫﻮ اﻟﴚء اﻟﺴﻠﻴﻢ أﺧﻼﻗﻴٍّﺎ اﻟﺬي ﻳﺘﻮﺟﱠ ﺐ‬
‫ﺷﺨﺺ واﺣﺪ ﻣﻘﻴﱠﺪ ﺑﻪ وﻟﻜﻨﻪ ﺷﺨﺺ ِ‬
‫ﻋﻠﻴﻚ اﻟﻘﻴﺎم ﺑﻪ؟ ﺑﺎملﺜﻞ‪ ،‬ﻳﻘﻮل أﻧﺼﺎر ﻫﺬا اﻟﻨﻬﺞ إن اﻟﺴﻴﺎرة اﻟﺬاﺗﻴﺔ اﻟﻘﻴﺎدة ﻗﺪ ﺗُﻀ َ‬
‫ﻄ ﱡﺮ إﱃ‬
‫اﺗﺨﺎذ ﺧﻴﺎر أﺧﻼﻗﻲ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺑني ﻗﺘْﻞ املﺸﺎة اﻟﻌﺎﺑﺮﻳﻦ ﻋﲆ اﻟﻄﺮﻳﻖ واﻻﺻﻄﺪام‬
‫ﻳﺠﺐ أن ﺗﺘﱠﺨِ ﺬه اﻟﺴﻴﺎرة؟ ﻳﺒﺪو أﻧﻪ‬
‫ﺑﺤﺎﺋﻂ‪ ،‬ﻣﻤﺎ ﻳﺆدي إﱃ ﻣﻮت اﻟﺴﺎﺋﻖ‪ .‬ﻣﺎ اﻟﺨﻴﺎر اﻟﺬي ِ‬
‫ﺳﻴﺘﻌني ﻋﻠﻴﻨﺎ اﺗﺨﺎذ ﻫﺬه اﻟﻘﺮارات اﻷﺧﻼﻗﻴﺔ )ﻣ ً‬
‫ُﺴﺒﻘﺎ( واﻟﺘﺄ ﱡﻛﺪ ﻣﻦ ﺗﻐﺬﻳﺔ اﻟﺴﻴﺎرات ﺑﻬﺎ‬
‫ﻣﻦ ﻗِ ﺒَﻞ ا ُملﻄﻮﱢرﻳﻦ‪ .‬أو رﺑﻤﺎ ﻧﺤﺘﺎج إﱃ ﺑﻨﺎء ﺳﻴﺎرات ﻣﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺘﻌ ﱠﻠﻢ ﻣﻦ‬
‫اﺧﺘﻴﺎرات اﻟﺒﴩ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻗﺪ ﻳُﺜﺎر ﺳﺆال ﻋﻤﺎ إذا ﻛﺎن إﻋﻄﺎء اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﻮاﻋﺪ ﻫﻮ‬
‫وﺳﻴﻠﺔ ﺟﻴﺪة ﻟﺘﻤﺜﻴﻞ اﻷﺧﻼق اﻟﺒﴩﻳﺔ‪ ،‬ﻫﺬا إن ﻛﺎن ﻣﻦ ا ُملﻤﻜﻦ »ﺗﻤﺜﻴﻞ« اﻷﺧﻼق ﻣﻦ اﻷﺳﺎس‪،‬‬
‫وإذا ﻛﺎﻧﺖ ﻣُﻌﻀﻠﺔ ﻋﺮﺑﺔ اﻟﱰام ﺗﺒني ﺷﻴﺌًﺎ ﺟﻮﻫﺮﻳٍّﺎ ﰲ اﻟﺤﻴﺎة واﻟﺘﺠﺮﺑﺔ اﻷﺧﻼﻗﻴﺔ‪ .‬أو‪ ،‬ﻣﻦ‬
‫ﻣﻨﻈﻮر ﻣﺨﺘﻠﻒ ﺗﻤﺎﻣً ﺎ‪ ،‬ﻳﻤﻜﻦ ﻟﻠﻤﺮء أن ﻳﺘﺴﺎءل ﻋﻤﺎ إذا ﻛﺎن اﻟﺒﴩ ﰲ اﻟﻮاﻗﻊ ﻗﺎدِ رﻳﻦ ﻋﲆ‬
‫ٍ‬
‫اﺗﺨﺎذ ﻗﺮارات أﺧﻼﻗﻴﺔ ﺑﻜﻔﺎءة‪ .‬وملﺎذا ﻧُﻘﻠﺪ أﺧﻼق اﻟﺒﴩ ﻣﻦ اﻷﺳﺎس؟ إن ﻣُﻨﺎﴏي ﺗﺠﺎوز‬
‫ﺑﺄﺧﻼق ﻓﺎﺋﻘﺔ ﻷﻧﻪ‬
‫اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳ َﺮون أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺳﻮف ﻳﺘﻤﺘﻊ‬
‫ٍ‬
‫ﺳﻴﻜﻮن أﻛﺜﺮ ذﻛﺎءً ﻣﻨﱠﺎ‪.‬‬
‫ً‬
‫ﻣﻮﻗﻒ َ‬
‫ٍ‬
‫وﻛﺎﻟﺔ أﺧﻼﻗﻴﺔ‬
‫آﺧﺮ‪ ،‬ﻻ ﻳﺘﻄ ﱠﻠﺐ‬
‫ﻫﺬا اﻟﺘﺸﻜﻴﻚ ﰲ اﻟﱰﻛﻴﺰ ﻋﲆ اﻹﻧﺴﺎن ﻳُﻮﺟﱢ ﻬﻨﺎ إﱃ‬
‫ﺘﻤﺤﻮر ﺣﻮل اﻹﻧﺴﺎن‪ .‬وﻗﺪ داﻓﻊ ﻟﻮﺗﺸﻴﺎﻧﻮ ﻓﻠﻮرﻳﺪي‬
‫ﻛﺎﻣﻠﺔ وﻳُﺤﺎول ﺗﺮك املﻮﻗﻒ اﻷﺧﻼﻗﻲ ا ُمل ِ‬
‫‪44‬‬

‫أﻫﻲ ٍّ‬
‫ﺣﻘﺎ ﻣﺠﺮد آﻻت؟‬

‫أﺧﻼق ﻻ ﻋﻘﻞ ﻟﻬﺎ وﻏري ﻣُﺴﺘﻨﺪة إﱃ ﺧﺼﺎﺋﺺ ﻳﻤﺘﻠﻜﻬﺎ‬
‫وﺟﻴﻪ دﺑﻠﻴﻮ ﺳﺎﻧﺪرز )‪ (٢٠٠٤‬ﻋﻦ‬
‫ٍ‬
‫ٍ‬
‫ِ‬
‫ﻛﺎف ﻣﻦ اﻟﺘﻔﺎﻋﻞ‬
‫ﺗﻌﺘﻤﺪ ﻋﲆ اﻟﺘﻤﺘﱡﻊ ﺑﻤﺴﺘﻮًى‬
‫اﻟﺒﴩ‪ .‬وﻳُﻤﻜﻨﻨﺎ ﺟﻌﻞ اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‬
‫واﻻﺳﺘﻘﻼل واﻟﻘﺪرة ﻋﲆ اﻟﺘﻜﻴﱡﻒ وﻛﺬﻟﻚ اﻟﻘﺪرة ﻋﲆ اﻟﻘﻴﺎم ﺑﺘﴫﱡﻓﺎت ذات ﻃﺎﺑﻊ أﺧﻼﻗﻲ‪.‬‬
‫ً‬
‫ووﻓﻘﺎ ﻟﻬﺬه املﻌﺎﻳري‪ ،‬ﻓﺈن ﻛﻠﺐ اﻟﺒﺤﺚ واﻹﻧﻘﺎذ ﻳﺘﻤﺘﱠﻊ ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‪ ،‬وﻟﻜﻦ ﻛﺬﻟﻚ روﺑﻮت‬
‫ﱠ‬
‫ﻳﺘﻮﱃ ﺗﺼﻔﻴﺔ اﻟﺮﺳﺎﺋﻞ اﻟﱪﻳﺪﻳﺔ ﻏري املﺮﻏﻮب ﻓﻴﻬﺎ‪ .‬وﺑﺎملِ ﺜﻞ‪ ،‬ﻳﻤﻜﻦ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬي‬
‫ﺗﻄﺒﻴﻖ ﻣﻌﺎﻳري ﻻ ﺗﺘﻤﺤﻮَر ﺣﻮل اﻹﻧﺴﺎن ملﻨﺢ اﻟﺮوﺑﻮﺗﺎت اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‪ ،‬ﻛﻤﺎ اﻗﱰح ﺟﻮن‬
‫ٍّ‬
‫ُﺴﺘﻘﻼ ﻋﻦ ا ُملﱪﻣﺠني وﻳﻤﻜﻨﻨﺎ ﺗﻔﺴري ﺳﻠﻮﻛﻪ‬
‫ﺳﺎﻟﻴﻨﺰ )‪ :(٢٠٠٦‬إذا ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣ‬
‫ﺑﺄن ﻧﻌﺰو إﻟﻴﻪ اﻟﻘﺼﺪ اﻷﺧﻼﻗﻲ )ﻣﺜﻞ ﻗﺼﺪ ﻓﻌﻞ اﻟﺨري أو اﻟﴩ(‪ ،‬وإذا ﻧ ﱠﻢ ﺳﻠﻮﻛﻪ ﻋﻦ‬
‫آﺧﺮﻳﻦ‪ ،‬ﻓﺈن ﻫﺬا اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺘﻤﺘﱠﻊ ﺑﺎﻟﻮﻛﺎﻟﺔ‬
‫َﻓﻬﻢ ﻣﺴﺌﻮﻟﻴﺘﻪ ﺗﺠﺎه وﻛﻼء أﺧﻼﻗِ ﻴﱢني ِ‬
‫اﻷﺧﻼﻗﻴﺔ‪ .‬وﻣﻦ ﺛَﻢ‪ ،‬ﻓﺈن ﻫﺬه اﻵراء ﻻ ﺗﺘﻄ ﱠﻠﺐ اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﻜﺎﻣﻠﺔ إذا ﻛﺎن ذﻟﻚ ﻳﻌﻨﻲ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﺗﻜﻮن ﻣﻦ ﺣﻴﺚ املﺒﺪأ‬
‫اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﺒﴩﻳﺔ‪ ،‬وﻟﻜﻨﻬﺎ ﺗُﻌ ﱢﺮف اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‬
‫ُﺴﺘﻘﻠﺔ ﻋﻦ اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﻜﺎﻣﻠﺔ ﻟﻠﺒﴩ ُ‬
‫ً‬
‫واﻟﻘﺪرات اﻟﺒﴩﻳﺔ املﻄﻠﻮﺑﺔ ﻟﺬﻟﻚ‪ .‬وﻣﻊ ذﻟﻚ‪،‬‬
‫ﻣ‬
‫ً‬
‫ً‬
‫ﻫﻞ ﺳﺘﻜﻮن ﻣﺜﻞ ﻫﺬه اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻻﺻﻄﻨﺎﻋﻴﺔ ﻛﺎﻓﻴﺔ إذا ﺣُ ِﻜ َﻢ ﻋﻠﻴﻬﺎ وﻓﻘﺎ ﻟﻠﻤﻌﺎﻳري‬
‫اﻷﺧﻼﻗﻴﺔ اﻟﺒﴩﻳﺔ؟ ﻋﻤﻠﻴٍّﺎ‪ ،‬ﻳﻜﻤﻦ اﻟﻘﻠﻖ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ أن اﻟﺴﻴﺎرات ذاﺗﻴﺔ اﻟﻘﻴﺎدة‬
‫ﻗﺪ ﻻ ﺗُﻄﺒﻖ اﻟﻘﻮاﻋﺪ اﻷﺧﻼﻗﻴﺔ اﻟﻜﺎﻓﻴﺔ‪ .‬أﻣﺎ ﻣﻦ ﺣﻴﺚ املﺒﺪأ‪ ،‬ﻓﻴﻜﻤﻦ اﻟﻘﻠﻖ ﰲ أﻧﻨﺎ ﻧﺒﺘﻌِ ﺪ‬
‫ﻛﺜريًا ﻋﻦ اﻷﺧﻼق اﻟﺒﴩﻳﺔ ﻫﻨﺎ‪ .‬وﻳﻌﺘﻘﺪ اﻟﻜﺜريون أن اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ ﻣُﺮﺗﺒﻄﺔ وﻳﺠﺐ أن‬
‫ﺗﻜﻮن ﻣﺮﺗﺒﻄﺔ ﺑﺎﻹﻧﺴﺎﻧﻴﺔ واﻟﺸﺨﺼﻴﺔ‪ .‬وﻫﺆﻻء ﻻ ﻳﻤﻴﻠﻮن إﱃ اﻋﺘﻨﺎق أﻓﻜﺎر ﻣﺆﻳﱢﺪي ﻣﺎ ﺑﻌﺪ‬
‫اﻹﻧﺴﺎﻧﻴﺔ أو ﻣﺆﻳﺪي ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪.‬‬
‫اﻛﺘﺴﺎب املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ‬
‫ﺛﻤﺔ ﻣﻮﺿﻮع َ‬
‫ٍ‬
‫ملﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ‪ .‬ﺗﺨﻴﱠﻞ‬
‫آﺧﺮ ﻣُﺜري ﻟﻠﺠﺪَل وﻳﺘﻌ ﱠﻠﻖ ﺑﺎﻛﺘﺴﺎب اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫أن ﻟﺪَﻳﻨﺎ ذﻛﺎءً اﺻﻄﻨﺎﻋﻴٍّﺎ ً‬
‫ﻓﺎﺋﻘﺎ‪ .‬ﻫﻞ ﻣﻦ ا َملﻘﺒﻮل أﺧﻼﻗﻴٍّﺎ إﻳﻘﺎف ﺗﺸﻐﻴﻠ ِﻪ‪ ،‬أو »ﻗﺘﻠﻪ«؟ وإذا‬
‫ﻛﻠﺐ آﱄ ﻣُﺰود‬
‫ﻣﺎ ﻧﻈﺮﻧﺎ ﻋﻦ ﻛﺜَﺐ إﱃ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺤﺎﱄ‪ :‬ﻫﻞ ﻣﻦ املﻘﺒﻮل رﻛﻞ ٍ‬
‫ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ ‪ 1‬إذا ﻛﺎﻧﺖ اﻵﻻت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺳﺘﻜﻮن ﺟﺰءًا ﻣﻦ‬
‫اﻟﺤﻴﺎة اﻟﻴﻮﻣﻴﺔ‪ ،‬ﻛﻤﺎ ﱠ‬
‫ﻳﺘﻮﻗﻊ اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﺒﺎﺣﺜني‪ ،‬ﻓﺈن ﻣﺜﻞ ﻫﺬه اﻟﺤﺎﻻت ﺳﺘﻈﻬﺮ ﺑﺎﻟﴬورة‬
‫ﻳﺠﺐ ﻋﲆ اﻟﺒﴩ اﻟﺘﴫﱡف ﺗﺠﺎه ﻫﺬه اﻟﻜﻴﺎﻧﺎت اﻻﺻﻄﻨﺎﻋﻴﺔ‪ .‬وﻣﻊ ذﻟﻚ‪،‬‬
‫وﺗُﺜري ﻣﺴﺄﻟﺔ ﻛﻴﻒ ِ‬
‫ﻟﻴﺲ ﻋ َﻠﻴﻨﺎ أن ﻧﻨ ُ‬
‫ﻈﺮ إﱃ ا ُملﺴﺘﻘﺒﻞ اﻟﺒﻌﻴﺪ أو إﱃ اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ‪ .‬ﻓﻘﺪ أﻇﻬﺮت اﻷﺑﺤﺎث أن‬
‫اﻟﻨﺎس ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ ﻳﺘﻌﺎﻃﻔﻮن ﻣﻊ اﻟﺮوﺑﻮﺗﺎت وﻳﱰدﱠدون ﰲ »ﻗﺘﻠﻬﺎ« أو »ﺗﻌﺬﻳﺒﻬﺎ«‬
‫‪45‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫)‪ ،(Suzuki et al. 2015; Darling, Nandy, and Breazeal 2015‬ﺣﺘﻰ إذا ﻟﻢ ﺗ ُﻜﻦ‬
‫ﻣﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻳﺒﺪو أن اﻟﺒﴩ ﻻ ﻳﺤﺘﺎﺟﻮن ﻣﻦ اﻟﻜﻴﺎﻧﺎت اﻻﺻﻄﻨﺎﻋﻴﺔ ﺳﻮى‬
‫اﻟﻘﻠﻴﻞ ﺟﺪٍّا ﻣﻦ أﺟﻞ إﺿﻔﺎء اﻹﻧﺴﺎﻧﻴﺔ أو اﻟﺸﺨﺼﻴﺔ ﻋﻠﻴﻬﻢ واﻟﺘﻌﺎ ُ‬
‫ﻃﻒ ﻣﻌﻬﻢ‪ .‬ﻓﺈذا أﺻﺒﺤﺖ‬
‫ﻫﺬه اﻟﻜﻴﺎﻧﺎت اﻵن ﻣﺰود ًة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻣﻤﺎ ﻳﺠﻌﻠﻬﺎ أﺷﺒَﻪ ﺑﺎﻹﻧﺴﺎن )أو ﺑﺎﻟﺤﻴﻮان(‪،‬‬
‫ﻳﺒﺪو أن ﻫﺬا ﻳﺠﻌﻞ ﻣﺴﺄﻟﺔ إﻛﺴﺎب املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ أﻛﺜﺮ إﻟﺤﺎﺣً ﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﺎذا‬
‫ﻳﻨﺒﻐﻲ أن ﻳﻜﻮن ر ﱡد ﻓﻌﻠﻨﺎ ﺗﺠﺎه اﻷﺷﺨﺎص اﻟﺬﻳﻦ ﻳﺘﻌﺎﻃﻔﻮن ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ ﻫﻞ‬
‫ُﻫﻢ ﻣُﺨﻄﺌﻮن؟‬
‫رﺑﻤﺎ ﻳﻜﻮن ﻗﻮل إن اﻵﻻت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻲ ﻣﺠﺮد آﻻت وإن اﻷﺷﺨﺎص‬
‫ٍ‬
‫ﺑﺒﺴﺎﻃﺔ ﻣُﺨﻄﺌﻮن ﰲ ﺗﻘﺪﻳﺮﻫﻢ ﻟﻸﻣﻮر وﰲ ﻋﻮاﻃﻔﻬﻢ وﺗﺠﺮﺑﺘِﻬﻢ‬
‫اﻟﺬﻳﻦ ﻳﺘﻌﺎﻃﻔﻮن ﻣﻌﻬﺎ‬
‫اﻷﺧﻼﻗﻴﺔ ﻫﻮ اﻷﻗﺮب إﱃ اﻟﺒﺪﻳﻬﺔ‪ .‬إذ ﻳﺒﺪو ﻟﻨﺎ‪ ،‬ﻋﻨﺪ اﻟﻨﻈﺮة اﻷوﱃ‪ ،‬أﻧﻨﺎ ﻻ ﻧﺪﻳﻦ ﺑﴚءٍ‬
‫ً‬
‫أﺷﺨﺎﺻﺎ‪ .‬وﻳُﻔﻜﺮ اﻟﻜﺜري ﻣﻦ اﻟﺒﺎﺣِ ﺜني ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء‬
‫إﱃ اﻵﻻت‪ .‬ﻓﻬﻲ أﺷﻴﺎء‪ ،‬وﻟﻴﺴﺖ‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﺑﻬﺬا املﻨﻄﻖ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗﺮى ﺟﻮاﻧﺎ ﺑﺮاﻳﺴﻮن أن اﻟﺮوﺑﻮﺗﺎت ﻫﻲ أدوات‬
‫ٍ‬
‫اﻟﺘﺰاﻣﺎت ﺗﺠﺎﻫﻬﺎ )‪ .(Bryson 2010‬ﻗﺪ ﻳﺘﻔﻖ اﻟﺬﻳﻦ ﻳﺘﺒﻨﱠﻮن‬
‫وﻣُﻤﺘﻠﻜﺎت وأﻧﻪ ﻟﻴﺲ ﻟﺪَﻳﻨﺎ أي‬
‫ﻫﺬا املﻮﻗﻒ ﺑﺸﺪﱠة ﻋﲆ أﻧﻪ إذا ﻛﺎن ﻟﺪى اﻵﻻت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ُ‬
‫اﻟﻘﺪرة ﻋﲆ‬
‫ً‬
‫ﻣﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ‪.‬‬
‫اﻟﻮﻋﻲ‪ ،‬وﻟﺪَﻳﻬﺎ ﺣﺎﻻت ﻋﻘﻠﻴﺔ‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪ ،‬ﻓﺈﻧﻨﺎ ﻣُﻄﺎﻟﺒﻮن ﺑﺄن ﻧﻤﻨﺤﻬﺎ‬
‫ﱠ‬
‫ﻳﺘﺤﻘﻖ اﻟﻴﻮم‪ .‬وﻛﻤﺎ رأﻳﻨﺎ ﰲ اﻟﻔﺼﻮل اﻟﺴﺎﺑﻘﺔ‪ ،‬ﻗﺪ‬
‫وﻟﻜﻨﻬﻢ ﺳﻴﻘﻮﻟﻮن إن ﻫﺬا اﻟﴩط ﻻ‬
‫ُ‬
‫ﱠ‬
‫ﻳﺘﺤﻘﻖ أﺑﺪًا؛ وﻳﻘﻮل َ‬
‫ﺗﺤﻘﻴﻘﻪ ﻣﻦ ﺣﻴﺚ املﺒﺪأ‪ ،‬وﻟﻜﻦ‬
‫آﺧﺮون إﻧﻪ ﻳﻤﻜﻦ‬
‫ﻳﻘﻮل اﻟﺒﻌﺾ إﻧﻪ ﻟﻦ‬
‫ُ‬
‫ُ‬
‫ﱢ‬
‫ﻫﺬا ﻟﻦ ﻳﺤﺪث ﰲ املﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ‪ .‬وﻟﻜﻦ اﻟﻨﺘﻴﺠﺔ املﱰﺗﺒﺔ ﻋﲆ اﻟﺴﺆال املﺘﻌﻠﻖ ﺑﺎملﻜﺎﻧﺔ‬
‫اﻷﺧﻼﻗﻴﺔ ﻫﻲ أﻧﻪ ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ وﰲ املﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ‪ ،‬ﻳُﻔﱰَض أن ﻧﺘﻌﺎﻣَ ﻞ ﻣﻊ اﻵﻻت‬
‫املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻛﺄﺷﻴﺎء‪ ،‬إﻻ إذا ﺛﺒﺖ ﺧﻼف ذﻟﻚ‪.‬‬
‫ﺗﻮاﺟﻬﻨﺎ ﻋﻨﺪ اﺗﺨﺎذ ﻫﺬا املﻮﻗﻒ‪ ،‬وﻫﻲ أﻧﻪ‬
‫ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ ذﻟﻚ‪ ،‬ﻓﺜﻤﱠ ﺔ ﻣﺸﻜﻠﺔ واﺣﺪة‬
‫ِ‬
‫ﻻ ﻳﻔﴪ وﻻ ﻳُﱪر إﺣﺴﺎﺳﻨﺎ اﻟﺒﺪﻳﻬﻲ اﻷﺧﻼﻗﻲ وﻻ ﺗﺠﺎرﺑﻨﺎ اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ ﺗُﺨﱪﻧﺎ ﺑﺄن ﺛﻤﱠ ﺔ‬
‫ﺷﻴﺌًﺎ ﺧﺎﻃﺌًﺎ ﰲ »إﺳﺎءة ﻣﻌﺎﻣﻠﺔ« اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺣﺘﻰ إذا ﻟﻢ ﺗ ُﻜﻦ ﻟﺪَﻳﻪ ﺧﺼﺎﺋﺺ‬
‫ﺷﺒﻴﻬﺔ ﺑﺎﻟﺒﴩ أو اﻟﺤﻴﻮاﻧﺎت ﻣﺜﻞ اﻟﻮﻋﻲ أو اﻹﺣﺴﺎس‪ .‬ﻟﻠﻌﺜﻮر ﻋﲆ ﻣﺜﻞ ﻫﺬه اﻟﺘﱪﻳﺮات‪،‬‬
‫ﻳُﻤﻜﻦ ﻟﻠﻤﺮء اﻟﻠﺠﻮء إﱃ ﻛﺎﻧﻂ‪ ،‬اﻟﺬي اﻋﺘﱪ أﻧﻪ ﻣﻦ اﻟﺨﻄﺄ إﻃﻼق اﻟﻨﺎر ﻋﲆ ﻛﻠﺐ؛ ﻟﻴﺲ ﻷن‬
‫ٍ‬
‫اﻟﺘﺰاﻣﺎت ﺗﺠﺎه ﻫﺬا اﻟﻜﻠﺐ‪ ،‬وﻟﻜﻦ ﻷن ﻣﺜﻞ ﻫﺬا اﻟﺸﺨﺺ‬
‫ﻛﻠﺐ ﻳﻨﺘﻬﻚ أي‬
‫إﻃﻼق اﻟﻨﺎر ﻋﲆ ٍ‬
‫»ﻳﴬﱡ ﺑﺼﻔﺎت اﻟﺮﺣﻤﺔ واﻹﻧﺴﺎﻧﻴﺔ ﰲ ﻧﻔﺴﻪ‪ ،‬واﻟﺘﻲ ﻳﺠﺐ أن ﻳُﻤﺎرﺳﻬﺎ ﺑﻨﺎءً ﻋﲆ واﺟﺒﺎﺗﻪ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻣﺨﺘﻠﻔﺔ ﺗﺠﺎه‬
‫ﺗﺠﺎه اﻟﺒﴩ« )‪ .(Kant 1997‬أﻣﺎ اﻟﻴﻮم ﻓﻨﺤﻦ ﻧﻤﻴﻞ إﱃ اﻟﺘﻔﻜري‬
‫‪46‬‬

‫أﻫﻲ ٍّ‬
‫ﺣﻘﺎ ﻣﺠﺮد آﻻت؟‬

‫اﻟﻜﻼب )ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن ﻫﺬا ﻟﻴﺲ ﺣﺎل اﻟﺠﻤﻴﻊ وﻟﻴﺲ اﻟﺤﺎل ﰲ ﻛﻞ ﻣﻜﺎن(‪ .‬وﻟﻜﻦ‬
‫ﻳﺒﺪو أﻧﻪ ﻳُﻤﻜﻦ ﺗﻄﺒﻴﻖ اﻟﺤﺠﱠ ﺔ ﻧﻔﺴﻬﺎ ﻋﲆ اﻵﻻت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ :‬ﻳُﻤﻜﻨﻨﺎ أن‬
‫ﻧﻘﻮل إﻧﻨﺎ ﻻ ﻧﺪﻳﻦ ﺑﴚءٍ إﱃ اﻵﻻت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻟﻜﻨﱠﻨﺎ ﻣﻊ ذﻟﻚ ﻳﻨﺒﻐﻲ‬
‫ﻟﻨﺎ ﻋﺪم ر ْﻛﻞ أو »ﺗﻌﺬﻳﺐ« آﻟﺔ ﻣﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؛ ﻷن ذﻟﻚ ﻳﺠﻌﻠﻨﺎ ﻏري رﺣﻤﺎء‬
‫أﻳﻀﺎ اﺳﺘﺨﺪام ﺣﺠﱠ ِﺔ أﺧﻼﻗﻴﺎت اﻟﻔﻀﻴﻠﺔ‪ ،‬وﻫﻲ ﺣُ ﺠﱠ ﺔ ﻏري ﻣﺒﺎﴍة ً‬
‫ﺗﺠﺎه اﻟﺒﴩ‪ .‬ﻳُﻤﻜﻦ ً‬
‫أﻳﻀﺎ‬
‫َ‬
‫ﺑﺎﻟﺒﴩ وﻟﻴﺲ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪» :‬إﺳﺎءة ﻣﻌﺎﻣﻠﺔ« اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺧﻄﺄ‬
‫ﻷﻧﻬﺎ ﺗﺘﻌ ﱠﻠﻖ‬
‫ﻟﻴﺲ ﻷن ﺛﻤﱠ ﺔ ﴐ ًرا ﺳﻴﻠﺤﻖ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻟﻜﻦ ﻷن ﻃﺎﺑﻌﻨﺎ اﻷﺧﻼﻗﻲ ﺳﻴﺘﺄذﱠى إذا‬
‫ً‬
‫أﺷﺨﺎﺻﺎ أﻓﻀﻞ‪ .‬وﻋﲆ اﻟﻨﻘﻴﺾ ﻣﻦ ﻫﺬا اﻟﻨﻬﺞ ﻳُﻤﻜﻨﻨﺎ أن‬
‫ﻣﺎ ﻓﻌﻠﻨﺎ ذﻟﻚ‪ .‬وﻫﺬا ﻻ ﻳﺠﻌﻠﻨﺎ‬
‫ٍ‬
‫ﺑﻘﻴﻤﺔ ﺟﻮﻫﺮﻳﺔ‬
‫ﻧﻘﻮل إﻧﻪ ﰲ ا ُملﺴﺘﻘﺒﻞ ﻗﺪ ﺗﺘﻤﺘﱠﻊ ﺑﻌﺾ اﻵﻻت املﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﱡ‬
‫وﺗﺴﺘﺤﻖ اﻫﺘﻤﺎﻣﻨﺎ اﻷﺧﻼﻗﻲ‪ ،‬ﺑﴩط أن ﺗﻜﻮن ﻟﺪَﻳﻬﺎ ﺧﺼﺎﺋﺺ ﻣﺜﻞ اﻹﺣﺴﺎس‪ .‬وﻻ ﻳﺒﺪو‬
‫َ‬
‫»اﻵﺧﺮ« ﻣﻦ اﻟﻌﻼﻗﺔ اﻷﺧﻼﻗﻴﺔ‬
‫أن اﻟﻨﻬﺞ ﻏري املﺒﺎﴍ ﻟﻠﻮاﺟﺐ أو اﻟﻔﻀﻴﻠﺔ ﻳﺄﺧﺬ ﻫﺬا اﻟﺠﺎﻧﺐ‬
‫ﻋﲆ ﻣﺤﻤﻞ اﻟﺠﺪ‪ .‬ﻓﻬﻮ ﻳُﻌﻨﻰ ﻓﻘﻂ ﺑﺎﻟﺒﴩ‪ .‬ﻓﻤﺎذا ﻋﻦ اﻵﻻت املﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟‬
‫وﻟﻜﻦ ﻫﻞ ﻳُﻤﻜﻦ ﻟﻶﻻت املﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أو اﻟﺮوﺑﻮﺗﺎت أن ﺗﻜﻮن ﻫﻲ »اﻵﺧﺮ«‬
‫ﻛﻤﺎ ﺳﺄل دﻳﻔﻴﺪ ﺟﻨﻜﻞ )‪(٢٠١٨‬؟ ﻣﺮة أﺧﺮى‪ ،‬ﻳﺒﺪو أن املﻨﻄﻖ ﻳﻘﻮل‪ :‬ﻻ‪ ،‬اﻵﻻت املﺰوﱠدة‬
‫ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺴﺖ ﻟﺪَﻳﻬﺎ اﻟﺨﺼﺎﺋﺺ املﻄﻠﻮﺑﺔ‪.‬‬
‫»إﺳﺎءة ﻣﻌﺎﻣﻠﺔ« اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺧﻄﺄ؛ ﻟﻴﺲ ﻷن ﺛﻤﺔ ﴐ ًرا ﺳﻴﻠﺤﻖ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻟﻜﻦ‬
‫ﻷن ﻃﺎﺑﻌﻨﺎ اﻷﺧﻼﻗﻲ ﺳﻴﺘﺄذﱠى إذا ﻣﺎ ﻓﻌﻠﻨﺎ ذﻟﻚ‪.‬‬

‫ﺛﻤﺔ ﻧﻬﺞ ﻣﺨﺘﻠﻒ ﺗﻤﺎﻣً ﺎ ﻳﺮى أن ﻃﺮﻳﻘﺔ ﺗﻌﺎﻣُﻠﻨﺎ ﻣﻊ ﻣﺴﺄﻟﺔ املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ ﻫﻲ‬
‫ﻧﻔﺴﻬﺎ ﺗﻨﻄﻮي ﻋﲆ إﺷﻜﺎﻟﻴﺔ‪ .‬ﻳﻌﺘﻤﺪ اﻟﺘﻔﻜري اﻷﺧﻼﻗﻲ اﻟﺸﺎﺋﻊ ﺑﺸﺄن املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ ﻋﲆ ﻣﺎ‬
‫ﺗﻤﻠِﻜﻪ اﻟﻜﻴﺎﻧﺎت ﻣﻦ ﺧﺼﺎﺋﺺ ذات ٍ‬
‫ﺻﻠﺔ ﺑﺎﻷﺧﻼق؛ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﻟﻮﻋﻲ أو اﻹﺣﺴﺎس‪.‬‬
‫وﻟﻜﻦ ﻛﻴﻒ ﻧﻌ َﻠﻢ ﻣﺎ إذا ﻛﺎن ﻟﺪى اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫ٍ‬
‫ﺻﻠﺔ‬
‫ﻓﻌﻼ ﺧﺼﺎﺋﺺ ﻣُﻌﻴﻨﺔ ذات‬
‫ﺑﺎﻷﺧﻼق أم ﻻ؟ وﻫﻞ ﻧﺤﻦ ﻣﺘﺄ ﱢﻛﺪون ﻣﻦ ذﻟﻚ ﰲ ﺣﺎﻟﺔ اﻟﺒﴩ؟ ﻳﻘﻮل ا ُملﺘﺸ ﱢﻜﻜﻮن إﻧﻨﺎ ﻟﺴﻨﺎ‬
‫ً‬
‫ﻣﻜﺎﻧﺔ‬
‫ﻣﺘﺄ ﱢﻛﺪﻳﻦ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﺣﺘﻰ دون ﻫﺬا اﻟﻴﻘني ا َملﻌﺮﰲ‪ ،‬ﻓﺈﻧﻨﺎ ﻻ ﻧﺰال ﻧُﻀﻔﻲ ﻋﲆ اﻹﻧﺴﺎن‬
‫أﺧﻼﻗﻴﺔ ﻋﲆ أﺳﺎس املﻈﻬﺮ‪ .‬وﻣﻦ ا ُملﺮﺟﱠ ﺢ أن ﻳﺤﺪُث اﻟﴚء ﻧﻔﺴﻪ إذا ُﻗﺪﱢر ﻟﻶﻻت املﺰوﱠدة‬
‫ﺑﻤﻈﻬﺮ وﺳﻠﻮك ﺷﺒﻴﻬَ ني ﺑﺎﻟﺒﴩ ﰲ املﺴﺘﻘﺒﻞ‪ .‬ﻳﺒﺪو أﻧﻪ ﱢ‬
‫ﺑﻐﺾ‬
‫ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن ﺗﺘﻤﺘﱠﻊ‬
‫ٍ‬
‫‪47‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﻨﻈﺮ ﻋﻤﱠ ﺎ ﻳﻌﺘﱪه اﻟﻔﻼﺳﻔﺔ ﻣﻦ اﻟﺼﻮاب أﺧﻼﻗﻴٍّﺎ‪ ،‬ﺳﻴُﻀﻔﻲ اﻟﺒﴩ‪ ،‬ﺑﺄﻳﺔ ﺣﺎل‪ ،‬ﻋﲆ ﻫﺬه‬
‫ً‬
‫ﺣﻘﻮﻗﺎ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ .‬ﻋﻼوة ﻋﲆ ذﻟﻚ‪ ،‬إذا ﻧﻈﺮﻧﺎ ﻋﻦ‬
‫اﻵﻻت ﻣﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ‪ ،‬وﻳﻤﻨﺤﻮﻧﻬﺎ‬
‫َ‬
‫ِ‬
‫املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ »ﰲ اﻟﻮاﻗﻊ«‪ ،‬ﻓﺈﻧﻪ ﻳﺘﱠﻀﺢ ﻋﲆ‬
‫اﻟﻄﺮﻳﻘﺔ ا ﱠﻟﺘﻲ ﻳُﻀﻔﻲ ﺑﻬﺎ اﻟﺒﴩ‬
‫ﻛﺜَﺐ إﱃ‬
‫ﺳﺒﻴﻞ املﺜﺎل أن ٍّ‬
‫ﻛﻼ ﻣﻦ اﻟﻌﻼﻗﺎت اﻻﺟﺘﻤﺎﻋﻴﺔ اﻟﻘﺎﺋﻤﺔ واﻟ ﱡﻠﻐﺔ ﺗﻠﻌﺐ دو ًرا‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫ري أﺧﻼﻗﻲ ﺑﺸﺄن ﻗِ ﱠ‬
‫إذا ﻋﺎﻣَ ْﻠﻨﺎ ﻗ ﱠ‬
‫ﻄﺘﻨﺎ‪ ،‬وﻟﻜﻦ ﻷن‬
‫ﻄﺘﻨﺎ ﺑﻠُﻄﻒ‪ ،‬ﻓﻬﺬا ﻟﻴﺲ ﻷﻧﻨﺎ‬
‫ﻧﻨﺨﺮط ﰲ ﺗﻔﻜ ٍ‬
‫ِ‬
‫ﻟﺪَﻳﻨﺎ ﺑﺎﻟﻔﻌﻞ ﻧﻮﻋً ﺎ ﻣﻦ اﻟﻌﻼﻗﺔ اﻻﺟﺘﻤﺎﻋﻴﺔ ﻣﻌﻬﺎ‪ .‬إﻧﻬﺎ ﺑﺎﻟﻔﻌﻞ ﺣﻴﻮا ٌن أﻟﻴﻒ وﻣُﺮاﻓﻖ ﻟﻨﺎ ﻗﺒﻞ‬
‫أن ﻧﻘﻮم ﺑﺎﻟﻌﻤﻞ اﻟﻔﻠﺴﻔﻲ اﻟﺬي ﻧ ُ ِ‬
‫ﻜﺴﺒﻬﺎ ﺑﻤﻮﺟ ِﺒﻪ ﻣﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ؛ ﻫﺬا إذا ﺷﻌﺮﻧﺎ ﻣﻦ اﻷﺳﺎس‬
‫ٍ‬
‫ٍّ‬
‫ﺧﺎﺻﺎ ﻋﲆ ﻛﻠﺒﻨﺎ‪ ،‬ﻓﺈﻧﻨﺎ — ﻋﲆ ﻋﻜﺲ‬
‫ﺑﺤﺎﺟﺔ إﱃ ﻣﺜﻞ ﻫﺬه املﻤﺎرﺳﺔ‪ .‬وإذا أﻃﻠﻘﻨﺎ اﺳﻤً ﺎ‬
‫ً‬
‫اﻟﺤﻴﻮاﻧﺎت اﻟﺘﻲ ﻻ ِ‬
‫ﱠ‬
‫ﺧﺎﺻﺔ‪،‬‬
‫ﻣﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ‬
‫ﺗﺤﻤﻞ اﺳﻤً ﺎ اﻟﺘﻲ ﻧﺄ ُﻛﻠﻬﺎ — ﻗﺪ ﻣﻨﺤﻨﺎه ﺑﺎﻟﻔﻌﻞ‬
‫ﺑﴫْف اﻟﻨﻈﺮ ﻋﻦ ﺧﺼﺎﺋﺼﻪ املﻮﺿﻮﻋﻴﺔ‪ .‬ﺑﺎﺳﺘﺨﺪام ﻣﺜﻞ ﻫﺬا اﻟﻨﻬﺞ اﻟﻌﻼﻗﺎﺗﻲ واﻟﻨﻘﺪي‬
‫وﻏري ا ُملﺘﺰﻣﱢ ﺖ )‪ ،(Coeckelbergh 2012‬ﻳُﻤﻜﻨﻨﺎ اﻟﻘﻮل إن اﻟﺒﴩ ﺳﻮف ﻳﻤﻨﺤﻮن اﻵﻻت‬
‫ً‬
‫ﻣﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ ﺑﻨﺎءً ﻋﲆ ﻛﻴﻔﻴﺔ ﺗﻀﻤﻴﻨﻬﺎ ﰲ ﺣﻴﺎﺗﻨﺎ اﻻﺟﺘﻤﺎﻋﻴﺔ‬
‫املﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫وﰲ ﻟُﻐﺘﻨﺎ وﰲ ﺛﻘﺎﻓﺘﻨﺎ اﻟﺒﴩﻳﺔ‪.‬‬
‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻧﻈ ًﺮا إﱃ أن ﻣﺜﻞ ﻫﺬه اﻟﻈﺮوف ﻣُﺘﻐرية ﺗﺎرﻳﺨﻴٍّﺎ — ﻓﻜﺮ ﻣﺮة أﺧﺮى‬
‫ﰲ ﻛﻴﻔﻴﺔ ﻣُﻌﺎﻣﻠﺘﻨﺎ وﺗﻔﻜريﻧﺎ ﺑﺸﺄن اﻟﺤﻴﻮاﻧﺎت — رﺑﻤﺎ ﺗﻜﻮن ﻫﻨﺎك ﺣﺎﺟﺔ إﱃ اﺗﺨﺎذ ﺳﺒُﻞ‬
‫ﺑﺸﻜﻞ ﻋﺎم أو ﻵﻟﺔ‬
‫اﻟﺤﻴﻄﺔ اﻷﺧﻼﻗﻴﺔ ﻗﺒﻞ »ﺗﺤﺪﻳﺪ« املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ﺑﺸﻜﻞ ﻋﺎم‬
‫ﻣُﻌﻴﱠﻨﺔ ﻣﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وملﺎذا ﺣﺘﻰ ﻧﺘﺤﺪﱠث ﻋﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ﺑﺸﻜﻞ ﻣﺠﺮد؟ ﻳﺒﺪو أن ﻫﻨﺎك ﺷﻴﺌًﺎ ﺧﺎﻃﺌًﺎ ﰲ اﻹﺟﺮاء اﻷﺧﻼﻗﻲ ملﻨﺢ املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ‪:‬‬
‫أو‬
‫ٍ‬
‫ﻛﻴﺎن ﻣﺎ‪ ،‬ﻧُﺨﺮج ﻫﺬا اﻟﻜﻴﺎن ﻣﻦ ﺳﻴﺎق ﻋﻼﻗﺎﺗﻪ‪ ،‬وﻗﺒﻞ أن ﻧﺤﺼﻞ ﻋﲆ‬
‫ﻓﻤﻦ أﺟﻞ اﻟﺤُ ﻜﻢ ﻋﲆ ٍ‬
‫ٍ‬
‫ﻛﻜﻴﺎن ﻧﺘﺨﺬ‬
‫ﺑﻄﺮﻳﻘﺔ رﺗﺒﻮﻳﺔ‪ ،‬ﺳﻠﻄﻮﻳﺔ‪ ،‬ﻣُﻬﻴﻤﻨﺔ‪،‬‬
‫ﻧﺘﻴﺠﺔ إﺟﺮاﺋﻨﺎ اﻷﺧﻼﻗﻲ‪ ،‬ﻧﺘﻌﺎﻣَ ﻞ ﻣﻌﻪ‬
‫ٍ‬
‫ﻧﺤﻦ اﻟﺒﴩ ا ُملﺘﻔﻮﱢﻗني ﻗﺮا ًرا ﺑﺸﺄﻧﻪ‪ .‬وﻳﺒﺪو أﻧﻨﺎ ﻗﺒﻞ ﺣﺘﻰ أن ﻧﻔﻜﺮ ﰲ ﻣﻜﺎﻧﺘﻪ اﻷﺧﻼﻗﻴﺔ‪ ،‬ﻗﺪ‬
‫ﻣﻨﺰﻟﺔ ﻣُﻌﻴﱠﻨﺔ ورﺑﻤﺎ ً‬
‫ٍ‬
‫ﻛﻜﺎﺋﻦ ﻧﺘﱠﺨﺬ‬
‫أﻳﻀﺎ ﻣﺎ َر ْﺳﻨﺎ ﻋﻠﻴﻪ اﻟﻌﻨﻒ ﺑﻤﻌﺎﻣﻠﺘﻪ‬
‫وﺿﻌﻨﺎه ﺑﺎﻟﻔﻌﻞ ﰲ‬
‫ٍ‬
‫وﻧﺼﺒﻨﺎ أﻧﻔﺴﻨﺎ ً‬
‫آﻟﻬﺔ ﻣﺤﻮرﻳﺔ ﻗﻮﻳﺔ ﻋﺎملﺔ ﻋﲆ اﻷرض ﱡ‬
‫ﻗﺮارات ﺑﺸﺄﻧﻪ‪ ،‬ﱠ‬
‫ﻳﺤﻖ ﻟﻬﺎ ﻣﻨﺢ املﻜﺎﻧﺔ‬
‫ً‬
‫ِ‬
‫ﻟﻠﻜﺎﺋﻨﺎت اﻷﺧﺮى‪ .‬ﻟﻘﺪ ﺟﻌﻠﻨﺎ أﻳﻀﺎ ﺟﻤﻴﻊ اﻟﺴﻴﺎﻗﺎت واملﻼﺑﺴﺎت اﻻﺟﺘﻤﺎﻋﻴﺔ ﻏري‬
‫اﻷﺧﻼﻗﻴﺔ‬
‫ﻣَ ﺮﺋﻴﺔ‪ .‬ﻛﻤﺎ ﰲ ﺣﺎﻟﺔ ﻣُﻌﻀﻠﺔ ﻋﺮﺑﺔ اﻟﱰام‪ ،‬ﻟﻘﺪ اﺧﺘﺰﻟﻨﺎ اﻷﺧﻼق ﰲ ﺻﻮرة ﻛﺎرﻳﻜﺎﺗريﻳﺔ‪.‬‬
‫ُ‬
‫اﻟﻔﻼﺳﻔﺔ املﺆﻳﺪون‬
‫ﺑﺎﺳﺘﺨﺪام ﻣﺜﻞ ﻫﺬا اﻟﺘﻔﻜري‪ ،‬ﻳﺒﺪو أن ﻓﻼﺳﻔﺔ اﻷﺧﻼق ﻳﻔﻌﻠﻮن ﻣﺎ اﺗﱡ ِﻬﻢ‬
‫ﻟﺪرﻳﻔﻮس اﻟﺒﺎﺣﺜني ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺮﻣﺰي ﺑ ِﻔﻌﻠﻪ‪ :‬ﺗﺸﻜﻴﻞ وﺗﺠﺮﻳﺪ ﺛﺮوة ﻣﻦ‬
‫ﱢ‬
‫اﻟﺘﺨﲇ ﻋﻤﺎ ﻳﺠﻌﻠﻨﺎ ﺑﴩًا‪ ،‬وﻟﻴﺲ ذﻟﻚ‬
‫اﻟﺘﺠﺮﺑﺔ اﻷﺧﻼﻗﻴﺔ واملﻌﺮﻓﺔ اﻷﺧﻼﻗﻴﺔ ﻋﲆ ﺣﺴﺎب‬
‫‪48‬‬

‫أﻫﻲ ٍّ‬
‫ﺣﻘﺎ ﻣﺠﺮد آﻻت؟‬

‫ﻓﺤﺴﺐ‪ ،‬ﺑﻞ وﻋﲆ ﺣﺴﺎب اﻟﺘﻀﺤﻴﺔ ﺑﻤﺴﺄﻟﺔ املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ ﻟﻐري اﻟﺒﴩ‪ .‬وﺑﴫف اﻟﻨﻈﺮ‬
‫ﻋﻦ املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ اﻟﻔﻌﻠﻴﺔ ﻟﻶﻻت املﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻛﻤﺎ ﻟﻮ ﻛﺎن ﻫﺬا ﻳﻤﻜﻦ‬
‫ﺑﻤﻜﺎن أن ﻧﻔﺤﺺ ﺗﻮﺟﱡ ﻬﻨﺎ‬
‫ﺑﺸﻜﻞ ﻣُﺴﺘﻘﻞ ﺗﻤﺎﻣً ﺎ ﻋﻦ ذاﺗﻴﺔ اﻹﻧﺴﺎن‪ ،‬ﻓﻤﻦ اﻷﻫﻤﻴﺔ‬
‫ﺗﺤﺪﻳﺪه‬
‫ٍ‬
‫ٍ‬
‫اﻷﺧﻼﻗﻲ وﻣﴩوع اﻟﺘﻔﻜري اﻷﺧﻼﻗﻲ املﺠﺮد ﻧﻔﺴﻪ‪ ،‬ﺑﺄﺳﻠﻮب ﻧﻘﺪي‪.‬‬
‫»إﺳﺎءة ﻣﻌﺎﻣﻠﺔ« اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺧﻄﺄ؛ ﻟﻴﺲ ﻷن ﺛﻤﺔ ﴐ ًرا ﺳﻴﻠﺤﻖ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻟﻜﻦ‬
‫ﻷن ﻃﺎﺑﻌﻨﺎ اﻷﺧﻼﻗﻲ ﺳﻴﺘﺄذﱠى إذا ﻣﺎ ﻓﻌﻠﻨﺎ ذﻟﻚ‪.‬‬

‫ﻧﺤﻮ ﻗﻀﺎﻳﺎ أﺧﻼﻗﻴﺔ أﻛﺜﺮ ﻋﻤﻠﻴﺔ‬
‫ﻛﻤﺎ ﺗُﻈﻬﺮ املﻨﺎﻗﺸﺎت ﰲ ﻫﺬا اﻟﻔﺼﻞ واﻟﻔﺼﻞ اﻟﺴﺎﺑﻖ‪ ،‬ﻓﺈن اﻟﺘﻔﻜري ﰲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻳُﻌ ﱢﻠﻤﻨﺎ أﺷﻴﺎء أﺧﺮى إﱃ ﺟﺎﻧﺐ ﻣﺎ ﻧﺘﻌ ﱠﻠﻤﻪ ﺑﺸﺄن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬إﻧﻪ ﻳُﻌﻠﻤﻨﺎ ً‬
‫أﻳﻀﺎ أﺷﻴﺎء‬
‫ﻋﻦ أﻧﻔﺴﻨﺎ‪ :‬ﻋﻦ ﻃﺮﻳﻘﺔ ﺗﻔﻜريﻧﺎ‪ ،‬وﻃﺮﻳﻘﺔ ﺗﴫﱡﻓﻨﺎ ﰲ اﻟﻮاﻗﻊ‪ ،‬واﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻳﻨﺒﻐﻲ أن‬
‫ِ‬
‫ﻷﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫ﻧﺘﻌﺎﻣَ ﻞ ﺑﻬﺎ ﻣﻊ ﻏري اﻟﺒﴩ‪ .‬ﻓﺈذا ﻧﻈﺮﻧﺎ إﱃ اﻷُﺳﺲ اﻟﻔﻠﺴﻔﻴﺔ‬
‫ً‬
‫ﻋﻤﻴﻘﺔ ﺣﻮل ﻃﺒﻴﻌﺔ وﻣﺴﺘﻘﺒﻞ اﻹﻧﺴﺎﻧﻴﺔ واﻟﻌﻠﻢ واﻟﺤﺪاﺛﺔ‪ .‬إن اﻟﺘﺸﻜﻴﻚ ﰲ‬
‫ﻧﺮى ﺧﻼﻓﺎت‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻳﻜﺸﻒ اﻟﻠﺜﺎم ﻋﻦ ﻋﺎ َﻟﻢ ﻣُﻈﻠﻢ ﻣﻦ اﻷﺳﺌﻠﺔ اﻟﻨﻘﺪﻳﺔ ﺣﻮل املﻌﺮﻓﺔ اﻟﺒﴩﻳﺔ‬
‫اﻟﺬﻛﺎء‬
‫ﱢ‬
‫وا ُملﺠﺘﻤﻊ اﻟﺒﴩي وﻃﺒﻴﻌﺔ اﻷﺧﻼق اﻟﺒﴩﻳﺔ‪.‬‬
‫ﻫﺬه املﻨﺎﻗﺸﺎت اﻟﻔﻠﺴﻔﻴﺔ أﻗﻞ ﺑُﻌﺪًا وأﻗﻞ »أﻛﺎدﻳﻤﻴﺔ« ﻣﻤﺎ ﻗﺪ ﻳﻌﺘﻘِ ﺪ اﻟﺒﻌﺾ‪ .‬وﺳﺘﻈ ﱡﻞ‬
‫ً‬
‫ﻻﺣﻘﺎ ﰲ ﻫﺬا اﻟﻜﺘﺎب‪ ،‬املﺰﻳ َﺪ ﻣﻦ املﺴﺎﺋﻞ اﻷﺧﻼﻗﻴﺔ‬
‫ﺗُﻌﺎود اﻟﻈﻬﻮر أﻣﺎﻣﻨﺎ ﻋﻨﺪﻣﺎ ﻧﺘﻨﺎول‪،‬‬
‫ً‬
‫ﻋﻤﻠﻴﺔ اﻟﺘﻲ ﻳُﺜريﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﴎﻋﺎن ﻣﺎ ﺳﺘُﻮاﺟﻬﻨﺎ‬
‫واﻟﻘﺎﻧﻮﻧﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ اﻷﻛﺜﺮ‬
‫ٍ‬
‫ﻣﻮﺿﻮﻋﺎت ﻣﺜﻞ املﺴﺌﻮﻟﻴﺔ واﻟﺴﻴﺎرات ذاﺗﻴﺔ اﻟﻘﻴﺎدة‪،‬‬
‫ﻣﻦ ﺟﺪﻳ ٍﺪ ﺑﻤﺠ ﱠﺮد أن ﻧُﺤﺎول اﻟﺘﻄ ﱡﺮق إﱃ‬
‫أو ﺷﻔﺎﻓﻴﺔ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ ،‬أو اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺘﺤﻴﺰ‪ ،‬أو أﺧﻼﻗﻴﺎت اﻟﺮوﺑﻮﺗﺎت اﻟﺠﻨﺴﻴﺔ‪ .‬إذا‬
‫ﻛﺎﻧﺖ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗُﺮﻳﺪ أن ﺗﻜﻮن أﻛﺜﺮ ِﻣﻦ ﻣﺠ ﱠﺮد ﻗﺎﺋﻤﺔ ﺑﺎﻟﻘﻀﺎﻳﺎ‪ ،‬ﻓﻴﺠﺐ‬
‫أن ﻳﻜﻮن ﻟﺪَﻳﻬﺎ ﻣﺎ ﺗﻘﻮﻟﻪ ﺣﻮل ﻣﺜﻞ ﻫﺬه املﺴﺎﺋﻞ‪.‬‬
‫ﺑﻌﺪ ﻛ ﱢﻞ ﻣﺎ ﻗﻴﻞ‪ ،‬ﺣﺎن اﻟﻮﻗﺖ اﻵن ﻟﻠﺘﺤﻮﱡل إﱃ ﻗﻀﺎﻳﺎ أﻛﺜﺮ ﻋﻤﻠﻴﺔ‪ .‬ﻫﺬه اﻟﻘﻀﺎﻳﺎ ﻻ‬
‫ﺗﺘﻌ ﱠﻠﻖ ﺑﺎ ُملﺸﻜﻼت اﻟﻔﻠﺴﻔﻴﺔ اﻟﺘﻲ ﻳﻄﺮﺣُ ﻬﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم ا ُملﻔﱰض‪ ،‬أو ﺑﺎملﺨﺎﻃﺮ‬
‫املﺘﱠﺼﻠﺔ ﺑﺎﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ ﰲ املﺴﺘﻘﺒﻞ اﻟﺒﻌﻴﺪ‪ ،‬أو ﺑﺎﻟﻮﺣﻮش املﺨﻴﻔﺔ اﻷﺧﺮى اﻟﺘﻲ ﻳﺨﻠﻘﻬﺎ‬
‫اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ‪ .‬إﻧﻬﺎ ﺗﺘﻌﻠﻖ ﺑﺤﻘﺎﺋﻖ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺋﻤﺔ ﺑﺎﻟﻔﻌﻞ‪ ،‬واﻟﺘﻲ ﻫﻲ أﻗ ﱡﻞ‬
‫‪49‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫وﺿﻮﺣً ﺎ ورﺑﻤﺎ أﻗﻞ ﺟﺎذﺑﻴﺔ‪ ،‬وﻟﻜﻨﻬﺎ ﻻ ﺗﺰال ﺷﺪﻳﺪة اﻷﻫﻤﻴﺔ‪ .‬إن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ‬
‫اﻟﻮﻗﺖ اﻟﺤﺎﱄ ﻻ ُ‬
‫ﻳﺄﺧﺬ دور وﺣﺶ ﻓﺮاﻧﻜﻨﺸﺘﺎﻳﻦ أو اﻟﺮوﺑﻮﺗﺎت ا ُملﺬﻫﻠﺔ املﺰوﱠدة ﺑﺎﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻟﺘﻲ ﺗُﻬﺪﱢد اﻟﺤﻀﺎرة‪ ،‬ﻛﻤﺎ أﻧﻪ أﻛﺜﺮ ﻣﻦ ﻣﺠﺮد ﺗﺠﺮﺑ ٍﺔ ﻓﻜﺮﻳﺔ ﻓﻠﺴﻔﻴﺔ‪ .‬اﻟﺬﻛﺎء‬
‫ٍ‬
‫ﺑﺘﻘﻨﻴﺎت ﴎﻳ ٍﺔ ﻏري ﻣﺮﺋﻴﺔ وﻟﻜﻨﻬﺎ ﻣُﺘﻐﻠﻐﻠﺔ وﻣﻨﺘﴩة وﻗﻮﻳﺔ وﻣﺘﺰاﻳﺪة‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻳﺘﻌ ﱠﻠﻖ‬
‫اﻟﺬﻛﺎء‪ ،‬ﺗﻠﻚ اﻟﺘﻘﻨﻴﺎت اﻟﺘﻲ ﺗُﺸ ﱢﻜﻞ ﺑﺎﻟﻔﻌﻞ ﺣﻴﺎﺗﻨﺎ اﻟﻴﻮم‪ .‬وﻣﻦ ﺛَﻢﱠ‪ ،‬ﻓﺈن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻟﺘﺤﺪﻳﺎت اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ ﻳُﺜريﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ وﰲ‬
‫ا ُملﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ‪ ،‬ﻛﻤﺎ ﺗﺘﻌﻠﻖ ﺑﺘﺄﺛري ﻫﺬه اﻟﺘﺤﺪﱢﻳﺎت ﻋﲆ ﻣﺠﺘﻤﻌﺎﺗﻨﺎ ودﻳﻤﻘﺮاﻃﻴﺎﺗﻨﺎ ﱠ‬
‫اﻟﻬﺸﺔ‪.‬‬
‫إن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺘﻌ ﱠﻠﻖ ﺑﺤﻴﺎة اﻟﻨﺎس وﺑﺎﻟﺴﻴﺎﺳﺔ‪ .‬إﻧﻬﺎ ﺗﺘﻌﻠﻖ ﺑﺤﺎﺟﺘﻨﺎ‪،‬‬
‫ﻛﺄﻓﺮادٍ وﻛﻤﺠﺘﻤﻌﺎت‪ ،‬إﱃ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﻘﻀﺎﻳﺎ اﻷﺧﻼﻗﻴﺔ اﻵن‪.‬‬

‫‪50‬‬

‫اﻟﻔﺼﻞ اﻟﺨﺎﻣﺲ‬

‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬

‫ﻗﺒﻞ ﻣﻨﺎﻗﺸﺔ اﻟﻘﻀﺎﻳﺎ اﻷﺧﻼﻗﻴﺔ اﻟﻮاﻗﻌﻴﺔ ا ُملﺘﻌﻠﻘﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﻤﺰﻳ ٍﺪ ﻣﻦ اﻟﺘﻔﺎﺻﻴﻞ‪،‬‬
‫ٌ‬
‫ﻣﻬﻤﺔ أﺧﺮى ﻋﻠﻴﻨﺎ إﻧﺠﺎزﻫﺎ ﻟﺘﻤﻬﻴﺪ اﻟﻄﺮﻳﻖ‪ :‬ﺑﻌﻴﺪًا ﻋﻦ اﻟﻀﺠﱠ ﺔ ا ُملﺜﺎرة ﺣﻮل‬
‫ﻟﺪَﻳﻨﺎ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻋﻠﻴﻨﺎ أن ﻧﻔﻬﻢ ﻫﺬه اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﺗﻄﺒﻴﻘﺎﺗﻬﺎ‪ .‬ﻓﻠﻨُﻨَﺢﱢ ﺟﺎﻧﺒًﺎ اﻟﺨﻴﺎلَ‬
‫اﻟﻌﻠﻤﻲ ﻟﺘﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ واﻟﺘﻄ ﱡﻠﻌﺎت اﻟﻔﻠﺴﻔﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم‪ ،‬وﻟﻨ ُ ْﻠ ِﻖ ﻧﻈﺮ ًة ﻋﲆ‬
‫ﱠ‬
‫ﻣﺎﻫﻴﺔ ﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻛﻴﻔﻴﺔ اﺳﺘﺨﺪاﻣﻬﺎ اﻟﻴﻮم‪ .‬وﺑﻤﺎ أن ﺗﻌﺮﻳﻔﺎت اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ وﻏريﻫﺎ ﻣﻦ ا ُملﺼﻄ َﻠﺤﺎت ﻫﻲ ﻧﻔﺴﻬﺎ ﻏري ُﻣﺘﱠﻔﻖ ﻋﻠﻴﻬﺎ‪ ،‬ﻓﺈﻧﻨﻲ ﻟﻦ أﺗﻌﻤﱠ ﻖ ﰲ‬
‫ٍ‬
‫ﻧﻘﺎﺷﺎت ﻓﻠﺴﻔﻴﺔ أو ﺳﻴﺎﻗﺎت ﺗﺎرﻳﺨﻴﺔ‪ .‬إن ﻫﺪﰲ اﻟﺮﺋﻴﴘ ﻫﻨﺎ ﻫﻮ أن أُﻋﻄﻲ اﻟﻘﺎرئ ﻓﻜﺮ ًة‬
‫ﻋﻦ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ املﻌﻨﻴﺔ وﻛﻴﻔﻴﺔ اﺳﺘﺨﺪاﻣﻬﺎ‪ .‬وﺳﻮف أﺑﺪأ ﺑﺎﻟﺘﺤﺪﱡث ﻋﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺑﺸﻜﻞ ﻋﺎم؛ أﻣﺎ اﻟﻔﺼﻞ اﻟﺘﺎﱄ‪ ،‬ﻓﺴﻴﺘﻨﺎول ﺗﻘﻨﻴﺎت ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت وﺗﻄﺒﻴﻘﺎﺗﻬﻤﺎ‪.‬‬
‫ٍ‬
‫ﻣﺎ ﻫﻮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟‬
‫ﻳﻤﻜﻦ ﺗﻌﺮﻳﻒ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺄﻧﻪ اﻟﺬﻛﺎء اﻟﺬي ﺗُﻈﻬﺮه أو ﺗُﺤﺎﻛﻴﻪ اﻟﺮﻣﻮز اﻟﱪﻣﺠﻴﺔ‬
‫ً‬
‫ﺳﺆاﻻ ﺣﻮل ﻛﻴﻔﻴﺔ ﺗﻌﺮﻳﻒ اﻟﺬﻛﺎء‪ .‬ﻣﻦ‬
‫)اﻟﺨﻮارزﻣﻴﺎت( أو اﻵﻻت‪ .‬وﻳُﺜري ﻫﺬا اﻟﺘﻌﺮﻳﻒ‬
‫ً‬
‫اﻟﻨﺎﺣﻴﺔ اﻟﻔﻠﺴﻔﻴﺔ‪ ،‬ﻳ َ‬
‫ﻏﺎﻣﻀﺎ‪ .‬وﻳﻤﻜﻦ اﻟﻘﻮل ﺑﺄﻧﻪ ذﻛﺎءٌ ﺷﺒﻴﻪ ﺑﺎﻟﺬﻛﺎء‬
‫ُﻌﺘﱪ اﻟﺬﻛﺎء ﻣﻔﻬﻮﻣً ﺎ‬
‫اﻟﺒﴩي‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳُﻌ ﱢﺮف ﻓﻴﻠﻴﺐ ﺟﺎﻧﺴﻦ وآﺧﺮون اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺄﻧﻪ »ﻋﻠﻢ‬
‫ذﻛﻴﺔ ً‬
‫ً‬
‫وﻓﻘﺎ ملﻌﺎﻳري اﻟﺬﻛﺎء اﻟﺒﴩي« )‪،٢٠١٨‬‬
‫وﻫﻨﺪﺳﺔ اﻵﻻت ذات اﻟﻘﺪرات اﻟﺘﻲ ﺗُﻌﺘﱪ‬
‫‪ً .(٥‬‬
‫وﻓﻘﺎ ﻟﻬﺬا اﻟﺘﻌﺮﻳﻒ‪ ،‬ﻳﺘﻌﻠﻖ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺈﻧﺸﺎء آﻻت ذﻛﻴﱠﺔ ﺗُﻔﻜﺮ أو ﺗﺘﻔﺎﻋﻞ‬
‫ﻣﺜﻞ اﻟﺒﴩ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻳﻌﺘﻘﺪ اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﺒﺎﺣﺜني ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أﻧﻪ ﻟﻴﺲ‬
‫ﺗﻌﺮﻳﻔﺎ أﻛﺜﺮ ﺣﻴﺎدًا ِﺻ َ‬
‫ً‬
‫ﻴﻎ‬
‫داع ﻷن ﻳﻜﻮن اﻟﺬﻛﺎء ﺷﺒﻴﻬً ﺎ ﺑﺎﻟﺬﻛﺎء اﻟﺒﴩي‪ ،‬وﻳﻔﻀﻠﻮن‬
‫ﻫﻨﺎك ٍ‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺑﺸﻜﻞ ﻣُﺴﺘﻘﻞ ﻋﻦ اﻟﺬﻛﺎء اﻟﺒﴩي وأﻫﺪاف اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم أو اﻟﻘﻮي ذات اﻟﺼﻠﺔ‪.‬‬
‫ٍ‬
‫وﻳﴪدون ﺟﻤﻴﻊ أﻧﻮاع اﻟﻮﻇﺎﺋﻒ املﻌﺮﻓﻴﺔ واملﻬﺎم ﻣﺜﻞ اﻟﺘﻌ ﱡﻠﻢ واﻹدراك واﻟﺘﺨﻄﻴﻂ وﻣُﻌﺎﻟﺠﺔ‬
‫اﻟﻠﻐﺔ اﻟﻄﺒﻴﻌﻴﺔ واﻟﺘﻔﻜري واﺗﺨﺎذ اﻟﻘﺮارات وﺣ ﱢﻞ املﺸﻜﻼت؛ وﻏﺎﻟﺒًﺎ ﻣﺎ ﻳُﻌﺎدل ذﻟﻚ اﻟﺬﻛﺎءَ‬
‫ﻧﻔﺴﻪ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗﺰﻋﻢ ﻣﺎرﺟﺮﻳﺖ ﺑﻮدﻳﻦ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ »ﻳﺴﻌﻰ إﱃ ﺟﻌﻞ‬
‫أﺟﻬﺰة اﻟﻜﻤﺒﻴﻮﺗﺮ ﺗﻘﻮم ﺑﺎﻷﺷﻴﺎء اﻟﺘﻲ ﻳُﻤﻜﻦ ﻟﻠﻌﻘﻮل اﻟﺒﴩﻳﺔ اﻟﻘﻴﺎم ﺑﻬﺎ«‪ .‬ﻳﺒﺪو اﻷﻣﺮ ﰲ‬
‫اﻟﺒﺪاﻳﺔ وﻛﺄن اﻟﺒﴩ ﻫﻢ اﻟﻨﻤﻮذج اﻟﻮﺣﻴﺪ‪ .‬إﻻ أﻧﻬﺎ‪ ،‬ﺗﴪد ﺑﻌﺪ ذﻟﻚ ﻛﻞ أﻧﻮاع املﻬﺎرات اﻟﻨﻔﺴﻴﺔ‬
‫ﻣﺜﻞ اﻹدراك واﻟﺘﻨﺒﱡﺆ واﻟﺘﺨﻄﻴﻂ‪ ،‬اﻟﺘﻲ ﺗُﺸﻜﻞ ﺟﺰءًا ﻣﻦ »اﻟﻔﻀﺎء اﻟﻐﻨﻲ ﺑﻘﺪرات ﻣُﻌﺎﻟﺠﺔ‬
‫املﻌﻠﻮﻣﺎت املﺘﻨﻮﻋﺔ« )‪ .(١ ،٢٠١٦‬وﻳﻤﻜﻦ أن ﺗﻜﻮن ﻣُﻌﺎﻟﺠﺔ املﻌﻠﻮﻣﺎت ﻫﺬه ﻟﻴﺴﺖ ﺣﻜ ًﺮا ﻋﲆ‬
‫اﻹﻧﺴﺎن‪ .‬ﻓﺎﻟﺬﻛﺎء اﻟﻌﺎم‪ً ،‬‬
‫وﻓﻘﺎ ملﺎرﺟﺮﻳﺖ ﺑﻮدﻳﻦ‪ ،‬ﻻ ﻳﻜﻮن ﺑﺎﻟﴬورة ﺑﴩﻳٍّﺎ‪ .‬ﻓﻬﻨﺎك ﺑﻌﺾ‬
‫ﺑﻌﻘﻮل ﻣُﺴﺘﻘﺒﻠﻴﺔ ﻻ‬
‫اﻟﺤﻴﻮاﻧﺎت اﻟﺘﻲ ﻳُﻤﻜﻨﻨﺎ اﻋﺘﺒﺎرﻫﺎ ذﻛﻴﺔ‪ .‬وﻳﺤﻠﻢ ﻣﺆﻳﺪو ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‬
‫ٍ‬
‫ﺗﻜﻮن ﻣﻀﻤﻨﺔ ﺑﻴﻮﻟﻮﺟﻴٍّﺎ ﻣﺜﻠﻤﺎ ﻫﻮ اﻟﺤﺎل اﻵن‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻛﺎن ﻫﺪف ﺗﺤﻘﻴﻖ ﻗﺪرات ﺷﺒﻴﻬﺔ‬
‫ﺑﻘﺪرات اﻟﺒﴩ ورﺑﻤﺎ ذﻛﺎء ﻋﺎم ﺷﺒﻴﻪ ﺑﺬﻛﺎء اﻟﺒﴩ ﺟﺰءًا ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻨﺬ اﻟﺒﺪاﻳﺔ‪.‬‬
‫ﻃﺎ ً‬
‫ﻳﺮﺗﺒﻂ ﺗﺎرﻳﺦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ارﺗﺒﺎ ً‬
‫ﱡ‬
‫واﻟﺘﺨﺼﺼﺎت‬
‫وﺛﻴﻘﺎ ﺑﺘﺎرﻳﺦ ﻋﻠﻮم اﻟﻜﻤﺒﻴﻮﺗﺮ‬
‫ذات ﱢ‬
‫اﻟﺼﻠﺔ ﻣﺜﻞ اﻟﺮﻳﺎﺿﻴﺎت واﻟﻔﻠﺴﻔﺔ‪ ،‬وﻣﻦ ﺛ َ ﱠﻢ ﻓﻬﻮ ﻳﻤﺘ ﱡﺪ ﻋﲆ اﻷﻗﻞ إﱃ اﻟﻌﺼﻮر اﻟﺤﺪﻳﺜﺔ‬
‫اﻟﺒﺎﻛﺮة )ﻣﺜﻞ ﺟﻮﺗﻔﺮﻳﺪ ﻓﻴﻠﻬﻠﻢ ﻻﻳﺒﻨﻴﺘﺲ ورﻳﻨﻴﻪ دﻳﻜﺎرت( إن ﻟﻢ ﻳﻜﻦ إﱃ اﻟﻌﺼﻮر اﻟﻘﺪﻳﻤﺔ‪،‬‬
‫ٍ‬
‫ٍ‬
‫ِ‬
‫وآﻻت ذﻛﻴﺔ ﻳُﻤﻜﻨﻬﺎ‬
‫ﻛﺎﺋﻨﺎت اﺻﻄﻨﺎﻋﻴﺔ‬
‫ﺗﻨﺘﴩ ﻓﻴﻬﺎ ﻗﺼﺺ ﻋﻦ ﺣﺮﻓﻴﱢني ﻳﺼﻨﻌﻮن‬
‫اﻟﺘﻲ‬
‫ُ‬
‫ﺧﺪاع اﻟﻨﺎس )ﺗﺬ ﱠﻛﺮ اﻟﺸﺨﺼﻴﺎت املﺘﺤﺮﻛﺔ ﰲ اﻟﻴﻮﻧﺎن اﻟﻘﺪﻳﻤﺔ أو اﻟﺸﺨﺼﻴﺎت اﻵﻟﻴﺔ‬
‫اﻟﺸﺒﻴﻬﺔ ﺑﺎﻟﺒﴩ ﰲ اﻟﺼني اﻟﻘﺪﻳﻤﺔ(‪ .‬وﻟﻜﻦ ﻋﲆ اﻟﻌﻤﻮم ﻳُﻌﺘﱪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﺪ ﺑﺪأ‬
‫ٍّ‬
‫ﱡ‬
‫ﺗﺨﺼ ً‬
‫ﻣﺴﺘﻘﻼ‪ ،‬ﺑﻌﺪ اﺧﱰاع اﻟﻜﻤﺒﻴﻮﺗﺮ‬
‫ﺼﺎ‬
‫ﰲ اﻟﺨﻤﺴﻴﻨﻴﺎت ﻣﻦ اﻟﻘﺮن اﻟﻌﴩﻳﻦ ﺑﻮﺻﻔﻪ‬
‫ﱡ‬
‫ﺗﺨﺼﺺ ﻋﻠﻢ اﻟﺘﺤ ﱡﻜﻢ اﻵﱄ‬
‫اﻟﺮﻗﻤﻲ اﻟﻘﺎﺑﻞ ﻟﻠﱪﻣﺠﺔ ﰲ أرﺑﻌﻴﻨﻴﱠﺎت اﻟﻘﺮن اﻟﻌﴩﻳﻦ ووﻻدة‬
‫)اﻟﺴﻴﱪاﻧﻴﺔ(‪ ،‬اﻟﺬي ﻋ ﱠﺮﻓﻪ ﻧﻮرﺑﺮت وﻳﻨﺮ ﰲ ﻋﺎم ‪ ١٩٤٨‬ﻋﲆ أﻧﻪ اﻟﺪراﺳﺔ اﻟﻌﻠﻤﻴﺔ »ﻟﻠﺘﺤ ﱡﻜﻢ‬
‫ُ‬
‫واﻟﺘﻮاﺻﻞ ﰲ اﻟﺤﻴﻮان واﻵﻟﺔ« )‪ .(Wiener 1948‬وﻛﺎن ﻧﴩ ورﻗﺔ أﻻن ﺗﻮرﻳﻨﺞ اﻟﺒﺤﺜﻴﺔ‬
‫ﻟﻌﺎم ‪ ١٩٥٠‬ﺑﻌﻨﻮان »اﻵﻻت اﻟﺤﺎﺳﺒﺔ واﻟﺬﻛﺎء« ﰲ ﻣﺠﻠﺔ »ﻣﺎﻳﻨﺪ«‪ ،‬واﻟﺘﻲ ﻗﺪﻣﺖ اﺧﺘﺒﺎر‬
‫ﺑﺸﻜﻞ ﻋﺎم ﺳﺆال ﻣﺎ إذا ﻛﺎﻧﺖ اﻵﻻت ﻗﺎدر ًة ﻋﲆ‬
‫ﺗﻮرﻳﻨﺞ اﻟﺸﻬري وﻟﻜﻦ ﻛﺎﻧﺖ ﺗﺘﻨﺎول‬
‫ٍ‬
‫اﻟﺘﻔﻜري‪ ،‬وﺳﺒﻘﺖ ﺑﺎﻟﻔﻌﻞ ﰲ اﻟﺘﻜﻬﱡ ﻦ ﺑﺎﻵﻻت اﻟﺘﻲ ﻳُﻤﻜﻨﻬﺎ اﻟﺘﻌ ﱡﻠﻢ وأداء ﻣﻬﺎم ﻣﺠ ﱠﺮدة‪ ،‬ﻛﺎﻧﺖ‬
‫ﻟﺤﻈﺔ ﻫﺎﻣﺔ ﰲ ﺗﺎرﻳﺦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﺗُﻌﺘﱪ ورﺷﺔ اﻟﻌﻤﻞ اﻟﺘﻲ ﻋُ ﻘﺪت ﰲ‬
‫ﺑﺸﻜﻞ ﻋﺎم ﻫﻲ ﻣﺤﻞ‬
‫ﺟﺎﻣﻌﺔ دارﺗﻤﻮث ﰲ ﺻﻴﻒ ﻋﺎم ‪ ١٩٥٦‬ﰲ ﻫﺎﻧﻮﻓﺮ‪ ،‬ﻧﻴﻮ ﻫﺎﻣﺒﺸﺎﻳﺮ‪،‬‬
‫ٍ‬
‫ﻣﻴﻼد اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﻌﺎﴏ‪ .‬وﻗﺪ ﺻﺎغ ﻣُﻨﻈﻤﻬﺎ ﺟﻮن ﻣﻜﺎرﺛﻲ ﻓﻴﻬﺎ ﻣﺼﻄﻠﺢ اﻟﺬﻛﺎء‬
‫‪52‬‬

‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬

‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﺷﺎرﻛﺖ ﻓﻴﻬﺎ أﺳﻤﺎءٌ ﻣﻬﻤﺔ ﻣﺜﻞ ﻣﺎرﻓﻦ ﻣﻴﻨﺴﻜﻲ‪ ،‬وﻛﻠﻮد ﺷﺎﻧﻮن‪ ،‬وأﻟﻦ ﻧﻴﻮﻳﻞ‪،‬‬
‫وﻫريﺑﺮت ﺳﺎﻳﻤﻮن‪ .‬وﰲ ﺣني ﻛﺎن ﻳُﻨﻈﺮ إﱃ ﻋﻠﻢ اﻟﺘﺤ ﱡﻜﻢ اﻵﱄ ﻋﲆ أﻧﻪ ﺷﺪﻳﺪ اﻻﻧﺸﻐﺎل‬
‫ﺑﺎﻵﻻت اﻟﺘﻨﺎﻇﺮﻳﺔ‪ ،‬اﻫﺘﻤﱠ ﺖ ورﺷﺔ ﻋﻤﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ دارﺗﻤﻮث ﺑﺎﻵﻻت اﻟﺮﻗﻤﻴﺔ‪.‬‬
‫ﻛﺎﻧﺖ اﻟﻔﻜﺮة ﻫﻲ »ﻣﺤﺎﻛﺎة« اﻟﺬﻛﺎء اﻟﺒﴩي )وﻟﻴﺲ إﻋﺎدة ﺧﻠﻘِ ﻪ‪ :‬ﻓﺎﻟﻌﻤﻠﻴﺔ ﻣﺨﺘﻠﻔﺔ ﻋﻤﺎ‬
‫ﱠ‬
‫وﻇﻦ اﻟﻜﺜري ﻣﻦ املﺸﺎرﻛني ﰲ ورﺷﺔ اﻟﻌﻤﻞ ﻫﺬه أن إﻧﺸﺎء ٍ‬
‫آﻟﺔ ﺗﺘﻤﺘﱠﻊ ﺑﻨﻔﺲ‬
‫ﻳﺤﺪث ﰲ اﻟﺒﴩ(‪.‬‬
‫ﺟﻴﻞ واﺣﺪ‪.‬‬
‫ذﻛﺎء اﻟﺒﴩ أﻣﺮ وﺷﻴﻚ اﻟﺤﺪوث‪ :‬ﺗﻮﻗﻌﻮا أﻧﻬﺎ ﻟﻦ ﺗﺴﺘﻐﺮق ﰲ ﻇﻬﻮرﻫﺎ أﻛﺜﺮ ﻣﻦ ٍ‬
‫ﻫﺬا ﻫﻮ ﻫﺪف »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﻮي«‪ .‬اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ »اﻟﻘﻮي« أو »اﻟﻌﺎم«‬
‫ﻗﺎدر ﻋﲆ أداء أي ﻣﻬﺎم ﻣﻌﺮﻓﻴﺔ ﻳﻤﻜﻦ ﻟﻠﺒﴩ أداؤﻫﺎ‪ ،‬ﰲ ﺣني أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ﻣﺠﺎﻻت ﻣُﺤﺪدة ﻣﺜﻞ اﻟﺸﻄﺮﻧﺞ‪،‬‬
‫»اﻟﻀﻌﻴﻒ« أو »املﺤﺪود« ﻳﻤﻜﻦ أن ﻳﺆدي ﻓﻘﻂ ﰲ‬
‫وﺗﺼﻨﻴﻒ اﻟﺼﻮر‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪ .‬ﺣﺘﻰ اﻟﻴﻮم‪ ،‬ﻟﻢ ﻧ ُ ﱢ‬
‫ﺤﻘﻖ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم‪ ،‬وﻛﻤﺎ رأﻳﻨﺎ‬
‫ﰲ اﻟﻔﺼﻮل اﻟﺴﺎﺑﻘﺔ‪ ،‬ﻓﺈن اﻟﺸﻜﻮك ﺗﺤُ ﻮم ﺣﻮل ﻣﺎ إذا ﻛﻨﱠﺎ ﺳﻨ ُ ﱢ‬
‫ﺤﻘﻘﻪ ﻋﲆ اﻹﻃﻼق‪ .‬وﻋﲆ‬
‫اﻟﺮﻏﻢ ﻣﻦ أن ﺑﻌﺾ اﻟﺒﺎﺣﺜني واﻟﴩﻛﺎت ﻳُﺤﺎوﻟﻮن ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم‪ ،‬وﻻ‬
‫ﺳﻴﻤﺎ ﻫﺆﻻء اﻟﺬﻳﻦ ﻳﺆﻣِﻨﻮن ﺑﻨﻈﺮﻳﺔ ﺣﺎﺳﻮﺑﻴﺔ اﻟﻌﻘﻞ‪ ،‬ﻓﺈﻧﻪ ﻟﻦ ﻳﺘﻢ ﺗﻄﻮﻳﺮه ﰲ ا ُملﺴﺘﻘﺒﻞ‬
‫اﻟﻘﺮﻳﺐ‪ .‬وﻟﺬا‪ ،‬ﺗُﺮﻛﺰ اﻷﺳﺌﻠﺔ اﻷﺧﻼﻗﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ ﰲ اﻟﻔﺼﻞ اﻟﺘﺎﱄ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻟﻀﻌﻴﻒ أو املﺤﺪود‪ ،‬املﻮﺟﻮد ﺑﺎﻟﻔﻌﻞ ﺣﺎﻟﻴٍّﺎ واﻟﺬي ﻣﻦ ا ُملﺮﺟﱠ ﺢ أن ﻳُﺼﺒﺢ أﻛﺜﺮ ﻗﻮ ًة واﻧﺘﺸﺎ ًرا‬
‫ﰲ املﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ‪.‬‬
‫ﻳﻤﻜﻦ ﺗﻌﺮﻳﻒ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻋﺘﺒﺎره ﻋﻠﻤً ﺎ وﻛﺬﻟﻚ ﺑﺎﻋﺘﺒﺎره ﺗﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬ﻳﻤﻜﻦ‬
‫أن ﻳﻜﻮن اﻟﻬﺪف ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻮ ﺗﻔﺴري اﻟﺬﻛﺎء واﻟﻮﻇﺎﺋﻒ املﻌﺮﻓﻴﺔ املﺬﻛﻮرة‬
‫ﺗﻔﺴريًا ﻋﻠﻤﻴٍّﺎ أدق‪ .‬وﻳُﻤﻜﻦ أن ﻳُﺴﺎﻋﺪﻧﺎ ﰲ ﻓﻬﻢ اﻟﺒﴩ وﻏريﻫﻢ ﻣﻦ اﻟﻜﺎﺋﻨﺎت اﻟﺘﻲ ﺗﻤﺘﻠﻚ ذﻛﺎءً‬
‫ﱡ‬
‫وﺗﺨﺼ ً‬
‫ﺼﺎ ﻳﺪرس‬
‫ﻃﺒﻴﻌﻴٍّﺎ ﻓﻬﻤً ﺎ أﻓﻀﻞ‪ .‬وﺑﻬﺬه اﻟﻄﺮﻳﻘﺔ‪ ،‬ﻳﻜﻮن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﻠﻤً ﺎ‬
‫ﺑﺸﻜﻞ ﻣﻨﻬﺠﻲ )‪ ،(Jansen et al. 2018‬وأﺣﻴﺎﻧًﺎ ﻳﺪرس اﻟﻌﻘﻞ أو اﻟﺪﻣﺎغ‪.‬‬
‫ﻇﺎﻫﺮة اﻟﺬﻛﺎء‬
‫ٍ‬
‫وﻣﻦ ﻫﺬا املﻨﻄﻠﻖ‪ ،‬ﻳﺮﺗﺒﻂ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﻌﻠﻮ ٍم أﺧﺮى ﻣﺜﻞ اﻟﻌﻠﻮم املﻌﺮﻓﻴﺔ وﻋﻠﻢ اﻟﻨﻔﺲ‬
‫وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت )اﻧﻈﺮ اﻟﻘﺴﻢ اﻟﻼﺣﻖ(‪ ،‬وأﺣﻴﺎﻧًﺎ ً‬
‫أﻳﻀﺎ ﻋِ ﻠﻢ اﻷﻋﺼﺎب‪ ،‬اﻟﺬي ﻳﺴﻌﻰ ﺣﺜﻴﺜًﺎ إﱃ‬
‫َﻓﻬﻢ اﻟﺬﻛﺎء اﻟﻄﺒﻴﻌﻲ‪ .‬وﻟﻜﻦ ﻗﺪ ﻳﻜﻮن اﻟﻬﺪف ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫أﻳﻀﺎ ﻫﻮ ﺗﻄﻮﻳﺮ‬
‫ٍ‬
‫ﻷﻏﺮاض ﻋﻤﻠﻴﺔ ﻣُﺨﺘﻠﻔﺔ‪ ،‬أو ﻛﻤﺎ ﻳﻘﻮل ﺑﻮدن »ﻹﻧﺠﺎز أﺷﻴﺎء ﻣُﻔﻴﺪة«‪ :‬ﻳﻤﻜﻦ أن ﻳﺄﺧﺬ‬
‫ﺗﻘﻨﻴﺎت‬
‫ٍ‬
‫ﻷﻏﺮاض ﻋﻤﻠﻴﺔ‪ .‬وﻳﻤﻜﻦ‬
‫ﺷﻜ َﻞ أدوات‪ ،‬ﺻﻤﱠ ﻤﻬﺎ اﻟﺒﴩ‪ ،‬وﺗﺨﻠﻖ ﻣﻈﻬﺮ اﻟﺬﻛﺎء واﻟﺴﻠﻮك اﻟﺬﻛﻲ‬
‫ٍ‬
‫ﻟﻶﻻت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن ﺗﻔﻌﻞ ذﻟﻚ ﻋﻦ ﻃﺮﻳﻖ ﺗﺤﻠﻴﻞ اﻟﺒﻴﺌﺔ )ﰲ ﺻﻮرة‬
‫ٍ‬
‫ﺑﺪرﺟﺔ ﻛﺒرية ﻣﻦ اﻻﺳﺘﻘﻼﻟﻴﺔ‪ .‬ﰲ ﺑﻌﺾ اﻷﺣﻴﺎن‪ ،‬ﺗﻠﺘﻘﻲ اﻻﻫﺘﻤﺎﻣﺎت‬
‫ﺑﻴﺎﻧﺎت( واﻟﺘﴫﱡف‬
‫‪53‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﻌﻠﻤﻴﺔ‪-‬اﻟﻨﻈﺮﻳﺔ واﻷﻏﺮاض اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﰲ ﻋِ ﻠﻢ اﻷﻋﺼﺎب اﻟﺤﻮﺳﺒﻲ‪،‬‬
‫ٍ‬
‫ٍ‬
‫ﻣﴩوﻋﺎت ﻣُﺤﺪدة‬
‫أدوات ﻣﻦ ﻋﻠﻮم اﻟﻜﻤﺒﻴﻮﺗﺮ ﻟﻔﻬﻢ اﻟﺠﻬﺎز اﻟﻌﺼﺒﻲ‪ ،‬أو ﰲ‬
‫اﻟﺬي ﻳﺴﺘﺨﺪِم‬
‫‪1‬‬
‫ً‬
‫وأﻳﻀﺎ اﻟﺮوﺑﻮﺗﺎت‬
‫ﻣﺜﻞ »ﻣﴩوع اﻟﺪﻣﺎغ اﻟﺒﴩي« اﻷوروﺑﻲ‪ ،‬اﻟﺬي ﻳﺸﻤﻞ اﻟﻌﻠﻮم اﻟﻌﺼﺒﻴﺔ‬
‫واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؛ وﺗﺠﻤﻊ ﺑﻌﺾ ﻣﴩوﻋﺎﺗﻪ ﻣﺎ ﺑني ﻋِ ﻠﻢ اﻷﻋﺼﺎب وﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ﻓﻴﻤﺎ ﻳُﻌ َﺮف‬
‫ﺑﻌﻠﻢ أﻋﺼﺎب اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ )ﻣﺜﻞ ﻓﻮ وآﺧﺮﻳﻦ ‪.(٢٠١٨‬‬
‫ﱡ‬
‫اﻟﺘﺨﺼﺼﺎت وﻳﺮﺗﺒﻂ ﺑﻬﺎ‪ ،‬ﺑﻤﺎ‬
‫ﺑﺸﻜﻞ أﻋﻢ‪ ،‬ﻳﻌﺘﻤﺪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ اﻟﻌﺪﻳﺪ ﻣﻦ‬
‫ٍ‬
‫ﰲ ذﻟﻚ اﻟﺮﻳﺎﺿﻴﺎت )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﻹﺣﺼﺎء(‪ ،‬واﻟﻬﻨﺪﺳﺔ‪ ،‬واﻟﻠﻐﻮﻳﺎت‪ ،‬واﻟﻌﻠﻮم املﻌﺮﻓﻴﺔ‪،‬‬
‫وﻋﻠﻮم اﻟﻜﻤﺒﻴﻮﺗﺮ‪ ،‬وﻋﻠﻢ اﻟﻨﻔﺲ‪ ،‬وﺣﺘﻰ اﻟﻔﻠﺴﻔﺔ‪ .‬وﻛﻤﺎ رأﻳﻨﺎ‪ ،‬ﻳﻬﺘﻢ اﻟﻔﻼﺳﻔﺔ واﻟﺒﺎﺣﺜﻮن ﰲ‬
‫ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ ﺣ ﱟﺪ ﺳﻮاء ﺑﻔﻬﻢ اﻟﻌﻘﻞ وﻇﻮاﻫﺮ ﻣﺜﻞ اﻟﺬﻛﺎء واﻟﻮﻋﻲ واﻹدراك‬
‫واﻟﻔﻌﻞ واﻹﺑﺪاع‪ .‬وﻗﺪ أﺛﱠﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ اﻟﻔﻠﺴﻔﺔ واﻟﻌﻜﺲ ﺻﺤﻴﺢ‪ .‬وﻗﺪ أﻗ ﱠﺮ ﻛﻴﺚ‬
‫ﻓﺮاﻧﻜﻴﺶ ووﻳﻠﻴﺎم راﻣﺰي ﺑﻬﺬا اﻻرﺗﺒﺎط ﺑني اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﻔﻠﺴﻔﺔ‪ ،‬وﺷﺪﱠدا ﻋﲆ‬
‫ﱡ‬
‫ﺗﺨﺼﺼﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﺟﻤﻌﺎ اﻟﺠﺎﻧﺒَني اﻟﻌﻠﻤﻲ واﻟﺘﻜﻨﻮﻟﻮﺟﻲ ﰲ ﺗﻌﺮﻳﻔﻬﻤﺎ‬
‫ﺗﻌﺪﱡد‬
‫ﱡ‬
‫اﻟﺘﺨﺼﺼﺎت ﻟﻔﻬﻢ وﻧﻤﺬﺟﺔ وﻣُﺤﺎﻛﺎة اﻟﺬﻛﺎء‬
‫ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻋﺘﺒﺎره »ﻧﻬﺠً ﺎ ﻣُﺘﻌﺪﱢد‬
‫واﻟﻌﻤﻠﻴﺎت املﻌﺮﻓﻴﺔ ﻋﻦ ﻃﺮﻳﻖ اﻻﺳﺘﻨﺎد إﱃ ﻣﺒﺎدئ وأﺟﻬﺰة ﺣﻮﺳ ِﺒﻴﱠﺔ ورﻳﺎﺿﻴﺔ وﻣﻨﻄﻘﻴﺔ‬
‫وﻣﻴﻜﺎﻧﻴﻜﻴﺔ وﺣﺘﻰ ﺑﻴﻮﻟﻮﺟﻴﺔ ﻣﺘﻨﻮﻋﺔ« )‪ .(١ ،٢٠١٤‬ﻟﺬﻟﻚ‪ ،‬ﻳﻌﺘﱪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻧﻈﺮﻳٍّﺎ‬
‫وﻋﻤﻠﻴٍّﺎ‪ ،‬ﻋﻠﻤً ﺎ وﺗﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬وﻳﺮﻛﺰ ﻫﺬا اﻟﻜﺘﺎب ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻋﺘﺒﺎره ﺗﻜﻨﻮﻟﻮﺟﻴﺎ‪،‬‬
‫ﻋﲆ اﻟﺠﺎﻧﺐ اﻷﻛﺜﺮ ﻋﻤﻠﻴﺔ‪ :‬ﻟﻴﺲ ﻓﻘﻂ ﻷن اﻟﱰﻛﻴﺰ داﺧﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﺪ ﺗﺤﻮﱠل ﰲ‬
‫ﻫﺬا اﻻﺗﺠﺎه‪ ،‬وﻟﻜﻦ‪ ،‬ﻋﲆ وﺟﻪ اﻟﺨﺼﻮص‪ ،‬ﻷن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﻫﺬه اﻟﺼﻮرة ﻟﻪ ﻋﻮاﻗﺐ‬
‫أﺧﻼﻗﻴﺔ واﺟﺘﻤﺎﻋﻴﺔ؛ ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن اﻟﺒﺤﺚ اﻟﻌﻠﻤﻲ ً‬
‫أﻳﻀﺎ ﻟﻴﺲ ﺧﺎﻟﻴًﺎ ﺗﻤﺎﻣً ﺎ ﻣﻦ اﻟﻌﻮاﻗﺐ‬
‫اﻷﺧﻼﻗﻴﺔ‪.‬‬
‫ً‬
‫ً‬
‫ﺑﺎﻋﺘﺒﺎره ﺗﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬ﻳُﻤﻜﻦ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن ﻳﺄﺧﺬ أﺷﻜﺎﻻ ﻣﺨﺘﻠﻔﺔ وﻋﺎدة ﻣﺎ‬
‫ﻳﻜﻮن ﺟﺰءًا ﻣﻦ ﻧُﻈﻢ ﺗﻜﻨﻮﻟﻮﺟﻴﺔ أﻛﱪ‪ :‬اﻟﺨﻮارزﻣﻴﺎت‪ ،‬واﻵﻻت‪ ،‬واﻟﺮوﺑﻮﺗﺎت‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪.‬‬
‫ﻟﺬﻟﻚ‪ ،‬ﰲ ﺣني ﻗﺪ ﻳﺘﻌ ﱠﻠﻖ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑ »اﻵﻻت«‪ ،‬ﻓﺈن ﻫﺬا املﺼﻄﻠﺢ ﻻ ﻳُﺸري إﱃ‬
‫ً‬
‫ﺷﻜﻼ ﺑﴩﻳٍّﺎ‪ .‬ﻳُﻤﻜﻦ أن ﻳُﻀﻤﱠ ﻦ اﻟﺬﻛﺎء‬
‫اﻟﺮوﺑﻮﺗﺎت وﺣﺪَﻫﺎ‪ ،‬ﻧﺎﻫﻴﻚ ﻋﻦ اﻟﺮوﺑﻮﺗﺎت اﻟﺘﻲ ﺗﺘﱠﺨِ ﺬ‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻟﻌﺪﻳﺪ ﻣﻦ أﻧﻮاع اﻷﻧﻈﻤﺔ واﻷﺟﻬﺰة اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ اﻷﺧﺮى‪ .‬وﻳﻤﻜﻦ ﻷﻧﻈﻤﺔ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن ُ‬
‫ﺑﺮﻧﺎﻣﺞ ﻳﻌﻤﻞ ﻋﲆ اﻟﻮﻳﺐ )ﻣﺜﻞ اﻟﺪردﺷﺔ اﻵﻟﻴﺔ وﻣُﺤﺮﻛﺎت‬
‫ﺗﺄﺧﺬ ﺷﻜ َﻞ‬
‫ٍ‬
‫اﻟﺒﺤﺚ وﺗﺤﻠﻴﻞ اﻟﺼﻮر(‪ ،‬وﻟﻜﻦ ﻳُﻤﻜﻦ أن ﻳُﻀﻤﱠ ﻦ ً‬
‫أﻳﻀﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻷﺟﻬﺰة‬
‫‪2‬‬
‫املﻠﻤﻮﺳﺔ ﻣﺜﻞ اﻟﺮوﺑﻮﺗﺎت أو اﻟﺴﻴﺎرات أو ﺗﻄﺒﻴﻘﺎت »إﻧﱰﻧﺖ اﻷﺷﻴﺎء«‪ .‬ﺑﺎﻟﻨﺴﺒﺔ إﱃ إﻧﱰﻧﺖ‬
‫‪54‬‬

‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬

‫اﻷﺷﻴﺎء‪ ،‬ﻳُﺴﺘﺨﺪَم أﺣﻴﺎﻧًﺎ ﻣﺼﻄﻠﺢ »اﻷﻧﻈﻤﺔ اﻹﻟﻜﱰوﻧﻴﺔ‪-‬املﺎدﻳﺔ«‪ ،‬وﻫﻲ ﻋﺒﺎرة ﻋﻦ أﺟﻬﺰة‬
‫ﺗﻌﻤﻞ ﰲ اﻟﻌﺎ َﻟﻢ املﺎدي وﺗﺘﻔﺎﻋﻞ ﻣﻌﻪ‪ .‬وﺗُﻌَ ﺪ اﻟﺮوﺑﻮﺗﺎت ﻧﻮﻋً ﺎ ﻣﻦ اﻷﻧﻈﻤﺔ اﻹﻟﻜﱰوﻧﻴﺔ‪-‬املﺎدﻳﺔ‪،‬‬
‫اﻟﺘﻲ ﺗﺆﺛﱢﺮ ﺗﺄﺛريًا ﻣﺒﺎﴍًا ﻋﲆ اﻟﻌﺎﻟﻢ )‪.(Lin, Abney, and Bekey 2011‬‬
‫إذا ﺗ ﱠﻢ ﺗﻀﻤني اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ روﺑﻮت‪ ،‬ﻓﺈﻧﻪ ﻳُﻄﻠﻖ ﻋﻠﻴﻪ أﺣﻴﺎﻧًﺎ اﻟﺬﻛﺎء‬
‫ﱢ‬
‫»املﺘﺠﺴﺪ«‪ .‬وﺗﻌﺘﻤﺪ اﻟﺮوﺑﻮﺗﺎت ﰲ ﺗﺄﺛريﻫﺎ ﻋﲆ اﻟﻌﺎ َﻟﻢ املﺎدي ﺗﺄﺛريًا ﻣﺒﺎﴍًا‬
‫اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻋﲆ ﻣﻜﻮﱢﻧﺎت ﻣﺎدﻳﺔ‪ .‬وﻟﻜﻦ ﻛﻞ ﻧﻈﺎم ذﻛﺎءٍ اﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ اﻟﱪاﻣﺞ اﻟﻨﺸﻄﺔ ﻋﲆ‬
‫اﻟﻮﻳﺐ‪» ،‬ﻳﻔﻌﻞ« ﺷﻴﺌًﺎ وﻟﺪَﻳﻪ ً‬
‫أﻳﻀﺎ ﻣﻜﻮﱢﻧﺎت ﻣﺎدﻳﺔ ﻣﺜﻞ اﻟﻜﻤﺒﻴﻮﺗﺮ اﻟﺬي ﻳﻌﻤﻞ ﻋﻠﻴﻪ‪ ،‬وا ُملﻜﻮﻧﺎت‬
‫املﺎدﻳﺔ ﻟﻠﺸﺒﻜﺔ واﻟ ِﺒﻨﻴﺔ اﻷﺳﺎﺳﻴﺔ اﻟﺘﻲ ِ‬
‫ﻳﻌﺘﻤﺪ ﻋﻠﻴﻬﺎ‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪ .‬وﻫﺬا ﻳﺠﻌﻞ اﻟﺘﻔﺮﻗﺔ ﻣﺎ ﺑني‬
‫ﺗﻄﺒﻴﻘﺎت اﻟﻮﻳﺐ »اﻻﻓﱰاﺿﻴﺔ« واﻟﺘﻄﺒﻴﻘﺎت »اﻟﱪﻣﺠﻴﺔ« ﻣﻦ ﻧﺎﺣﻴﺔ‪ ،‬واﻟﺘﻄﺒﻴﻘﺎت املﺎدﻳﺔ أو‬
‫ً‬
‫ﻣﺴﺄﻟﺔ ﺻﻌﺒﺔ وﻣ ﱢ‬
‫ُﺤرية‪ .‬إن ﺑﺮاﻣﺞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺗﻄﺒﻴﻘﺎت »اﻷﺟﻬﺰة« ﻣﻦ ﻧﺎﺣﻴ ٍﺔ أﺧﺮى‬
‫ﺗﺤﺘﺎج إﱃ ﻣﻜﻮﱢﻧﺎت ﻣﺎدﻳﺔ و ِﺑﻨﻴﺔ أﺳﺎﺳﻴﺔ ﻣﺎدﻳﺔ ﻟﻜﻲ ﺗﻌﻤﻞ‪ ،‬واﻷﻧﻈﻤﺔ اﻹﻟﻜﱰوﻧﻴﺔ‪-‬املﺎدﻳﺔ ﻻ‬
‫ﻳﻤﻜﻦ اﻋﺘﺒﺎرﻫﺎ ذﻛﺎءً اﺻﻄﻨﺎﻋﻴٍّﺎ إﻻ إذا ﺗﻢ ﺗﻮﺻﻴﻠﻬﺎ ﺑﺎﻟﱪاﻣﺞ املﻨﺎﺳﺒﺔ‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻣﻦ‬
‫وﺟﻬﺔ ﻧﻈﺮ اﻟﻈﺎﻫﺮﻳﺔ‪ ،‬ﻗﺪ ﺗﻨﺪﻣﺞ املﻜﻮﻧﺎت املﺎدﻳﺔ واﻟﱪﻣﺠﻴﺔ أﺣﻴﺎﻧًﺎ ﰲ ﺗﺠﺮﺑﺘﻨﺎ واﺳﺘﺨﺪاﻣﻨﺎ‬
‫ً‬
‫ﺷﻜﻼ ﺑﴩﻳٍّﺎ وﻳﻌﻤﻞ ﺑﻮاﺳﻄﺔ‬
‫ﻟﻸﺟﻬﺰة‪ :‬ﻓﻨﺤﻦ ﻻ ﻧﺸﻌﺮ ﺑﺄن اﻟﺮوﺑﻮت اﻟﺘﻔﺎﻋُ ﲇ اﻟﺬي ﻳﺄﺧﺬ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬أو أن ﺟﻬﺎز املﺤﺎدﺛﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺜﻞ أﻟﻴﻜﺴﺎ‪ ،‬ﻋﺒﺎرة ﻋﻦ‬
‫ﻣﻜﻮﻧﺎت ﺑﺮﻣﺠﻴﺔ أو ﻣﻜﻮﻧﺎت ﻣﺎدﻳﺔ‪ ،‬وﻟﻜﻨﻨﺎ ﻧﺸﻌﺮ أﻧﻬﻤﺎ ﺟﻬﺎز ﺗﻜﻨﻮﻟﻮﺟﻲ واﺣﺪ )وأﺣﻴﺎﻧًﺎ‬
‫ﻧﺸﻌﺮ أﻧﻬﻤﺎ ِﺷﺒﻪ أﺷﺨﺎص‪ ،‬ﻣﺜﻞ دُﻣﻴﺔ »ﻫﺎﻟﻮ ﺑﺎرﺑﻲ«(‪.‬‬
‫ﻣﻦ ا ُملﺮﺟﱠ ﺢ أن ﻳﻜﻮن ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺄﺛري ﻛﺒري ﻋﲆ ﻋﻠﻢ اﻟﺮوﺑﻮﺗﺎت‪ ،‬وذﻟﻚ ﻋﲆ‬
‫ُ‬
‫واﻟﺘﻮاﺻﻞ اﻟﺸﺒﻴﻪ ﺑﺘﻮاﺻﻞ اﻹﻧﺴﺎن‪.‬‬
‫ﺳﺒﻴﻞ املﺜﺎل ﻣﻦ ﺧﻼل اﻟﺘﻘﺪﱡم ﰲ ﻣﻌﺎﻟﺠﺔ اﻟﻠﻐﺔ اﻟﻄﺒﻴﻌﻴﺔ‬
‫ري ﻣﻦ اﻷﺣﻴﺎن ﻳُﻄ َﻠﻖ ﻋﲆ ﻫﺬه اﻟﺮوﺑﻮﺗﺎت اﺳﻢ »اﻟﺮوﺑﻮﺗﺎت اﻻﺟﺘﻤﺎﻋﻴﺔ«؛ ﻷﻧﻬﺎ ﻣُﺼﻤﱠ ﻤﺔ‬
‫ﰲ ﻛﺜ ٍ‬
‫ﻛﺮﻓﺎق أو ﻣﺴﺎﻋِ ﺪﻳﻦ‪،‬‬
‫ﺑﻬﺪف املﺸﺎرﻛﺔ ﰲ اﻟﺤﻴﺎة اﻻﺟﺘﻤﺎﻋﻴﺔ اﻟﻴﻮﻣﻴﺔ ﻟﻠﺒﴩ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫ٍ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻃﺒﻴﻌﻴﺔ‪ .‬وﻣﻦ ﺛَﻢﱠ‪ ،‬ﻳﻤﻜﻦ أن ﻳُﻌﺰز اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻣﻦ ﺧﻼل اﻟﺘﻔﺎﻋُ ﻞ ﻣﻊ اﻟﺒﴩ‬
‫ﻣﺰﻳﺪًا ﻣﻦ اﻟﺘﻄﻮرات ﰲ اﻟﺮوﺑﻮﺗﺎت اﻻﺟﺘﻤﺎﻋﻴﺔ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﺑﻐﺾ اﻟﻨﻈﺮ ﻋﻦ املﻈﻬﺮ واﻟﺴﻠﻮك اﻟﻜﲇ ﻟﻠﻨﻈﺎم وﺗﺄﺛريه ﻋﲆ اﻟﺒﻴﺌﺔ ا ُملﺤﻴﻄﺔ‬
‫ﺑﻪ‪ ،‬وﻫﻮ ﻣﺎ ﻳُﻌﺘﱪ ﻣﻬﻤٍّ ﺎ ﺟﺪٍّا ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻟﻈﺎﻫﺮﻳﺔ واﻷﺧﻼﻗﻴﺔ‪ ،‬ﻓﺈن أﺳﺎس »اﻟﺬﻛﺎء« ﰲ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻮ ﺑﺮﻧﺎﻣﺞ‪» :‬ﺧﻮارزﻣﻴﺔ« أو ﻣﺠﻤﻮﻋﺔ ﻣﻦ اﻟﺨﻮارزﻣﻴﺎت‪ .‬واﻟﺨﻮارزﻣﻴﺔ‬
‫ُ‬
‫وﺗﺴﻠﺴﻞ ﻣﻦ اﻟﺘﻌﻠﻴﻤﺎت‪ ،‬ﻣﺜﻞ اﻟﻮﺻﻔﺔ‪ ،‬ﺗُﺨﱪ اﻟﻜﻤﺒﻴﻮﺗﺮ أو اﻟﻬﺎﺗﻒ اﻟﺬﻛﻲ أو‬
‫ﻫﻲ ﻣﺠﻤﻮﻋﺔ‬
‫اﻵﻟﺔ أو اﻟﺮوﺑﻮت أو أي ﳾءٍ َ‬
‫آﺧﺮ ﻳﺘﻢ ﺗﻀﻤﻴﻨﻬﺎ ﻓﻴﻪ ﺑﻤﺎ ﻳﺠﺐ أن ﻳﻔﻌﻞ‪ .‬وﻫﻲ ﺗﺆدي إﱃ‬
‫‪55‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ٍ‬
‫ﻣﺸﻜﻠﺔ‬
‫ﻣُﺨﺮﺟﺎت ﻣُﻌﻴﱠﻨﺔ ﺑﻨﺎءً ﻋﲆ املﻌﻠﻮﻣﺎت املﺘﺎﺣﺔ )املﺪﺧﻼت(‪ .‬وﺗُﻄﺒﱠﻖ اﻟﺨﻮارزﻣﻴﺔ ﻟﺤ ﱢﻞ‬
‫ﻣﺎ‪ .‬وﻟﻜﻲ ﻧﻔﻬﻢ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻋﻠﻴﻨﺎ ً‬
‫أوﻻ أن ﻧﻔﻬﻢ ﻛﻴﻔﻴﺔ ﻋﻤﻞ ﺧﻮارزﻣﻴﺎت‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻣﺎ ﺗﻘﻮم ﺑﻪ‪ .‬وﺳﻮف أﺗﺤﺪﱠث أﻛﺜﺮ ﻋﻦ ﻫﺬا املﻮﺿﻮع ﻫﻨﺎ وﰲ اﻟﻔﺼﻞ‬
‫اﻟﻘﺎدم‪.‬‬
‫املﻨﺎﻫﺞ واملﺠﺎﻻت اﻟﻔﺮﻋﻴﺔ ا ُملﺨﺘﻠﻔﺔ‬
‫ﻫﻨﺎك أﻧﻮاع ﻣﺨﺘﻠﻔﺔ ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻳﻤﻜﻦ اﻟﻘﻮل ً‬
‫أﻳﻀﺎ إن ﻫﻨﺎك ﻣﻨﺎﻫﺞ أو ﻧﻤﺎذج‬
‫ٍ‬
‫ﺑﺤﺚ ﻣﺨﺘﻠﻔﺔ‪ .‬ﻛﻤﺎ رأﻳﻨﺎ ﰲ اﻧﺘﻘﺎد درﻳﻔﻮس‪ ،‬ﻏﺎﻟﺒًﺎ ﻣﺎ ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ ﻣﺪار‬
‫اﻟﺘﺎرﻳﺦ ذﻛﺎءً اﺻﻄﻨﺎﻋﻴٍّﺎ رﻣﺰﻳٍّﺎ‪ .‬وﻛﺎن ﻫﺬا ﻫﻮ اﻟﻨﻤﻮذج اﻟﺴﺎﺋﺪ ﺣﺘﻰ أواﺧﺮ اﻟﺜﻤﺎﻧﻴﻨﻴﺎت‪.‬‬
‫ِ‬
‫وﻳﻌﺘﻤﺪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺮﻣﺰي ﻋﲆ اﻟﺘﻤﺜﻴﻼت اﻟﺮﻣﺰﻳﺔ ﻟﻠﻤﻬﺎ ﱢم املﻌﺮﻓﻴﺔ اﻟﻌﺎﻟﻴﺔ ا ُملﺴﺘﻮى‬
‫ﻣﺜﻞ اﻟﺘﻔﻜري اﻟﺘﺠﺮﻳﺪي واﺗﺨﺎذ اﻟﻘﺮارات‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﻳﺘﱠﺨِ ﺬ ﻗﺮا ًرا اﺳﺘﻨﺎدًا إﱃ‬
‫ﻧﻤﻮذج ﻟﻠﻘﺮارات وﻋﻮاﻗِ ﺒﻬﺎ ا ُملﻤﻜﻨﺔ‪ ،‬وﻳُﻤﺜﱠﻞ‬
‫اﻟﻬﻴﻜﻞ اﻟﺸﺠﺮي ﻻﺗﺨﺎذ اﻟﻘﺮار؛ وﻫﻮ ﻋﺒﺎرة ﻋﻦ‬
‫ٍ‬
‫ﺑﺸﻜﻞ رﺳﻮﻣﻲ ﻳُﺸﺒﻪ ا ُملﺨﻄﻂ اﻻﻧﺴﻴﺎﺑﻲ‪ .‬وﺗﺤﺘﻮي اﻟﺨﻮارزﻣﻴﺔ اﻟﺘﻲ ﺗﻔﻌﻞ ذﻟﻚ ﻋﲆ‬
‫ﻏﺎﻟﺒًﺎ‬
‫ٍ‬
‫ٍ‬
‫ﻋﺒﺎرات ﴍﻃﻴﺔ‪ :‬ﻗﻮاﻋﺪ ﻻﺗﺨﺎذ اﻟﻘﺮار ﻋﲆ ﺻﻮرة … ‪ ،if … then‬ﺑﺤﻴﺚ ﻳﲇ ‪ if‬اﻟﴩط وﻳﲇ‬
‫ٍ‬
‫ﺑﻴﺎﻧﺎت ﺗُﻤﺜﱢﻞ‬
‫‪ then‬اﻟﻨﺘﻴﺠﺔ‪ .‬وﻫﺬه اﻟﻌﻤﻠﻴﺔ ﺣﺎﺳﻤﺔ وﻏري ﻋﺸﻮاﺋﻴﺔ‪ .‬وﺑﺎﻻﺳﺘﻨﺎد إﱃ ﻗﺎﻋﺪة‬
‫املﻌﺮﻓﺔ اﻟﺨﺒرية اﻟﺒﴩﻳﺔ‪ ،‬ﻳُﻤﻜﻦ ملِ ﺜﻞ ﻫﺬا اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﺗﺨﺎذ اﻟﻘﺮار‪ ،‬ﻣُﻌﺘﻤﺪًا ﻋﲆ ﻛ ﱟﻢ‬
‫ٍ‬
‫ﻗﺮارات ﺣﻜﻴﻤﺔ أو ﻳﺼﻞ‬
‫ﻫﺎﺋﻞ ﻣﻦ املﻌﻠﻮﻣﺎت‪ ،‬واﻟﺘﴫﱡف ﻛﻨﻈﺎ ٍم ﺧﺒري‪ .‬وﻳﺴﺘﻄﻴﻊ أن ﻳﺘﱠﺨﺬ‬
‫إﱃ ﺗﻮﺻﻴﺎت اﺳﺘﻨﺎدًا إﱃ ٍ‬
‫ﻛﺘﻠﺔ ﺿﺨﻤﺔ ﻣﻦ املﻌﺮﻓﺔ‪ ،‬ﻗﺪ ﻳﻜﻮن ﻣﻦ اﻟﺼﻌﺐ أو ﻣﻦ ا ُملﺴﺘﺤﻴﻞ‬
‫ﺑﺎﻟﻨﺴﺒﺔ إﱃ اﻟﺒﴩ اﻻﻃﻼع ﻋﻠﻴﻬﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗُﺴﺘﺨﺪَم ﻫﺬه اﻷﻧﻈﻤﺔ اﻟﺨﺒرية ﰲ اﻟﻘﻄﺎع‬
‫ِ‬
‫ﺧﻄﺔ اﻟﻌﻼج‪ .‬وﻗﺪ ﻇ ﱠﻠﺖ ﻫﺬه اﻷﻧﻈﻤﺔ ﻫﻲ اﻷﻧﺠﺢ ﰲ ﻣﺠﺎل‬
‫اﻟﻄﺒﱢﻲ ﻟﺘﺸﺨﻴﺺ املﺮض ووﺿﻊ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻔﱰ ٍة ﻃﻮﻳﻠﺔ‪.‬‬
‫وﻻ ﻳﺰال اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺮﻣﺰي ﻣُﻔﻴﺪًا ﺣﺘﻰ اﻟﻴﻮم‪ ،‬وﻟﻜﻦ ﻇﻬﺮت ً‬
‫أﻳﻀﺎ أﻧﻮاع‬
‫ﺟﺪﻳﺪة ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻳُﻤﻜﻦ دﻣﺠﻬﺎ أو ﻋﺪم دﻣﺠﻬﺎ ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻟﺮﻣﺰي‪ ،‬وﻫﻲ ﻗﺎدرة ﻋﲆ اﻟﺘﻌ ﱡﻠﻢ ذاﺗﻴٍّﺎ ﻣﻦ اﻟﺒﻴﺎﻧﺎت‪ ،‬ﻋﲆ ﻋﻜﺲ اﻷﻧﻈﻤﺔ اﻟﺨﺒرية‪ .‬وﻳﺘﻢ‬
‫ﻧﻬﺞ ﻣﺨﺘﻠﻒ ﺗﻤﺎﻣً ﺎ‪ .‬وﻳﻌﺘﻤﺪ ﻧﻤﻮذج اﻟﺒﺤﺚ »اﻟﺘﺸﺎﺑُﻜﻲ«‪ ،‬اﻟﺬي ﺗﻢ‬
‫ذﻟﻚ ﻣﻦ ﺧﻼل اﺳﺘﺨﺪام ٍ‬
‫ﻛﺒﺪﻳﻞ ملﺎ أُﻃﻠِﻖ ﻋﻠﻴﻪ اﺳﻢ »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺗﻄﻮﻳﺮه ﰲ اﻟﺜﻤﺎﻧﻴﻨﻴﺎت ﻣﻦ اﻟﻘﺮن اﻟﻌﴩﻳﻦ‬
‫ٍ‬
‫اﻟﻘﺪﻳﻢ« وﻳﻌﺮف اﺧﺘﺼﺎ ًرا ﺑ ‪ ،GOFAI‬وﺗﻜﻨﻮﻟﻮﺟﻴﺎ »اﻟﺸﺒﻜﺎت اﻟﻌﺼﺒﻴﺔ« ﻋﲆ ﻓﻜﺮة أﻧﻨﺎ ً‬
‫ﺑﺪﻻ‬
‫‪56‬‬

‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬

‫ٍ‬
‫وﺣﺪات‬
‫ﻣﻦ ﺗﻤﺜﻴﻞ اﻟﻮﻇﺎﺋﻒ املﻌﺮﻓﻴﺔ اﻟﻌُ ﻠﻴﺎ‪ ،‬ﻳﺠﺐ ﻋﻠﻴﻨﺎ ﺑﻨﺎء ﺷﺒﻜﺎت ﻣُﱰاﺑﻄﺔ ﺑﺎﻻﺳﺘﻨﺎد إﱃ‬
‫ﺑﺴﻴﻄﺔ‪ .‬وﻳﺪﻋﻲ ﻣﺆﻳﺪو ﻫﺬا اﻟﻨﻬﺞ أن ﻫﺬا ﻳُﺸﺒﻪ اﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻳﻌﻤﻞ ﺑﻬﺎ اﻟﺪﻣﺎغ اﻟﺒﴩي؛‬
‫ِ‬
‫وﺣﺪات املﻌﺎﻟﺠﺔ اﻟﺒﺴﻴﻄﺔ ا ُملﺴﻤﱠ ﺎة »اﻟﺨﻼﻳﺎ اﻟﻌﺼﺒﻴﺔ«‬
‫إذ ﻳﻨﺸﺄ اﻹدراك ﻣﻦ ﺗﻔﺎﻋُ ﻼت ﺑني‬
‫)وﻣﻊ ذﻟﻚ‪ ،‬ﻓﻬﻲ ﻻ ﺗُﺸﺒﻪ اﻟﺨﻼﻳﺎ اﻟﻌﺼﺒﻴﺔ اﻟﺒﻴﻮﻟﻮﺟﻴﺔ(‪ .‬وﻳُﺴﺘﺨﺪَم اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﺨﻼﻳﺎ‬
‫اﻟﻌﺼﺒﻴﺔ ا ُملﱰا ِﺑﻄﺔ‪ .‬ﻳُﺴﺘﺨﺪَم ﻫﺬا اﻟﻨﻬﺞ وﻫﺬه اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻛﺜريًا ﰲ »ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ« )اﻧﻈﺮ‬
‫اﻟﻔﺼﻞ اﻟﺘﺎﱄ(‪ ،‬واﻟﺬي ﻳُﻄﻠﻖ ﻋﻠﻴﻪ ﺑﻌﺪ ذﻟﻚ »اﻟﺘﻌ ﱡﻠﻢ اﻟﻌﻤﻴﻖ« إذا ﻛﺎﻧﺖ اﻟﺸﺒﻜﺎت اﻟﻌﺼﺒﻴﺔ‬
‫ﻃﺒﻘﺎت ﻣﻦ اﻟﺨﻼﻳﺎ اﻟﻌﺼﺒﻴﺔ‪ .‬وﺗُ َ‬
‫ٍ‬
‫ﻌﺘﱪ ﺑﻌﺾ اﻷﻧﻈﻤﺔ ﻫﺠﻴﻨﺔ؛ ﻋﲆ ﺳﺒﻴﻞ‬
‫ﺗﺘﻜﻮﱠن ﻣﻦ ﻋﺪة‬
‫املﺜﺎل‪ ،‬ﻳ َ‬
‫ُﻌﺘﱪ »أﻟﻔﺎ ﺟﻮ« اﻟﺬي ﻃ ﱠﻮ َرﺗﻪ ﴍﻛﺔ »دﻳﺐ ﻣﺎﻳﻨﺪ« ﻧﻈﺎﻣً ﺎ ﻫﺠﻴﻨًﺎ‪ .‬وﻗﺪ أدﱠى اﻟﺘﻌ ﱡﻠﻢ‬
‫اﻟﻌﻤﻴﻖ إﱃ ﺣﺪوث ﺗﻄﻮﱡر ﰲ ﻣﺠﺎﻻت ﻣﺜﻞ رؤﻳﺔ اﻵﻟﺔ وﻣُﻌﺎﻟﺠﺔ اﻟﻠﻐﺔ اﻟﻄﺒﻴﻌﻴﺔ‪ .‬وﻳﻤﻜﻦ أن‬
‫ﻳﻜﻮن ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ اﻟﺬي ﻳَﺴﺘﺨﺪِم ﺷﺒﻜﺔ ﻣُﺤﺎﻳﺪة ﺑﻤﻨﺰﻟﺔ »ﺻﻨﺪوق أﺳﻮد«؛ ﺑﻤﻌﻨﻰ أﻧﻪ ﰲ ﺣني‬
‫أن ا ُملﱪﻣِﺠني ﻳﻌﺮﻓﻮن ﺗﺼﻤﻴﻢ اﻟﺸﺒﻜﺔ‪ ،‬ﻓﺈﻧﻪ ﻟﻴﺲ واﺿﺤً ﺎ ﻟﻶﺧﺮﻳﻦ ﻣﺎذا ﻳﺤﺪث ﺑﺎﻟﻀﺒﻂ‬
‫ﰲ ﻃﺒﻘﺎﺗﻬﺎ اﻟﻮﺳﻴﻄﺔ )ﺑني املﺪﺧﻼت واملﺨﺮﺟﺎت( وﺑﺎﻟﺘﺎﱄ ﻛﻴﻒ ﺗﺘﱠﺨِ ﺬ ﻗﺮا ًرا‪ .‬وﻫﺬا ﻋﻜﺲ‬
‫ً‬
‫وﻗﺎﺑﻼ ﻟﻠﺘﻔﺴري‪ ،‬وﻣﻦ ﺛَﻢ‬
‫ﻣﺎ ﻳﺤﺪُث ﰲ اﻟﻬﻴﻜﻞ اﻟﺸﺠﺮي ﻻﺗﺨﺎذ اﻟﻘﺮار‪ ،‬اﻟﺬي ﻳﻜﻮن واﺿﺤً ﺎ‬
‫ُ‬
‫ﻓﺤﺼﻪ وﺗﻘﻴﻴﻤﻪ ﻣﻦ ﻗِ ﺒﻞ اﻟﺒﴩ‪.‬‬
‫ﻳﻤﻜﻦ‬
‫ﺛﻤﱠ ﺔ ﻧﻤﻮذج ﻣُﻬﻢ َ‬
‫آﺧﺮ ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻫﻮ ذﻟﻚ اﻟﺬي ﻳَﺴﺘﺨﺪِم ﻣﻨﺎﻫﺞ أﻛﺜﺮ‬
‫ً‬
‫ﺗﺠﺴﻴﺪﻳﺔ وأﻛﺜﺮ اﻋﺘﻤﺎدًا ﻋﲆ املﻮاﻗﻒ‪ ،‬ﻣﺮﻛ ًﺰا ﻋﲆ اﻟﺘﻔﺎﻋُ ﻞ واملﻬﺎم اﻟﺤﺮﻛﻴﺔ ً‬
‫ﺑﺪﻻ ﻣﻤﺎ ﻧُﻄﻠﻖ‬
‫ﻋﻠﻴﻪ املﻬﺎم املﻌﺮﻓﻴﺔ اﻟﻌُ ﻠﻴﺎ‪ .‬واﻟﺮوﺑﻮﺗﺎت اﻟﺘﻲ ﺻﻨﻌﻬﺎ ﺑﺎﺣﺜﻮن ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ﺗﻤﺜﻴﻼت رﻣﺰﻳﺔ وﻟﻜﻦ‬
‫ﻣﺜﻞ رودﻧﻲ ﺑﺮوﻛﺲ ﻣﻦ »إم آي ﺗﻲ« ﻻ ﺗﺤ ﱡﻞ املﺸﻜﻼت ﺑﺎﺳﺘﺨﺪام‬
‫ﻋﻦ ﻃﺮﻳﻖ اﻟﺘﻔﺎﻋُ ﻞ ﻣﻊ اﻟﺒﻴﺌﺔ ا ُملﺤﻴﻄﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ُ ،‬‬
‫ﺻﻤﱢ َﻢ اﻟﺮوﺑﻮت »ﻛﻮج« اﻟﺸﺒﻴﻪ‬
‫ﺑﺎﻟﺒﴩ‪ ،‬اﻟﺬي ﺗ ﱠﻢ ﺗﻄﻮﻳﺮه ﰲ اﻟﺘﺴﻌﻴﻨﻴﺎت ﻣﻦ اﻟﻘﺮن اﻟﻌﴩﻳﻦ‪ ،‬ﺑﺤﻴﺚ ﻳﺘﻌ ﱠﻠﻢ ﻣﻦ ﺧﻼل‬
‫اﻟﺘﻔﺎﻋﻞ ﻣﻊ اﻟﻌﺎﻟﻢ‪ ،‬ﻛﻤﺎ ﻳﻔﻌﻞ اﻷﻃﻔﺎل‪ .‬وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻳﻌﺘﻘﺪ ﺑﻌﺾ اﻷﺷﺨﺎص أن اﻟﻌﻘﻞ‬
‫ﻳﻤﻜﻦ أن ﻳﻨﺸﺄ ﻓﻘﻂ ﻣﻦ اﻟﺤﻴﺎة؛ وﺑﺎﻟﺘﺎﱄ‪ ،‬ﻹﻧﺸﺎء اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻳﺠﺐ أن ﻧُﺤﺎول‬
‫إﻧﺸﺎء ﺣﻴﺎ ٍة اﺻﻄﻨﺎﻋﻴﺔ‪ .‬وﻳﺘﺒﻊ ﺑﻌﺾ املﻬﻨﺪﺳني ﻧﻬﺠً ﺎ أﻗ ﱠﻞ ِﻣﻴﺘﺎﻓﻴﺰﻳﻘﻴﺔ وأﻛﺜﺮ ﻋﻤﻠﻴﺔ؛ إذ‬
‫ﺗﻄﺒﻴﻘﺎت ﺗﻜﻨﻮﻟﻮﺟﻴﺔ ﻋﻤﻠﻴﺔ‪ .‬وﻫﻨﺎك ً‬
‫ٍ‬
‫أﻳﻀﺎ آﻻت ﺗﻄﻮﱡرﻳﺔ‬
‫ﻳﺄﺧﺬون اﻷﺣﻴﺎء ﻧﻤﻮذﺟً ﺎ ﻟﺘﻄﻮﻳﺮ‬
‫ﻣﺰودة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺴﺘﻄﻴﻊ أن ﺗﺘﻄﻮﱠر‪ .‬وﻳﻤﻜﻦ ﻟﺒﻌﺾ اﻟﱪاﻣﺞ‪ ،‬ﺑﺎﺳﺘﺨﺪام ﻣﺎ‬
‫ﻳُﺴﻤﱠ ﻰ ﺑﺨﻮارزﻣﻴﺎت اﻟﻮراﺛﺔ‪ ،‬ﺗﻐﻴري ﻧﻔﺴﻬﺎ‪.‬‬
‫ﻫﺬا اﻟﺘﻨ ﱡﻮع ﰲ ﻣﻨﺎﻫﺞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ووﻇﺎﺋﻔﻪ ﻳﺸري إﱃ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻟﻴﻮم ﻟﻪ اﻟﻌﺪﻳﺪ ﻣﻦ املﺠﺎﻻت اﻟﻔﺮﻋﻴﺔ‪ :‬ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ ،‬ورؤﻳﺔ اﻟﻜﻤﺒﻴﻮﺗﺮ‪ ،‬وﻣﻌﺎﻟﺠﺔ اﻟﻠﻐﺔ‬
‫‪57‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﻄﺒﻴﻌﻴﺔ‪ ،‬واﻷﻧﻈﻤﺔ اﻟﺨﺒرية‪ ،‬واﻟﺤﻮﺳﺒﺔ اﻟﺘﻄﻮﱡرﻳﺔ‪ ،‬وﻫﻠ ﱠﻢ ﺟ ٍّﺮا‪ .‬وﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﻜﻮن اﻟﱰﻛﻴﺰ‬
‫ﻣﺠﺎل واﺣﺪ ﻣﻦ ﻣﺠﺎﻻت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫اﻟﻴﻮم ﻋﲆ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ ،‬وﻟﻜﻦ ﻫﺬا ﻟﻴﺲ ﺳﻮى‬
‫ٍ‬
‫ﺣﺘﻰ وإن ﻛﺎﻧﺖ ﻫﺬه املﺠﺎﻻت اﻷﺧﺮى ﻣ ً‬
‫ُﺘﺼﻠﺔ ﻏﺎﻟﺒًﺎ ﺑﺘﻌ ﱡﻠﻢ اﻵﻟﺔ‪ .‬وﻗﺪ ﺗﻢ ﺗﺤﻘﻴﻖ ﺗﻄﻮرات‬
‫ﻫﺎﺋﻠﺔ ﻣﺆﺧ ًﺮا ﰲ رؤﻳﺔ اﻟﻜﻤﺒﻴﻮﺗﺮ وﻣﻌﺎﻟﺠﺔ اﻟﻠﻐﺔ اﻟﻄﺒﻴﻌﻴﺔ وﺗﺤﻠﻴﻞ اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ ﻋﻦ‬
‫ﻃﺮﻳﻖ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻤﻜﻦ اﺳﺘﺨﺪام ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ملﻌﺎﻟﺠﺔ اﻟﻠﻐﺔ اﻟﻄﺒﻴﻌﻴﺔ‬
‫اﺳﺘﻨﺎدًا إﱃ ﺗﺤﻠﻴﻞ اﻟﻜﻼم واملﺼﺎدر املﻜﺘﻮﺑﺔ ﻣﺜﻞ اﻟﻨﺼﻮص املﻮﺟﻮدة ﻋﲆ اﻹﻧﱰﻧﺖ‪ .‬وﻗﺪ‬
‫أﺛﻤﺮ ﻫﺬا اﻟﻌﻤﻞ ﻋﻦ إﻧﺸﺎء أﺟﻬﺰة املﺤﺎدﺛﺔ اﻟﺤﺪﻳﺜﺔ‪ .‬ﻣﺜﺎل َ‬
‫آﺧﺮ ﻫﻮ اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻟﻮﺟﻮه‬
‫اﺳﺘﻨﺎدًا إﱃ رؤﻳﺔ اﻟﻜﻤﺒﻴﻮﺗﺮ واﻟﺘﻌ ﱡﻠﻢ اﻟﻌﻤﻴﻖ‪ ،‬وﻳﻤﻜﻦ اﺳﺘﺨﺪاﻣﻪ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﻣﺠﺎل‬
‫املﺮاﻗﺒﺔ‪.‬‬
‫اﻟﺘﻄﺒﻴﻘﺎت واﻟﺘﺄﺛري‬
‫ٍ‬
‫ﻣﺠﺎﻻت ﻣﺨﺘﻠﻔﺔ )ﻟﻬﺎ ﺗﻄﺒﻴﻘﺎت ﻣﺘﻨﻮﻋﺔ(‪،‬‬
‫ﻳﻤﻜﻦ ﺗﻄﺒﻴﻖ ﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ‬
‫ﺗﱰاوح ﻣﺎ ﺑني اﻟﺘﺼﻨﻴﻊ واﻟﺰراﻋﺔ واﻟﻨﻘﻞ‪ ،‬واﻟﺮﻋﺎﻳﺔ اﻟﺼﺤﻴﺔ واﻟﺘﻤﻮﻳﻞ واﻟﺘﺴﻮﻳﻖ واﻟﺠﻨﺲ‬
‫ُ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ .‬ﰲ ﻣﺠﺎل اﻟﺒﻴﻊ ﺑﺎﻟﺘﺠﺰﺋﺔ واﻟﺘﺴﻮﻳﻖ‪،‬‬
‫واﻟﱰﻓﻴﻪ واﻟﺘﻌﻠﻴﻢ ووﺳﺎﺋﻞ‬
‫ٍ‬
‫إﻋﻼﻧﺎت ﻣﺴﺘﻬﺪﻓﺔ‪ .‬أﻣﺎ ﰲ‬
‫ﺗُﺴﺘﺨﺪَم أﻧﻈﻤﺔ اﻟﺘﻮﺻﻴﺔ ﻟﻠﺘﺄﺛري ﰲ ﻗﺮارات اﻟﴩاء وﻟﺘﻘﺪﻳﻢ‬
‫َ‬
‫ُ‬
‫ﻳﺸﻐﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺮوﺑﻮﺗﺎت‪ :‬وﻫﻲ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ ،‬ﻳﻤﻜﻦ أن‬
‫ﻣﺠﺎل وﺳﺎﺋﻞ‬
‫ِ‬
‫ٌ‬
‫أﺷﺨﺎص ﺣﻘﻴﻘﻴﻮن وﻟﻜﻨﻬﺎ ﰲ اﻟﻮاﻗﻊ‬
‫ﺣﺴﺎﺑﺎت ﻣُﺴﺘﺨﺪﻣني ﺗﻈﻬﺮ ﻋﲆ أﻧﻬﺎ‬
‫ﻋﺒﺎرة ﻋﻦ‬
‫ً‬
‫ﺑﺮاﻣﺞ‪ .‬وﻳُﻤﻜﻦ ملِ ﺜﻞ ﻫﺬه اﻟﺮوﺑﻮﺗﺎت أن ﺗﻨﴩ ﻣﺤﺘﻮًى ﺳﻴﺎﺳﻴٍّﺎ أو ﺗُﺠﺮي دردﺷﺔ ﻣﻊ‬
‫ﻣُﺴﺘﺨﺪِﻣني ﻣﻦ اﻟﺒﴩ‪ .‬وﰲ ﻣﺠﺎل اﻟﺮﻋﺎﻳﺔ اﻟﺼﺤﻴﺔ‪ ،‬ﻳُﺴﺘﺨﺪَم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﺘﺤﻠﻴﻞ‬
‫ﺑﻴﺎﻧﺎت ﻣﻦ ﻣﻼﻳني املﺮﴇ‪ .‬وﻣﺎ زاﻟﺖ اﻷﻧﻈﻤﺔ اﻟﺨﺒرية ﺗُﺴﺘﺨﺪَم ً‬
‫ٍ‬
‫أﻳﻀﺎ ﰲ ﻫﺬا املﺠﺎل‪.‬‬
‫ٍ‬
‫ﻣﺠﻤﻮﻋﺎت ﺿﺨﻤﺔ ﻣﻦ اﻟﺒﻴﺎﻧﺎت‬
‫ﰲ ﻣﺠﺎل اﻟﺘﻤﻮﻳﻞ‪ ،‬ﻳُﺴﺘﺨﺪَم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﺘﺤﻠﻴﻞ‬
‫ﻟﺘﺤﻠﻴﻞ اﻟﺴﻮق وأﺗْﻤَ ﺘَ ِﺔ اﻟﺘﻌﺎﻣُﻼت املﺎﻟﻴﺔ‪ .‬وﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﺘﻢ ﺗﻀﻤني ﻧﻮع ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫ﻣﺮاﻓﻘﺎ ﻟﻺﻧﺴﺎن‪ .‬واﻟﻄﻴﺎر اﻵﱄ واﻟﺴﻴﺎرات ذاﺗﻴﺔ اﻟﻘﻴﺎدة‬
‫ﰲ اﻟﺮوﺑﻮﺗﺎت ا ُملﺼﻤﱠ ﻤﺔ ﻟﺘﻜﻮن‬
‫ﺗﺴﺘﺨﺪم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻳﻤﻜﻦ ﻷﺻﺤﺎب اﻟﻌﻤﻞ اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ملﺮاﻗﺒﺔ‬
‫ٍ‬
‫ﺷﺨﺼﻴﺎت ﻣﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫املﻮﻇﻔني‪ .‬ﻛﻤﺎ أن أﻟﻌﺎب اﻟﻔﻴﺪﻳﻮ ﺗﺤﺘﻮي ﻋﲆ‬
‫ُ‬
‫وﺗﺴﺘﻄﻴﻊ اﻵﻻت املﺰوﱠدة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺄﻟﻴﻒ املﻮﺳﻴﻘﻰ أو ﻛﺘﺎﺑﺔ ﻣﻘﺎﻻت اﻷﺧﺒﺎر‪.‬‬
‫ﻛﻤﺎ ﺗﺴﺘﻄﻴﻊ ﺗﻘﻠﻴ َﺪ أﺻﻮات اﻷﺷﺨﺎص وﺣﺘﻰ إﻧﺸﺎء ﻣﻘﺎﻃﻊ ﻓﻴﺪﻳﻮ ﻣُﺰﻳﻔﺔ ﻟﺨﻄﺎﺑﺎت‪.‬‬
‫‪58‬‬

‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬

‫ﻧﻈ ًﺮا إﱃ ﺗﻨﻮﱡع ﺗﻄﺒﻴﻘﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻣﻦ ا ُملﺮﺟﱠ ﺢ أن ﻳﻜﻮن ﻟﻪ ﺗﺄﺛري واﺳﻊ‬
‫اﻟﻨﻄﺎق‪ ،‬ﺳﻮاء اﻟﻴﻮم أو ﰲ ا ُملﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ‪ .‬ﻓﺈذا ﻓ ﱠﻜ ْﺮﻧﺎ ً‬
‫ﻣﺜﻼ ﰲ اﻟﴩﻃﺔ اﻟﺘﻨﺒﱡﺆﻳﺔ وإﻣﻜﺎﻧﻴﺔ‬
‫اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻟﻜﻼم‪ ،‬اﻟﻠﺬَﻳﻦ ﻳﺨﻠﻘﺎن إﻣﻜﺎﻧﻴﺎت ﺟﺪﻳﺪة ﻟﻸﻣﺎن واملﺮاﻗﺒﺔ‪ ،‬ووﺳﺎﺋﻞ اﻟﻨﻘﻞ ﺑني‬
‫ﻣﺪن ﺑﺄﻛﻤﻠﻬﺎ‪ ،‬واﻟﺘﺪاول‬
‫اﻷﻓﺮاد واﻟﺴﻴﺎرات ذاﺗﻴﺔ اﻟﻘﻴﺎدة اﻟﺘﻲ ﻳُﻤﻜﻦ أن ﺗُﺤﺪِث ﺗﺤﻮ ًﱡﻻ ﰲ ٍ‬
‫اﻟﺨﻮارزﻣﻲ اﻟﻌﺎﱄ اﻟﱰدﱡد اﻟﺬي ﻳُﺸ ﱢﻜﻞ ﺑﺎﻟﻔﻌﻞ اﻷﺳﻮاق املﺎﻟﻴﺔ‪ ،‬أو اﻟﺘﻄﺒﻴﻘﺎت اﻟﺘﺸﺨﻴﺼﻴﺔ‬
‫ﰲ اﻟﻘﻄﺎع اﻟﻄﺒﻲ اﻟﺘﻲ ﺗﺆﺛﺮ ﰲ اﺗﺨﺎذ اﻟﻘﺮارات اﻟﺴﻠﻴﻤﺔ‪ .‬ﻳﺠﺐ ً‬
‫أﻳﻀﺎ أﻻ ﻧﻨﴗ اﻟﻌﻠﻮم‬
‫ﻛﺄﺣﺪ املﺠﺎﻻت اﻟﺮﺋﻴﺴﻴﺔ اﻟﺘﻲ ﺗﺄﺛ ﱠ َﺮت إﱃ ﺣ ﱟﺪ ﻛﺒري ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ :‬ﻋﻦ ﻃﺮﻳﻖ ﺗﺤﻠﻴﻞ‬
‫ٍ‬
‫ﻣﺠﻤﻮﻋﺎت ﺿﺨﻤﺔ ﻣﻦ اﻟﺒﻴﺎﻧﺎت‪ ،‬ﻳﻤﻜﻦ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺴﺎﻋﺪة اﻟﻌﻠﻤﺎء ﰲ اﻛﺘﺸﺎف‬
‫ارﺗﺒﺎﻃﺎت ﻟﻢ ﻳﻜﻮﻧﻮا ﻟﻴُﺪرﻛﻮﻫﺎ ﻟﻮﻻه‪ .‬وﻫﺬا ﻳﻨﻄﺒﻖ ﻋﲆ اﻟﻌﻠﻮم اﻟﻄﺒﻴﻌﻴﺔ ﻣﺜﻞ اﻟﻔﻴﺰﻳﺎء‪ ،‬وﻟﻜﻦ‬
‫ً‬
‫أﻳﻀﺎ ﻋﲆ اﻟﻌﻠﻮم اﻻﺟﺘﻤﺎﻋﻴﺔ واﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ‪ .‬وﻣﻦ ا ُملﺆ ﱠﻛﺪ أن ﻳﺆﺛﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ‬
‫ﻣﺠﺎل اﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ اﻟﺮﻗﻤﻴﺔ اﻟﻨﺎﺷﺊ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻦ ﻃﺮﻳﻖ ﺗﻌﻠﻴﻤﻨﺎ املﺰﻳﺪ ﻋﻦ‬
‫اﻟﺒﴩ وﻋﻦ ا ُملﺠﺘﻤﻌﺎت اﻟﺒﴩﻳﺔ‪.‬‬
‫ﻳﺆﺛﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫أﻳﻀﺎ ﻋﲆ اﻟﻌﻼﻗﺎت اﻻﺟﺘﻤﺎﻋﻴﺔ‪ ،‬ﻛﻤﺎ أن ﻟﻪ ﺗﺄﺛريًا اﺟﺘﻤﺎﻋﻴٍّﺎ‬
‫واﻗﺘﺼﺎدﻳٍّﺎ وﺑﻴﺌﻴٍّﺎ أوﺳﻊ )‪ .(Jansen et al. 2018‬وﻣﻦ ا ُملﺮﺟﱠ ﺢ أن ﻳﺸﻜﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻟﺘﻔﺎﻋﻼت اﻟﺒﴩﻳﺔ وﻳﺆﺛﺮ ﻋﲆ اﻟﺨﺼﻮﺻﻴﺔ‪ .‬وﻳُﻘﺎل إﻧﻪ ﻗﺪ ﻳﺰﻳﺪ ﻣﻦ اﻟﺘﺤﻴﱡﺰ واﻟﺘﻤﻴﻴﺰ‪ .‬وﻣﻦ‬
‫ﱠ‬
‫املﺘﻮﻗﻊ أن ﻳﺆدي إﱃ ﻓﻘﺪان اﻟﻮﻇﺎﺋﻒ ورﺑﻤﺎ إﱃ إﺣﺪاث ﺗﺤﻮ ٍﱡل اﻗﺘﺼﺎدي ﻛﺎﻣﻞ‪ .‬ﻓﻤﻦ ا ُملﻤﻜﻦ‬
‫ً‬
‫ﻣﻌﺠﻼ اﻟﻈﻠﻢ‬
‫أن ﻳﺰﻳﺪ اﻟﻔﺠﻮة ﺑني اﻷﻏﻨﻴﺎء واﻟﻔﻘﺮاء وﺑني أﺻﺤﺎب اﻟﻨﻔﻮذ وا ُملﺴﺘﻀﻌَ ﻔني‪،‬‬
‫واﻟﺘﻔﺎوت اﻻﺟﺘﻤﺎﻋﻲ‪ .‬أﻣﺎ اﻟﺘﻄﺒﻴﻘﺎت اﻟﻌﺴﻜﺮﻳﺔ‪ ،‬ﻓﻘﺪ ﺗُ ﱢ‬
‫ﻐري اﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻳﺘﻢ ﺑﻬﺎ ﺗﻨﻔﻴﺬ‬
‫ﻳﺠﺐ أن‬
‫اﻟﺤﺮوب‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻨﺪ اﺳﺘﺨﺪام اﻷﺳﻠﺤﺔ اﻟﻘﺎﺗﻠﺔ ذاﺗﻴﺔ اﻟﺘﺸﻐﻴﻞ‪ .‬ﻛﺬﻟﻚ ِ‬
‫ُ‬
‫ﻧﺄﺧﺬ ﰲ اﻋﺘﺒﺎرﻧﺎ اﻟﺘﺄﺛري اﻟﺒﻴﺌﻲ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬واﻟﺬي ﻳﺸﻤﻞ زﻳﺎدة اﺳﺘﻬﻼك اﻟﻄﺎﻗﺔ‬
‫ً‬
‫ﻻﺣﻘﺎ ﺑﻌﺾ اﻵﺛﺎر اﻷﺧﻼﻗﻴﺔ واﻻﺟﺘﻤﺎﻋﻴﺔ ﺑﻤﺰﻳ ٍﺪ ﻣﻦ اﻟﺘﻔﺼﻴﻞ‪،‬‬
‫واﻟﺘﻠﻮﱡث‪ .‬وﺳﻮف أُﻧﺎﻗﺶ‬
‫ﻣﺮﻛ ًﺰا ﻋﲆ ﻣﺸﻜﻼت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻣَ ﺨﺎﻃﺮه‪ .‬وﻟﻜﻦ ﻳﻤﻜﻦ أن ﻳﻜﻮن ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫أﻳﻀﺎ آﺛﺎر إﻳﺠﺎﺑﻴﺔ؛ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻤﻜﻦ أن ﻳﺨﻠﻖ ﻣُﺠﺘﻤﻌﺎت ﺟﺪﻳﺪة ﻋﻦ ﻃﺮﻳﻖ وﺳﺎﺋﻞ‬
‫ُ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ ،‬وﻳُﻘ ﱢﻠﻞ املﻬﺎم املﺘﻜ ﱢﺮرة واﻟﺨﻄرية ﻋﻦ ﻃﺮﻳﻖ ﺗﻜﻠﻴﻒ اﻟﺮوﺑﻮﺗﺎت ﺑﻬﺎ‪،‬‬
‫وﻳ ﱢ‬
‫ُﺤﺴﻦ ﺳﻼﺳﻞ اﻹﻣﺪاد‪ ،‬وﻳُﻘ ﱢﻠﻞ اﺳﺘﻬﻼك املﻴﺎه‪ ،‬وﻫﻜﺬا‪.‬‬
‫ﻓﻴﻤﺎ ﻳﺘﻌ ﱠﻠﻖ ﺑﺎﻟﺘﺄﺛري — إﻳﺠﺎﺑﻲ أو ﺳﻠﺒﻲ — ﻳﺠﺐ أﻻ ﻧﺴﺄل ﻓﻘﻂ ﻋﻦ ﻃﺒﻴﻌﺔ اﻟﺘﺄﺛري‬
‫وﻣﺪاه؛ ﺑﻞ أن ﻧﺴﺄل ً‬
‫أﻳﻀﺎ »ﻣَ ﻦ« ﻫﻢ املﺘﺄﺛﺮون وﻛﻴﻒ ﺳﻴﺘﺄﺛﱠﺮون‪ .‬ﻗﺪ ﻳﻜﻮن اﻟﺘﺄﺛري أﻛﺜﺮ‬
‫إﻳﺠﺎﺑﻴﺔ ﺑﺎﻟﻨﺴﺒﺔ إﱃ اﻟﺒﻌﺾ ﻣﻨﻪ ﺑﺎﻟﻨﺴﺒﺔ إﱃ َ‬
‫اﻵﺧﺮﻳﻦ‪ .‬ﻓﻬﻨﺎك اﻟﻌﺪﻳﺪ ﻣﻦ اﻷﻃﺮاف ا َملﻌﻨﻴﺔ‪،‬‬
‫‪59‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺑﺪءًا ﻣﻦ اﻟﻌﻤﺎل واملﺮﴇ وا ُملﺴﺘﻬﻠﻜني‪ ،‬إﱃ اﻟﺤﻜﻮﻣﺎت وا ُملﺴﺘﺜﻤﺮﻳﻦ واﻟﴩﻛﺎت‪ ،‬وﺟﻤﻴﻌﻬﻢ‬
‫ﻗﺪ ﻳﺘﺄﺛﺮون ﺑﻄ ُﺮق ﻣﺨﺘﻠﻔﺔ‪ .‬وﺗﻨﺸﺄ ﻫﺬه اﻻﺧﺘﻼﻓﺎت ﰲ املﻜﺎﺳﺐ واﻟﺨﺴﺎﺋﺮ ﻣﻦ ﺗﺄﺛريات‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺲ ﻓﻘﻂ داﺧﻞ اﻟﺒﻠﺪان وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﺑني اﻟﺒﻠﺪان وأﺟﺰاء اﻟﻌﺎ َﻟﻢ‪ .‬ﻓﻬﻞ‬
‫ﺳﻴﻌﻮد اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻟﻨﱠﻔﻊ ﻋﲆ اﻟﺒﻠﺪان املﺘﻘﺪﱢﻣﺔ وا ُملﺘﻄﻮرة ﰲ املﻘﺎم اﻷول؟ وﻫﻞ‬
‫ﻣﻦ ا ُملﻤﻜﻦ أن ﻳﻜﻮن ﻣﻔﻴﺪًا ً‬
‫أﻳﻀﺎ ﻟﻸﺷﺨﺎص ذوي اﻟﺘﻌﻠﻴﻢ ا ُملﻨﺨﻔِ ﺾ واﻟﺪﺧﻞ ا ُملﻨﺨﻔﺾ‪،‬‬
‫ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل؟ ﻣَ ﻦ ﺳﺘﻜﻮن ﻟﺪَﻳﻪ اﻟﻘﺪرة ﻋﲆ اﻟﻮﺻﻮل إﱃ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﻳﻜﻮن ﻗﺎد ًرا ﻋﲆ‬
‫ﺟﻨْﻲ ﻓﻮاﺋﺪﻫﺎ؟ ﻣَ ﻦ ﺳﻴﺘﻤﻜﻦ ﻣﻦ ﺗﻤﻜني ﻧﻔﺴﻪ ﺑﺎﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ وﻣَ ﻦ ﺳﻴﻜﻮن‬
‫ﻣُﺴﺘﺒﻌﺪًا ﻣﻦ ﻫﺬه اﻟﻔﻮاﺋﺪ؟‬
‫ﻣَ ﻦ ﺳﺘﻜﻮن ﻟﺪَﻳﻪ اﻟﻘﺪرة ﻋﲆ اﻟﻮﺻﻮل إﱃ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﻳﻜﻮن ﻗﺎد ًرا ﻋﲆ ﺟﻨﻲ ﻓﻮاﺋﺪﻫﺎ؟ ﻣَ ﻦ ﺳﻴﺘﻤ ﱠﻜﻦ‬
‫ﻣﻦ ﺗﻤﻜني ﻧﻔﺴﻪ ﺑﺎﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ وﻣَ ﻦ ﺳﻴﻜﻮن ﻣُﺴﺘﺒﻌﺪًا ﻣﻦ ﻫﺬه اﻟﻔﻮاﺋﺪ؟‬

‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺲ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺮﻗﻤﻴﺔ اﻟﻮﺣﻴﺪة اﻟﺘﻲ ﺗُﺜري ﻣﺜﻞ ﻫﺬه اﻷﺳﺌﻠﺔ‪.‬‬
‫ﻓﻬﻨﺎك ﺗﻘﻨﻴﺎت رﻗﻤﻴﺔ أﺧﺮى ﺧﺎﺻﺔ ﺑﺎملﻌﻠﻮﻣﺎت واﻻﺗﺼﺎﻻت‪ ،‬وﻫﻲ ً‬
‫أﻳﻀﺎ ﺗﺆﺛﺮ ﺗﺄﺛريًا ﻛﺒريًا‬
‫ﻋﲆ ﺣﻴﺎﺗﻨﺎ وﻣُﺠﺘﻤﻌﺎﺗﻨﺎ‪ .‬وﻛﻤﺎ ﺳﻨﺮى‪ ،‬ﺑﻌﺾ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ ﻳُﺜريﻫﺎ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺴﺖ ﺣﻜ ًﺮا ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺣﺪَه‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻫﻨﺎك ﻣﺸﻜﻼت‬
‫ﻣﻮازﻳﺔ ﰲ ﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻷﺟﻬﺰة اﻟﺬاﺗﻴﺔ اﻟﺘﺸﻐﻴﻞ‪ .‬ﺗﺬ ﱠﻛﺮ ً‬
‫ﻣﺜﻼ اﻟﺮوﺑﻮﺗﺎت اﻟﺼﻨﺎﻋﻴﺔ اﻟﺘﻲ ﺗﻤﱠ ﺖ‬
‫ﺑﺮﻣﺠﺘﻬﺎ وﻻ ﺗُ َ‬
‫ﻌﺘﱪ ذﻛﺎءً اﺻﻄﻨﺎﻋﻴٍّﺎ‪ ،‬وﻟﻜﻨﻬﺎ ﻻ ﺗﺰال ﻟﻬﺎ ﺗﺄﺛريات اﺟﺘﻤﺎﻋﻴﺔ ﻋﻨﺪﻣﺎ ﺗﺆدي إﱃ‬
‫اﻟﺒﻄﺎﻟﺔ‪ .‬وﺑﻌﺾ ﻣﺸﻜﻼت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣُﺮﺗﺒﻄﺔ ﺑﺎﻟﺘﻘﻨﻴﺎت اﻟﺘﻲ ﻳﺘﱠ ِﺼﻞ ﺑﻬﺎ اﻟﺬﻛﺎء‬
‫ٍ‬
‫ُ‬
‫ﺑﺘﺤﺪﻳﺎت ﺟﺪﻳﺪة‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ واﻹﻧﱰﻧﺖ‪ ،‬اﻟﺘﻲ ﺗُﻮاﺟﻬﻨﺎ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻣﺜﻞ وﺳﺎﺋﻞ‬
‫ﱠ‬
‫ﻣﻨﺼﺎت‬
‫ﻋﻨﺪﻣﺎ ﻳﺘﻢ دﻣﺠُ ﻬﺎ ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻨﺪﻣﺎ ﺗﺴﺘﺨﺪم‬
‫ﻟﺘﻌﺮف املﺰﻳﺪ ﻋﻦ ﻣُﺴﺘﺨﺪﻣﻴﻬﺎ‪،‬‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ ﻣﺜﻞ »ﻓﻴﺴﺒﻮك« اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ِ‬
‫ﻓﺈن ﻫﺬا ﻳُﺜري ﻣﺨﺎوف ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻟﺨﺼﻮﺻﻴﺔ‪.‬‬
‫ﻫﺬا اﻻﺗﺼﺎل ﻣﻊ اﻟﺘﻘﻨﻴﺎت اﻷﺧﺮى ﻳﻌﻨﻲ ً‬
‫أﻳﻀﺎ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻜﻮن ﻏري‬
‫ٍ‬
‫ﻣﻠﺤﻮظ ﰲ ﻛﺜري ﻣﻦ اﻷﺣﻴﺎن‪ .‬وﻳﺮﺟﻊ ﻫﺬا ﰲ املﻘﺎم اﻷول إﱃ ﻛﻮﻧﻪ أﺻﺒﺢ ﺑﺎﻟﻔﻌﻞ ﺟﺰءًا‬
‫ﻻ ﻳﺘﺠﺰأ ﻣﻦ ﺣﻴﺎﺗﻨﺎ اﻟﻴﻮﻣﻴﺔ‪ .‬ﻓﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻛﺜريًا ﻣﺎ ﻳُﺴﺘﺨﺪَم ﰲ ﺗﻄﺒﻴﻘﺎت ﺟﺪﻳﺪة‬
‫ﻳﺠﺐ أﻻ ﻧﻨﴗ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬي ﻳﺸﻐﻞ ﺑﺎﻟﻔﻌﻞ‬
‫وﻣﺬﻫﻠﺔ ﻣﺜﻞ »أﻟﻔﺎ ﺟﻮ«‪ .‬وﻟﻜﻨﻨﺎ ِ‬
‫ُ‬
‫ﱠ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ ،‬وﻣُﺤﺮﻛﺎت اﻟﺒﺤﺚ‪ ،‬وﻏريﻫﺎ ﻣﻦ اﻟﻮﺳﺎﺋﻂ واﻟﺘﻘﻨﻴﺎت اﻟﺘﻲ‬
‫ﻣﻨﺼﺎت‬
‫‪60‬‬

‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬

‫أﺿﺤﺖ ﺟﺰءًا ﻣﻦ ﺗﺠﺮﺑﺘﻨﺎ اﻟﻴﻮﻣﻴﺔ‪ .‬إن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣ ﱢ‬
‫ُﺘﻮﻏﻞ ﰲ ﻛﻞ ﳾء‪ .‬وﻳﻤﻜﻦ‬
‫ً‬
‫ﻏﺎﻣﻀﺎ‪،‬‬
‫وأﺷﻜﺎل أُﺧﺮى ﻣﻦ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫أن ﻳﻜﻮن اﻟﻔﺎرق ﺑني اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻔﻌﲇ‬
‫ٍ‬
‫ﻣﻤﱠ ﺎ ﻳﺠﻌﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻏري ﻣﺮﺋﻲ‪ :‬إذا ﺗﻢ ﺗﻀﻤني أﻧﻈﻤﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ‬
‫ﻧﻌﺮف ﺑﺎﻟﻔﻌﻞ أﻧﻪ ﻣُﻀﻤﱠ ﻦ‪ ،‬ﻓﺈﻧﻪ ﻣﻦ اﻟﺼﻌﺐ‬
‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬ﻓﺈﻧﻨﺎ ﻋﺎد ًة ﻻ ﻧُﻼﺣﻈﻬﺎ‪ .‬وإذا ﻛﻨﺎ ِ‬
‫أن ﻧﻘﻮل ﻣﺎ إذا ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻮ اﻟﺬي ﻳُﺴﺒﱢﺐ املﺸﻜﻠﺔ أو اﻟﺘﺄﺛري‪ ،‬أو إذا ﻛﺎﻧﺖ‬
‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻷﺧﺮى ا ُملﺘﱠﺼﻠﺔ ﺑﻪ ﻫﻲ املﺴﺌﻮﻟﺔ ﻋﻦ ذﻟﻚ‪ .‬ﺑﻌﺒﺎرة أﺧﺮى‪ ،‬ﻻ ﻳﻮﺟﺪ »ذﻛﺎء‬
‫اﺻﻄﻨﺎﻋﻲ« ﰲ ﺣ ﱢﺪ ذاﺗﻪ‪ :‬ﻓﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻌﺘﻤﺪ داﺋﻤً ﺎ ﻋﲆ ﺗﻘﻨﻴﺎت أُﺧﺮى وﻳﺘﻢ ﺗﻀﻤﻴﻨﻪ‬
‫ﰲ ﻣُﻤﺎرﺳﺎت وإﺟﺮاءات ﻋﻠﻤﻴﺔ وﺗﻜﻨﻮﻟﻮﺟﻴﺔ أوﺳﻊ‪ .‬وﰲ ﺣني أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫أﻳﻀﺎ‬
‫ٍ‬
‫ﻣﺸﻜﻼت أﺧﻼﻗﻴﺔ ﺧﺎﺻﺔ ﺑﻪ‪ ،‬ﻓﺈن »أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ« ﺗﺤﺘﺎج إﱃ أن‬
‫ﻳُﺜري‬
‫ﺗﻜﻮن ﻣُﺮﺗﺒﻄﺔ ﺑﺎﻷﺧﻼﻗﻴﺎت اﻟﻌﺎﻣﺔ ﻟﻠﻤﻌﻠﻮﻣﺎت اﻟﺮﻗﻤﻴﺔ وﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻻﺗﺼﺎﻻت‪ ،‬وأﺧﻼﻗﻴﺎت‬
‫اﻟﻜﻤﺒﻴﻮﺗﺮ‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪.‬‬
‫ﻳﺠﺐ أﻻ ﻧﻨﴗ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬي ﻳﺸﻐﻞ ﺑﺎﻟﻔﻌﻞ ﱠ‬
‫ﻣﻨﺼﺎت اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ ،‬وﻣُﺤﺮﻛﺎت اﻟﺒﺤﺚ‪،‬‬
‫وﻏريﻫﺎ ﻣﻦ اﻟﻮﺳﺎﺋﻂ واﻟﺘﻘﻨﻴﺎت اﻟﺘﻲ أﺿﺤﺖ ﺟﺰءًا ﻣﻦ ﺗﺠﺮﺑﺘﻨﺎ اﻟﻴﻮﻣﻴﺔ‪ .‬إن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣ ﱢ‬
‫ُﺘﻮﻏﻞ‬
‫ﰲ ﻛ ﱢﻞ ﳾء‪.‬‬

‫ﺛﻤﱠ ﺔ ﻣﻨﻄﻖ َ‬
‫آﺧﺮ ﻳﺆﻛﺪ أﻧﻪ ﻻ ﻳﻮﺟﺪ ﳾء ﻳُﻌﺮف ﺑﺎﺳﻢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﺣ ﱢﺪ ذاﺗﻪ‪،‬‬
‫ً‬
‫وﻫﻮ أن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ً‬
‫اﺟﺘﻤﺎﻋﻴﺔ وإﻧﺴﺎﻧﻴﺔ‪ :‬ﻓﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻻ ﻳﺘﻌﻠﻖ‬
‫أﻳﻀﺎ داﺋﻤً ﺎ ﻣﺎ ﺗﻜﻮن‬
‫ﻓﻘﻂ ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﺑﻤﺎ ﻳﻔﻌﻠﻪ اﻟﺒﴩ ﺑﻬﺎ‪ ،‬وﻛﻴﻒ ﻳﺴﺘﺨﺪﻣﻮﻧﻬﺎ‪ ،‬وﻛﻴﻒ ﻳُﺪرﻛﻮﻧﻬﺎ‬
‫ٍ‬
‫ﺑﻴﺌﺎت اﺟﺘﻤﺎﻋﻴﺔ وﺗﻘﻨﻴﺔ أوﺳﻊ‪ .‬وﻫﺬا أﻣﺮ ﻣُﻬﻢ ﻟﻸﺧﻼﻗﻴﺎت‬
‫وﻳﻌﻴﺸﻮﻧﻬﺎ‪ ،‬وﻛﻴﻒ ﻳُﻀﻤﱢ ﻨﻮﻧﻬﺎ ﰲ‬
‫أﻳﻀﺎ ﺑﻘﺮارات اﻹﻧﺴﺎن — وﻳﻌﻨﻲ ً‬
‫— اﻟﺘﻲ ﺗﺘﻌﻠﻖ ً‬
‫ﻣﻨﻈﻮر ﺗﺎرﻳﺨﻲ‬
‫أﻳﻀﺎ أﻧﻪ ﻳﺠﺐ ﺗﻀﻤني‬
‫ٍ‬
‫واﺟﺘﻤﺎﻋﻲ ﺛﻘﺎﰲ‪ .‬اﻟﻀﺠﺔ اﻹﻋﻼﻣﻴﺔ املﺜﺎرة ﺣﺎﻟﻴٍّﺎ ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺴﺖ اﻟﻀﺠﱠ ﺔ‬
‫اﻷوﱃ اﻟﺘﻲ ﺗُﺜﺎر ﺣﻮل اﻟﺘﻘﻨﻴﺎت املﺘﻘﺪﻣﺔ‪ .‬ﻗﺒﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻛﺎﻧﺖ »اﻟﺮوﺑﻮﺗﺎت«‬
‫أو »اﻵﻻت« ﻫﻲ اﻟﻜﻠﻤﺎت اﻟﺮﺋﻴﺴﻴﺔ‪ .‬ﻛﻤﺎ ﺷﻬﺪت ﺗﻘﻨﻴﺎت ﻣُﺘﻘﺪﻣﺔ أﺧﺮى ﻣﺜﻞ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫اﻟﻨﻮوﻳﺔ‪ ،‬وﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﻨﺎﻧﻮ‪ ،‬واﻹﻧﱰﻧﺖ‪ ،‬واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺤﻴﻮﻳﺔ اﻟﻜﺜري ﻣﻦ اﻟﺠﺪل‪ .‬وﻣﻦ ا ُملﻔﻴﺪ‬
‫أن ﻧﻀﻊ ذﻟﻚ ﰲ اﻋﺘﺒﺎرﻧﺎ ﺧﻼل ﻣﻨﺎﻗﺸﺎﺗﻨﺎ ﺣﻮل أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؛ إذ رﺑﻤﺎ‬
‫ﻳُﻤﻜﻨﻨﺎ أن ﻧﺴﺘﻔﻴﺪ ﻣﻦ ﻫﺬه اﻟﻨﻘﺎﺷﺎت واﻟﺠﺪاﻻت‪ .‬إن اﺳﺘﺨﺪام اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﺗﻄﻮﻳﺮﻫﺎ‬
‫‪61‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻳﺤﺪث ﰲ ﺳﻴﺎق اﺟﺘﻤﺎﻋﻲ‪ .‬وﻛﻤﺎ ﻳﻌ َﻠﻢ اﻷﺷﺨﺎص ا ُملﻬﺘﻤﻮن ﺑﺘﻘﻴﻴﻢ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬ﻋﻨﺪﻣﺎ‬
‫ﺗﻜﻮن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﺟﺪﻳﺪة‪ ،‬ﻳﻤﻴﻞ اﻟﻨﺎس إﱃ أن ﻳُﺜريوا ﺣﻮﻟﻬﺎ اﻟﻜﺜري ﻣﻦ اﻟﺠﺪل‪ ،‬وﻟﻜﻦ‬
‫ﺑﻤﺠﺮد أن ﺗُﺼﺒﺢ ﺟﺰءًا ﻣﻦ اﻟﺤﻴﺎة اﻟﻴﻮﻣﻴﺔ‪ ،‬ﺗﻨﺨﻔِ ﺾ اﻟﻀﺠﺔ ا ُملﺜﺎرة ﺣﻮﻟﻬﺎ واﻟﺠﺪل ﺑﺸﺄﻧﻬﺎ‬
‫ﺑﺸﻜﻞ ﻛﺒري‪ .‬وﻣﻦ ا ُملﺮﺟﺢ أن ﻳﺤﺪث ﻫﺬا ً‬
‫أﻳﻀﺎ ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﰲ ﺣني أن ﻣﺜﻞ‬
‫ٍ‬
‫ﱡ‬
‫اﻟﺘﻮﻗﻊ ﻟﻴﺲ ﺳﺒﺒًﺎ وﺟﻴﻬً ﺎ ﻟﱰك ﻣُﻬﻤﺔ ﺗﻘﻴﻴﻢ اﻟﺠﻮاﻧﺐ اﻷﺧﻼﻗﻴﺔ واﻟﻌﻮاﻗﺐ اﻻﺟﺘﻤﺎﻋﻴﺔ‬
‫ﻫﺬا‬
‫ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻓﺈﻧﻪ ﻳُﺴﺎﻋﺪﻧﺎ ﰲ رؤﻳﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﺳﻴﺎﻗﻪ‪ ،‬وﻣﻦ ﺛ َ ﱠﻢ ﻳﺴﺎﻋﺪﻧﺎ‬
‫ﰲ ِ‬
‫ﻧﺤﻮ أﻓﻀﻞ‪.‬‬
‫ﻓﻬﻤﻪ ﻋﲆ ٍ‬

‫‪62‬‬

‫اﻟﻔﺼﻞ اﻟﺴﺎدس‬

‫ﺗﻨﺲ )ﻋﻠﻢ( اﻟﺒﻴﺎﻧﺎت‬
‫ﻻ َ‬
‫ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‬
‫ﺑﻤﺎ أن اﻟﻌﺪﻳﺪ ﻣﻦ اﻷﺳﺌﻠﺔ اﻷﺧﻼﻗﻴﺔ ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺘﻌﻠﻖ ﺑﺘﻘﻨﻴﺎت ِ‬
‫ﺗﻌﺘﻤﺪ ﻛﻠﻴٍّﺎ أو‬
‫ﺟﺰﺋﻴٍّﺎ ﻋﲆ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت ذي ﱢ‬
‫اﻟﺼﻠﺔ‪ ،‬ﻓﺈﻧﻪ ﻳﺠﺪُر ﺑﻨﺎ أن ﻧُﻠﻘﻲ اﻟﻀﻮء ﻋﲆ ﻫﺬه‬
‫اﻟﺘﻘﻨﻴﺔ واﻟﻌﻠﻢ‪.‬‬
‫ﻳُﺸري »ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ« إﱃ اﻟﱪاﻣﺞ اﻟﺘﻲ ﻳُﻤﻜﻨﻬﺎ »اﻟﺘﻌ ﱡﻠﻢ«‪ .‬واملﺼﻄﻠﺢ ﻣُﺜري ﻟﻠﺠﺪل‪ :‬ﻓﺎﻟﺒﻌﺾ‬
‫ﻳﻘﻮﻟﻮن إن ﻣﺎ ﺗﻘﻮم ﺑﻪ ﻟﻴﺲ ﺗﻌ ﱡﻠﻤً ﺎ ﺣﻘﻴﻘﻴٍّﺎ ﻷﻧﻬﺎ ﻻ ﺗﺘﻤﺘﱠﻊ ﺑﺈدراكٍ ﺣﻘﻴﻘﻲ؛ واﻟﺘﻌ ﱡﻠﻢ ﻣﻘﺼﻮر‬
‫ﻋﲆ َ‬
‫ً‬
‫ﺿﺌﻴﻼ أو ﻣُﻨﻌﺪﻣً ﺎ ﻣﻊ‬
‫اﻟﺒﴩ ﻓﺤﺴﺐ‪ .‬ﻋﲆ أي ﺣﺎل‪ ،‬ﻳﺤﻤﻞ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ اﻟﺤﺪﻳﺚ »ﺗﺸﺎﺑﻬً ﺎ‬
‫ﻣﺎ ﻗﺪ ﻳﺤﺪُث ﰲ ﻋﻘﻮل اﻟﺒﴩ« )‪ .(Boden 2016, 46‬وﻫﻮ ﻳﻌﺘﻤﺪ ﻋﲆ اﻹﺣﺼﺎءات؛ إذ إﻧﻪ‬
‫ﻋﻤﻠﻴﺔ إﺣﺼﺎﺋﻴﺔ‪ .‬وﻳُﻤﻜﻦ اﺳﺘﺨﺪاﻣﻪ ملﻬﺎ ﱠم ﻣﺘﻨﻮﻋﺔ‪ ،‬وﻟﻜﻦ املﻬﻤﺔ اﻷﺳﺎﺳﻴﺔ ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﻜﻮن‬
‫ﻫﻲ اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻷﻧﻤﺎط‪ .‬وﻳُﻤﻜﻦ ﻟﻠﺨﻮارزﻣﻴﺎت اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻷﻧﻤﺎط أو اﻟﻘﻮاﻋﺪ املﻮﺟﻮدة‬
‫ﱡ‬
‫وﺗﻮﻗﻊ اﻟﺒﻴﺎﻧﺎت ا ُملﺴﺘﻘﺒﻠﻴﺔ‪.‬‬
‫ﰲ اﻟﺒﻴﺎﻧﺎت واﺳﺘﺨﺪام ﺗﻠﻚ اﻷﻧﻤﺎط أو اﻟﻘﻮاﻋﺪ ﻟﺘﻔﺴري اﻟﺒﻴﺎﻧﺎت‬
‫ٍ‬
‫ﺗﻌﻠﻴﻤﺎت وﻗﻮاﻋﺪ ﻣﺒﺎﴍة ﻳُﻌﻄﻴﻬﺎ املﱪﻣﺞ‪.‬‬
‫ﻳﺤﺪُث ذﻟﻚ ذاﺗﻴٍّﺎ؛ ﺑﻤﻌﻨﻰ أﻧﻪ ﻳﺤﺪُث دون‬
‫ِ‬
‫ﺗﻌﺘﻤﺪ ﻋﲆ ﺧﱪاء ﺑﴩﻳني ﰲ املﺠﺎل ﻳﴩﺣﻮن اﻟﻘﻮاﻋﺪ‬
‫وﻋﲆ ﻋﻜﺲ اﻷﻧﻈﻤﺔ اﻟﺨﺒرية اﻟﺘﻲ‬
‫ﻟﻠﻤُﱪﻣِﺠني اﻟﺬﻳﻦ ﻳﺘﻮ ﱠﻟﻮن ﺑﻌﺪ ذﻟﻚ ﺑﺮﻣﺠﺔ ﻫﺬه اﻟﻘﻮاﻋﺪ‪ ،‬ﺗﺒﺤﺚ ﺧﻮارزﻣﻴﺔ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ﻋﻦ‬
‫ٍ‬
‫أﻧﻤﺎط ﻟﻢ ﻳُﺤﺪﱢدﻫﺎ املﱪﻣﺞ‪ .‬ﻛﻞ ﻣﺎ ﻋﻠﻴﻚ ﻫﻮ ﺗﺤﺪﻳﺪ اﻟﻬﺪف أو املﻬﻤﺔ ﻓﻘﻂ‪ .‬وﺳﻮف‬
‫ﻗﻮاﻋﺪ أو‬
‫ﻳﺴﺘﻄﻴﻊ اﻟﱪﻧﺎﻣﺞ أن ﻳُﻜﻴﱢﻒ ﺳﻠﻮ َﻛﻪ ﺑﻤﺎ ﻳﺘﻮاﻓﻖ ﻣﻊ ﻣُﺘﻄﻠﺒﺎت املﻬﻤﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫ﻳﻤﻜﻦ ﻟﺘﻌ ﱡﻠﻢ اﻵﻟﺔ املﺴﺎﻋﺪة ﰲ اﻟﺘﻤﻴﻴﺰ ﺑني اﻟﱪﻳﺪ اﻹﻟﻜﱰوﻧﻲ اﻟﻌﺸﻮاﺋﻲ ﻏري املﺮﻏﻮب ﻓﻴﻪ‬
‫ُﻌﺘﱪ ﻋﺸﻮاﺋﻴٍّﺎ‪ .‬ﻣﺜﺎل َ‬
‫واﻟﱪﻳﺪ ا ُملﻬﻢ ﻣﻦ ﺧﻼل ﻓﺤﺺ ﻋﺪدٍ ﻛﺒري ﻣﻦ اﻟﺮﺳﺎﺋﻞ وﺗﻌ ﱡﻠﻢ ﻣﺎ ﻳ َ‬
‫آﺧﺮ‪:‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ً‬
‫ﻣﺠﻤﻮﻋﺔ ﻣﻦ‬
‫ﻹﻧﺸﺎء ﺧﻮارزﻣﻴﺔ ﺗﺘﻌ ﱠﺮف ﻋﲆ ﺻﻮر اﻟﻘﻄﻂ‪ ،‬ﻻ ﻳُﻘﺪﱢم املﱪﻣﺠﻮن ﻟﻠﻜﻤﺒﻴﻮﺗﺮ‬
‫ﱟ‬
‫ﺧﺎص ﺑﻬﺎ‬
‫ﻧﻤﻮذج‬
‫اﻟﻘﻮاﻋﺪ ﺗُﻌ ﱠﺮف ﻓﻴﻬﺎ ﻣﺎ ﻫﻲ اﻟﻘﻄﻂ‪ ،‬وﻟﻜﻨﻬﻢ ﻳُﺘﻴﺤﻮن ﻟﻠﺨﻮارزﻣﻴﺔ إﻧﺸﺎء‬
‫ٍ‬
‫ﺤﺴﻦ اﻟﺨﻮارزﻣﻴﺔ ﻣﻦ أداﺋﻬﺎ ذاﺗﻴٍّﺎ ﻟﺘﺤﻘﻴﻖ أﻋﲆ ﱠ‬
‫ﻟﺼﻮر اﻟﻘﻄﻂ‪ .‬وﺗُ ﱢ‬
‫دﻗﺔ ﺗﻨﺒﺆ ﺑﺎﻻﺳﺘﻨﺎد إﱃ‬
‫ٍ‬
‫ﻣﺠﻤﻮﻋﺔ ﻣﻦ ﺻﻮر اﻟﻘﻄﻂ وﻏري اﻟﻘﻄﻂ‪ .‬وﺑﺎﻟﺘﺎﱄ‪ ،‬ﺗﻬﺪف إﱃ ﺗﻌ ﱡﻠﻢ ﻣﺎ ﻫﻲ ﺻﻮر اﻟﻘﻄﻂ‪.‬‬
‫ٍ‬
‫ﺑﺘﻌﻠﻴﻤﺎت أو ﻗﻮاﻋﺪ ﻣُﺤﺪدة‪.‬‬
‫وﻳُﻘﺪﱢم اﻟﺒﴩ ﺗﻘﺎرﻳﺮ‪ ،‬وﻟﻜﻨﻬﻢ ﻻ ﻳُﻐﺬﱡوﻧﻬﺎ‬
‫ٍ‬
‫ﻧﻈﺮﻳﺎت ﻟﺘﻔﺴري اﻟﺒﻴﺎﻧﺎت واﻟﺘﻨﺒﱡﺆ ﺑﻬﺎ؛ ﰲ ﺣني ﻳُﻨﺸﺊ‬
‫ﻛﺎن اﻟﻌﻠﻤﺎء ﰲ اﻟﺴﺎﺑﻖ ﻳُﻨﺸﺌﻮن‬
‫اﻟﻜﻤﺒﻴﻮﺗﺮ ﰲ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ﻧﻤﺎذج ﺧﺎﺻﺔ ﺑﻪ ﺗﺘﻨﺎﺳﺐ ﻣﻊ اﻟﺒﻴﺎﻧﺎت‪ .‬إذَن ﻓﻨﻘﻄﺔ اﻟﺒﺪاﻳﺔ ﻫﻲ‬
‫اﻟﺒﻴﺎﻧﺎت‪ ،‬وﻟﻴﺲ اﻟﻨﻈﺮﻳﺎت‪ .‬وﻣﻦ ﻫﺬا ا ُملﻨﻄﻠﻖ‪ ،‬ﻟﻢ ﺗﻌُ ﺪ اﻟﺒﻴﺎﻧﺎت »ﺳﻠﺒﻴﺔ« ﺑﻞ »ﻧﺸﻄﺔ«‪:‬‬
‫»ﻓﺎﻟﺒﻴﺎﻧﺎت ﻧﻔﺴﻬﺎ ﻫﻲ اﻟﺘﻲ ﺗُﺤﺪﱢد ﻣﺎ ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ ﺑﻌﺪ ذﻟﻚ« )‪Alpaydin 2016,‬‬
‫‪ .(11‬ﻳُﺪ ﱢرب اﻟﺒﺎﺣﺜﻮن اﻟﺨﻮارزﻣﻴﺔ ﺑﺎﺳﺘﺨﺪام ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت املﻮﺟﻮدة )ﻋﲆ ﺳﺒﻴﻞ‬
‫املﺜﺎل‪ ،‬رﺳﺎﺋﻞ اﻟﱪﻳﺪ اﻹﻟﻜﱰوﻧﻲ اﻟﻘﺪﻳﻤﺔ(‪ ،‬وﻋﻨﺪﺋ ٍﺬ ﺗﺴﺘﻄﻴﻊ اﻟﺨﻮارزﻣﻴﺔ اﻟﺘﻨﺒﱡﺆ ﺑﺎﻟﻨﺘﺎﺋﺞ ﻣﻦ‬
‫اﻟﺒﻴﺎﻧﺎت اﻟﺠﺪﻳﺪة )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﻟﱪﻳﺪ اﻹﻟﻜﱰوﻧﻲ اﻟﻮارد اﻟﺠﺪﻳﺪ( )‪ .(CDT 2018‬ﻳُﺸﺎر‬
‫ٍ‬
‫ﻛﻤﻴﺎت ﻛﺒرية ﻣﻦ املﻌﻠﻮﻣﺎت )اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ( ﺑﺎﺳﻢ‬
‫أﺣﻴﺎﻧًﺎ إﱃ اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻷﻧﻤﺎط ﰲ‬
‫»اﻟﺘﻨﻘﻴﺐ ﻋﻦ اﻟﺒﻴﺎﻧﺎت«‪ ،‬ﺗﺸﺒﻴﻬً ﺎ ﻟﻪ ﺑﺎﺳﺘﺨﺮاج املﻌﺎدن َ‬
‫اﻟﻘﻴﱢﻤﺔ ﻣﻦ اﻷرض‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻓﺈن‬
‫ٍ‬
‫أﻧﻤﺎط ﻣﻦ اﻟﺒﻴﺎﻧﺎت‪ ،‬وﺗﺤﻠﻴﻞ اﻟﺒﻴﺎﻧﺎت‪ ،‬وﻟﻴﺲ‬
‫املﺼﻄﻠﺢ ﻣُﻀ ﱢﻠﻞ ﻷن اﻟﻬﺪف ﻫﻮ اﺳﺘﺨﺮاج‬
‫اﺳﺘﺨﺮاج اﻟﺒﻴﺎﻧﺎت ﻧﻔﺴﻬﺎ‪.‬‬
‫ﻣﺘﻐري ﻣ ﱠ‬
‫ﱢ‬
‫ُﻌني‬
‫ﻳﻤﻜﻦ أن ﻳﻜﻮن ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ »ﻣُﻮﺟﱠ ﻬً ﺎ«‪ ،‬ﻣﻤﺎ ﻳَﻌﻨﻲ أن اﻟﺨﻮارزﻣﻴﺔ ﺗﺮ ﱢﻛﺰ ﻋﲆ‬
‫ﻳُﻌ َﺮف ﺑﺎﺳﻢ ﻫﺪف اﻟﺘﻨﺒﺆ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إذا ﻛﺎن اﻟﻬﺪف ﻫﻮ ﺗﻘﺴﻴﻢ اﻷﺷﺨﺎص إﱃ ﻓﺌﺘَني‬
‫)ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺧﻄﻮرة أﻣﻨﻴﺔ ﻋﺎﻟﻴﺔ أو ﻣﻨﺨﻔﻀﺔ(‪ ،‬ﻓﺈن ا ُملﺘﻐريات اﻟﺘﻲ ﺗﺘﻨﺒﺄ ﺑﻬﺎﺗَني‬
‫اﻟﻔﺌﺘَني ﻣﻌﺮوﻓﺔ ﺑﺎﻟﻔﻌﻞ‪ ،‬وﺑﺎﻟﺘﺎﱄ ﺗﺘﻌ ﱠﻠﻢ اﻟﺨﻮارزﻣﻴﺔ اﻟﺘﻨﺒﱡﺆ ﺑﺎﻻﻧﺘﻤﺎء إﱃ إﺣﺪى اﻟﻔﺌﺘَني‬
‫)اﻟﺨﻄﻮرة اﻷﻣﻨﻴﺔ اﻟﻌﺎﻟﻴﺔ أو اﻟﺨﻄﻮرة اﻷﻣﻨﻴﺔ املﻨﺨﻔﻀﺔ(‪ .‬ﻳُﺪ ﱢرب املﱪﻣﺞ اﻟﻨﻈﺎم ﻋﻦ ﻃﺮﻳﻖ‬
‫ﺗﻮﻓري أﻣﺜﻠﺔ وﻏريﻫﺎ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺻﻮر ﻟﻸﺷﺨﺎص اﻟﺬﻳﻦ ﻳُﺸ ﱢﻜﻠﻮن ﺧﻄﻮرة أﻣﻨﻴﺔ‬
‫ﻋﺎﻟﻴﺔ وأﻣﺜﻠﺔ ﻟﻸﺷﺨﺎص اﻟﺬﻳﻦ ﻻ ﻳُﺸﻜﻠﻮن ﺧﻄﻮرة أﻣﻨﻴﺔ‪ .‬ﻳﻜﻮن اﻟﻬﺪف أن ﻳﺘﻌ ﱠﻠﻢ اﻟﻨﻈﺎم‬
‫اﻟﺘﻨﺒﱡﺆ ﺑﻤَ ﻦ ﻳﻨﺘﻤﻲ إﱃ ﻛﻞ ﻓﺌﺔ‪ ،‬أي ﻣَ ﻦ ﻳُﺸﻜﻞ ﺧﻄﻮر ًة أﻣﻨﻴﺔ ﻋﺎﻟﻴﺔ وﻣَ ﻦ ﻻ ﻳﺸﻜﻞ ﺑﻨﺎءً ﻋﲆ‬
‫اﻟﺒﻴﺎﻧﺎت اﻟﺠﺪﻳﺪة‪ .‬إذا أ ُ ِ‬
‫ﻋﻄﻲ اﻟﻨﻈﺎم ﻣﺎ ﻳﻜﻔﻲ ﻣﻦ اﻷﻣﺜﻠﺔ‪ ،‬ﻓﺈﻧﻪ ﺳﻴﻜﻮن ﻗﺎد ًرا ﻋﲆ اﻟﺘﻌﻤﻴﻢ‬
‫ﻟﺮاﻛﺐ ﻳﻤ ﱡﺮ‬
‫ﻣﻦ ﻫﺬه اﻷﻣﺜﻠﺔ وﻣﻌﺮﻓﺔ ﻛﻴﻔﻴﺔ ﺗﺼﻨﻴﻒ اﻟﺒﻴﺎﻧﺎت اﻟﺠﺪﻳﺪة‪ ،‬ﻣﺜﻞ ﺻﻮر ٍة ﺟﺪﻳﺪة‬
‫ٍ‬
‫ﻋَ ْﱪ أﻣﻦ املﻄﺎر‪ .‬أﻣﺎ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ »ﻏري ا ُملﻮﺟﱠ ﻪ« ﻓﻴﻌﻨﻲ ﻋﺪم ﺗﻘﺪﻳﻢ ﻫﺬا اﻟﻨﻮع ﻣﻦ اﻟﺘﺪرﻳﺐ‪،‬‬
‫ٍ‬
‫ﱠ‬
‫ﺧﺎﺻﺔ ﺑﻬﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫ﻓﺌﺎت‬
‫وأن اﻟﻔﺌﺎت ﻏري ﻣﻌﺮوﻓﺔ‪ :‬وﻣﻦ ﺛَﻢ ﺗُﻨﺸﺊ اﻟﺨﻮارزﻣﻴﺎت‬
‫‪64‬‬

‫ﻻ َ‬
‫ﺗﻨﺲ )ﻋﻠﻢ( اﻟﺒﻴﺎﻧﺎت‬

‫ً‬
‫ً‬
‫ٍ‬
‫ﺧﺎﺻﺔ ﺑﻪ اﺳﺘﻨﺎدًا إﱃ ا ُملﺘﻐريات اﻟﺘﻲ ﻳُﺤﺪدﻫﺎ؛ ﻻ‬
‫أﻣﻨﻴﺔ‬
‫ﻓﺌﺎت‬
‫ﻳُﻨﺸﺊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫أﻧﻤﺎط ﻟﻢ ﻳُﺤﺪﱢدﻫﺎ ﺧﱪاء‬
‫اﻟﺘﻲ ﻳُﻘﺪﻣﻬﺎ إﻟﻴﻪ املﱪﻣﺞ‪ .‬ورﺑﻤﺎ ﻳﻌﺜﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ‬
‫املﺠﺎل )ﰲ ﻫﺬا اﻟﺴﻴﺎق‪ :‬اﻟﺨﱪاء اﻷﻣﻨﻴﻮن(‪ .‬وﻳﻤﻜﻦ أن ﺗﺒﺪو اﻟﻔﺌﺎت اﻟﺘﻲ أﻧﺸﺄﻫﺎ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ ﻣﻨﻈﻮر اﻟﺒﴩ ﻋﺸﻮاﺋﻴﺔ ﻟﻠﻐﺎﻳﺔ‪ .‬ورﺑﻤﺎ ﻻ ﻳﻜﻮن ﻟﻬﺎ ﻣﻌﻨﻰ‪ .‬وﻟﻜﻨﻬﺎ ﻣﻮﺟﻮدة‬
‫ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻹﺣﺼﺎﺋﻴﺔ‪ .‬وﰲ ﺑﻌﺾ اﻷﺣﻴﺎن ﻳﻜﻮن ﻟﻬﺎ ﻣﻌﻨﻰ‪ ،‬وﰲ ﻫﺬه اﻟﺤﺎﻟﺔ ﻳﻤﻜﻦ ﻟﻬﺬه‬
‫ً‬
‫ﻣﻌﺮﻓﺔ ﺟﺪﻳﺪة ﺣﻮل اﻟﻔﺌﺎت ﰲ اﻟﻌﺎﻟﻢ اﻟﻮاﻗﻌﻲ‪ .‬أﻣﺎ اﻟﺘﻌ ﱡﻠﻢ »ا ُملﻌ ﱠﺰز«‪،‬‬
‫اﻟﻄﺮﻳﻘﺔ أن ﺗُﻌﻄﻴﻨﺎ‬
‫ﻓﺈﻧﻪ ﻳﺘﻄﻠﺐ ﺗﻘﻴﻴﻤً ﺎ ﻟﻠﻤُﺨﺮﺟﺎت إن ﻛﺎﻧﺖ ﺟﻴﺪة أم ﺳﻴﺌﺔ‪ .‬وﻫﺬا ﻳُﺸﺒﻪ ﻓﻜﺮة اﻟﺜﻮاب واﻟﻌﻘﺎب‪.‬‬
‫ُﺨﱪ أيﱡ اﻹﺟﺮاءات ﻳﺠﺐ أن ﻳ َ‬
‫ﻓﺎﻟﱪﻧﺎﻣﺞ ﻻ ﻳ َ‬
‫ُﺘﺨﺬ‪ ،‬وﻟﻜﻨﻪ »ﻳﺘﻌﻠﻢ« ﻣﻦ ﺧﻼل ﻋﻤﻠﻴﺔ ﺗﻜﺮارﻳﺔ‬
‫أي اﻹﺟﺮاءات اﻟﺘﻲ ﺗﺆدي إﱃ اﻟﺜﻮاب‪ .‬ﻓﻔﻲ املﺜﺎل اﻷﻣﻨﻲ اﻟﺴﺎﺑﻖ‪ ،‬ﻳﺘﻠﻘﻰ اﻟﻨﻈﺎم ﺗﻘﺮﻳ ًﺮا )أو‬
‫ﺑﻌﻤﻞ ﺟﻴﺪ ﻋﻨﺪﻣﺎ ﻳﺠﺮي‬
‫ﺑﻴﺎﻧﺎت( ﻣﻦ اﻟﺨﱪاء اﻷﻣ ِﻨﻴﱢني ﺑﺤﻴﺚ »ﻳﻌﺮف« ﻣﺎ إذا ﻛﺎن ﻗﺪ ﻗﺎم‬
‫ٍ‬
‫ﺗﻨﺒ ًﺆا ﻣﻌﻴﻨًﺎ‪ .‬ﻓﺈذا ﻟﻢ ﻳُﺴﺒﺐ اﻟﺸﺨﺺ اﻟﺬي ﺗﻨﺒﺄ اﻟﻨﻈﺎم ﺑﺄﻧﻪ ذو ﺧﻄﻮرة أﻣﻨﻴﺔ ﻣﻨﺨﻔﻀﺔ‬
‫ٍ‬
‫ﻣﺸﻜﻼت أﻣﻨﻴﺔ‪ ،‬ﻓﺈن اﻟﻨﻈﺎم ﻳﺘﻠﻘﻰ ﺗﻘﺮﻳ ًﺮا ﺑﺄن ﻣﺨﺮﺟﺎﺗﻪ ﻛﺎﻧﺖ ﺟﻴﺪة وﻣﻦ ﺛَﻢ »ﻳﺘﻌﻠﻢ«‬
‫أيﱠ‬
‫ً‬
‫ً‬
‫دﻗﻴﻘﺎ ﺑﻨﺴﺒﺔ ‪ ١٠٠‬ﰲ‬
‫ﻧﺴﺒﺔ ﻣﻦ اﻟﺨﻄﺄ‪ :‬ﻓﺎﻟﻨﻈﺎم ﻟﻴﺲ‬
‫ﻣﻨﻪ‪ .‬ﻳﺠﺐ ﻣﻼﺣﻈﺔ أن ﻫﻨﺎك داﺋﻤً ﺎ‬
‫املﺎﺋﺔ‪ .‬ﻳﺠﺐ ً‬
‫أﻳﻀﺎ ﻣﻼﺣﻈﺔ أن ا ُملﺼﻄ َﻠﺤَ ني اﻟﻔﻨﱢﻴﱠني »ﻣﻮﺟﱠ ﻪ« و»ﻏري ﻣﻮﺟﱠ ﻪ« ﻻ ﻋﻼﻗﺔ ﻟﻬﻤﺎ‬
‫ﱡ‬
‫اﻟﺘﺪﺧﻞ اﻟﺒﴩي ﰲ اﺳﺘﺨﺪام اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ :‬ﻓﻔﻲ ﺣني أن اﻟﺨﻮارزﻣﻴﺔ ﺗﺘﻤﺘﱠﻊ ﺑﺒﻌﺾ‬
‫ﺑﻤﺪى‬
‫ﱠ‬
‫ﺑﻄﺮق ﻣﺨﺘﻠﻔﺔ‪.‬‬
‫ﻳﺘﺪﺧﻠﻮن‬
‫اﻻﺳﺘﻘﻼﻟﻴﺔ‪ ،‬ﻓﺈن اﻟﺒﴩ ﰲ ﺟﻤﻴﻊ أﻧﻮاع ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‬
‫ٍ‬
‫ﻫﺬا ﺻﺤﻴﺢ ً‬
‫ﱡ‬
‫ﻳﺨﺺ اﻟﺒﻴﺎﻧﺎت ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ‬
‫أﻳﻀﺎ ﻓﻴﻤﺎ‬
‫ﻣﺎ ﻳُﺴﻤﱠ ﻰ ﺑ »اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ«‪ .‬اﻛﺘﺴﺐ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ اﻟﻘﺎﺋﻢ ﻋﲆ اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ اﻟﻜﺜري‬
‫ﻣﻦ اﻻﻫﺘﻤﺎم ﺑﺴﺒﺐ ﺗﻮﻓﺮ ﻛﻤﻴﺎت ﻛﺒرية ﻣﻦ اﻟﺒﻴﺎﻧﺎت وزﻳﺎدة ﻗﺪرة اﻟﻜﻤﺒﻴﻮﺗﺮ )اﻷرﺧﺺ(‪.‬‬
‫ﻳﺘﺤﺪﱠث ﺑﻌﺾ اﻟﺒﺎﺣﺜني ﻋﻦ »زﻟﺰال اﻟﺒﻴﺎﻧﺎت« )‪ .(Alpaydin 2016, x‬ﻧﺤﻦ ﺟﻤﻴﻌً ﺎ ﻧُﻨﺘِﺞ‬
‫ﺑﻴﺎﻧﺎت ﻣﻦ ﺧﻼل أﻧﺸﻄﺘﻨﺎ اﻟﺮﻗﻤﻴﺔ‪ ،‬ﻣﺜﻠﻤﺎ ﻳﺤﺪُث ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﻋﻨﺪﻣﺎ ﻧﺴﺘﺨﺪِم وﺳﺎﺋﻞ‬
‫ٍ‬
‫ﻣﻨﺘﺠﺎت ﻋﱪ اﻹﻧﱰﻧﺖ‪ .‬ﻫﺬه اﻟﺒﻴﺎﻧﺎت ﻣﻬﻤﺔ ﺑﺎﻟﻨﺴﺒﺔ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ أو ﻋﻨﺪﻣﺎ ﻧﺸﱰي‬
‫ً‬
‫وأﻳﻀﺎ ﺑﺎﻟﻨﺴﺒﺔ إﱃ اﻟﺤﻜﻮﻣﺎت واﻟﻌﻠﻤﺎء‪ .‬ﻟﻘﺪ ﺻﺎر ﺟﻤﻊ اﻟﺒﻴﺎﻧﺎت‬
‫إﱃ اﻟﺠﻬﺎت اﻟﺘﺠﺎرﻳﺔ‬
‫وﺗﺨﺰﻳﻨﻬﺎ وﻣﻌﺎﻟﺠﺘﻬﺎ أﺳﻬﻞ ﺑﻜﺜري ﻋﲆ املﺆﺳﺴﺎت )‪.(Kelleher and Tierney 2018‬‬
‫وﻟﻴﺲ ذﻟﻚ ﺑﺴﺒﺐ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ﻓﻘﻂ‪ :‬ﻓﺎﻟﺒﻴﺌﺔ اﻟﺮﻗﻤﻴﺔ اﻷوﺳﻊ وﺗﻘﻨﻴﺎت اﻟﻮﺳﺎﺋﻂ اﻟﺮﻗﻤﻴﺔ‬
‫ُ‬
‫اﻟﺘﻮاﺻﻞ‬
‫اﻷﺧﺮى ﺗﻠﻌﺐ دو ًرا ﻣُﻬﻤٍّ ﺎ ﰲ ﻫﺬا اﻟﺼﺪد‪ .‬إذ ﺗﻴﴪ اﻟﺘﻄﺒﻴﻘﺎت ﻋﱪ اﻹﻧﱰﻧﺖ ووﺳﺎﺋﻞ‬
‫اﻻﺟﺘﻤﺎﻋﻲ ﺟﻤﻊ اﻟﺒﻴﺎﻧﺎت ﻣﻦ اﻷﻓﺮاد‪ .‬ﻛﻤﺎ أن ﺗﺨﺰﻳﻦ اﻟﺒﻴﺎﻧﺎت أﺻﺒﺢ أﻗ ﱠﻞ ﺗﻜﻠﻔﺔ‪ ،‬وأﺻﺒﺤﺖ‬
‫‪65‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ٍ‬
‫ﺑﺸﻜﻞ‬
‫إﻣﻜﺎﻧﻴﺎت أﻛﱪ‪ .‬ﻛﻞ ﻫﺬا ﻛﺎن ﻣُﻬﻤٍّ ﺎ ﻟﺘﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫أﺟﻬﺰة اﻟﻜﻤﺒﻴﻮﺗﺮ ذات‬
‫ٍ‬
‫ﻋﺎم‪ ،‬وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت ﺑﺸﻜﻞ ﺧﺎص‪.‬‬
‫ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت‬
‫ﻧﺴﺘﻨﺘِﺞ ﻣﻤﺎ ﺳﺒﻖ أن ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ﻳﺮﺗﺒﻂ ﺑ »ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت«‪ .‬إذ ﻳﻬﺪف ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت إﱃ‬
‫ٍ‬
‫أﻧﻤﺎط ﻣﻔﻴﺪة وذات ﻣﻌﻨًﻰ ﻣﻦ ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت‪ ،‬وﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ ﻫﺬه‬
‫اﺳﺘﺨﺮاج‬
‫املﺠﻤﻮﻋﺎت ﻛﺒرية ﺟﺪٍّا‪ .‬ﻳﺴﺘﻄﻴﻊ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ﺗﺤﻠﻴﻞ ﻫﺬه املﺠﻤﻮﻋﺎت اﻟﻜﺒرية ﻣﻦ اﻟﺒﻴﺎﻧﺎت آﻟﻴٍّﺎ‪.‬‬
‫ِ‬
‫وﻳﻌﺘﻤﺪ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت ﻋﲆ اﻹﺣﺼﺎءات‪ ،‬أو ﻋﲆ اﻻﻧﺘﻘﺎل ﻣﻦ املﻼﺣﻈﺎت اﻟﻔﺮدﻳﺔ‬
‫ٍ‬
‫ٍ‬
‫ارﺗﺒﺎﻃﺎت ﰲ اﻟﺒﻴﺎﻧﺎت ﻣﻦ ﺧﻼل‬
‫ﺗﻮﺻﻴﻔﺎت ﻋﺎﻣﺔ‪ .‬ﻓﻌﻠﻤﺎء اﻹﺣﺼﺎء ﻳﻬﺘﻤﱡ ﻮن ﺑﺎﻟﻌﺜﻮر ﻋﲆ‬
‫إﱃ‬
‫اﻟﺘﺤﻠﻴﻞ اﻹﺣﺼﺎﺋﻲ‪ .‬وﺗﺒﺤﺚ ﻋﻤﻠﻴﺎت إﻧﺸﺎء اﻟﻨﻤﺎذج اﻹﺣﺼﺎﺋﻴﺔ ﻋﻦ اﻟﻌﻼﻗﺎت اﻟﺮﻳﺎﺿﻴﺔ ﺑني‬
‫املﺪﺧﻼت واملﺨﺮﺟﺎت‪ .‬وﻫﺬا ﻫﻮ ﻣﺎ ﺗﺴﺎﻋﺪ ﻓﻴﻪ ﺧﻮارزﻣﻴﺎت ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪.‬‬
‫ﻧﺤﻦ ﺟﻤﻴﻌً ﺎ ﻧُﻨﺘﺞ ﺑﻴﺎﻧﺎت ﻣﻦ ﺧﻼل أﻧﺸﻄﺘﻨﺎ اﻟﺮﻗﻤﻴﺔ‪ ،‬ﻛﻤﺎ ﻳﺤﺪث ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﻋﻨﺪﻣﺎ ﻧﺴﺘﺨﺪِم‬
‫ُ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ أو ﻋﻨﺪﻣﺎ ﻧﺸﱰي ﻣُﻨﺘﺠﺎت ﻋﱪ اﻹﻧﱰﻧﺖ‪.‬‬
‫وﺳﺎﺋﻞ‬

‫وﻟﻜﻦ ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت ﻳﻨﻄﻮي ﻋﲆ أﻛﺜﺮ ﻣﻦ ﻣﺠﺮد ﺗﺤﻠﻴﻞ اﻟﺒﻴﺎﻧﺎت ﺑﻮاﺳﻄﺔ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ .‬إذ‬
‫ﻳﺠﺐ ﺟﻤﻊ اﻟﺒﻴﺎﻧﺎت وإﻋﺪادﻫﺎ ﻗﺒﻞ ﺗﺤﻠﻴﻠﻬﺎ‪ ،‬وﺑﻌﺪ ذﻟﻚ ﻳﺠﺐ ﺗﻔﺴري ﻧﺘﺎﺋﺞ اﻟﺘﺤﻠﻴﻞ‪ .‬وﻳﻨﻄﻮي‬
‫ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت ﻋﲆ ﺗﺤﺪﱢﻳﺎت ﻣﺜﻞ ﻛﻴﻔﻴﺔ اﻟﺤﺼﻮل ﻋﲆ اﻟﺒﻴﺎﻧﺎت وﺗﻨﻘﻴﺘﻬﺎ )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫ﻣﻦ وﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ واﻟﻮﻳﺐ(‪ ،‬وﻛﻴﻔﻴﺔ اﻟﻮﺻﻮل إﱃ ﻛﻤﻴ ٍﺔ ﻛﺎﻓﻴﺔ ﻣﻦ اﻟﺒﻴﺎﻧﺎت‪،‬‬
‫وﻛﻴﻔﻴﺔ ﺟﻤﻊ ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت ﻣﻌً ﺎ‪ ،‬وﻛﻴﻔﻴﺔ إﻋﺎدة ﻫﻴﻜﻠﺔ ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت‪ ،‬وﻛﻴﻔﻴﺔ‬
‫اﺧﺘﻴﺎر ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت ذات اﻟﺼﻠﺔ‪ ،‬وأي ﻧﻮع ﻣﻦ اﻟﺒﻴﺎﻧﺎت ﻳﺘﻢ اﺳﺘﺨﺪاﻣﻪ‪ .‬ﻟﺬﻟﻚ ﻻ ﻳﺰال‬
‫َ‬
‫اﻟﺒﴩ ﻳﻠﻌﺒﻮن دو ًرا ﻣﻬﻤٍّ ﺎ ﰲ ﺟﻤﻴﻊ املﺮاﺣﻞ وﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﺠﻤﻴﻊ ﻫﺬه اﻟﺠﻮاﻧﺐ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ‬
‫ﺻﻴﺎﻏﺔ املﺸﻜﻠﺔ‪ ،‬واﻟﺤﺼﻮل ﻋﲆ اﻟﺒﻴﺎﻧﺎت‪ ،‬وإﻋﺪاد اﻟﺒﻴﺎﻧﺎت )ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﺗﺘﺪ ﱠرب‬
‫ﻋﻠﻴﻬﺎ اﻟﺨﻮارزﻣﻴﺔ وﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﺳﺘُﻄﺒﻖ ﻋﻠﻴﻬﺎ(‪ ،‬وإﻧﺸﺎء ﺧﻮارزﻣﻴﺔ اﻟﺘﻌ ﱡﻠﻢ أو‬
‫اﺧﺘﻴﺎرﻫﺎ‪ ،‬وﺗﻔﺴري اﻟﻨﺘﺎﺋﺞ‪ ،‬واﺗﺨﺎذ ﻗﺮار ﺣﻮل اﻹﺟﺮاء اﻟﺬي ﻳﺠﺐ اﺗﺨﺎذه )‪Kelleher and‬‬

‫‪.(Tierney 2018‬‬
‫‪66‬‬

‫ﻻ َ‬
‫ﺗﻨﺲ )ﻋﻠﻢ( اﻟﺒﻴﺎﻧﺎت‬

‫ﺗﻈﻬﺮ اﻟﺘﺤﺪﱢﻳﺎت اﻟﻌﻠﻤﻴﺔ ﰲ ﻛﻞ ﻣﺮﺣﻠﺔ ﻣﻦ ﻫﺬه اﻟﻌﻤﻠﻴﺔ‪ ،‬وﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن اﻟﱪاﻣﺞ‬
‫ﻗﺪ ﺗﻜﻮن ﺳﻬﻠﺔ اﻻﺳﺘﺨﺪام‪ ،‬ﻓﺈن ﻣﻮاﺟﻬﺔ ﻫﺬه اﻟﺘﺤﺪﻳﺎت ﺗﺘﻄ ﱠﻠﺐ وﺟﻮد املﻌﺮﻓﺔ اﻟﺒﴩﻳﺔ‬
‫ﺘﺨﺼﺼﺔ‪ .‬وﻋﺎد ًة ﻣﺎ ﻳﻜﻮن اﻟﺘﻌﺎون ﺑني اﻟﺒﴩ أﻣ ًﺮا ﴐورﻳٍّﺎ ً‬
‫اﻟﺨﺒرية ا ُمل ﱢ‬
‫أﻳﻀﺎ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ‬
‫املﺜﺎل‪ ،‬ﺑني ﻋﻠﻤﺎء اﻟﺒﻴﺎﻧﺎت واملﻬﻨﺪﺳني‪ .‬وﻣﻦ اﻟﻮارد ﺣﺪوث أﺧﻄﺎء ﻃﻮال اﻟﻮﻗﺖ‪ ،‬ﻟﺬا ﻓﺈن‬
‫اﻻﺧﺘﻴﺎر اﻟﺒﴩي واملﻌﺮﻓﺔ اﻟﺒﴩﻳﺔ واﻟﺘﻔﺴري اﻟﺒﴩي أﻣﺮ ﺣﺎﺳﻢ اﻷﻫﻤﻴﺔ‪ .‬ﻓﺎﻟﺒﴩ ﻣﻬﻤﱡ ﻮن ﰲ‬
‫ﻧﺤﻮ ﻣﻌﻘﻮل وﺗﻮﺟﻴﻪ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻧﺤﻮ اﻟﺒﺤﺚ ﻋﻦ ﻋﻮاﻣﻞ‬
‫ﻫﺬا اﻟﺴﻴﺎق ﻟﺘﻔﺴري اﻷﻣﻮر ﻋﲆ ٍ‬
‫وﻋﻼﻗﺎت ﻣﺨﺘﻠﻔﺔ‪ .‬واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻣﻦ وﺟﻬﺔ ﻧﻈﺮ ﺑﻮدن )‪ ،(٢٠١٦‬ﻳﻔﺘﻘﺮ إﱃ ﻓﻬﻤﻨﺎ‬
‫ﻟﻠﺼﻼت واﻟﻌﻼﻗﺎت‪ .‬وﻳﻤﻜﻨﻨﺎ أن ﻧُﻀﻴﻒ أﻧﻪ ﻳﻔﺘﻘﺮ ً‬
‫ﱢ‬
‫أﻳﻀﺎ إﱃ اﻟﻔﻬﻢ واﻟﺘﺠﺮﺑﺔ واﻟﺤﺴﺎﺳﻴﺔ‬
‫واﻟﺤﻜﻤﺔ‪ .‬وﻫﺬه ﺣﺠﺔ ﺟﻴﺪة ﺗﺪﻋﻢ ﻧﻈﺮﻳٍّﺎ وﻣﺒﺪﺋﻴٍّﺎ ﴐورة ﻣﺸﺎرﻛﺘﻨﺎ ﻧﺤﻦ اﻟﺒﴩ ﰲ‬
‫اﻷﻣﺮ‪ .‬وﻟﻜﻦ ﺛﻤﺔ ﺣﺠﺔ ﻋﻤﻠﻴﺔ ً‬
‫أﻳﻀﺎ ﺗﺪﻋﻢ ﻋﺪم ﺧﺮوج اﻟﺒﴩ ﻣﻦ املﺸﻬﺪ؛ وﻫﻲ أن اﻟﺒﴩ‬
‫ﻳﺸﺎرﻛﻮن ﺑﺎﻟﻔﻌﻞ ﻋﻤﻠﻴٍّﺎ ﰲ اﻷﻣﺮ‪ .‬ﻓﺪون املﱪﻣﺠني وﻋﻠﻤﺎء اﻟﺒﻴﺎﻧﺎت‪ ،‬ﻟﻦ ﺗﺴﺘﻄﻴﻊ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫اﻟﻘﻴﺎم ﺑﻮﻇﻴﻔﺘﻬﺎ ﺑﺒﺴﺎﻃﺔ‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻛﺜريًا ﻣﺎ ﻳﺘﻢ دﻣﺞ اﻟﺨﱪة اﻟﺒﴩﻳﺔ ﻣﻊ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻨﺪﻣﺎ ﻳﺴﺘﺨﺪم اﻟﻄﺒﻴﺐ اﺳﱰاﺗﻴﺠﻴﺔ ﻋﻼج ﴎﻃﺎن ﻳﻮﴆ ﺑﻬﺎ‬
‫اﻟﺬﻛﺎءُ اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻟﻜﻨﻪ ﰲ اﻟﻮﻗﺖ ﻧﻔﺴﻪ ﻳﻌﺘﻤﺪ ﻋﲆ ﺗﺠﺎرﺑﻪ وﺣﺪﺳﻪ ﻛﺨﺒري‪ .‬ﻓﺈذا أﻟﻐﻲ‬
‫اﻟﺘﺪﺧﻞ اﻟﺒﴩي‪ ،‬ﻳﻤﻜﻦ أن ﺗﺴﻮء اﻷﻣﻮر أو ﺗﻔﻘﺪ ﻣﻌﻨﺎﻫﺎ أو ﺑﺒﺴﺎﻃﺔ ﺗُﺼﺒﺢ ﻏري ﻣﻨﻄﻘﻴﺔ‪.‬‬
‫وﻟﻨﴬب ً‬
‫ﻣﺜﻼ ﺑﺎملﺸﻜﻠﺔ املﻌﺮوﻓﺔ اﻟﺘﺎﻟﻴﺔ ﻣﻦ اﻹﺣﺼﺎء‪ ،‬واﻟﺘﻲ ﺗﺆﺛﺮ ﺑﺪَورﻫﺎ ﻋﲆ‬
‫ٍ‬
‫ﻋﻼﻗﺎت ﺳﺒﺒﻴﺔ‪ .‬ﻳُﻘﺪم ﺗﺎﻳﻠﺮ ﻓﻴﺠني ﰲ‬
‫اﺳﺘﺨﺪام ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ :‬اﻻرﺗﺒﺎﻃﺎت ﻻ ﺗﻌﻨﻲ ﺑﺎﻟﴬورة‬
‫ﻛﺘﺎﺑﻪ »اﻻرﺗﺒﺎﻃﺎت اﻟﺰاﺋﻔﺔ« )‪ (٢٠١٥‬ﺑﻌﺾ اﻷﻣﺜﻠﺔ اﻟﺠﻴﺪة ﻋﲆ ذﻟﻚ‪ .‬ﰲ اﻹﺣﺼﺎء‪ ،‬اﻻرﺗﺒﺎط‬
‫ٍ‬
‫ﺑﻌﻼﻗﺎت ﺳﺒﺒﻴﺔ‬
‫اﻟﺰاﺋﻒ ﻫﻮ اﻻرﺗﺒﺎط اﻟﺬي ﺗﻜﻮن ﻓﻴﻪ ا ُملﺘﻐريات ﻏري ﻣﺮﺗﺒﻄﺔ ﻓﻴﻤﺎ ﺑﻴﻨﻬﺎ‬
‫ﻋﺎﻣﻞ ﺛﺎﻟﺚ ﻏري ﻣﺮﺋﻲ‪ .‬ﻣﻦ ﺑني‬
‫وﻟﻜﻨﻬﺎ ﻗﺪ ﺗﺒﺪو ﻛﺬﻟﻚ؛ وﻳﻜﻮن اﻻرﺗﺒﺎط ﻧﺎﺟﻤً ﺎ ﻋﻦ وﺟﻮد‬
‫ٍ‬
‫اﻷﻣﺜﻠﺔ اﻟﺘﻲ ﻳُﻘﺪﱢﻣﻬﺎ ﻓﻴﺠني اﻻرﺗﺒﺎط ﺑني ﻣﻌﺪل اﻟﻄﻼق ﰲ وﻻﻳﺔ ﻣني وﻣﻌﺪل اﺳﺘﻬﻼك‬
‫اﻟﺴﻤﻦ اﻟﻨﺒﺎﺗﻲ ﻟﻠﻔﺮد اﻟﻮاﺣﺪ‪ ،‬أو اﻻرﺗﺒﺎط ﺑني ﻣﻌﺪل اﺳﺘﻬﻼك ﺟﺒﻦ املﻮﺗﺰارﻳﻼ ﻟﻠﻔﺮد اﻟﻮاﺣﺪ‬
‫واﻟﺤﺼﻮل ﻋﲆ دﻛﺘﻮراه ﰲ اﻟﻬﻨﺪﺳﺔ ا َملﺪﻧﻴﺔ‪ 1 .‬رﺑﻤﺎ ﻳﻌﺜﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ ﻣﺜﻞ ﻫﺬه‬
‫ﱠ‬
‫ﱡ‬
‫ﺗﺴﺘﺤﻖ ﻣﺰﻳﺪًا ﻣﻦ‬
‫ﻳﺘﺪﺧﻞ اﻟﺒﴩ ﻟﺘﻘﺮﻳﺮ اﻻرﺗﺒﺎﻃﺎت اﻟﺘﻲ‬
‫اﻻرﺗﺒﺎﻃﺎت‪ ،‬وﻟﻜﻦ ﻳﺠﺐ أن‬
‫ٍ‬
‫ﻋﻼﻗﺎت ﺳﺒﺒﻴﺔ‪.‬‬
‫اﻟﺪراﺳﺔ ﻣﻦ أﺟﻞ اﻟﻌﺜﻮر ﻋﲆ‬
‫ً‬
‫ﻓﻀﻼ ﻋﻦ ذﻟﻚ‪ ،‬ﰲ املﺮﺣﻠﺔ اﻟﺘﻲ ﻳﺘﻢ ﻓﻴﻬﺎ ﺟﻤﻊ اﻟﺒﻴﺎﻧﺎت وﺗﺼﻤﻴﻢ أو إﻧﺸﺎء ﻣﺠﻤﻮﻋﺔ‬
‫ٍ‬
‫ﱡ‬
‫ﻳﺨﺺ ﻛﻴﻔﻴﺔ اﻟﺘﺠﺮﻳﺪ ﻋﻦ اﻟﻮاﻗﻊ )‪Kelleher and Tierney‬‬
‫اﺧﺘﻴﺎرات ﻓﻴﻤﺎ‬
‫اﻟﺒﻴﺎﻧﺎت‪ ،‬ﻧﺠﺮي‬
‫‪ .(2018‬واﻟﺘﺠﺮﻳﺪ ﻋﻦ اﻟﻮاﻗﻊ ﻻ ﻳﻜﻮن ﻣُﺤﺎﻳﺪًا أﺑﺪًا‪ ،‬واﻟﺘﺠﺮﻳﺪ ﻧﻔﺴﻪ ﻟﻴﺲ واﻗﻌً ﺎ؛ وإﻧﻤﺎ ﻫﻮ‬
‫‪67‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺗﻤﺜﻴﻞ ﻟﻠﻮاﻗﻊ‪ .‬وﻫﺬا ﻳَﻌﻨﻲ أﻧﻪ ﻳُﻤﻜﻨﻨﺎ ﻣﻨﺎﻗﺸﺔ ﻣﺪى ﺟﻮدة ﻫﺬا اﻟﺘﻤﺜﻴﻞ وﻣﻼءﻣﺘﻪ‪ ،‬ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ‬
‫ﺑﻐ َﺮض ﻣُﻌني‪ .‬ﻗﺎرن ﻫﺬا ﺑﺄﻳﺔ ﺧﺮﻳﻄﺔ‪ :‬اﻟﺨﺮﻳﻄﺔ ﻧﻔﺴﻬﺎ ﻟﻴﺴﺖ ﻫﻲ اﻹﻗﻠﻴﻢ‪ ،‬وﻗﺪ اﺧﺘﺎر َ‬
‫اﻟﺒﴩ‬
‫ﻃﺮﻳﻘﺔ ﺗﺼﻤﻴﻢ اﻟﺨﺮﻳﻄﺔ ﻟﻐ َﺮ ٍض ﻣُﻌني )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺧﺮﻳﻄﺔ ملﻼﺣﺔ اﻟﺴﻴﺎرات ﻣﻘﺎﺑﻞ‬
‫ﺧﺮﻳﻄﺔ ﻃﻮﺑﻮﻏﺮاﻓﻴﺔ ﻟﻠﺘﻨ ﱡﺰه ﺳريًا ﻋﲆ اﻷﻗﺪام(‪ .‬ﰲ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ ،‬ﻳﻌﻤﻞ اﻟﺘﺠﺮﻳﺪ ﺑﺎﺳﺘﺨﺪام‬
‫اﻷﺳﺎﻟﻴﺐ اﻹﺣﺼﺎﺋﻴﺔ ﻋﲆ إﻧﺸﺎء ﻧﻤﻮذج ﻟﻠﻮاﻗﻊ؛ إﻧﻪ ﻟﻴﺲ اﻟﻮاﻗﻊ اﻟﻔﻌﲇ‪ .‬ﻛﻤﺎ ﻳﺘﻀﻤﱠ ﻦ ذﻟﻚ‬
‫اﺧﺘﻴﺎرات‪ :‬اﺧﺘﻴﺎرات ﺑﺸﺄن اﻟﺨﻮارزﻣﻴﺔ ﻧﻔﺴﻬﺎ اﻟﺘﻲ ﺗُ ﱢ‬
‫ﻮﻓﺮ اﻟﻌﻤﻠﻴﺔ اﻹﺣﺼﺎﺋﻴﺔ اﻟﺘﻲ ﺗﺄﺧﺬﻧﺎ‬
‫ﻣﻦ اﻟﺒﻴﺎﻧﺎت إﱃ اﻟﻨﻤﻂ‪/‬اﻟﻘﺎﻋﺪة‪ ،‬وﻟﻜﻦ ً‬
‫أﻳﻀﺎ اﺧﺘﻴﺎرات ﺑﺸﺄن ﺗﺼﻤﻴﻢ ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت‬
‫اﻟﺘﻲ ﺗﺘﺪ ﱠرب ﻋﻠﻴﻬﺎ اﻟﺨﻮارزﻣﻴﺔ‪ .‬ﻳﻌﻨﻲ ﻫﺬا اﻟﺠﺎﻧﺐ اﻻﺧﺘﻴﺎري‪ ،‬وﻣﻦ ﺛَﻢ اﻟﺠﺎﻧﺐ اﻟﺒﴩي‪ ،‬ﰲ‬
‫ً‬
‫أﺳﺌﻠﺔ ﻧﻘﺪﻳﺔ ﺣﻮل اﻻﺧﺘﻴﺎرات اﻟﺘﻲ ﺗُﺘﱠ َﺨﺬ‪ ،‬ﺑﻞ ﻳﺠﺐ ﻋﻠﻴﻨﺎ‬
‫ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ أﻧﻪ ﻳُﻤﻜﻨﻨﺎ أن ﻧﻄﺮح‬
‫أن ﻧﻔﻌﻞ ذﻟﻚ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻫﻞ ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﺳﻴﺘﻢ اﻟﺘﺪرﻳﺐ ﻋﻠﻴﻬﺎ ﺗُﻤﺜﻞ‬
‫ً‬
‫ﺗﻤﺜﻴﻼ ﺟﻴﺪًا؟ ﻫﻞ ﻫﻨﺎك أي ﺗﺤﻴﱡﺰات ﰲ اﻟﺒﻴﺎﻧﺎت؟ ﻛﻤﺎ ﺳﻨﺮى ﰲ اﻟﻔﺼﻞ اﻟﻘﺎدم‪،‬‬
‫اﻟﺴﻜﺎن‬
‫ﻫﺬه اﻻﺧﺘﻴﺎرات واﻟﻘﻀﺎﻳﺎ ﻟﻴﺴﺖ ﻣﺠﺮد أﺳﺌﻠﺔ ﻓﻨﻴﺔ وﻟﻜﻦ ﻟﻬﺎ ً‬
‫أﻳﻀﺎ ﺟﺎﻧﺐ أﺧﻼﻗﻲ ﺷﺪﻳﺪ‬
‫اﻷﻫﻤﻴﺔ‪.‬‬
‫اﻟﺘﻄﺒﻴﻘﺎت‬
‫ﺗﻄﺒﻴﻘﺎت ﻋﺪﻳﺪة‪ ،‬ذَ ُ‬
‫ٌ‬
‫ﻛﺮت ﺑﻌﻀﻬﺎ ﺑﺎﻟﻔﻌﻞ ﺗﺤﺖ اﻟﻌﻨﻮان اﻷﻋﻢ‬
‫ﻟﺘﻌ ﱡﻠﻢ اﻵﻟﺔ وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت‬
‫ا ُملﺘﻤﺜﻞ ﰲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻫﺬه اﻟﺘﻘﻨﻴﺎت ﻳُﻤﻜﻦ اﺳﺘﺨﺪاﻣﻬﺎ ﻟﻠﺘﻌ ﱡﺮف ﻋﲆ اﻟﻮﺟﻮه‬
‫)ﺑﻞ ﻟﻠﺘﻌ ﱡﺮف ﻋﲆ اﻻﻧﻔﻌﺎﻻت ﺑﻨﺎءً ﻋﲆ ﺗﺤﻠﻴﻞ اﻟﻮﺟﻮه(‪ ،‬أو ﺗﻘﺪﻳﻢ اﻗﱰاﺣﺎت ﺑﺤﺚ‪ ،‬أو‬
‫ﻗﻴﺎدة اﻟﺴﻴﺎرة‪ ،‬أو إﺟﺮاء ﱡ‬
‫ﺗﻮﻗﻌﺎت ﺷﺨﺼﻴﺔ‪ ،‬أو اﻟﺘﻨﺒﱡﺆ ﺑﻤَ ﻦ ﺳﻴﻌﺎود ارﺗﻜﺎب اﻟﺠﺮﻳﻤﺔ‪،‬‬
‫أو اﻟﺘﻮﺻﻴﺔ ﺑﻤﻮﺳﻴﻘﻰ ﻣُﻌﻴﻨﺔ ﻟﻼﺳﺘﻤﺎع إﻟﻴﻬﺎ‪ .‬وﺗﺴﺘﺨﺪَم ﰲ ﻣﺠﺎل املﺒﻴﻌﺎت واﻟﺘﺴﻮﻳﻖ‪،‬‬
‫ﻟﻠﺘﻮﺻﻴﺔ ﺑﻤﻨﺘﺠﺎت وﺧﺪﻣﺎت‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻨﺪﻣﺎ ﺗﺸﱰي ﺷﻴﺌًﺎ ﻋﲆ ﻣﻮﻗﻊ أﻣﺎزون‪،‬‬
‫ٍ‬
‫ﺑﻴﺎﻧﺎت ﻋﻨﻚ ﺛﻢ ﻳُﻘﺪم ﺗﻮﺻﻴﺎت ﻋﲆ أﺳﺎس ﻧﻤﻮذج إﺣﺼﺎﺋﻲ ﻳﺴﺘﻨﺪ إﱃ‬
‫ﺳﻴﺠﻤﻊ املﻮﻗﻊ‬
‫ٍ‬
‫ﺑﻴﺎﻧﺎت ﻣﻦ ﺟﻤﻴﻊ اﻟﻌﻤﻼء‪ .‬اﺳﺘﺨﺪﻣﺖ ﴍﻛﺔ ووملﺎرت ﰲ ﻣﺘﺎﺟﺮﻫﺎ ﺗﻘﻨﻴﺔ اﻟﺘﻌ ﱡﺮف ﻋﲆ‬
‫ُ‬
‫اﻟﻮﺟﻮه ﻟﻠﺘﺼﺪي ﻟﻠﴪﻗﺔ؛ وﻗﺪ ﺗﺴﺘﺨﺪم ﰲ املﺴﺘﻘﺒﻞ اﻟﺘﻘﻨﻴﺔ ﻧﻔﺴﻬﺎ ﻟﺘﺤﺪﻳﺪ ﻣﺎ إذا ﻛﺎن‬
‫ا ُملﺘﺴﻮﻗﻮن ﺳﻌﺪاء أم ﻣُﺤﺒَﻄني‪ .‬ﻛﻤﺎ أن ﻟﻠﺘﻘﻨﻴﺎت ﺗﻄﺒﻴﻘﺎت ﻣﺨﺘﻠﻔﺔ ﰲ ﻣﺠﺎل اﻟﺘﻤﻮﻳﻞ‪.‬‬
‫ﺗﻌﺎوﻧﺖ وﻛﺎﻟﺔ إﻛﺴﱪﻳﺎن ﻟﻠﻤﺮﺟﻌﻴﺔ اﻻﺋﺘﻤﺎﻧﻴﺔ ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ املﺪﻋﻮم ﺑﺘﻌ ﱡﻠﻢ اﻵﻟﺔ‬
‫ﻟﺘﺤﻠﻴﻞ اﻟﺒﻴﺎﻧﺎت ا ُملﺘﻌﻠﻘﺔ ﺑﺎ ُملﻌﺎﻣﻼت واﻟﻘﻀﺎﻳﺎ املﻨﻈﻮرة ﰲ املﺤﺎﻛﻢ ﻣﻦ أﺟﻞ اﻟﺘﻮﺻﻴﺔ ﺑﻤﺎ‬
‫ﻗﺮض ُملﻘﺪﱢم ﻃﻠﺐ ﻟﺮﻫﻦ ﻋﻘﺎري‪ .‬وﺗﺴﺘﺨﺪم أﻣﺮﻳﻜﺎن إﻛﺴﱪﻳﺲ ﺗﻌ ﱡﻠﻢ‬
‫إذا ﻛﺎن ﻳﺠﺐ ﺗﻘﺪﻳﻢ ٍ‬
‫‪68‬‬

‫ﻻ َ‬
‫ﺗﻨﺲ )ﻋﻠﻢ( اﻟﺒﻴﺎﻧﺎت‬

‫اﻵﻟﺔ ﻟﺘﻮﻗﻊ املﻌﺎﻣﻼت اﻻﺣﺘﻴﺎﻟﻴﺔ‪ .‬وﰲ ﻣﺠﺎل اﻟﻨﻘﻞ‪ ،‬ﻳُﺴﺘﺨﺪَم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺒﻴﺎﻧﺎت‬
‫اﻟﻀﺨﻤﺔ ﻹﻧﺸﺎء ﺳﻴﺎرات ذاﺗﻴﺔ اﻟﻘﻴﺎدة‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗﺴﺘﺨﺪِم ﴍﻛﺔ ﺑﻲ إم دﺑﻠﻴﻮ ﻧﻮﻋً ﺎ‬
‫ﻣﻦ ﺗﻘﻨﻴﺔ اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻟﺼﻮر ﻟﺘﺤﻠﻴﻞ اﻟﺒﻴﺎﻧﺎت اﻟﻮاردة ﻣﻦ أﺟﻬﺰة اﻻﺳﺘﺸﻌﺎر واﻟﻜﺎﻣريات‬
‫ﰲ اﻟﺴﻴﺎرة‪ .‬وﰲ ﻣﺠﺎل اﻟﺮﻋﺎﻳﺔ اﻟﺼﺤﻴﺔ‪ ،‬ﻳﻤﻜﻦ أن ﻳُﺴﺎﻋﺪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ املﺪﻋﻮم ﺑﺘﻌ ﱡﻠﻢ‬
‫اﻵﻟﺔ ﰲ ﺗﺸﺨﻴﺺ اﻟﴪﻃﺎن )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﺗﺤﻠﻴﻞ ﺻﻮر اﻷﺷﻌﺔ ﻟﺘﺸﺨﻴﺺ ﻣﺮض‬
‫اﻟﴪﻃﺎن( أو اﻛﺘﺸﺎف اﻷﻣﺮاض ا ُملﻌﺪﻳﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬أﺟﺮى ﻧﻈﺎم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫ﺗﺤﻠﻴﻼ ملﻠﻴﻮن ﺻﻮرة ﻣﻦ ﺻﻮر أﺷﻌﺔ اﻟﻌﻴﻮن وﺑﻴﺎﻧﺎت املﺮﴇ‪ ،‬ﻣُﺪرﺑًﺎ‬
‫ﻟﴩﻛﺔ دﻳﺐ ﻣﺎﻳﻨﺪ‬
‫ﻧﻔﺴﻪ ﻋﲆ ﺗﺸﺨﻴﺺ أﻋﺮاض ﺣﺎﻻت اﻟﻌﻴﻮن املﺮﺿﻴﺔ ا ُملﺘﺪﻫﻮرة‪ .‬وﻗﺪ ﺗﺠﺎوز ﻧﻈﺎم واﺗﺴﻮن‬
‫ٍ‬
‫ﺗﻮﺻﻴﺎت ﺑﺸﺄن‬
‫اﻟﺬي أﻧﺸﺄﺗﻪ ﴍﻛﺔ آي ﺑﻲ إم ﻣُﻤﺎرﺳﺔ ﻟﻌﺒﺔ »ﺟﻴﻮﺑﺎردي« وﻳﺴﺘﺨﺪم ﻟﺘﻘﺪﻳﻢ‬
‫ﻋﻼج اﻟﴪﻃﺎن‪ .‬ﻛﻤﺎ ﺗُﺰوﱢد أﺟﻬﺰة اﻟﺮﻳﺎﺿﺔ واﻟﺼﺤﺔ اﻟﺘﻲ ﻳﻤﻜﻦ ارﺗﺪاؤﻫﺎ ﺗﻄﺒﻴﻘﺎت ﺗﻌ ﱡﻠﻢ‬
‫اﻵﻟﺔ ﺑﺎﻟﺒﻴﺎﻧﺎت‪ .‬وﰲ ﻣﺠﺎل اﻟﺼﺤﺎﻓﺔ‪ ،‬ﻳﻤﻜﻦ ﻟﺘﻌ ﱡﻠﻢ اﻵﻟﺔ ﻛﺘﺎﺑﺔ ﺗﻘﺎرﻳﺮ إﺧﺒﺎرﻳﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ‬
‫املﺜﺎل‪ ،‬ﰲ املﻤﻠﻜﺔ املﺘﺤﺪة‪ ،‬ﺗﺴﺘﺨﺪِم وﻛﺎﻟﺔ أﻧﺒﺎء »ﺑﺮﻳﺲ أﺳﻮﺳﻴﻴﺸﻦ« اﻟﺮوﺑﻮﺗﺎت ﰲ ﻛﺘﺎﺑﺔ‬
‫ﺗﻘﺎرﻳﺮ اﻷﺧﺒﺎر املﺤﻠﻴﺔ‪ .‬وﻳﺪﺧﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫أﻳﻀﺎ إﱃ املﻨﺰل واملﺠﺎل اﻟﺸﺨﴢ‪ ،‬ﻋﲆ‬
‫ﱠ‬
‫ﺗﺘﻮﱃ ﺟﻤﻊ اﻟﺒﻴﺎﻧﺎت وأﺟﻬﺰة ﺗﻔﺎﻋُ ﻠﻴﺔ ﻣﺴﺎﻋﺪة ﻣﺘﱠﺼﻠﺔ‬
‫ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﺷﻜﻞ روﺑﻮﺗﺎت‬
‫ﺑﻤﻌﺎﻟﺠﺔ اﻟﻠﻐﺔ اﻟﻄﺒﻴﻌﻴﺔ‪ .‬ﺗﺘﺤﺪﱠث دُﻣﻴﺔ »ﻫﺎﻟﻮ ﺑﺎرﺑﻲ« إﱃ اﻷﻃﻔﺎل ﺑﺎﺳﺘﺨﺪام ﻣُﻌﺎﻟﺠﺔ اﻟﻠﻐﺔ‬
‫اﻟﻄﺒﻴﻌﻴﺔ اﻟﺘﻲ ﺗُﺤﻠﻞ املﺤﺎدﺛﺎت املﺴﺠﻠﺔ‪ .‬ﻓﻜ ﱡﻞ ﻣﺎ ﻳﻘﻮﻟﻪ اﻷﻃﻔﺎل ﻳﺘﻢ ﺗﺴﺠﻴﻠُﻪ وﺗﺨﺰﻳﻨﻪ‬
‫وﺗﺤﻠﻴﻠﻪ ﰲ وﺣﺪات اﻟﺨﺪﻣﺔ اﻟﺨﺎﺻﺔ ﺑ »ﺗﻮي ﺗﻮك«‪ .‬ﺛﻢ ﻳُﺮﺳﻞ ردٍّا إﱃ اﻟﺠﻬﺎز‪ :‬وﺗﺠﻴﺐ دﻣﻴﺔ‬
‫»ﻫﺎﻟﻮ ﺑﺎرﺑﻲ« ﻋﲆ أﺳﺎس ﻣﺎ »ﺗﻌﻠﻤﺘﻪ« ﻋﻦ ﻣُﺴﺘﺨﺪﻣﻬﺎ‪ .‬وﻳﺴﺘﺨﺪِم ﻓﻴﺴﺒﻮك ﺗﻘﻨﻴﺎت اﻟﺘﻌ ﱡﻠﻢ‬
‫اﻟﻌﻤﻴﻖ واﻟﺸﺒﻜﺎت اﻟﻌﺼﺒﻴﺔ ﻟﻬﻴﻜﻠﺔ وﺗﺤﻠﻴﻞ اﻟﺒﻴﺎﻧﺎت اﻵﺗﻴﺔ ﻣﻤﺎ ﻳﻘ ُﺮب ﻣﻦ ﻣﻠﻴﺎ َري ﻣﺴﺘﺨﺪم‬
‫ٍ‬
‫ﱠ‬
‫ﺑﻴﺎﻧﺎت ﻏري ﻣُﻬﻴﻜﻠﺔ‪ .‬وﻫﺬا ﻳﺴﺎﻋﺪ اﻟﴩﻛﺔ ﰲ ﺗﻘﺪﻳﻢ إﻋﻼﻧﺎت ﻣُﺴﺘﻬﺪﻓﺔ‪.‬‬
‫ﻟﻠﻤﻨﺼﺔ ﻳُﻨﺘﺠﻮن‬
‫وﻳﺤ ﱢﻠﻞ إﻧﺴﺘﺠﺮام ﺻﻮر ‪ ٨٠٠‬ﻣﻠﻴﻮن ﻣُﺴﺘﺨﺪِم ﺑﻬﺪف ﺑﻴﻊ اﻹﻋﻼﻧﺎت إﱃ اﻟﴩﻛﺎت‪ .‬وﻳﺴﺘﺨﺪم‬
‫ﻧﺘﻔﻠﻴﻜﺲ ﻣﺤﺮﻛﺎت اﻟﺘﻮﺻﻴﺔ اﻟﺘﻲ ﺗُﺤ ﱢﻠﻞ ﺑﻴﺎﻧﺎت اﻟﻌﻤﻼء‪ ،‬ﻟﻜﻲ ﻳُﺤﻮﱢل ﻧﻔﺴﻪ ﻣﻦ ﻣﻮزع إﱃ‬
‫ﻣﻨﺘﺞ ﻣﺤﺘﻮى‪ :‬ﻓﺈذا َ‬
‫ﻛﻨﺖ ﺗﺴﺘﻄﻴﻊ اﻟﺘﻨﺒﱡﺆ ﺑﻤﺎ ﻳﺮﻏﺐ اﻟﻨﺎس ﰲ ﻣﺸﺎﻫﺪﺗﻪ‪ ،‬ﻓﻴُﻤﻜﻨﻚ إﻧﺘﺎﺟﻪ‬
‫ﺑﻨﻔﺴﻚ وﺗﺤﻘﻴﻖ رﺑﺢ ﻣﻨﻪ‪ .‬ﺑﻞ إن ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت اﺳﺘُﺨﺪِم ﰲ ﻣﺠﺎل اﻟﻄﻬﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫ﺑﻨﺎءً ﻋﲆ ﺗﺤﻠﻴﻞ ﻧﺤﻮ ‪ ١٠٠٠٠‬وﺻﻔﺔ‪ ،‬ﻳُﻨﺸﺊ ﻧﻈﺎم ﺷﻴﻒ واﺗﺴﻮن اﻟﺬي أﻧﺘﺠﺘﻪ ﴍﻛﺔ آي‬
‫ﺑﻲ إم وﺻﻔﺎﺗﻪ اﻟﺨﺎﺻﺔ اﻟﺘﻲ ﺗﻘﱰح ﺗﻮﻟﻴﻔﺎت ﺟﺪﻳﺪة ﻟﻠﻤﻜﻮﻧﺎت‪ 2 .‬وﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ اﺳﺘﺨﺪام‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ املﺪﻋﻮم ﺑﺘﻌ ﱡﻠﻢ اﻵﻟﺔ ﰲ اﻟﺘﻌﻠﻴﻢ‪ ،‬واﻟﺘﻮﻇﻴﻒ‪ ،‬واﻟﻌﺪاﻟﺔ اﻟﺠﻨﺎﺋﻴﺔ‪ ،‬واﻷﻣﻦ‬
‫‪69‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫)ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﻟﴩﻃﺔ اﻟﺘﻨﺒﺆﻳﺔ(‪ ،‬واﺳﱰﺟﺎع املﻮﺳﻴﻘﻰ‪ ،‬واﻷﻋﻤﺎل املﻜﺘﺒﻴﺔ‪ ،‬واﻟﺰراﻋﺔ‪،‬‬
‫واﻷﺳﻠﺤﺔ اﻟﻌﺴﻜﺮﻳﺔ‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪.‬‬
‫ﰲ املﺎﴈ‪ ،‬ﻛﺎﻧﺖ اﻹﺣﺼﺎء ﻣﻦ املﺠﺎﻻت ﻏري اﻟﺠﺬاﺑﺔ‪ .‬أﻣﺎ اﻟﻴﻮم‪ ،‬ﻓﺒﻌﺪ أن أﺻﺒﺤﺖ ﺟﺰءًا‬
‫ﺷﻜﻞ ﻳُﺪﻣَ ﺞ ﻓﻴﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻊ اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ‪ ،‬أﺻﺒﺤﺖ‬
‫ﻣﻦ ﻋِ ﻠﻢ اﻟﺒﻴﺎﻧﺎت وﰲ‬
‫ٍ‬
‫اﻹﺣﺼﺎء ﺷﺪﻳﺪة اﻟﺠﺎذﺑﻴﺔ‪ .‬إﻧﻬﺎ اﻟﺴﺤﺮ اﻟﺠﺪﻳﺪ‪ .‬إﻧﻬﺎ املﺠﺎل اﻟﺬي ﺗُ ﱢ‬
‫ﻔﻀﻠﻪ وﺳﺎﺋﻞ اﻹﻋﻼم‪.‬‬
‫ﻧﻮع ﺟﺪﻳﺪ ﻣﻦ اﻟﺘﻨﻘﻴﺐ ﻋﻦ‬
‫ﻛﻤﺎ أﻧﻬﺎ ﺗُﻌﺘﱪ ﻣﺠﺎ َل أﻋﻤﺎل ﺿﺨﻤً ﺎ‪ .‬ﻓﺎﻟﺒﻌﺾ ﻳﺘﺤﺪﱠﺛﻮن ﻋﻦ ٍ‬
‫ً‬
‫ﺧﻴﺎﻻ‬
‫اﻟﺬﻫﺐ؛ واﻟﺘﻮﻗﻌﺎت ﻫﺎﺋﻠﺔ‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻓﻬﺬا اﻟﻨﻮع ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺲ‬
‫َ‬
‫ﻣﺤﺾ ﻧﺒﻮءة‪ ،‬ﻛﻤﺎ ﺗُﺒني اﻷﻣﺜﻠﺔ اﻟﺘﻲ ﴐﺑﻨﺎﻫﺎ أن ﻣﺎ ﻳُﺴﻤﱠ ﻰ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻋﻠﻤﻴٍّﺎ أو‬
‫املﺤﺪود أو اﻟﻀﻌﻴﻒ ﻣﻮﺟﻮد ﺑﺎﻟﻔﻌﻞ وواﺳﻊ اﻻﻧﺘﺸﺎر‪ .‬وﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﺘﺄﺛريه ا ُملﺤﺘﻤَ ﻞ‪ ،‬ﻓﻠﻴﺲ‬
‫ﻫﻨﺎك ﻣﺎ ﻳُﻤﻜﻨﻨﺎ أن ِ‬
‫ﻧﺼﻔﻪ ﺑﺄﻧﻪ ﻣﺤﺪود أو ﺿﻌﻴﻒ‪ .‬ﻟﺬﻟﻚ‪ ،‬ﻓﺈﻧﻪ ﻣﻦ اﻟﴬوري ﺟﺪٍّا أن ﻧُﺤ ﱢﻠﻞ‬
‫وﻧُﻨﺎﻗﺶ اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﻘﻀﺎﻳﺎ اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ أﺛﺎرﺗﻬﺎ ﺗﻘﻨﻴﺎت ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ وﻏريﻫﺎ ﻣﻦ ﺗﻘﻨﻴﺎت‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺗﻄﺒﻴﻘﺎﺗﻬﺎ‪ .‬وﻫﺬا ﻫﻮ ﻣﻮﺿﻮع اﻟﻔﺼﻮل اﻟﻘﺎدﻣﺔ‪.‬‬
‫ﰲ املﺎﴈ‪ ،‬ﻛﺎﻧﺖ اﻹﺣﺼﺎء ﻣﻦ املﺠﺎﻻت ﻏري اﻟﺠﺬﱠاﺑﺔ‪ .‬أﻣﺎ اﻟﻴﻮم‪ ،‬ﻓﺒﻌﺪ أن أﺻﺒﺤﺖ ﺟﺰءًا ﻣﻦ ﻋﻠﻢ‬
‫ﺷﻜﻞ ﻳُﺪﻣﺞ ﻓﻴﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻊ اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ‪ ،‬أﺻﺒﺤﺖ اﻹﺣﺼﺎء ﺷﺪﻳﺪة‬
‫اﻟﺒﻴﺎﻧﺎت وﰲ‬
‫ٍ‬
‫اﻟﺠﺎذﺑﻴﺔ‪ .‬إﻧﻬﺎ اﻟﺴﺤﺮ اﻟﺠﺪﻳﺪ‪.‬‬

‫‪70‬‬

‫اﻟﻔﺼﻞ اﻟﺴﺎﺑﻊ‬

‫اﳋﺼﻮﺻﻴﺔ وﻏﲑﻫﺎ ﻣﻦ اﻟﻘﻀﺎﻳﺎ‬

‫إن اﻟﻌﺪﻳﺪ ﻣﻦ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ ا ُملﺘﻌﻠﻘﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻌﺮوﻓﺔ ﻣﻦ ﻣﺠﺎل أﺧﻼﻗﻴﺎت‬
‫ﺑﺸﻜﻞ أﻋﻢ‪ ،‬ﻣﻦ ﻣﺠﺎل أﺧﻼﻗﻴﺎت اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺮﻗﻤﻴﺔ وﺗﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫اﻟﺮوﺑﻮﺗﺎت واﻷﺗﻤﺘﺔ أو‪،‬‬
‫ٍ‬
‫اﻻﺗﺼﺎﻻت‪ .‬وﻟﻜﻦ ﻫﺬا ﰲ ﺣ ﱢﺪ ذاﺗﻪ ﻻ ﻳُﻘ ﱢﻠﻞ ﻣﻦ أﻫﻤﻴﺘﻬﺎ‪ .‬وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻓﺈن ﻫﺬه اﻟﻘﻀﺎﻳﺎ‬
‫ٍ‬
‫ﺑﺘﻘﻨﻴﺎت أﺧﺮى — ﺗﻜﺘﺴﺐ ﺑُﻌﺪًا ﺟﺪﻳﺪًا وﺗُﺼﺒﺢ‬
‫— ﺑﺴﺒﺐ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﻃﺮﻳﻘﺔ ارﺗﺒﺎﻃﻬﺎ‬
‫أﻛﺜﺮ أﻫﻤﻴﺔ وإﻟﺤﺎﺣً ﺎ‪.‬‬
‫اﻟﺨﺼﻮﺻﻴﺔ وﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت‬
‫ﻓﻠﻨُﻔﻜﺮ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﻣﺴﺄﻟﺔ اﻟﺨﺼﻮﺻﻴﺔ وﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت‪ .‬ﻳﻨﻄﻮي اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻻ ﺳﻴﻤﺎ ﺗﻄﺒﻴﻘﺎت ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ اﻟﺘﻲ ﺗﺘﻌﺎﻣَ ﻞ ﻣﻊ اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ‪ ،‬ﻏﺎﻟﺒًﺎ‬
‫ﻋﲆ ﺟﻤﻊ املﻌﻠﻮﻣﺎت اﻟﺸﺨﺼﻴﺔ واﺳﺘﺨﺪاﻣﻬﺎ‪ .‬وﻳُﻤﻜﻦ ً‬
‫أﻳﻀﺎ اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫وأﻳﻀﺎ ﰲ ﻣﻜﺎن اﻟﻌﻤﻞ وﰲ ﻛﻞ ﻣﻜﺎن‪ ،‬وذﻟﻚ ﻣﻦ ﺧﻼل اﻟﻬﻮاﺗﻒ اﻟﺬﻛﻴﺔ‬
‫ﻟﻠﻤُﺮاﻗﺒﺔ‪ ،‬ﰲ اﻟﺸﺎرع‬
‫ُ‬
‫ري ﻣﻦ اﻷﺣﻴﺎن‪ ،‬ﻻ ﻳﻌﻠﻢ اﻟﻨﺎس ﺣﺘﻰ أن اﻟﺒﻴﺎﻧﺎت‬
‫ووﺳﺎﺋﻞ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ .‬وﰲ ﻛﺜ ٍ‬
‫ﺳﻴﺎق‬
‫ﺳﻴﺎق ﻣﺎ ﺗُﺴﺘﺨﺪَم ﺑﻮاﺳﻄﺔ أﻃﺮاف أﺧﺮى ﰲ‬
‫ﺗُﺠﻤَ ﻊ‪ ،‬أو أن اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﻗﺪﻣﻮﻫﺎ ﰲ‬
‫ٍ‬
‫ٍ‬
‫آﺧﺮ‪ .‬ﻛﻤﺎ أن اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗَﻌﻨﻲ أن )ﻣﺠﻤﻮﻋﺎت( اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﺗﺤﺼﻞ ﻋﻠﻴﻬﺎ‬
‫املﻨﻈﻤﺎت املﺨﺘﻠﻔﺔ ﻳﺘﻢ دﻣﺠﻬﺎ ﻣﻌً ﺎ‪.‬‬
‫ﻳﺘﻄ ﱠﻠﺐ اﻻﺳﺘﺨﺪام اﻷﺧﻼﻗﻲ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺟﻤﻊ اﻟﺒﻴﺎﻧﺎت وﻣﻌﺎﻟﺠﺘﻬﺎ وﻣﺸﺎرﻛﺘﻬﺎ‬
‫ﱠ‬
‫ٍ‬
‫وﺣﻘﻬﻢ ﰲ ﻣﻌﺮﻓﺔ ﻣﺎ ﻳﺤﺪث ﻟﺒﻴﺎﻧﺎﺗﻬﻢ‪ ،‬واﻟﻮﺻﻮل إﱃ‬
‫ﺑﻄﺮﻳﻘﺔ ﺗﺤﱰم ﺧﺼﻮﺻﻴﺔ اﻷﻓﺮاد‬
‫ﺑﻴﺎﻧﺎﺗﻬﻢ‪ ،‬واﻻﻋﱰاض ﻋﲆ ﺟﻤﻊ ﺑﻴﺎﻧﺎﺗِﻬﻢ أو ﻋﲆ ﻣُﻌﺎﻟﺠﺘﻬﺎ‪ ،‬وﻣﻌﺮﻓﺔ أن ﺑﻴﺎﻧﺎﺗﻬﻢ ﺗُﺠﻤﻊ‬
‫ٍ‬
‫ﻟﻘﺮارات ﻳﺘﺨﺬﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ )ﰲ ﺣﺎﻟﺔ ﺣﺪوث ذﻟﻚ‬
‫وﺗُﻌﺎﻟﺞ وأﻧﻬﻢ ﺑﻌﺪﺋ ٍﺬ ﻳﺨﻀﻌﻮن‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺑﺎﻟﻔﻌﻞ(‪ .‬وﺗُﺜﺎر اﻟﻌﺪﻳﺪ ﻣﻦ ﻫﺬه اﻟﻘﻀﺎﻳﺎ ً‬
‫أﻳﻀﺎ ﰲ ﺳﻴﺎﻗﺎت ﺗﻜﻨﻮﻟﻮﺟﻴﺎ املﻌﻠﻮﻣﺎت وﺗﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫ﻃﺎ ﻣُﻬﻤٍّ ﺎ ً‬
‫اﻻﺗﺼﺎﻻت اﻷﺧﺮى‪ ،‬وﻛﻤﺎ ﺳﻨﺮى ﻓﻴﻤﺎ ﺑﻌ ُﺪ ﰲ ﻫﺬا اﻟﻔﺼﻞ‪ ،‬ﺗﻌﺘﱪ اﻟﺸﻔﺎﻓﻴﺔ ﴍ ً‬
‫أﻳﻀﺎ‬
‫ﰲ ﺗﻠﻚ اﻟﺤﺎﻻت )اﻧﻈﺮ ً‬
‫ﻻﺣﻘﺎ ﰲ ﻫﺬا اﻟﻔﺼﻞ(‪ .‬ﻛﻤﺎ ﺗُﺜﺎر ﻗﻀﺎﻳﺎ ﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت ﰲ أﺧﻼﻗﻴﺎت‬
‫اﻟﺒﺤﺚ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ أﺧﻼﻗﻴﺎت ﺟﻤﻊ اﻟﺒﻴﺎﻧﺎت ﻷﺑﺤﺎث اﻟﻌﻠﻮم اﻻﺟﺘﻤﺎﻋﻴﺔ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻋﻨﺪ اﻟﻨﻈﺮ إﱃ اﻟﺴﻴﺎﻗﺎت اﻟﺘﻲ ﻳُﺴﺘﺨﺪَم ﻓﻴﻬﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻴﻮم‪،‬‬
‫ﺗُﺼﺒﺢ ﻗﻀﺎﻳﺎ اﻟﺨﺼﻮﺻﻴﺔ وﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت أﻛﺜﺮ ﺗﻌﻘﻴﺪًا‪ .‬ﻓﺈن اﺣﱰام ﻫﺬه اﻟﻘِ ﻴَﻢ واﻟﺤﻘﻮق‬
‫ً‬
‫اﺳﺘﺒﻴﺎن ﻛﻌﺎﻟِﻢ اﺟﺘﻤﺎع‪ :‬إذ ﻳﻤﻜﻦ ﻟﻠﺒﺎﺣﺚ إﺑﻼغ املﺸﺎرﻛني‬
‫ﺳﻬﻼ إﱃ ﺣ ﱟﺪ ﻣﺎ ﻋﻨﺪ إﺟﺮاء‬
‫ﻳﻜﻮن‬
‫ٍ‬
‫ﺑﺸﻜﻞ ﴏﻳﺢ‪ ،‬وﻣِﻦ ﺛَﻢ ﺳﻴﻜﻮن ﻣﻦ املﻌﺮوف ﻧﺴﺒﻴٍّﺎ ﻣﺎ‬
‫ﰲ اﻻﺳﺘﺒﻴﺎن وﻃﻠﺐ ﻣﻮاﻓﻘﺘﻬﻢ‬
‫ٍ‬
‫ﺳﻴﺤﺪث ﻟﻠﺒﻴﺎﻧﺎت‪ .‬وﻟﻜﻦ اﻟﺒﻴﺌﺔ اﻟﺘﻲ ﻳُﺴﺘﺨﺪَم ﻓﻴﻬﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت اﻟﻴﻮم‬
‫ً‬
‫ﻣﺨﺘﻠﻔﺔ ﺗﻤﺎﻣً ﺎ‪ .‬ﻓﻠﻨﺘﻨﺎول ً‬
‫ﻣﺜﻼ وﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ :‬ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ‬
‫ﻋﺎد ًة ﻣﺎ ﺗﻜﻮن‬
‫ﻣﻌﻠﻮﻣﺎت اﻟﺨﺼﻮﺻﻴﺔ واﻟﺘﻄﺒﻴﻘﺎت اﻟﺘﻲ ﺗﻄﻠُﺐ ﻣﻦ ا ُملﺴﺘﺨﺪِﻣني املﻮاﻓﻘﺔ‪ ،‬ﻓﺈن ا ُملﺴﺘﺨﺪِﻣني‬
‫ﻻ ﻳﻌﺮﻓﻮن ﺑﻮﺿﻮح ﻣﺎ ﻳﺤﺪُث ﻟﺒﻴﺎﻧﺎﺗﻬﻢ أو ﺣﺘﻰ أي ﺑﻴﺎﻧﺎت ﻳﺘﻢ ﺟﻤﻌﻬﺎ؛ وإذا ﻛﺎﻧﻮا ﻳﺮﻏﺒﻮن‬
‫ري ﻣﻦ اﻷﺣﻴﺎن‪ ،‬ﻻ‬
‫ﰲ اﺳﺘﺨﺪام اﻟﺘﻄﺒﻴﻖ واﻻﺳﺘﻤﺘﺎع ﺑﻔﻮاﺋﺪه‪ ،‬ﻓﻌﻠﻴﻬﻢ أن ﻳﻮاﻓﻘﻮا‪ .‬وﰲ ﻛﺜ ٍ‬
‫ﻳﻌﻠﻢ ا ُملﺴﺘﺨﺪﻣﻮن ﺣﺘﻰ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳ ﱢ‬
‫ُﺸﻐﻞ اﻟﺘﻄﺒﻴﻖ اﻟﺬي ﻳﺴﺘﺨﺪﻣﻮﻧﻪ‪ .‬وﻏﺎﻟﺒًﺎ‬
‫ﻣﺎ ﺗُ َ‬
‫ﻧﻄﺎق َ‬
‫آﺧﺮ واﺳﺘﺨﺪاﻣﻬﺎ ﻷﻏﺮاض ﻣﺨﺘﻠﻔﺔ )إﻋﺎدة‬
‫ﺳﻴﺎق ﻣﺎ إﱃ‬
‫ﻨﻘﻞ اﻟﺒﻴﺎﻧﺎت ا ُملﻌﻄﺎة ﰲ‬
‫ٍ‬
‫ٍ‬
‫أﻏﺮاض أﺧﺮى(‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻨﺪﻣﺎ ﺗﺒﻴﻊ اﻟﴩﻛﺎت ﺑﻴﺎﻧﺎﺗﻬﺎ إﱃ‬
‫اﺳﺘﺨﺪام اﻟﺒﻴﺎﻧﺎت ﰲ‬
‫ٍ‬
‫ﴍﻛﺎت أﺧﺮى أو ﺗﻨﻘﻞ اﻟﺒﻴﺎﻧﺎت ﺑني أﺟﺰاءٍ ﻣﺨﺘﻠﻔﺔ ﻣﻦ ﻧﻔﺲ اﻟﴩﻛﺔ دون ﻋِ ﻠﻢ املﺴﺘﺨﺪِﻣني‬
‫ﺑﻬﺬا‪.‬‬
‫اﻟﺘﻼﻋُ ﺐ واﻻﺳﺘﻐﻼل وا ُملﺴﺘﺨﺪِﻣني ا ُملﺴﺘﻬﺪﻓني‬
‫ﺗُﺸري ﻫﺬه اﻟﻈﺎﻫﺮة اﻷﺧرية ً‬
‫أﻳﻀﺎ إﱃ اﺣﺘﻤﺎﻟﻴﺔ اﻟﺘﻼﻋُ ﺐ ﺑﺎ ُملﺴﺘﺨﺪﻣني واﺳﺘﻐﻼﻟﻬﻢ‪ .‬ﻳُﺴﺘﺨﺪَم‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻠﺘﺤﻜﻢ ﻓﻴﻤﺎ ﻧﺸﱰﻳﻪ‪ ،‬وﰲ اﻷﺧﺒﺎر اﻟﺘﻲ ﻧُﺘﺎﺑﻌﻬﺎ‪ ،‬وﰲ اﻵراء اﻟﺘﻲ ﻧﺜِﻖ‬
‫ﺑﻬﺎ‪ ،‬وﻏري ذﻟﻚ‪ .‬وﻗﺪ أﺷﺎر اﻟﺒﺎﺣﺜﻮن ﰲ اﻟﻨﻈﺮﻳﺔ اﻟﻨﻘﺪﻳﺔ إﱃ اﻟﺴﻴﺎق اﻟﺮأﺳﻤﺎﱄ اﻟﺬي ﻳﺤﺪث‬
‫ﻓﻴﻪ اﺳﺘﺨﺪام وﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻤﻜﻦ اﻟﻘﻮل إن ﻣُﺴﺘﺨﺪﻣﻲ‬
‫ً‬
‫»ﻋﻤﻼ رﻗﻤﻴٍّﺎ« ﻣﺠﺎﻧﻴٍّﺎ )‪ (Fuchs 2014‬ﻣﻦ ﺧﻼل إﻧﺘﺎج‬
‫وﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ ﻳﺆدﱡون‬
‫اﻟﺒﻴﺎﻧﺎت ﻟﺼﺎﻟﺢ اﻟﴩﻛﺎت‪ .‬وﻳُﻤﻜﻦ أن ﻳﺸﻤﻞ ﻫﺬا اﻟﺸﻜﻞ ﻣﻦ أﺷﻜﺎل اﻻﺳﺘﻐﻼل ً‬
‫أﻳﻀﺎ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻓﺒﻮﺻﻔﻨﺎ ﻣُﺴﺘﺨﺪﻣني ﻟﻮﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ‪ ،‬ﻧﺤﻦ ﻧﺘﻌﺮض ﻟﺨﻄﺮ أن‬
‫ﻧﺼﺒﺢ اﻟﻘﻮة اﻟﻌﺎﻣﻠﺔ ا ُملﺴﺘﻐ ﱠﻠﺔ ﻏري املﺄﺟﻮرة‪ ،‬اﻟﺘﻲ ﺗﻨﺘﺞ اﻟﺒﻴﺎﻧﺎت ﻟﺼﺎﻟﺢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫‪72‬‬

‫اﻟﺨﺼﻮﺻﻴﺔ وﻏريﻫﺎ ﻣﻦ اﻟﻘﻀﺎﻳﺎ‬

‫اﻟﺬي ﻳُﺤﻠﻞ ﺑﻴﺎﻧﺎﺗﻨﺎ ﺑﻌﺪ ذﻟﻚ ﻟﺼﺎﻟﺢ اﻟﴩﻛﺎت اﻟﺘﻲ ﺗﺴﺘﺨﺪم اﻟﺒﻴﺎﻧﺎت‪ ،‬واﻟﺘﻲ ﻋﺎد ًة ﻣﺎ‬
‫ً‬
‫أﻳﻀﺎ‪ .‬وﻫﺬا ﻳُﺬ ﱢﻛﺮﻧﺎ ً‬
‫أﻃﺮاﻓﺎ أﺧﺮى ً‬
‫أﻳﻀﺎ ﺑﺘﺤﺬﻳﺮ ﻫريﺑﺮت ﻣﺎرﻛﻮزه ﰲ ﺳﺘﻴﻨﻴﺎت‬
‫ﺗﺘﻀﻤﱠ ﻦ‬
‫اﻟﻘﺮن اﻟﻌﴩﻳﻦ ﺑﺄﻧﻪ ﺣﺘﻰ ﰲ املﺠﺘﻤﻌﺎت ا ُملﺴﻤﱠ ﺎة ﻣﺠﺘﻤﻌﺎت »ﺣﺮة«‪ ،‬و»ﻏري ﺷﻤﻮﻟﻴﺔ«‪ ،‬ﻫﻨﺎك‬
‫أﺷﻜﺎل ﺧﺎﺻﺔ ﻣﻦ اﻟﺴﻴﻄﺮة‪ ،‬وﺧﺎﺻﺔ اﺳﺘﻐﻼل ا ُملﺴﺘﻬﻠﻜني )‪ .(Marcuse 1991‬ﻳﻜﻤﻦ‬
‫أﺷﻜﺎل‬
‫اﻟﺨﻄﺮ ﻫﻨﺎ ﰲ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﺪ ﻳﺆدي ﺣﺘﻰ ﰲ اﻟﺪﻳﻤﻘﺮاﻃﻴﺎت اﻟﺤﺪﻳﺜﺔ إﱃ‬
‫ٍ‬
‫ﺟﺪﻳﺪة ﻣﻦ اﻟﺘﻼﻋُ ﺐ واملﺮاﻗﺒﺔ واﻻﺳﺘﺒﺪاد‪ ،‬ﻟﻴﺲ ﺑﺎﻟﴬورة ﰲ ﺷﻜﻞ ﺳﻴﺎﺳﺎت اﺳﺘﺒﺪادﻳﺔ‬
‫وﻟﻜﻦ ﺑﻄﺮﻳﻘﺔ أﻛﺜﺮ ﺧﻔﺎءً وﻓﻌﺎﻟﻴﺔ‪ :‬ﻣﻦ ﺧﻼل ﺗﻐﻴري اﻻﻗﺘﺼﺎد ﺑﻄﺮﻳﻘﺔ ﺗُﺤﻮﱢﻟﻨﺎ ﺟﻤﻴﻌً ﺎ —‬
‫ﰲ اﺳﺘﺨﺪاﻣﻨﺎ ﻟﻠﻬﻮاﺗﻒ اﻟﺬﻛﻴﺔ واﻟﺘﻔﺎﻋﻼت اﻟﺮﻗﻤﻴﺔ اﻷﺧﺮى — إﱃ ﻣﺎ ﻳُﺸﺒﻪ اﻷﺑﻘﺎر اﻟﺘﻲ ﻳﺘﻢ‬
‫ﺣﻠﺒﻬﺎ ﻟﻠﺤﺼﻮل ﻋﲆ ﺑﻴﺎﻧﺎﺗﻬﺎ‪ .‬وﻟﻜﻦ ﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻠﺘﻼﻋُ ﺐ ﰲ‬
‫ُ‬
‫اﻟﺘﻮاﺻﻞ‬
‫ﺑﺸﻜﻞ أﻛﺜﺮ ﻣﺒﺎﴍة‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﻦ ﺧﻼل ﺗﺤﻠﻴﻞ ﺑﻴﺎﻧﺎت وﺳﺎﺋﻞ‬
‫اﻟﺴﻴﺎﺳﺔ‬
‫ٍ‬
‫اﻻﺟﺘﻤﺎﻋﻲ ﻟﺪﻋﻢ ﺣﻤﻼت ﺳﻴﺎﺳﻴﺔ ﻣُﻌﻴﻨﺔ )ﻛﻤﺎ ﰲ اﻟﺤﺎﻟﺔ اﻟﺸﻬرية ﻟﴩﻛﺔ ﻛﺎﻣﱪﻳﺪج أﻧﺎﻟﻴﺘﻴﻜﺎ‪،‬‬
‫اﻟﺘﻲ اﺳﺘﺨﺪﻣﺖ ﺑﻴﺎﻧﺎت ﻣُﺴﺘﺨﺪﻣﻲ ﻓﻴﺴﺒﻮك — دون ﻣﻮاﻓﻘﺘﻬﻢ — ﻷﻏﺮاض ﺳﻴﺎﺳﻴﺔ ﰲ‬
‫اﻧﺘﺨﺎﺑﺎت اﻟﺮﺋﺎﺳﺔ اﻷﻣﺮﻳﻜﻴﺔ ﻋﺎم ‪ ،(٢٠١٦‬أو ﻋﻦ ﻃﺮﻳﻖ اﺳﺘﺨﺪام روﺑﻮﺗﺎت ﻟﻨﴩ رﺳﺎﺋﻞ‬
‫ُ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ اﺳﺘﻨﺎدًا إﱃ ﺗﺤﻠﻴﻞ ﺑﻴﺎﻧﺎت اﻷﻓﺮاد ﻣﻦ ﺣﻴﺚ‬
‫ﺳﻴﺎﺳﻴﺔ ﻋﲆ وﺳﺎﺋﻞ‬
‫ﺗﻔﻀﻴﻼﺗﻬﻢ اﻟﺴﻴﺎﺳﻴﺔ ﻟﻠﺘﺄﺛري ﻋﲆ ﻋﻤﻠﻴﺎت اﻟﺘﺼﻮﻳﺖ‪ .‬ﻛﻤﺎ أن اﻟﺒﻌﺾ ﻳُﺴﺎورﻫﻢ اﻟﻘﻠﻖ ﻣﻦ‬
‫ً‬
‫ﻧﻴﺎﺑﺔ ﻋﻦ اﻟﺒﴩ‪ ،‬ﻗﺪ ﻳُﺤﻮﱢل‬
‫أن ﻳُﺤﻮﱢل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻣﻦ ﺧﻼل ﺗﻮ ﱢﻟﻴﻪ املﻬﺎم املﻌﺮﻓﻴﺔ‬
‫ﻣُﺴﺘﺨﺪِﻣﻴﻪ إﱃ أﻃﻔﺎل ﻋﲆ املﺴﺘﻮى اﻟﻌﻘﲇ ﻋﻦ ﻃﺮﻳﻖ »ﺗﻘﻠﻴﻞ ﻗﺪرﺗﻬﻢ ﻋﲆ اﻟﺘﻔﻜري ﺑﻤﺤﺾ‬
‫أﻧﻔﺴﻬﻢ أو اﺗﺨﺎذ ﻗﺮاراﺗﻬﻢ اﻟﺨﺎﺻﺔ ﺑﻤﺎ ﻳﺠﺐ ﻓﻌﻠﻪ« )‪ .(Shanahan 2015, 170‬ﻋﻼو ًة ﻋﲆ‬
‫ذﻟﻚ‪ ،‬ﻻ ﻳﻜﻤﻦ ﺧﻄﺮ اﻻﺳﺘﻐﻼل ﰲ ﺟﺎﻧﺐ املﺴﺘﺨﺪم ﻓﺤﺴﺐ‪ :‬ﻓﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻌﺘﻤﺪ ﻋﲆ‬
‫أﺟﻬﺰة ﺻﻨﻌﻬﺎ أﺷﺨﺎص‪ ،‬وﻗﺪ ﻳﻨﻄﻮي إﻧﺸﺎء ﻫﺬه اﻷﺟﻬﺰة ﻋﲆ اﺳﺘﻐﻼل ﻫﺆﻻء اﻷﺷﺨﺎص‪.‬‬
‫وﻗﺪ ﻳﺪﺧﻞ اﻻﺳﺘﻐﻼل ً‬
‫أﻳﻀﺎ ﰲ ﺗﺪرﻳﺐ اﻟﺨﻮارزﻣﻴﺎت وإﻧﺘﺎج اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﺗُﺴﺘﺨﺪَم ﻟﺼﺎﻟﺢ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻋﻦ ﻃﺮﻳﻘﻪ‪ .‬إن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ رﺑﻤﺎ ﻳﺠﻌﻞ اﻟﺤﻴﺎة أﻳﴪ ﺑﺎﻟﻨﺴﺒﺔ‬
‫إﱃ ﻣُﺴﺘﺨﺪِﻣﻴﻪ‪ ،‬وﻟﻜﻦ ﻟﻴﺲ ﺑﺎﻟﴬورة ﺑﺎﻟﻨﺴﺒﺔ إﱃ أوﻟﺌﻚ اﻟﺬﻳﻦ ﻳ ﱢ‬
‫ُﻨﻘﺒﻮن ﻋﻦ املﻌﺎدن‪ ،‬أو‬
‫ﺑﺎﻟﻨﺴﺒﺔ إﱃ ﻣَ ﻦ ﻳﺘﻌﺎﻣﻠﻮن ﻣﻊ ا ُملﺨﻠﻔﺎت اﻹﻟﻜﱰوﻧﻴﺔ‪ ،‬أو إﱃ ﻣَ ﻦ ﻳُﺪرﺑﻮن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻻ ﻳﻘﺘﴫ ﻣﺎ ﻳﻘﻮم ﺑﻪ ﺗﻄﺒﻴﻖ »أﻟﻴﻜﺴﺎ« اﻟﺬي ﻃﻮﱠرﺗﻪ أﻣﺎزون إﻛﻮ ﻋﲆ‬
‫إﻧﺸﺎء ﻣُﺴﺘﺨﺪِﻣني ﻳﺆدﱡون ً‬
‫ﻋﻤﻼ ﻣﺠﺎﻧﻴٍّﺎ وﻳُﺼﺒﺤﻮن ﻣﺼﺎدر ﻟﻠﺒﻴﺎﻧﺎت وﻳُﺒﺎﻋﻮن ﻛﻤﻨﺘﺠﺎت؛‬
‫ﺑﻞ ﻫﻨﺎك ﻋﺎ َﻟﻢ ﻣﻦ اﻟﻌﻤﻞ اﻟﺒﴩي ﻳﻜﻤُﻦ ﺧﻠﻒ اﻟﻜﻮاﻟﻴﺲ‪ :‬ﻓﻌُ ﻤﱠ ﺎل اﻟﺘﻨﻘﻴﺐ ﻋﻦ املﻌﺎدن‪،‬‬
‫واﻟﻌﻤﺎل ﻋﲆ اﻟﺴﻔﻦ‪ ،‬واﻟﻌﻤﺎل اﻟﺬﻳﻦ ﻳُﺼﻨﻔﻮن ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت‪ ،‬ﻛﻞ ﻫﺆﻻء ﰲ ﺧﺪﻣﺔ‬
‫ﺗﺠﻤﻴﻊ رءوس اﻷﻣﻮال وﺗﺮا ُﻛﻤﻬﺎ ﻟﺪى ﻋﺪد ﻗﻠﻴﻞ ﺟﺪٍّا ﻣﻦ اﻷﺷﺨﺎص )‪.(Schwab 2018‬‬
‫‪73‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫أﺷﻜﺎل ﺟﺪﻳﺪة ﻣﻦ اﻟﺘﻼﻋُ ﺐ واملﺮاﻗﺒﺔ واﻻﺳﺘﺒﺪاد‪ ،‬ﻟﻴﺲ ﺑﺎﻟﴬورة ﰲ‬
‫ﻗﺪ ﻳﺆدي اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ‬
‫ٍ‬
‫ٍ‬
‫ﺳﻴﺎﺳﺎت اﺳﺘﺒﺪادﻳﺔ وﻟﻜﻦ ﺑﻄﺮﻳﻘﺔٍ أﻛﺜﺮ ﺧﻔﺎءً وﻓﻌﺎﻟﻴﺔ‪.‬‬
‫ﺷﻜﻞ‬

‫ﺑﻌﺾ ﻣُﺴﺘﺨﺪﻣﻲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أﻛﺜﺮ ﺗﻌ ﱡﺮ ً‬
‫ﺿﺎ ﻟﻠﺨﻄﺮ ﻣﻦ ﻏريﻫﻢ‪ .‬وﻧﻈﺮﻳﺎت‬
‫اﻟﺨﺼﻮﺻﻴﺔ واﻻﺳﺘﻐﻼل ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﻔﱰض أن املﺴﺘﺨﺪِم ﺷﺨﺺ ﺑﺎﻟِﻎ ﺳﻠﻴﻢ اﻟﺠﺴﻢ‪ ،‬ﺻﻐري‬
‫اﻟﺴﻦ ﻧﺴﺒﻴٍّﺎ‪ ،‬ﰲ ﻛﺎﻣﻞ ﻗﻮاه اﻟﻌﻘﻠﻴﺔ‪ .‬ﱠ‬
‫ﻟﻜﻦ اﻟﻌﺎ َﻟﻢ اﻟﺤﻘﻴﻘﻲ ﻣﲇء ﺑﺎﻷﻃﻔﺎل وﻛﺒﺎر اﻟﺴﻦ‬
‫واﻷﺷﺨﺎص اﻟﺬﻳﻦ ﻻ ﻳﺘﻤﺘﻌﻮن ﺑﻘﻮًى ﻋﻘﻠﻴﺔ »ﻃﺒﻴﻌﻴﺔ« أو »ﻛﺎﻣﻠﺔ«‪ ،‬وﻏريﻫﻢ‪ .‬ﻣﺜﻞ ﻫﺆﻻء‬
‫ا ُملﺴﺘﺨﺪﻣني اﻟﻀﻌﻔﺎء أﻛﺜﺮ ﻋُ ً‬
‫ﺮﺿﺔ ﻟﻠﺨﻄﺮ‪ .‬وﻳﻤﻜﻦ اﻧﺘﻬﺎك ﺧﺼﻮﺻﻴﺘﻬﻢ أو اﻟﺘﻼﻋﺐ ﺑﻬﻢ‬
‫ﱢ‬
‫ً‬
‫ﻓﺮﺻﺎ ﺟﺪﻳﺪة ﻟﻬﺬه اﻻﻧﺘﻬﺎﻛﺎت وﻋﻤﻠﻴﺎت اﻟﺘﻼﻋﺐ‪ .‬ﻓ ﱢﻜﺮ‬
‫وﻳﻮﻓﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺑﺴﻬﻮﻟﺔ‪،‬‬
‫ً‬
‫ﻣﺜﻼ ﰲ اﻷﻃﻔﺎل اﻟﺼﻐﺎر اﻟﺬﻳﻦ ﻳﺘﺤﺪﺛﻮن ﻣﻊ دﻣﻴ ٍﺔ ﻣﺘﺼﻠﺔ ﺑﻨﻈﺎم ﺗﻜﻨﻮﻟﻮﺟﻲ ﻣﺪﻋﻮم ﺑﺎﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ :‬ﻋﲆ اﻷرﺟﺢ‪ ،‬ﻫﺆﻻء اﻷﻃﻔﺎل ﻻ ﻳﻌﻠﻤﻮن ﺷﻴﺌًﺎ ﻋﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺴﺘﺨﺪَم‬
‫أو ﻋﻦ ﺟﻤﻊ ﺑﻴﺎﻧﺎﺗﻬﻢ‪ ،‬ﻓﻤﺎ ﺑﺎﻟﻚ ﺑﻤﺎ ﻳُﻔﻌَ ﻞ ﺑﻤﻌﻠﻮﻣﺎﺗﻬﻢ اﻟﺸﺨﺼﻴﺔ‪ .‬إن روﺑﻮت اﻟﺪردﺷﺔ‬
‫أو اﻟﺪﻣﻴﺔ اﻟﺬﻛﻴﺔ املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻻ ﺗﺴﺘﻄﻴﻊ ﻓﻘﻂ أن ﺗﺠﻤﻊ اﻟﻜﺜري ﻣﻦ‬
‫املﻌﻠﻮﻣﺎت اﻟﺸﺨﺼﻴﺔ ﻋﻦ اﻟﻄﻔﻞ وأﺑﻮﻳﻪ ﺑﻬﺬه اﻟﻄﺮﻳﻘﺔ‪ ،‬ﺑﻞ ﻳُﻤﻜﻨﻬﺎ ً‬
‫أﻳﻀﺎ اﻟﺘﻼﻋُ ﺐ ﺑﺎﻟﻄﻔﻞ‬
‫ﺑﺎﺳﺘﺨﺪام واﺟﻬﺔ اﻟﻠﻐﺔ واﻟﺼﻮت‪ .‬وﻣﻊ ﺗﺤﻮﱡل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ ﺟﺰءٍ ﻣﻦ »إﻧﱰﻧﺖ‬
‫اﻷﻟﻌﺎب« )‪ (Druga and Williams 2017‬وإﻧﱰﻧﺖ اﻷﺷﻴﺎء )اﻷﺧﺮى(‪ ،‬ﺗُﺼﺒﺢ ﻫﺬه ﻣﺸﻜﻠﺔ‬
‫أﺧﻼﻗﻴﺔ وﺳﻴﺎﺳﻴﺔ‪ .‬إن ﺷﺒﺢ اﻟﺸﻤﻮﻟﻴﺔ واﻻﺳﺘﺒﺪاد ﻳُﻌﺎود اﻟﻈﻬﻮر ﻣﺠﺪدًا‪ :‬ﻟﻴﺲ ﰲ ﻗﺼﺺ‬
‫اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ ا ُملﺘﺸﺎﺋﻤﺔ أو ﰲ ﻛﻮاﺑﻴﺲ ﻣﺎ ﺑﻌﺪ اﻟﺤﺮوب اﻟﻘﺪﻳﻤﺔ‪ ،‬وﻟﻜﻦ ﰲ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫اﻻﺳﺘﻬﻼﻛﻴﺔ املﻮﺟﻮدة ﺑﺎﻟﻔﻌﻞ ﰲ اﻷﺳﻮاق‪.‬‬
‫اﻷﺧﺒﺎر اﻟﻜﺎذﺑﺔ‪ ،‬وﺧﻄﺮ اﻟﺸﻤﻮﻟﻴﺔ‪ ،‬وﺗﺄﺛريﻫﺎ ﻋﲆ اﻟﻌﻼﻗﺎت اﻟﺸﺨﺼﻴﺔ‬
‫ﻳﻤﻜﻦ أن ﻳُﺴﺘﺨﺪَم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫أﻳﻀﺎ ﰲ إﻧﺘﺎج ﺧﻄﺎب اﻟﻜﺮاﻫﻴﺔ واملﻌﻠﻮﻣﺎت اﻟﺰاﺋﻔﺔ‪،‬‬
‫ﻛﺄﺷﺨﺎص وﻟﻜﻨﻬﺎ ﰲ اﻟﻮاﻗﻊ ﻣﺠﺮد ﺑﺮاﻣﺞ ﻣﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء‬
‫أو ﰲ إﻧﺸﺎء روﺑﻮﺗﺎت ﺗﺒﺪو‬
‫ٍ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻗﺪ ﺳﺒﻖ وأﴍت ﺑﺎﻟﻔﻌﻞ إﱃ روﺑﻮت اﻟﺪردﺷﺔ »ﺗﺎي« وﺧﻄﺎب أوﺑﺎﻣﺎ اﻟﺰاﺋﻒ‪.‬‬
‫ﻗﺪ ﻳﺆدي ذﻟﻚ إﱃ ﻋﺎ َﻟ ٍﻢ ﻻ ﻳﻤﻜﻦ ﻓﻴﻪ اﻟﺘﻤﻴﻴﺰ ﺑﻮﺿﻮح ﺑني ﻣﺎ ﻫﻮ ﺣﻘﻴﻘﻲ وﻣﺎ ﻫﻮ زاﺋﻒ‪،‬‬
‫َ‬
‫ﻳﺠﺐ ﺗﺴﻤﻴﺘﻬﺎ »ﻣﺎ ﺑﻌﺪ اﻟﺤﻘﻴﻘﺔ«‬
‫ﻋﺎﻟﻢ‬
‫ﺗﺘﺪاﺧﻞ ﻓﻴﻪ اﻟﺤﻘﺎﺋﻖ ﻣﻊ اﻟﺨﻴﺎل‪ .‬وﺳﻮاء ﻛﺎن ِ‬
‫ﺑﺸﻜﻞ واﺿﺢ ﰲ‬
‫أم ﻻ )‪ ،(McIntyre 2018‬ﺗﺴﺎﻫﻢ ﻫﺬه اﻟﺘﻄﺒﻴﻘﺎت ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫‪74‬‬

‫اﻟﺨﺼﻮﺻﻴﺔ وﻏريﻫﺎ ﻣﻦ اﻟﻘﻀﺎﻳﺎ‬

‫املﺸﻜﻠﺔ‪ .‬ﺑﺎﻟﻄﺒﻊ‪ ،‬ﻛﺎن ﻳُﻮﺟَ ﺪ ﺗﻼﻋﺐ وﻣﻌﻠﻮﻣﺎت ﻛﺎذﺑﺔ ﻗﺒﻞ ﻇﻬﻮر اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﻓﺎﻷﻓﻼم‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻛﺎﻧﺖ داﺋﻤً ﺎ ﺗﺨﻠُﻖ أوﻫﺎﻣً ﺎ‪ ،‬واﻟﺼﺤﻒ ﻛﺎﻧﺖ ﺗﻨﴩ اﻟﺪﻋﺎﻳﺔ‬
‫ﺟﻨﺐ ﻣﻊ إﻣﻜﺎﻧﻴﺎت وﺑﻴﺌﺔ اﻹﻧﱰﻧﺖ‬
‫اﻟﻜﺎذﺑﺔ‪ .‬وﻟﻜﻦ ﺑﻌﺪ ﻇﻬﻮر اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺟﻨﺒًﺎ إﱃ ٍ‬
‫ووﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ اﻟﺮﻗﻤﻴﺔ‪ ،‬ﻳﺒﺪو أن املﺸﻜﻠﺔ ﺗﺰداد ﺗﻌﻘﻴﺪًا وﺣِ ﺪﱠة‪ .‬وﻳﺒﺪو أن‬
‫ﻫﻨﺎك املﺰﻳﺪ ﻣﻦ ُ‬
‫اﻟﻔ َﺮص ﻟﻠﺘﻼﻋُ ﺐ‪ ،‬ﻣﻤﺎ ﻳﻌﺮض اﻟﺘﻔﻜري اﻟﻨﻘﺪي ﻟﻠﺨﻄﺮ‪ .‬وﻛﻞ ﻫﺬا ﻳُﺬﻛﺮﻧﺎ ﻣﺮة‬
‫أﺧﺮى ﺑﺨﻄﻮرة اﻟﺸﻤﻮﻟﻴﺔ‪ ،‬اﻟﺘﻲ ﺗﺴﺘﻔﻴﺪ ﻣﻦ ا ْﻟﺘﺒﺎس اﻟﺤﻘﻴﻘﺔ وﺗﻨﺘﺞ أﺧﺒﺎ ًرا زاﺋﻔﺔ ﻷﻏﺮاض‬
‫أﻳﺪﻳﻮﻟﻮﺟﻴﺔ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﺣﺘﻰ ﰲ اﻟﻴﻮﺗﻮﺑﻴﺎ اﻟﻠﻴﱪاﻟﻴﺔ ﻗﺪ ﻻ ﺗﻜﻮن اﻟﺤﻴﺎة ﻏﺎﻳﺔ ﰲ اﻹﴍاق واﻟﺒﻬﺎء‪ .‬إذ‬
‫إن املﻌﻠﻮﻣﺎت اﻟﻜﺎذﺑﺔ ﺗﻨﺨﺮ ﰲ ﺟﺪار اﻟﺜﻘﺔ وﻣﻦ ﺛَﻢ ﺗﻔﺴﺪ اﻟﻨﺴﻴﺞ اﻻﺟﺘﻤﺎﻋﻲ‪ .‬وﻳُﻤﻜﻦ أن‬
‫ُ‬
‫اﻟﺘﻮاﺻﻞ‪ ،‬أو ﻋﲆ اﻷﻗﻞ اﻟﺘﻮاﺻﻞ اﻟﻬﺎدف‪،‬‬
‫ﻳﺆدي اﻻﺳﺘﺨﺪام ا ُملﻔﺮط ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ إﱃ ﺗﻘﻠﻴﻞ‬
‫ﺑني اﻷﻓﺮاد‪ .‬ﰲ ﻋﺎم ‪ ،٢٠١١‬ﻗﺪﻣﺖ ﺷريي ﺗريﻛﻞ ادﻋﺎءً ﻳﺘﻌﻠﻖ ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻣﺜﻞ أﺟﻬﺰة‬
‫اﻟﻜﻤﺒﻴﻮﺗﺮ واﻟﺮوﺑﻮﺗﺎت‪ :‬ﻟﻘﺪ اﻧﺘﻬﻰ ﺑﻨﺎ اﻷﻣﺮ إﱃ ﱡ‬
‫ﺗﻮﻗﻊ املﺰﻳﺪ ﻣﻦ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬واﻟﻘﻠﻴﻞ ﻣﻦ‬
‫أﻧﻔﺴﻨﺎ‪ .‬وﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ اﺳﺘﺨﺪام ﻫﺬه اﻟﺤﺠﱠ ﺔ ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ :‬ﺗﻜﻤُﻦ املﺸﻜﻠﺔ‬
‫ﰲ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﰲ ﺷﻜﻞ وﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ أو ﰲ ﺷﻜﻞ »اﻟﺮﻓﺎق«‬
‫اﻟﺮﻗﻤﻴني‪ ،‬ﻳُﻌﻄﻴﻨﺎ ْ‬
‫وﻫﻢ اﻟﺮﻓﻘﺔ وﻟﻜﻨﻪ ﻳُﺰﻋﺰع اﺳﺘﻘﺮار اﻟﻌﻼﻗﺎت اﻟﺤﻘﻴﻘﻴﺔ ﻣﻊ اﻷﺻﺪﻗﺎء‬
‫واﻷﺣﺒﺎء واﻟﻌﺎﺋﻼت‪ .‬وﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن ﻫﺬه املﺸﻜﻠﺔ ﻛﺎﻧﺖ ﻣﻮﺟﻮد ًة ﺑﺎﻟﻔﻌﻞ ﻗﺒﻞ اﻟﺬﻛﺎء‬
‫ٍ‬
‫وﺳﻴﻂ ﺟﺪﻳﺪ ﻣﻦ اﻟﻮﺳﺎﺋﻂ )ﻗﺮاءة اﻟﺼﺤﻒ أو‬
‫اﻻﺻﻄﻨﺎﻋﻲ وﺗﺰداد ﺗﻔﺎﻗﻤً ﺎ ﻣﻊ ﻇﻬﻮر ﻛﻞ‬
‫ﻣﺸﺎﻫﺪة اﻟﺘﻠﻔﻴﺰﻳﻮن ً‬
‫ﺑﺪﻻ ﻣﻦ اﻟﺘﺤﺪﱡث وإدارة ﺣﻮار(‪ ،‬ﻓﺈﻧﻪ ﻳﻤﻜﻦ اﻟﻘﻮل إن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫ري ﰲ ﺧﻠﻖ ْ‬
‫وﻫﻢ اﻟﺮﻓﻘﺔ‪،‬‬
‫اﻵن‪ ،‬ﰲ وﺟﻮد اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺗﻄﺒﻴﻘﻪ‪ ،‬ﻗﺪ أﺻﺒﺤﺖ أﻓﻀﻞ ﺑﻜﺜ ٍ‬
‫وأن ﻫﺬا ﻳﺰﻳﺪ ﻣﻦ ﺧﻄﺮ اﻟﻮﺣﺪة أو ﺗﺪﻫﻮر اﻟﻌﻼﻗﺎت اﻟﺸﺨﺼﻴﺔ‪.‬‬
‫اﻟﺴﻼﻣﺔ واﻷﻣﺎن‬
‫ﻫﻨﺎك ً‬
‫أﻳﻀﺎ ﻣﺨﺎﻃﺮ أوﺿﺢ‪ .‬ﻓﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻻ ﺳﻴﱠﻤﺎ ﰲ ﺣﺎل ﺗﻀﻤﻴﻨﻪ ﰲ أﻧﻈﻤﺔ‬
‫أﻳﻀﺎ إﱃ أن ﻳﻜﻮن آﻣﻨًﺎ‪ .‬وﻟﻨﴬب ً‬
‫اﻷﺟﻬﺰة اﻟﺘﻲ ﺗﻌﻤﻞ ﰲ اﻟﻌﺎ َﻟﻢ اﻟﻔﻌﲇ‪ ،‬ﻳﺤﺘﺎج ً‬
‫ﻣﺜﻼ ﻋﲆ‬
‫ذﻟﻚ ﺑﺎﻟﺮوﺑﻮﺗﺎت اﻟﺼﻨﺎﻋﻴﺔ‪ :‬ﻳﻔﱰض أﻻ ﺗُﻠﺤِ ﻖ ﻫﺬه اﻟﺮوﺑﻮﺗﺎت اﻷذى ﺑﺎﻟﻌﻤﺎل‪ .‬وﻣﻊ ذﻟﻚ‪،‬‬
‫ﺗﺤﺪث أﺣﻴﺎﻧًﺎ ﺣﻮادث ﰲ املﺼﺎﻧﻊ‪ .‬وﻳﻤﻜﻦ ﻟﻠﺮوﺑﻮﺗﺎت أن ﺗﻘﺘُﻞ‪ ،‬ﺣﺘﻰ ﻟﻮ ﻛﺎن ذﻟﻚ ﻧﺎد ًرا‬
‫ﻧﺴﺒﻴٍّﺎ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﰲ اﻟﺮوﺑﻮﺗﺎت اﻟﺘﻲ ِ‬
‫ﺗﻌﺘﻤﺪ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺗُﺼﺒﺢ ﻣﺸﻜﻠﺔ اﻟﺴﻼﻣﺔ‬
‫ﺟﻨﺐ ﻣﻊ اﻟﺒﴩ‪ ،‬وﻗﺪ ﺗﺘﻤ ﱠﻜﻦ‬
‫أﻛﺜﺮ ﺗﺤ ﱢﺪﻳًﺎ‪ :‬ﻓﻬﺬه اﻟﺮوﺑﻮﺗﺎت ﻗﺪ ﺗﺘﻤ ﱠﻜﻦ ﻣﻦ اﻟﻌﻤﻞ ﺟﻨﺒًﺎ إﱃ‬
‫ٍ‬
‫‪75‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻧﺤﻮ ذﻛﻲ«‪ .‬وﻟﻜﻦ ﻣﺎذا ﻳﻌﻨﻲ ذﻟﻚ ﺑﺎﻟﻀﺒﻂ؟ ﻫﻞ ﻳﺠﺐ‬
‫ﻣﻦ ﺗﺠﻨﱡﺐ إﻟﺤﺎق اﻷذى ﺑﺎﻟﺒﴩ »ﻋﲆ ٍ‬
‫ً‬
‫ﻗﺮﻳﺒﺔ ﻣﻦ اﻟﺒﴩ‪ ،‬ﻣﻤﺎ ﻳُﺒﻄﺊ اﻟﻌﻤﻠﻴﺔ‪ ،‬أم أﻧﻪ ﻣﻦ املﻘﺒﻮل‬
‫أن ﺗﺘﺤﺮك ﺑﺒﻂءٍ أﻛﱪ ﻋﻨﺪﻣﺎ ﺗﻜﻮن‬
‫ٍ‬
‫ﺑﴪﻋﺔ ﻋﺎﻟﻴﺔ ﻣﻦ أﺟﻞ إﻧﺠﺎز اﻟﻌﻤﻞ ﺑﻜﻔﺎءة وﴎﻋﺔ؟ ﻫﻨﺎك داﺋﻤً ﺎ اﺣﺘﻤﺎﻻت ﻟﺤﺪوث‬
‫اﻟﺘﺤﺮك‬
‫ﺧﻄﺄ ﻣﻦ ﻧﻮع ﻣﺎ‪ .‬ﻓﻬﻞ ﻳﺠﺐ أن ﺗﻨﻄﻮي أﺧﻼﻗﻴﺎت اﻟﺴﻼﻣﺔ ﻋﲆ اﻟﻮﺻﻮل إﱃ ﺣﻠﻮل وﺳﻂ؟‬
‫ﺗُﺜري اﻟﺮوﺑﻮﺗﺎت املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﺑﻴﺌﺔ املﻨﺰل أو ﰲ اﻷﻣﺎﻛﻦ اﻟﻌﺎﻣﺔ ً‬
‫أﻳﻀﺎ‬
‫ﻗﻀﺎﻳﺎ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻟﺴﻼﻣﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻫﻞ ﻳﺠﺐ ﻋﲆ اﻟﺮوﺑﻮت داﺋﻤً ﺎ ﺗﺠﻨﱡﺐ اﻻﺻﻄﺪام‬
‫ً‬
‫ﺷﺨﺼﺎ ﻣﻦ أﺟﻞ اﻟﻮﺻﻮل إﱃ ﻫﺪﻓﻪ؟‬
‫ﺑﺎﻟﺒﴩ أم أﻧﻪ ﻣﻦ املﻘﺒﻮل أﺣﻴﺎﻧًﺎ أن ﻳُﻌﺮﻗﻞ اﻟﺮوﺑﻮت‬
‫ﻫﺬه ﻟﻴﺴﺖ ﻣﺴﺎﺋﻞ ﺗﻘﻨﻴﺔ ﺑﺤﺘﺔ وﻟﻜﻦ ﻟﻬﺎ ﺟﺎﻧﺐ أﺧﻼﻗﻲ‪ :‬إﻧﻬﺎ ﻣﺴﺄﻟﺔ ﺣﻴﺎة ﺑﴩﻳﺔ و ِﻗﻴَﻢ ﻣﺜﻞ‬
‫ٍ‬
‫ﻣﺸﻜﻼت ﺗﺘﻌ ﱠﻠﻖ ﺑﺎملﺴﺌﻮﻟﻴﺔ )ﺳﻨﺘﺤﺪث ﻋﻦ ﻫﺬا ﺑﺘﻔﺼﻴﻞ‬
‫اﻟﺤﺮﻳﺔ واﻟﻜﻔﺎءة‪ .‬ﻛﻤﺎ أﻧﻬﺎ ﺗُﺜري‬
‫أﻛﺜﺮ ً‬
‫ﻻﺣﻘﺎ(‪.‬‬
‫ﺛﻤﱠ ﺔ ﻣﺸﻜﻠﺔ أﺧﺮى ﻛﺎﻧﺖ ﻣﻮﺟﻮدة ﺑﺎﻟﻔﻌﻞ ﻗﺒﻞ ﻇﻬﻮر اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ املﺸﻬﺪ‪،‬‬
‫ﱡ‬
‫ﺗﺴﺘﺤﻖ ﺗﺠﺪﻳﺪ اﻫﺘﻤﺎﻣﻨﺎ ﺑﻬﺎ؛ أﻻ وﻫﻲ ﻣﺸﻜﻠﺔ اﻷﻣﺎن‪ .‬ﰲ ﻋﺎﻟﻢ ﻣُﺘﺼﻞ ﺑﺎﻟﺸﺒﻜﺎت‪،‬‬
‫وﻟﻜﻨﻬﺎ‬
‫ﻳﻤﻜﻦ اﺧﱰاق أي ﺟﻬﺎز إﻟﻜﱰوﻧﻲ أو ﺑﺮﻧﺎﻣﺞ واﺧﱰاﻗﻪ واﻟﺘﻼﻋُ ﺐ ﺑﻪ ﻣﻦ ﻗﺒﻞ أﺷﺨﺎص ﻟﺪﻳﻬﻢ‬
‫ﻧﻮاﻳﺎ ﺧﺒﻴﺜﺔ‪ .‬ﻓﻜ ﱡﻠﻨﺎ ﻧﻌﻠﻢ ﺑﺸﺄن ﻓريوﺳﺎت اﻟﻜﻤﺒﻴﻮﺗﺮ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﻟﺘﻲ ﻳﻤﻜﻦ أن ﺗُﺨ ﱢﺮب‬
‫ﺟﻬﺎز اﻟﻜﻤﺒﻴﻮﺗﺮ اﻟﺨﺎص ﺑﻚ‪ .‬وﻟﻜﻦ ﻋﻨﺪ ﺗﺰوﻳﺪ أﺟﻬﺰﺗﻨﺎ وﺑﺮاﻣﺠﻨﺎ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫ٍ‬
‫ﺑﻮﻛﺎﻟﺔ أﺧﻼﻗﻴﺔ أﻛﱪ وﻳﻜﻮن ﻟﻬﺬا‬
‫ﻳﻤﻜﻦ أن ﺗﺰﻳﺪ إﻣﻜﺎﻧﻴﺎﺗﻬﺎ وﻗﺪراﺗﻬﺎ‪ ،‬وﻋﻨﺪﻣﺎ ﺗﺤﻈﻰ‬
‫ﻋﻮاﻗﺐ ﻣﺎدﻳﺔ ﰲ اﻟﻌﺎﻟﻢ اﻟﻔﻌﲇ‪ ،‬ﺗُﺼﺒﺢ ﻣﺸﻜﻠﺔ اﻷﻣﺎن أﻛﱪ ﺑﻜﺜري‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إذا‬
‫ُ‬
‫اﺧﱰ َﻗﺖ ﺳﻴﺎرﺗﻚ اﻟﺬاﺗﻴﺔ اﻟﻘﻴﺎدة اﻟﺘﻲ ﺗﻌﻤﻞ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻓﺴﻮف ﺗُﻌﺎﻧﻲ ﻣﻤﺎ ﻫﻮ‬
‫أﻛﺜﺮ ﻣﻦ ﻣﺠﺮد »ﻣﺸﻜﻠﺔ ﰲ اﻟﻜﻤﺒﻴﻮﺗﺮ« أو »ﻣﺸﻜﻠﺔ ﰲ اﻟﱪﻧﺎﻣﺞ«؛ ﻗﺪ ﺗﻠﻘﻰ ﺣﺘﻔﻚ‪ .‬وإذا‬
‫اﺧﱰق ﺑﺮﻧﺎﻣﺞ إﺣﺪى اﻟﺒَﻨﻰ اﻟﺘﺤﺘﻴﺔ املﻬﻤﺔ )ﻣﺜﻞ اﻹﻧﱰﻧﺖ‪ ،‬أو املﻴﺎه‪ ،‬أو اﻟﻄﺎﻗﺔ … إﻟﺦ( أو‬
‫ُِ‬
‫ٍ‬
‫اﺿﻄﺮاب‬
‫ﻗﺪرات ﻣﺪﻣﺮة‪ ،‬ﻓﻤﻦ املﺮﺟﱠ ﺢ أن ﻳﺘﻌﺮض املﺠﺘﻤﻊ ﺑﺄﻛﻤﻠﻪ إﱃ‬
‫ﺟﻬﺎز ﻋﺴﻜﺮي ذي‬
‫ٍ‬
‫ﻛﺒري وﺳﻮف ﻳﺘﻌﺮض اﻟﻜﺜري ﻣﻦ اﻷﺷﺨﺎص ﻟﻠﴬر‪ .‬ﰲ اﻟﺘﻄﺒﻴﻘﺎت اﻟﻌﺴﻜﺮﻳﺔ‪ ،‬ﻳﺸ ﱢﻜﻞ‬
‫اﺳﺘﺨﺪام اﻷﺳﻠﺤﺔ اﻟﻔﺘﺎﻛﺔ اﻟﺬاﺗﻴﺔ اﻟﺘﺸﻐﻴﻞ ﺧﻄﻮرة أﻣﻨﻴﺔ واﺿﺤﺔ‪ ،‬ﻻ ﺳﻴﱠﻤﺎ ﻋﲆ ا ُملﺴﺘﻬﺪَﻓني‬
‫ﺑﺎﻟﻄﺒﻊ ﺑﻬﺬه اﻷﺳﻠﺤﺔ )وﻋﺎدة ﻣﺎ ﻻ ﻳﻜﻮﻧﻮن ﻣﻦ اﻟﻐﺮب( وﻟﻜﻨﻪ ﻳﺸ ﱢﻜﻞ ﺧﻄﻮرة ً‬
‫أﻳﻀﺎ ﻋﲆ‬
‫أوﻟﺌﻚ اﻟﺬﻳﻦ ﻳﻨﴩوﻧﻬﺎ‪ :‬إذ ﻳﻤﻜﻦ داﺋﻤً ﺎ اﺧﱰاﻗﻬﺎ وﺗﺤﻮﻳﻠﻬﺎ ﺿﺪﻫﻢ‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻗﺪ‬
‫ﺣﺮب ﻋﺎملﻴﺔ ﺟﺪﻳﺪة‪ .‬وﻻ ﻳﻠﺰﻣﻨﺎ أن ﻧﻨﻈﺮ‬
‫ﻳﺆدي ﺳﺒﺎق اﻟﺘﺴ ﱡﻠﺢ اﻟﺬي ﻳﺸﻤﻞ ﻫﺬه اﻷﺳﻠﺤﺔ إﱃ‬
‫ٍ‬
‫ﺑﻌﻴﺪًا ﰲ ا ُملﺴﺘﻘﺒﻞ‪ :‬ﻓﺈذا ﻛﺎﻧﺖ اﻟﻄﺎﺋﺮات دون ﻃﻴﺎر )ﻏري ا ُملﺰودة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ(‬
‫ﻣﻄﺎر ﻛﺒري ﰲ ﻟﻨﺪن‪ ،‬ﻓﺈﻧﻪ ﻟﻴﺲ ﻣﻦ اﻟﺼﻌﺐ ﺗﺨﻴﱡﻞ ﻣﺪى‬
‫ﻳُﻤﻜﻨﻬﺎ ﺑﺎﻟﻔﻌﻞ ﺣﺎﻟﻴٍّﺎ اﻟﺴﻴﻄﺮة ﻋﲆ‬
‫ٍ‬
‫‪76‬‬

‫اﻟﺨﺼﻮﺻﻴﺔ وﻏريﻫﺎ ﻣﻦ اﻟﻘﻀﺎﻳﺎ‬

‫ﻫﺸﺎﺷﺔ ﻣﻨﺸﺂت ِﺑﻨﻴﺘﻨﺎ اﻷﺳﺎﺳﻴﺔ اﻟﻴﻮﻣﻴﺔ وﻛﻴﻒ ﻳﻤﻜﻦ ﻟﻼﺳﺘﺨﺪام املﺆذي أو ﻻﺧﱰاق اﻟﺬﻛﺎء‬
‫ً‬
‫ﺗﺪﻣريﻳﺔ ﻫﺎﺋﻠﺔ‪ .‬ﻻﺣﻆ ً‬
‫ٍ‬
‫ٍ‬
‫أﻳﻀﺎ أﻧﻪ‪ ،‬ﻋﲆ‬
‫وﻋﻤﻠﻴﺎت‬
‫اﺿﻄﺮاﺑﺎت ﺟﺴﻴﻤﺔ‬
‫اﻻﺻﻄﻨﺎﻋﻲ أن ﻳُﺴﺒﺐ‬
‫ﻋﻜﺲ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﻨﻮوﻳﺔ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻓﺈن اﺳﺘﺨﺪام ﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫ﻃﻮﻳﻼ؛ وﻣِﻦ ﺛَﻢ ﻓﺎﻟﻌﺎﺋﻖ أﻣﺎم اﺳﺘﺨﺪام‬
‫اﻟﺤﺎﻟﻴﺔ ﻻ ﻳﺘﻄ ﱠﻠﺐ ﻣﻌﺪﱠات ﺑﺎﻫﻈﺔ اﻟﺜﻤﻦ أو ﺗﺪرﻳﺒًﺎ‬
‫ﻷﻏﺮاض ﺧﺒﻴﺜﺔ ﻣُﻨﺨﻔﺾ ﻧﺴﺒﻴٍّﺎ‪.‬‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ﺗُﺬﻛﺮﻧﺎ ً‬
‫أﻳﻀﺎ املﺸﻜﻼت اﻟﻌﺎدﻳﺔ املﺘﻌﻠﻘﺔ ﺑﺎﻷﻣﺎن ﻣﻊ اﻟﺴﻴﺎرات وﻣﻨﺸﺂت اﻟﺒﻨﻴﺔ اﻟﺘﺤﺘﻴﺔ‬
‫ً‬
‫ﻋﺮﺿﺔ ﻟﻠﺨﻄﺮ ﻣﻦ ﻏريﻫﻢ‪،‬‬
‫ﻣﺜﻞ املﻄﺎرات ﺑﺄﻧﻪ ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن ﺑﻌﺾ اﻷﺷﺨﺎص أﻛﺜﺮ‬
‫ٍ‬
‫ﺗﻘﻨﻴﺎت ﻣﺜﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻷﻧﻨﺎ‪ ،‬ﻣﻊ زﻳﺎدة‬
‫ﻓﺈﻧﻨﺎ »ﺟﻤﻴﻌً ﺎ« ﻣُﻌ ﱠﺮﺿﻮن ﻟﻠﺨﻄﺮ ﰲ ﻇ ﱢﻞ‬
‫ﺗﻤﺘﱡﻊ ﻫﺬه اﻟﺘﻘﻨﻴﺎت ﺑﺎﻟﻮﻛﺎﻟﺔ وزﻳﺎدة ﺗﻔﻮﻳﻀﻨﺎ ﻟﻬﺎ ﻟﺘﺄدﻳﺔ املﺰﻳﺪ ﻣﻦ املﻬﺎم‪ ،‬ﻧُﺼﺒﺢ ﺟﻤﻴﻌً ﺎ‬
‫أﻛﺜﺮ اﻋﺘﻤﺎدًا ﻋﻠﻴﻬﻢ‪ .‬وﻫﻨﺎك اﺣﺘﻤﺎ ٌل داﺋﻢ أن ﺗﺴري اﻷﻣﻮر ﻋﲆ ﻏري ﻣﺎ ﻧﺮوم‪ .‬وﻣﻦ ﺛَﻢ‪ ،‬ﻳُﻤﻜﻨﻨﺎ‬
‫اﻟﻘﻮل إن املﺨﺎﻃﺮ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ اﻟﺠﺪﻳﺪة ﻟﻴﺴﺖ ﻣﺠﺮد ﻣﺨﺎﻃﺮ ﺗﻜﻨﻮﻟﻮﺟﻴﺔ‪ ،‬وإﻧﻤﺎ ﺗﺘﺠﺎوز‬
‫ذﻟﻚ ﻟﺘُﺼﺒﺢ ﻣﺨﺎﻃﺮ ﺗﻬﺪﱢد وﺟﻮدﻧﺎ ﺑﺼﻔﺘﻨﺎ ﺑﴩًا )‪ .(Coeckelbergh 2013‬ﻳﻤﻜﻦ رؤﻳﺔ‬
‫املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ املﻄﺮوﺣﺔ ﻫﻨﺎ ﻋﲆ أﻧﻬﺎ ﻣﺨﺎﻃﺮ إﻧﺴﺎﻧﻴﺔ‪ :‬ﻓﺎملﺨﺎﻃﺮ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ ﺗُﻬﺪد‬
‫ﻛﺒﴩ ﰲ ﻧﻬﺎﻳﺔ املﻄﺎف‪ .‬وﺑﻘﺪْر ﻣﺎ ﻧﻌﺘﻤﺪ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﺑﻘﺪْر ﻣﺎ ﻳﻜﻮن‬
‫وﺟﻮدﻧﺎ‬
‫ٍ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أﻛﺜﺮ ﻣﻦ ﻣﺠﺮد أدا ٍة ﻧﺴﺘﺨﺪﻣﻬﺎ؛ ﻓﺈﻧﻪ ﻳُﺼﺒﺢ ﺟﺰءًا ﻣﻦ ﻫﻮﻳﺘﻨﺎ وﻣﻦ‬
‫املﺨﺎﻃﺮ اﻟﺘﻲ ﺗﺤﻴﻖ ﺑﻨﺎ ﰲ اﻟﻌﺎﻟﻢ‪.‬‬
‫ﰲ ﻋﺎﻟ ٍﻢ ﻣُﺘﺼﻞ ﺑﺎﻟﺸﺒﻜﺎت‪ ،‬ﻳﻤﻜﻦ اﺧﱰاق أي ﺟﻬﺎز إﻟﻜﱰوﻧﻲ أو ﺑﺮﻧﺎﻣﺞ واﺧﱰاﻗﻪ واﻟﺘﻼﻋُ ﺐ ﺑﻪ ﻣﻦ‬
‫ﻗﺒﻞ أﺷﺨﺎص ﻟﺪﻳﻬﻢ ﻧﻮاﻳﺎ ﺧﺒﻴﺜﺔ‪.‬‬

‫ﻛﺬﻟﻚ ﻳُﺜري ﺗﻤﺘﱡﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‪ ،‬ﻻ ﺳﻴﻤﺎ إذا ﻛﺎﻧﺖ ﺗﺤ ﱡﻞ ﻣﺤ ﱠﻞ‬
‫ً‬
‫أﻫﻤﻴﺔ ﻣﻊ ﻣﺮور اﻟﻮﻗﺖ‪ :‬أﻻ وﻫﻲ‬
‫اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﺒﴩﻳﺔ‪ ،‬ﻣﺸﻜﻠﺔ أﺧﻼﻗﻴﺔ أﺧﺮى ﺗﺰداد‬
‫املﺴﺌﻮﻟﻴﺔ‪ .‬وﻫﺬا ﻫﻮ ﻣﻮﺿﻮع اﻟﻔﺼﻞ اﻟﻘﺎدم‪.‬‬

‫‪77‬‬

‫اﻟﻔﺼﻞ اﻟﺜﺎﻣﻦ‬

‫ُ‬
‫ﻻﻣﺴﺌﻮﻟﻴﺔ اﻵﻻت واﻟﻘﺮارات ﻏﲑ اﳌُﱪرة‬
‫ﻛﻴﻒ ﻳﻤﻜﻦ أن ﻧﺴﻨﺪ املﺴﺌﻮﻟﻴﺔ اﻷﺧﻼﻗﻴﺔ وﻣﺎ اﻟﻜﻴﻔﻴﺔ اﻟﻮاﺟﺒﺔ ﻟﺬﻟﻚ؟‬
‫ﻧﻮاﺟ ُﻪ‬
‫ﻋﻨﺪ اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻻﺗﺨﺎذ ﻗﺮارات وﻟﻠﻘﻴﺎم ﺑﺄﺷﻴﺎء ﺑﺎﻟﻨﻴﺎﺑﺔ ﻋﻨﱠﺎ‪ ،‬ﻓﺈﻧﻨﺎ ِ‬
‫ً‬
‫أﻫﻤﻴﺔ ﻋﻨﺪﻣﺎ ﻳُﻤ ﱢﻜﻨﻨﺎ‬
‫ﻣﺸﻜﻠﺔ ﻣﺸﱰﻛﺔ ﰲ ﺟﻤﻴﻊ ﺗﻘﻨﻴﺎت اﻷﺗﻤﺘﺔ‪ ،‬ﻏري أن ﻫﺬه املﺸﻜﻠﺔ ﺗﺰداد‬
‫ري ﻣﻤﺎ ﻛﻨﺎ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ ﺗﻔﻮﻳﺾ املﺰﻳﺪ واملﺰﻳﺪ ﻣﻦ اﻟﻘﺮارات إﱃ اﻵﻻت أﻛﺜﺮ ﺑﻜﺜ ٍ‬
‫ﻧﻔﻌﻞ ﰲ املﺎﴈ‪ :‬وﻫﺬه املﺸﻜﻠﺔ ﻫﻲ إﺳﻨﺎد املﺴﺌﻮﻟﻴﺔ‪ 1 .‬إذا ﻣُﻨﺢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻛﺎﻟﺔ‬
‫أﻛﱪ وأﺧﺬ ﻋﲆ ﻋﺎﺗﻘﻪ ﻣﺎ ﻛﺎن ﱠ‬
‫ﻳﺘﻮﻻه اﻟﺒﴩ ﰲ املﺎﴈ‪ ،‬ﻓﻜﻴﻒ ﻧُﺴﻨﺪ املﺴﺌﻮﻟﻴﺔ اﻷﺧﻼﻗﻴﺔ ﻋﻦ‬
‫أﻓﻌﺎﻟﻪ؟ ﻣَ ﻦ املﺴﺌﻮل ﻋﻦ اﻷﴐار واﻟﻔﻮاﺋﺪ اﻟﺘﻲ ﺗﻨﺸﺄ ﻋﻦ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻋﻨﺪﻣﺎ ﻳﻔﻮض اﻟﺒﴩ‬
‫ﱡ‬
‫ﻳﺨﺺ املﺨﺎﻃﺮ ﺗﺤﺪﻳﺪًا‪ :‬ﻣَ ﻦ املﺴﺌﻮل ﻋﻨﺪ‬
‫اﻟﻮﻛﺎﻟﺔ واﻟﻘﺮارات إﱃ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ وﻓﻴﻤﺎ‬
‫ﺣﺪوث ﺧﻄﺄ ﻣﺎ؟‬
‫ﻋﻨﺪﻣﺎ ﻳﻘﻮم َ‬
‫اﻟﺒﴩ ﺑﺄداء ﻣﻬﺎ ﱠم واﺗﺨﺎذ ﻗﺮارات‪ ،‬ﻓﻨﺤﻦ ﻋﺎد ًة ﻣﺎ ﻧﺮﺑﻂ اﻟﻮﻛﺎﻟﺔ ﺑﺎملﺴﺌﻮﻟﻴﺔ‬
‫اﻷﺧﻼﻗﻴﺔ‪ .‬ﻓﺄﻧﺖ ﻣﺴﺌﻮل ﻋﻤﺎ ﺗﻔﻌﻠﻪ وﻋﻦ اﻟﻘﺮارات اﻟﺘﻲ ﺗﺘﱠﺨﺬﻫﺎ‪ .‬وإذا ﻛﺎن ﻟﺪَﻳﻚ ﺗﺄﺛري ﻋﲆ‬
‫اﻟﻌﺎﻟﻢ وﻋﲆ اﻵﺧﺮﻳﻦ‪ ،‬ﻓﺄﻧﺖ ﻣﺴﺌﻮل ﻋﻦ ﻋﻮاﻗﺐ أﻓﻌﺎﻟﻚ‪ً .‬‬
‫وﻓﻘﺎ ﻷرﺳﻄﻮ‪ ،‬ﻫﺬا ﻫﻮ اﻟﴩط اﻷول‬
‫ﻟﻠﻤﺴﺌﻮﻟﻴﺔ اﻷﺧﻼﻗﻴﺔ‪ ،‬املﻌﺮوف ﺑﺎﺳﻢ اﻟﴩط اﻟﺘﺤ ﱡﻜﻤﻲ‪ :‬ﰲ اﻷﺧﻼﻗﻴﺎت اﻟﻨﻴﻘﻮﻣﺎﺧﻴﺔ‪ ،‬ﻳﻘﻮل‬
‫أرﺳﻄﻮ إن اﻟﻔﻌﻞ ﻳﺠﺐ أن ﻳﻨﺸﺄ ﻣﻦ اﻟﻔﺎﻋﻞ‪ .‬وﻟﻬﺬا اﻟﺮأي ً‬
‫أﻳﻀﺎ ﺟﺎﻧﺐ ﺗﻘﻴﻴﻤﻲ‪ :‬إذا ﻛﺎن‬
‫ﻟﺪﻳﻚ وﻛﺎﻟﺔ وإذا َ‬
‫ﻛﻨﺖ ﻗﺎد ًرا ﻋﲆ اﺗﺨﺎذ ﻗﺮارات‪ ،‬ﻓﻴﻨﺒﻐﻲ أن ﺗﺘﺤﻤﱠ ﻞ املﺴﺌﻮﻟﻴﺔ ﻋﻦ أﻓﻌﺎﻟﻚ‪.‬‬
‫وﻣﺎ ﻧﺮﻳﺪ ﺗﺠﻨﱡﺒﻪ ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ ﻫﻮ أن ﻳُﻮﺟَ ﺪ ﺷﺨﺺ ﻳﺘﻤﺘﻊ ﺑﺎﻟﻮﻛﺎﻟﺔ واﻟﻘﺪرة وﻟﻜﻨﻪ‬
‫ﻃﺎ َ‬
‫أﻳﻀﺎ ﴍ ً‬
‫ﻻ ﻳﺘﺤﻤﻞ املﺴﺌﻮﻟﻴﺔ‪ .‬أﺿﺎف أرﺳﻄﻮ ً‬
‫آﺧﺮ ﻓﻴﻤﺎ ﻳﺨﺺ املﺴﺌﻮﻟﻴﺔ اﻷﺧﻼﻗﻴﺔ‪ :‬أﻧﺖ‬
‫ﻣﺴﺌﻮل إذا ﻛﻨﺖ ﺗﻌﻠﻢ ﻣﺎ ﺗﻔﻌﻠﻪ‪ .‬وﻫﺬا ﴍط إدراﻛﻲ‪ :‬ﻳﺠﺐ أن ﺗﻜﻮن واﻋﻴًﺎ ﺑﻤﺎ ﺗﻔﻌﻞ وﻋﲆ‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫دراﻳﺔ ﺑﻌﻮاﻗﺒﻪ املﺤﺘﻤﻠﺔ‪ .‬وﻣﺎ ﻧﺤﺘﺎج إﱃ ﺗﺠﻨﱡﺒﻪ ﻫﻨﺎ ﻫﻮ ﺷﺨﺺ ﺗﺼﺪُر ﻋﻨﻪ أﻓﻌﺎل ﻻ ﻳﺪري‬
‫ﻣﺎﻫﻴﺘﻬﺎ‪ ،‬وﻫﻮ ﻣﺎ ﻗﺪ ﻳﺆدي ﰲ اﻟﻨﻬﺎﻳﺔ إﱃ ﻋﻮاﻗﺐ وﺧﻴﻤﺔ‪.‬‬
‫ً‬
‫وﻛﺎﻟﺔ أﻛﱪ وأﺧﺬ ﻋﲆ ﻋﺎﺗِﻘِ ﻪ ﻣﺎ ﻛﺎن ﱠ‬
‫ﻳﺘﻮﻻه اﻟﺒﴩ ﰲ املﺎﴈ‪ ،‬ﻓﻜﻴﻒ ﻧُﺴﻨﺪ‬
‫إذا ُﻣﻨِﺢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫املﺴﺌﻮﻟﻴﺔ اﻷﺧﻼﻗﻴﺔ ﻋﻦ أﻓﻌﺎﻟﻪ؟‬

‫ﱠ‬
‫ﺗﺘﺤﻘﻖ ﻫﺬه اﻟﴩوط ﻋﻨﺪ ﺗﻔﻮﻳﺾ اﻟﻘﺮارات واﻷﻋﻤﺎل إﱃ اﻟﺬﻛﺎء‬
‫اﻵن دﻋﻮﻧﺎ ﻧﺮى ﻫﻞ‬
‫ً‬
‫ٍ‬
‫أﻓﻌﺎﻻ‬
‫ﻗﺮارات وﻳﺆدي‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ .‬املﺸﻜﻠﺔ اﻷوﱃ ﻫﻲ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻤﻜﻦ أن ﻳﺘﱠﺨﺬ‬
‫ﻟﻬﺎ ﻋﻮاﻗﺐ أﺧﻼﻗﻴﺔ‪ ،‬وﻟﻜﻨﻪ ﻻ ﻳُﺪرك ﻣﺎ ﻳﻔﻌﻠﻪ وﻏري ﻗﺎدر ﻋﲆ اﻟﺘﻔﻜري اﻷﺧﻼﻗﻲ وﺑﺎﻟﺘﺎﱄ ﻻ‬
‫ً‬
‫ﻣﺴﺌﻮﻻ ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ ﻋﻤﺎ ﻳﻔﻌﻠﻪ‪ .‬ﻳﻤﻜﻦ أن ﺗﺘﻤﺘﱠﻊ اﻵﻻت ﺑﺎﻟﻮﻛﺎﻟﺔ‬
‫ﻳُﻤﻜﻦ اﻋﺘﺒﺎره‬
‫وﻟﻜﻦ ﻟﻴﺲ ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ؛ ﻷﻧﻬﺎ ﺗﻔﺘﻘِ ﺮ إﱃ اﻟﻮﻋﻲ واﻹرادة اﻟﺤﺮة واﻟﻌﻮاﻃﻒ ُ‬
‫واﻟﻘﺪرة‬
‫ﻋﲆ ﺗﻜﻮﻳﻦ اﻟﻨﻮاﻳﺎ وﻣﺎ ﺷﺎﺑَ َﻪ ذﻟﻚ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ً ،‬‬
‫وﻓﻘﺎ ﻟﺮؤﻳﺔ أرﺳﻄﻮ‪ ،‬ﻳﻤﻜﻦ ﻟﻠﺒﴩ‬
‫ﻓﻘﻂ أداء اﻷﻓﻌﺎل اﻟﺘﻄﻮﱡﻋﻴﺔ واﻟﺘﻔﻜري ﰲ أﻓﻌﺎﻟﻬﻢ‪ .‬إذا ﻛﺎن ﻫﺬا ﺻﺤﻴﺤً ﺎ‪ ،‬ﻓﺈن اﻟﺤ ﱠﻞ اﻟﻮﺣﻴﺪ‬
‫ﻫﻮ ﺟﻌْ ﻞ اﻟﺒﴩ ﻣﺴﺌﻮﻟني ﻋﻤﺎ ﺗﻔﻌﻠﻪ اﻵﻟﺔ‪ .‬وﻣﻦ ﺛَﻢ ﻓﺈن اﻟﺒﴩ ﻳُﻔﻮﱢﺿﻮن اﻟﻮﻛﺎﻟﺔ إﱃ اﻵﻟﺔ‪،‬‬
‫وﻟﻜﻨﻬﻢ ﻳﺤﺘﻔِ ﻈﻮن ﺑﺎملﺴﺌﻮﻟﻴﺔ‪ .‬وﻧﺤﻦ ﻧﻔﻌﻞ ذﻟﻚ ﺑﺎﻟﻔﻌﻞ ﰲ أﻧﻈﻤﺘﻨﺎ اﻟﻘﺎﻧﻮﻧﻴﺔ؛ إذ إﻧﻨﺎ ﻻ‬
‫ﻌﺘﱪ اﻟﻜﻼب أو اﻷﻃﻔﺎل اﻟﺼﻐﺎر ﻣﺴﺌﻮﻟني ﻋﻦ أﻓﻌﺎﻟﻬﻢ‪ ،‬وﻟﻜﻨﻨﺎ ﻧﻀﻊ املﺴﺌﻮﻟﻴﺔ اﻟﻘﺎﻧﻮﻧﻴﺔ‬
‫ﻧَ ِ‬
‫ً‬
‫ٍ‬
‫ﺷﺨﺺ ﻣﺎ‬
‫ﻣﻬﻤﺔ ﻣﻌﻴﻨﺔ إﱃ‬
‫ﻣﺆﺳﺴﺔ ﻣﺎ‪ ،‬ﻗﺪ ﻧُﻔﻮﱢض‬
‫ﻋﲆ ﻋﺎﺗﻖ ﻣَ ﻦ ﻳﺘﻮ ﱠﻟﻮن رﻋﺎﻳﺘﻬﻢ‪ .‬وﰲ‬
‫ٍ‬
‫وﻟﻜﻨﻨﺎ ﻧُﺤﻤﱢ ﻞ املﺴﺌﻮﻟﻴﺔ ﻟﻠﻤﺪﻳﺮ املﺴﺌﻮل ﻋﻦ املﴩوع اﻟﻌﺎم‪ ،‬ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن اﻟﺸﺨﺺ‬
‫ا ُملﻔﻮض ﰲ ﻫﺬه اﻟﺤﺎﻟﺔ ﻳﺘﺤﻤﱠ ﻞ ﺟﺰءًا ﻣﻦ املﺴﺌﻮﻟﻴﺔ‪ 2 .‬إذَن ملﺎذا ﻻ ﻧﺴﻤﺢ ﻟﻶﻟﺔ ﺑﺄداء اﻷﻋﻤﺎل‬
‫وﻧﺤﺘﻔﻆ ﺑﺎملﺴﺌﻮﻟﻴﺔ ﻋﲆ اﻟﺠﺎﻧﺐ اﻟﺒﴩي؟ ﻳﺒﺪو أن ﻫﺬه ﻫﻲ أﻓﻀﻞ وﺳﻴﻠﺔ ﻧﻤﴤ ﺑﻬﺎ ﻗﺪﻣً ﺎ‪،‬‬
‫ﺣﻴﺚ إن اﻟﺨﻮارزﻣﻴﺎت واﻵﻻت ﺑﻼ ﻣﺴﺌﻮﻟﻴﺔ‪.‬‬
‫ﻳﻮاﺟﻪ ﻫﺬا اﻟﺤ ﱡﻞ ﻋﺪة ﻣﺸﻜﻼت ﰲ ﺣﺎﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ً .‬‬
‫أوﻻ‪ ،‬ﻳﻤﻜﻦ‬
‫وﻣﻊ ذﻟﻚ‪،‬‬
‫ِ‬
‫ﱠ‬
‫ﻟﻠﻨﻈﺎم املﺰوﱠد ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن ﻳﺘﺨﺬ ﻗﺮاراﺗﻪ وﻳﺆدي أﻓﻌﺎﻟﻪ ﺑﴪﻋﺔ ﻛﺒرية ﻟﻠﻐﺎﻳﺔ‪،‬‬
‫ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ اﻟﺘﺪاول اﻟﻌﺎﱄ اﻟﱰدﱡد أو ﰲ اﻟﺴﻴﺎرات اﻟﺬاﺗﻴﺔ اﻟﻘﻴﺎدة‪ ،‬ﻣﻤﺎ ﻳﺤﺮم‬
‫ﱡ‬
‫اﻟﺘﺪﺧﻞ ﰲ اﻟﻔﻌﻞ‪ .‬ﻓﻜﻴﻒ ﻳُﻤﻜﻦ ﻟﻠﺒﴩ‬
‫اﻹﻧﺴﺎن ﻣﻦ اﻟﻮﻗﺖ اﻟﻜﺎﰲ ﻻﺗﺨﺎذ اﻟﻘﺮار اﻟﻨﻬﺎﺋﻲ أو‬
‫أن ﻳﺘﺤﻤﱠ ﻠﻮا املﺴﺌﻮﻟﻴﺔ ﻋﻦ ﻣﺜﻞ ﻫﺬه اﻷﻓﻌﺎل واﻟﻘﺮارات؟ ﺛﺎﻧﻴًﺎ‪ ،‬ﻷﻧﻈﻤﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺗﻄﺒﻴﻖ ﻣُﻌني‪ ،‬ﻓﺮﺑﻤﺎ ﻳُﺼﺒﺢ ﻣﻦ‬
‫ﺗﻮارﻳﺦ‪ .‬ﻋﻨﺪﻣﺎ ﻳﻘﻮم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺄﺷﻴﺎء ﰲ ﺳﻴﺎق‬
‫ٍ‬
‫‪80‬‬

‫ُ‬
‫ﻻﻣﺴﺌﻮﻟﻴﺔ اﻵﻻت واﻟﻘﺮارات ﻏري ا ُملﱪرة‬

‫ﻏري اﻟﻮاﺿﺢ ﻣَ ﻦ أﻧﺸﺄه‪ ،‬وﻣَ ﻦ اﺳﺘﺨﺪﻣﻪ ً‬
‫أوﻻ‪ ،‬واﻟﻜﻴﻔﻴﺔ اﻟﺘﻲ ﻳﺠﺐ ﺑﻬﺎ ﺗﻮزﻳﻊ املﺴﺌﻮﻟﻴﺔ ﺑني‬
‫ﻫﺬه اﻷﻃﺮاف املﺨﺘﻠﻔﺔ املﻌﻨﻴﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﺣﺎﻟﺔ إﻧﺸﺎء ﺧﻮارزﻣﻴﺔ ذﻛﺎء اﺻﻄﻨﺎﻋﻲ‬
‫ﻣﴩوع ﻋﻠﻤﻲ ﰲ اﻟﺠﺎﻣﻌﺔ‪ ،‬ﺛﻢ ﺗﻄﺒﻴﻖ ﻫﺬه اﻟﺨﻮارزﻣﻴﺔ ﻟﻠﻤﺮة اﻷوﱃ ﰲ ا ُملﺨﺘﱪ ﰲ‬
‫ﰲ ﺳﻴﺎق‬
‫ٍ‬
‫اﻟﺠﺎﻣﻌﺔ‪ ،‬ﺛﻢ ﰲ ﻗﻄﺎع اﻟﺮﻋﺎﻳﺔ اﻟﺼﺤﻴﺔ‪ ،‬وﰲ ٍ‬
‫ﺳﻴﺎق ﻋﺴﻜﺮي‪ .‬ﻓﻤَ ﻦ ﻳﺘﺤﻤﱠ ﻞ‬
‫وﻗﺖ ﻻﺣﻖ ﰲ‬
‫ٍ‬
‫املﺴﺌﻮﻟﻴﺔ؟ ﻗﺪ ﻳﻜﻮن ﻣﻦ اﻟﺼﻌﺐ ﺗﺘﺒﱡﻊ ﺟﻤﻴﻊ اﻟﺒﴩ املﺘﻮ ﱢرﻃني ﰲ ﺗﺎرﻳﺦ ﻫﺬه اﻟﺨﻮارزﻣﻴﺔ‬
‫ٍ‬
‫ﻧﺘﻴﺠﺔ ﻣﻌﻴﱠﻨﺔ ﺗَ ِ‬
‫ﺤﻤﻞ إﺷﻜﺎﻟﻴﺔ أﺧﻼﻗﻴﺔ‪ .‬ﻓﻨﺤﻦ ﻻ‬
‫ﺑﺎﻟﺬات‪ ،‬ﺑﻞ ﰲ اﻟﺘﺎرﻳﺦ اﻟﺴﺒﺒﻲ اﻟﺬي أدﱠى إﱃ‬
‫ﻧﻌﺮف داﺋﻤً ﺎ ﺟﻤﻴﻊ اﻷﺷﺨﺎص ا َملﻌﻨﻴﱢني ﰲ اﻟﻠﺤﻈﺔ اﻟﺘﻲ ﺗُﺜﺎر ﻓﻴﻬﺎ ﻣﺸﻜﻠﺔ ﺗﺘﻌﻠﻖ ﺑﺎملﺴﺌﻮﻟﻴﺔ‪.‬‬
‫ﻓﺨﻮارزﻣﻴﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﻜﻮن ﻟﻬﺎ ﺗﺎرﻳﺦ ﻃﻮﻳﻞ ﻳﺸﺎرك ﻓﻴﻪ اﻟﻌﺪﻳﺪ ﻣﻦ‬
‫اﻷﺷﺨﺎص‪ .‬وﻫﺬا ﻳُﻔﴤ ﺑﻨﺎ إﱃ ﻣﺸﻜﻠﺔ ﻧﻤﻄﻴﺔ ﰲ إﺳﻨﺎد املﺴﺌﻮﻟﻴﺔ ﻋﻦ اﻷﻓﻌﺎل اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ؛‬
‫إذ ﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﻜﻮن ﻫﻨﺎك اﻟﻜﺜري ﻣﻦ اﻷﻃﺮاف وﻳُﻤﻜﻨﻨﻲ أن أﺿﻴﻒ‪ ،‬اﻷﺷﻴﺎء‪.‬‬
‫ﻫﻨﺎك اﻟﻜﺜري ﻣﻦ اﻷﻃﺮاف ﺑﻤﻌﻨﻰ أن اﻟﻜﺜري ﻣﻦ اﻷﺷﺨﺎص ﻳﺸﺎرﻛﻮن ﰲ اﻟﻔﻌﻞ‬
‫اﻟﺘﻜﻨﻮﻟﻮﺟﻲ‪ .‬ﰲ ﺣﺎﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻳﺒﺪأ اﻷﻣﺮ ﺑﺎملﱪﻣﺞ‪ ،‬وﻟﻜﻦ ﻟﺪﻳﻨﺎ ً‬
‫أﻳﻀﺎ املﺴﺘﺨﺪم‬
‫اﻟﻨﻬﺎﺋﻲ وآﺧﺮون‪ .‬دﻋﻮﻧﺎ ﻧﻔﻜﺮ ً‬
‫ﻣﺜﻼ ﰲ اﻟﺴﻴﺎرة اﻟﺬاﺗﻴﺔ اﻟﻘﻴﺎدة‪ :‬ﻫﻨﺎك املﱪﻣﺞ‪ ،‬وﻣُﺴﺘﺨ ِﺪ ُم‬
‫اﻟﺴﻴﺎرة‪ ،‬وأﺻﺤﺎبُ ﴍﻛﺔ اﻟﺴﻴﺎرات‪ ،‬واملﺴﺘﺨﺪِﻣﻮن اﻵﺧﺮون ﻟﻠﻄﺮﻳﻖ‪ ،‬وﻫﻜﺬا‪ .‬ﰲ ﻣﺎرس‬
‫ٍ‬
‫ﺣﺎدث ﰲ أرﻳﺰوﻧﺎ أدﱠى إﱃ وﻓﺎة أﺣﺪ‬
‫‪ ،٢٠١٨‬ﺗﺴﺒﱠﺒَﺖ ﺳﻴﺎرة ذاﺗﻴﺔ اﻟﻘﻴﺎدة ﻟﴩﻛﺔ أوﺑﺮ ﰲ‬
‫ا ُملﺸﺎة‪ .‬ﻓﻤَ ﻦ املﺴﺌﻮل ﻋﻦ ﻫﺬه اﻟﻨﺘﻴﺠﺔ املﺄﺳﺎوﻳﺔ؟ ﻳﻤﻜﻦ أن ﻳﻜﻮن املﺴﺌﻮﻟﻮن ﻫﻢ ﻣَ ﻦ‬
‫ﺑﺮﻣﺠﻮا اﻟﺴﻴﺎرة‪ ،‬واﻷﺷﺨﺎص املﺴﺌﻮﻟني ﻋﻦ ﺗﻄﻮﻳﺮ املﻨﺘﺞ ﰲ اﻟﴩﻛﺔ‪ ،‬وﴍﻛﺔ أوﺑﺮ ﻧﻔﺴﻬﺎ‪،‬‬
‫وﻣﺴﺘﺨﺪم اﻟﺴﻴﺎرة‪ ،‬واﻟﺸﺨﺺ اﻟﺴﺎﺋﺮ‪ ،‬واملﴩع )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬وﻻﻳﺔ أرﻳﺰوﻧﺎ(‪ ،‬وﻫﻜﺬا‪.‬‬
‫إذَن ﻓﻠﻴﺲ ﻣﻦ اﻟﻮاﺿﺢ ﻋﲆ ﻣَ ﻦ ﺗﻘﻊ املﺴﺌﻮﻟﻴﺔ‪ .‬ﻗﺪ ﻳﻜﻮن اﻷﻣﺮ ﻫﻮ أن املﺴﺌﻮﻟﻴﺔ ﻻ ﻳﻤﻜﻦ‬
‫ﺷﺨﺺ واﺣﺪ؛ ورﺑﻤﺎ ﺗﻘﻊ ﻋﲆ أﻛﺜﺮ ﻣﻦ ﺷﺨﺺ‪ .‬وﻟﻜﻦ ﻫﺬا ﻳﻌﻨﻲ‬
‫وﻻ ﻳﺠﺐ إﺳﻨﺎدﻫﺎ إﱃ‬
‫ٍ‬
‫أﻧﻪ ﻟﻴﺲ ﻣﻦ اﻟﻮاﺿﺢ ﻛﻴﻔﻴﺔ ﺗﻮزﻳﻊ املﺴﺌﻮﻟﻴﺔ‪ .‬ﻓﻘﺪ ﺗﻘﻊ املﺴﺌﻮﻟﻴﺔ ﻋﲆ ﺑﻌﻀﻬﻢ أﻛﺜﺮ ﻣﻦ‬
‫اﻵﺧﺮﻳﻦ‪.‬‬
‫ﻫﻨﺎك ً‬
‫أﻳﻀﺎ اﻟﻜﺜري ﻣﻦ اﻷﺷﻴﺎء‪ ،‬ﺑﻤﻌﻨﻰ أن اﻟﻨﻈﺎم اﻟﺘﻜﻨﻮﻟﻮﺟﻲ ﻳﺘﺄﻟﻒ ﻣﻦ اﻟﻌﺪﻳﺪ‬
‫ﻣﻦ اﻟﻌﻨﺎﴏ املﺘﺼﻠﺔ؛ وﻋﺎد ًة ﻣﺎ ﻳﻜﻮن ﻫﻨﺎك اﻟﻌﺪﻳﺪ ﻣﻦ املﻜﻮﻧﺎت اﻟﺘﻲ ﺗﺪﺧﻞ ﰲ اﻟﻨﻈﺎم‪.‬‬
‫ﻫﻨﺎك ﺧﻮارزﻣﻴﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻟﻜﻦ ﻫﺬه اﻟﺨﻮارزﻣﻴﺔ ﺗﺘﻔﺎﻋﻞ ﻣﻊ أﺟﻬﺰة اﺳﺘﺸﻌﺎر‪،‬‬
‫وﺗﺴﺘﺨﺪم ﺟﻤﻴﻊ أﻧﻮاع اﻟﺒﻴﺎﻧﺎت‪ ،‬وﺗﺘﻔﺎﻋﻞ ﻣﻊ ﺟﻤﻴﻊ أﻧﻮاع املﻜﻮﻧﺎت املﺎدﻳﺔ واﻟﱪﻣﺠﻴﺔ‪.‬‬
‫ﻛﻞ ﻫﺬه اﻷﺷﻴﺎء ﻟﻬﺎ ﺗﺎرﻳﺨﻬﺎ وﻣﺘﺼﻠﺔ ﺑﺎﻷﺷﺨﺎص اﻟﺬﻳﻦ ﺑﺮﻣﺠﻮﻫﺎ أو أﻧﺘﺠﻮﻫﺎ‪ .‬وﻋﻨﺪﻣﺎ‬
‫ﻳﺤﺪُث ﺧﻄﺄ‪ ،‬ﻻ ﻳﻜﻮن واﺿﺤً ﺎ ﻟﻨﺎ ﺑﺎﻟﴬورة ﻣﺎ إذا ﻛﺎن »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ« ﻫﻮ اﻟﺬي‬
‫‪81‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺳﺒﱠﺐ املﺸﻜﻠﺔ أم ﻣُﻜﻮﱢن َ‬
‫آﺧﺮ ﻣﻦ ﻣﻜﻮﻧﺎت اﻟﻨﻈﺎم؛ ﺑﻞ إﻧﻨﺎ ﻻ ﻧﻌﺮف ﺑﺎﻟﴬورة أﻳﻦ ﺗﻨﺘﻬﻲ‬
‫ﻣﺴﺌﻮﻟﻴﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺗﺒﺪأ ﻣﺴﺌﻮﻟﻴﺔ ﺑﻘﻴﺔ املﻜﻮﻧﺎت اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‪ .‬وﻫﺬا ﻳﺠﻌﻞ ﻣﻦ‬
‫اﻟﺼﻌﺐ إﺳﻨﺎد املﺴﺌﻮﻟﻴﺔ وﺗﻮزﻳﻌﻬﺎ‪ .‬دﻋﻮﻧﺎ ﻧﻔﻜﺮ ً‬
‫أﻳﻀﺎ ﰲ ﺗﻌﻠﻢ اﻵﻟﺔ وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت‪ :‬ﻛﻤﺎ‬
‫رأﻳﻨﺎ‪ ،‬ﻟﻴﺲ ﻫﻨﺎك ﻓﻘﻂ ﺧﻮارزﻣﻴﺔ‪ ،‬وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﻋﻤﻠﻴﺔ ﺗﺸﻤﻞ ﻣﺮاﺣﻞ ﻣﺨﺘﻠﻔﺔ ﻣﺜﻞ ﺟﻤﻊ‬
‫اﻟﺒﻴﺎﻧﺎت وﻣﻌﺎﻟﺠﺘﻬﺎ‪ ،‬وﺗﺪرﻳﺐ اﻟﺨﻮارزﻣﻴﺔ‪ ،‬وﻫﻜﺬا؛ وﺟﻤﻴﻊ ﻫﺬه املﺮاﺣﻞ ﻳﺪﺧﻞ ﻓﻴﻬﺎ ﻋﻨﺎﴏ‬
‫ﺗﻘﻨﻴﺔ ﻣﺨﺘﻠﻔﺔ وﺗﺘﻄ ﱠﻠﺐ ﻗﺮارات ﺑﴩﻳﺔ‪ .‬ﻣﺮة أﺧﺮى‪ ،‬ﻫﻨﺎك ﺗﺎرﻳﺦ ﺳﺒﺒﻲ ﻳﺸﱰك ﻓﻴﻪ اﻟﻜﺜري‬
‫ﻣﻦ اﻟﺒﴩ واﻷﺟﺰاء‪ ،‬وﻫﺬا ﻳﺠﻌﻞ إﺳﻨﺎد املﺴﺌﻮﻟﻴﺔ أﻣ ًﺮا ﺻﻌﺒًﺎ‪.‬‬
‫ﻟﻜﻲ ﻧُﺤﺎول اﻟﺘﻌﺎﻣﻞ ﻣﻊ ﻫﺬه اﻟﻘﻀﺎﻳﺎ‪ ،‬ﻳﻤﻜﻨﻨﺎ أن ﻧﺘﻌﻠﻢ ﻣﻦ اﻷﻧﻈﻤﺔ اﻟﻘﺎﻧﻮﻧﻴﺔ أو‬
‫ﻧﻠﻘﻲ ﻧﻈﺮة ﻋﲆ ﻛﻴﻔﻴﺔ ﻋﻤﻞ اﻟﺘﺄﻣني؛ وﺳﻮف أﺗﺤﺪﱠث ﻋﻦ ﺑﻌﺾ املﻔﺎﻫﻴﻢ اﻟﻘﺎﻧﻮﻧﻴﺔ ﰲ‬
‫اﻟﻔﺼﻮل املﺘﻌﻠﻘﺔ ﺑﺎﻟﺴﻴﺎﺳﺔ‪ .‬وﻟﻜﻦ ﺛﻤﱠ ﺔ أﺳﺌﻠﺔ أﻛﺜﺮ ﻋﻤﻮﻣﻴﺔ ﺗﻠﻮح ﻟﻨﺎ ﻣﻦ وراء ﻫﺬه اﻷﻧﻈﻤﺔ‬
‫اﻟﻘﺎﻧﻮﻧﻴﺔ وأﻧﻈﻤﺔ اﻟﺘﺄﻣني ﺣﻮل وﻛﺎﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واملﺴﺌﻮﻟﻴﺔ ﻋﻨﻪ‪ :‬إﱃ أي ﻣﺪى‬
‫ﻧﺮﻳﺪ أن ﻧﻌﺘﻤﺪ ﻋﲆ ﺗﻘﻨﻴﺔ اﻷﺗﻤﺘﺔ‪ ،‬وﻫﻞ ﻳُﻤﻜﻨﻨﺎ أن ﻧﺘﺤﻤﻞ املﺴﺌﻮﻟﻴﺔ ﻋﻤﺎ ﻳﻘﻮم ﺑﻪ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻛﻴﻒ ﻳُﻤﻜﻨﻨﺎ إﺳﻨﺎد املﺴﺌﻮﻟﻴﺎت وﺗﻮزﻳﻌﻬﺎ؟ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﻔﻬﻮم اﻹﻫﻤﺎل‬
‫ﰲ اﻟﻘﺎﻧﻮن ﻳﺘﻌ ﱠﻠﻖ ﺑﻤﺎ إذا ﻛﺎن اﻟﺸﺨﺺ ﻗﺪ أدﱠى ﻣﺎ ﻋﻠﻴﻪ ﻣﻦ واﺟﺐ اﻟﻌﻨﺎﻳﺔ‪ .‬وﻟﻜﻦ ﻣﺎذا ﻳﻌﻨﻲ‬
‫ً‬
‫ﺧﺎﺻﺔ أﻧﻪ ﻣﻦ اﻟﺼﻌﺐ اﻟﺘﻨﺒﱡﺆ ﺑﺠﻤﻴﻊ اﻟﻌﻮاﻗﺐ‬
‫ﻫﺬا اﻟﻮاﺟﺐ ﰲ ﺣﺎﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫اﻷﺧﻼﻗﻴﺔ ا ُملﺤﺘﻤﻠﺔ؟‬
‫وﻫﺬا ﻳﻘﻮدﻧﺎ إﱃ اﻟﻘﻀﻴﺔ اﻟﺘﺎﻟﻴﺔ‪ .‬ﺣﺘﻰ إذا ﺗ ﱠﻢ ﺣ ﱡﻞ ﻣﺸﻜﻠﺔ اﻟﺘﺤﻜﻢ‪ ،‬ﻓﻬﻨﺎك اﻟﴩط‬
‫اﻟﺜﺎﻧﻲ ﻟﻠﻤﺴﺌﻮﻟﻴﺔ اﻷﺧﻼﻗﻴﺔ‪ ،‬واﻟﺬي ﻳﺘﻌ ﱠﻠﻖ ﺑﻤﺸﻜﻠﺔ املﻌﺮﻓﺔ‪ .‬ﻟﻜﻲ ﺗﺘﺤﻤﱠ ﻞ املﺴﺌﻮﻟﻴﺔ‪ ،‬ﻳﺠﺐ‬
‫أن ﺗﻌﺮف ﻣﺎ ﺗﻔﻌﻠﻪ واﻟﻨﺘﺎﺋﺞ ا ُملﱰﺗﱢﺒﺔ ﻋﲆ ﻓﻌﻠﻚ‪ ،‬وﻓﻴﻤﺎ ﺑﻌﺪ‪ ،‬ﺗﻌﺮف ﻣﺎ َ‬
‫ﻗﻤﺖ ﺑﻪ‪ .‬وﺑﺎﻹﺿﺎﻓﺔ‬
‫ﱠ‬
‫ﻧﺘﻮﻗﻊ أن ﻳﺘﻤﻜﻦ اﻟﺸﺨﺺ ﻣﻦ‬
‫إﱃ ذﻟﻚ‪ ،‬ﻫﺬه املﺴﺄﻟﺔ ﻟﻬﺎ ﺟﺎﻧﺐ ﴎدي‪ :‬ﰲ ﺣﺎﻟﺔ اﻟﺒﴩ‪،‬‬
‫ﴍح ﻣﺎ ﻗﺎم ﺑﻪ أو ﻗ ﱠﺮ َره‪ .‬املﺴﺌﻮﻟﻴﺔ إذَن ﺗﻌﻨﻲ اﻟﻘﺪرة ﻋﲆ اﻟﺮد واﻟﺘﻔﺴري‪ .‬ﻓﺈذا ﺣﺪث ﺧﻄﺄ‬
‫ﻣﺎ‪ ،‬ﻓﻨﺤﻦ ﻧﺮﻳﺪ ردٍّا وﺗﻔﺴريًا‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻧﻄﻠﺐ ﻣﻦ اﻟﻘﺎﴈ أن ﻳ ﱢ‬
‫ُﻔﴪ ﻗﺮاره‪ ،‬أو‬
‫ً‬
‫إﺷﻜﺎﻟﻴﺔ ﻟﻠﻐﺎﻳﺔ ﰲ ﺣﺎﻟﺔ اﻟﺬﻛﺎء‬
‫ﻧﺴﺄل اﻟﺠﺎﻧﻲ ملﺎذا ﻓﻌﻞ ﻣﺎ ﻓﻌ َﻠﻪ‪ .‬وﻫﺬه اﻟﴩوط ﺗُﺼﺒﺢ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ً .‬‬
‫أوﻻ‪ ،‬ﻣﻦ ﺣﻴﺚ املﺒﺪأ‪ ،‬ﻻ »ﻳﻌﺮف« اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﴐ ﻣﺎ‬
‫ﻳﻔﻌﻠﻪ‪ ،‬ﺑﻤﻌﻨﻰ أﻧﻪ ﻟﻴﺲ واﻋﻴًﺎ وﺑﺎﻟﺘﺎﱄ ﻻ ﻳﺪرك ﻣﺎ ﻳﻘﻮم ﺑﻪ وﻻ ﻳﺪرك ﻧﺘﺎﺋﺞ أﻓﻌﺎﻟﻪ‪ .‬ﻳﻤﻜﻨﻪ‬
‫ﺗﺨﺰﻳﻦ ﻣﺎ ﻳﻔﻌﻠﻪ وﺗﺴﺠﻴﻠﻪ‪ ،‬وﻟﻜﻨﻪ ﻻ »ﻳﻌﺮف ﻣﺎ ﻳﻘﻮم ﺑﻪ« ﻛﻤﺎ ﻳﻔﻌﻞ اﻟﺒﴩ‪ ،‬اﻟﺬﻳﻦ ﻳُﺪرﻛﻮن‪،‬‬
‫ﺑﻮﺻﻔﻬﻢ ﻛﺎﺋﻨﺎت واﻋﻴﺔ‪ ،‬ﻣﺎ ﻳﻔﻌﻠﻮن وﻳﻤﻜﻨﻬﻢ — ً‬
‫وﻓﻘﺎ ﻷرﺳﻄﻮ ﻣﺮة أﺧﺮى — اﻟﺘﻔﻜري‬
‫واﻟﺘﺄﻣﻞ ﰲ أﻓﻌﺎﻟﻬﻢ وﻋﻮاﻗﺐ ﺗﻠﻚ اﻷﻓﻌﺎل‪ .‬وﻋﻨﺪﻣﺎ ﻻ ﺗُﻠﺒﱠﻰ ﻫﺬه اﻟﴩوط ﰲ ﺣﺎﻟﺔ اﻟﺒﴩ‪ ،‬ﻋﲆ‬
‫‪82‬‬

‫ُ‬
‫ﻻﻣﺴﺌﻮﻟﻴﺔ اﻵﻻت واﻟﻘﺮارات ﻏري ا ُملﱪرة‬

‫ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﺣﺎﻟﺔ اﻷﻃﻔﺎل اﻟﺼﻐﺎر ﺟﺪٍّا‪ ،‬ﻓﺈﻧﻨﺎ ﻻ ﻧﺤﻤﱢ ﻠﻬﻢ املﺴﺌﻮﻟﻴﺔ‪ .‬وﻛﺬﻟﻚ ﻋﺎد ًة ﻣﺎ ﻻ‬
‫ﻧُﺤﻤﱢ ﻞ اﻟﺤﻴﻮاﻧﺎت املﺴﺌﻮﻟﻴﺔ ً‬
‫أﻳﻀﺎ‪ 3 .‬وإذا ﻟﻢ ﻳُﻠﺐﱢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﺬه اﻟﴩوط‪ ،‬ﻓﺈﻧﻨﺎ ﻻ‬
‫ﻧﺴﺘﻄﻴﻊ أن ﻧُﺤﻤﱢ ﻠﻪ املﺴﺌﻮﻟﻴﺔ‪ .‬واﻟﺤﻞ ﻣﺮة أﺧﺮى ﻫﻮ ﺗﺤﻤﻴﻞ املﺴﺌﻮﻟﻴﺔ ﻟﻠﺒﴩ ﻋﻦ أﻋﻤﺎل‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻋﲆ اﻓﱰاض أﻧﻬﻢ ﻳﻌﺮﻓﻮن ﻣﺎ ﻳﻘﻮم ﺑﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻣﺎ ﻳﻔﻌﻠﻮﻧﻪ‬
‫ﺑﺎﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ — وﺑﻤﺮاﻋﺎة اﻟﺠﺎﻧﺐ اﻟﴪدي — وأﻧﻬﻢ ﻗﺎدرون ﻋﲆ اﻟﺮ ﱢد ﻋﻦ‬
‫أﻓﻌﺎﻟﻪ وﻳُﻤﻜﻨﻬﻢ ﺗﻔﺴري ﻣﺎ ﻗﺎم ﺑﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻓﺈن ﻣﺪى ﺻﺤﺔ ﻫﺬا اﻻﻓﱰاض ﻟﻴﺲ أﻣ ًﺮا ﻣﻦ اﻟﺴﻬﻞ ﺗﻘﺮﻳﺮه ﻛﻤﺎ ﻗﺪ ﻳﺒﺪو‬
‫ﻟﻠﻮﻫﻠﺔ اﻷوﱃ‪ .‬ﻋﺎد ًة ﻣﺎ ﻳﻌﺮف املﱪﻣﺠﻮن واملﺴﺘﺨﺪﻣﻮن ﻣﺎ اﻟﺬي ﻳﺮﻏﺒﻮن ﰲ اﻟﻘﻴﺎم ﺑﻪ‬
‫ﺑﺎﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬أو ﱠ‬
‫ﺑﺪﻗﺔ أﻛﱪ‪ :‬ﻳﻌﺮﻓﻮن ﻣﺎ ﻳﺮﻳﺪون ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫أن ﻳﻔﻌﻠﻪ ﻟﻬﻢ‪ .‬إﻧﻬﻢ ﻳﻌﺮﻓﻮن اﻟﻬﺪف اﻟﻨﻬﺎﺋﻲ؛ وﻟﻬﺬا اﻟﺴﺒﺐ ﻳﻔﻮﱢﺿﻮن املﻬﻤﺔ إﱃ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻗﺪ ﻳﻜﻮﻧﻮن ً‬
‫ﺑﺸﻜﻞ ﻋﺎم‪ .‬وﻟﻜﻦ‪،‬‬
‫أﻳﻀﺎ ﻋﲆ دراﻳﺔ ﺑﻜﻴﻔﻴﺔ ﻋﻤﻞ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫ٍ‬
‫ﻛﻤﺎ ﺳﻨﺮى‪ ،‬ﻫﻢ ﻻ ﻳﻌﺮﻓﻮن ﱠ‬
‫ﺑﺪﻗ ٍﺔ داﺋﻤً ﺎ ﻣﺎ ﻳﻔﻌﻠﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ )ﰲ أي ﻟﺤﻈﺔ( وﻻ‬
‫ﻳُﻤﻜﻨﻬﻢ داﺋﻤً ﺎ ﺗﻔﺴري ﻣﺎ ﻓﻌﻠﻪ أو ﻛﻴﻒ وﺻﻞ إﱃ ﻗﺮاره‪.‬‬
‫اﻟﺸﻔﺎﻓﻴﺔ واﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري‬
‫ﻧﺤﻦ ﻧﻮاﺟﻪ ﻫﻨﺎ ﻣﺸﻜﻠﺔ اﻟﺸﻔﺎﻓﻴﺔ واﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري‪ .‬ﰲ ﺑﻌﺾ أﻧﻈﻤﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫ﺗﻜﻮن اﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻳﺴﺘﺨﺪﻣﻬﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻻﺗﺨﺎذ ﻗﺮاره واﺿﺤﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫إذا ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺴﺘﺨﺪم ﺷﺠﺮة اﺗﺨﺎذ اﻟﻘﺮارات‪ ،‬ﻓﺈن اﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻳﺼﻞ ﺑﻬﺎ‬
‫إﱃ ﻗﺮاره ﺗﻜﻮن واﺿﺤﺔ‪ .‬ﻓﻘﺪ ﺗﻤﱠ ﺖ ﺑﺮﻣﺠﺘُﻪ ﺑﻄﺮﻳﻘﺔ ﺗُﺤﺪﱢد اﻟﻘﺮار‪ ،‬ﺑﻨﺎءً ﻋﲆ ﻣﺪﺧﻼت‬
‫ﻣُﻌﻴﱠﻨﺔ‪ .‬وﺑﺎﻟﺘﺎﱄ ﻳﻤﻜﻦ ﻟﻠﺒﴩ ﺗﻔﺴري ﻛﻴﻒ وﺻﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ ﻗﺮاره‪ ،‬وﻳﻤﻜﻦ أن‬
‫»ﻧﻄﻠُﺐ« ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن »ﻳُﻔﴪ« ﻗﺮاره‪ .‬ﺑﻌﺪ ذﻟﻚ‪ ،‬ﻳﻤﻜﻦ ﻟﻠﺒﴩ ﺗﺤﻤﱡ ﻞ ﻣﺴﺌﻮﻟﻴﺔ‬
‫اﻟﻘﺮار أو‪ ،‬ﻋﲆ اﻷﺣﺮى‪ ،‬اﺗﺨﺎذ ﻗﺮار ﺑﻨﺎءً ﻋﲆ اﻟﺘﻮﺻﻴﺔ اﻟﺘﻲ ﻗﺪﱠﻣَ ﻬﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻣﻊ ﺑﻌﺾ أﻧﻈﻤﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻷﺧﺮى‪ ،‬وﻻ ﺳﻴﻤﺎ ﺗﻠﻚ اﻟﺘﻲ ﺗﺴﺘﺨﺪم ﺗﻌ ﱡﻠﻢ‬
‫اﻵﻟﺔ وﺧﺎﺻﺔ اﻟﺘﻌ ﱡﻠﻢ اﻟﻌﻤﻴﻖ اﻟﺬي ﻳﺴﺘﺨﺪم اﻟﺸﺒﻜﺎت اﻟﻌﺼﺒﻴﺔ‪ ،‬ﻟﻢ ﻳﻌُ ﺪ ﻣﻦ املﻤﻜﻦ ﻟﻺﻧﺴﺎن‬
‫ٍ‬
‫ﻗﺮارات ﻣﻦ ﻫﺬا اﻟﻨﻮع‪ .‬ﺣﻴﺚ ﻟﻢ ﻳﻌُ ﺪ واﺿﺤً ﺎ ﻛﻴﻒ ﻳﺼﻞ‬
‫ﺗﻘﺪﻳﻢ ﻫﺬا اﻟﺘﻔﺴري أو اﺗﺨﺎذ‬
‫اﻟﺬﻛﺎءُ اﻻﺻﻄﻨﺎﻋﻲ إﱃ ﻗﺮاره‪ ،‬وﺑﺎﻟﺘﺎﱄ ﻻ ﻳُﻤﻜﻦ َ‬
‫ﺑﺸﻜﻞ ﻛﺎﻣﻞ‪ .‬إﻧﻬﻢ‬
‫ﻟﻠﺒﴩ ﺗﻔﺴري اﻟﻘﺮار‬
‫ٍ‬
‫ﻗﺮار ﱠ‬
‫ﻣﻌني‪.‬‬
‫ﻳﻌﺮﻓﻮن ﻛﻴﻒ ﻳﻌﻤﻞ اﻟﻨﻈﺎم اﻟﺨﺎص ﺑﻬﻢ‪،‬‬
‫ٍ‬
‫ﺑﺸﻜﻞ ﻋﺎم‪ ،‬وﻟﻜﻦ ﻻ ﻳُﻤﻜﻨﻬﻢ ﺗﻔﺴري ٍ‬
‫وﻟﻨﴬب ً‬
‫ﻣﺜﻼ ﺑﻠﻌﺒﺔ اﻟﺸﻄﺮﻧﺞ املﺰودة ﺑﺎﻟﺘﻌ ﱡﻠﻢ اﻟﻌﻤﻴﻖ‪ :‬ﻳﻌﺮف املﱪﻣﺠﻮن ﻛﻴﻒ ﻳﻌﻤﻞ اﻟﺬﻛﺎء‬
‫‪83‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ٍ‬
‫ﺣﺮﻛﺔ ﻣﻌﻴﱠﻨﺔ )أي‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻟﻜﻦ اﻟﻄﺮﻳﻘﺔ اﻟﺪﻗﻴﻘﺔ اﻟﺘﻲ ﻳﺼﻞ ﻣﻦ ﺧﻼﻟﻬﺎ اﻟﺠﻬﺎز إﱃ‬
‫ﻣﺎ ﻳﺤﺪث ﰲ ﻃﺒﻘﺎت اﻟﺸﺒﻜﺔ اﻟﻌﺼﺒﻴﺔ( ﻟﻴﺴﺖ واﺿﺤﺔ وﻻ ﻳﻤﻜﻦ ﺗﻔﺴريﻫﺎ‪ .‬وﻫﺬه ﻣﺸﻜﻠﺔ‬
‫ﱡ‬
‫ﻳﺨﺺ ﺗﺤﻤﱡ ﻞ املﺴﺌﻮﻟﻴﺔ‪ ،‬ﺣﻴﺚ ﻻ ﻳﺴﺘﻄﻴﻊ اﻟﺒﴩ اﻟﺬﻳﻦ ﻳُﻨﺸﺌﻮن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أو‬
‫ﻓﻴﻤﺎ‬
‫ﻳﺴﺘﺨﺪﻣﻮﻧﻪ ﺗﻔﺴري ﻗﺮار ﻣﻌني‪ ،‬وﺑﺎﻟﺘﺎﱄ ﻳﻔﺸﻠﻮن ﰲ ﻣﻌﺮﻓﺔ ﻣﺎ ﻳﻘﻮم ﺑﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫وﻻ ﻳُﻤﻜﻨﻬﻢ ﺗﱪﻳﺮ أﻓﻌﺎﻟﻪ‪ِ .‬‬
‫ﻓﻤﻦ ﻧﺎﺣﻴﺔ‪ ،‬ﻳﻌﺮف اﻟﺒﴩ ﻣﺎ اﻟﺬي ﻳﻘﻮم ﺑﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫)ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻌﺮﻓﻮن اﻟﺮﻣﻮز اﻟﱪﻣﺠﻴﺔ اﻟﺨﺎﺻﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻳﻌﺮﻓﻮن ﻛﻴﻒ‬
‫ﺑﺸﻜﻞ ﻋﺎم(‪ ،‬وﻟﻜﻦ ﻣﻦ ﻧﺎﺣﻴﺔ أﺧﺮى‪ ،‬ﻫﻢ ﻻ ﻳﻌﺮﻓﻮن )ﻻ ﻳُﻤﻜﻨﻬﻢ ﺗﻔﺴري ﻗﺮار‬
‫ﻳﻌﻤﻞ‬
‫ٍ‬
‫ﱠ‬
‫ﻣﻌني(‪ ،‬وﺗﻜﻮن ﻧﺘﻴﺠﺔ ذﻟﻚ أن اﻟﺒﴩ اﻟﺬﻳﻦ ﻳﺘﺄﺛﺮون ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻻ ﻳﻤﻜﻦ إﻋﻄﺎؤﻫﻢ‬
‫ﱡ‬
‫ٍ‬
‫اﻟﺘﻮﻗﻊ‪ .‬وﺑﺎﻟﺘﺎﱄ‪ ،‬ﻋﲆ اﻟﺮﻏﻢ‬
‫ﻣﻌﻠﻮﻣﺎت دﻗﻴﻘﺔ ﺣﻮل ﻣﺎ اﻟﺬي دﻓﻊ اﻵﻟﺔ إﱃ اﻟﻮﺻﻮل إﱃ ﻫﺬا‬
‫ً‬
‫ﻣﺸﻜﻠﺔ‬
‫ﻣﻦ أن ﻛﻞ ﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻷﺗﻤﺘﺔ ﺗُﺜري ﻣﺸﻜﻼت ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﺎملﺴﺌﻮﻟﻴﺔ‪ ،‬ﻓﺈﻧﻨﺎ ﻫﻨﺎ ﻧﻮاﺟﻪ‬
‫ﱡ‬
‫ﺗﺨﺺ ﺑﻌﺾ أﻧﻮاع اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؛ وﻫﻲ ﻣﺎ ﻳﻄ َﻠﻖ ﻋﻠﻴﻬﺎ ﻣﺸﻜﻠﺔ اﻟﺼﻨﺪوق اﻷﺳﻮد‪.‬‬
‫ً‬
‫ٍ‬
‫ﺑﻤﻌﺮﻓﺔ ﺣﻮل‬
‫ﻋﻼوة ﻋﲆ ذﻟﻚ‪ ،‬ﺣﺘﻰ اﻻﻓﱰاض ﺑﺄن اﻟﺒﴩ ﰲ ﻣﺜﻞ ﻫﺬه اﻟﺤﺎﻻت ﻳﺘﻤﺘﻌﻮن‬
‫ﺑﺸﻜﻞ ﻋﺎم وﺣﻮل رﻣﻮزه اﻟﱪﻣﺠﻴﺔ ﻟﻴﺲ داﺋﻤً ﺎ ﺻﺤﻴﺤً ﺎ‪ .‬ﻓﻌﲆ اﻷرﺟﺢ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ﻳﻌﺮف املﱪﻣﺠﻮن اﻷﺻﻠﻴﻮن اﻟﺮﻣﻮز اﻟﱪﻣﺠﻴﺔ وﻛﻴﻔﻴﺔ ﻋﻤﻞ ﻛﻞ ﳾءٍ )أو ﻋﲆ اﻷﻗﻞ ﻳﻌﺮﻓﻮن‬
‫اﻟﺠﺰء اﻟﺬي ﺑﺮﻣﺠﻮه(‪ ،‬وﻟﻜﻦ ذﻟﻚ ﻻ ﻳﻌﻨﻲ أن املﱪﻣِﺠني وا ُملﺴﺘﺨﺪﻣني اﻟﻼﺣﻘِ ني اﻟﺬﻳﻦ‬
‫ٍ‬
‫ﻟﺘﻄﺒﻴﻘﺎت ﻣﺤﺪﱠدة ﻳﻌﺮﻓﻮن ﺗﻤﺎﻣً ﺎ ﻣﺎ ﻳﻔﻌﻠﻪ اﻟﺬﻛﺎء‬
‫ﻳُﻐريون اﻟﺨﻮارزﻣﻴﺔ أو ﻳﺴﺘﺨﺪﻣﻮﻧﻬﺎ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﻻ ﻳﻔﻬﻢ اﻟﺸﺨﺺ اﻟﺬي ﻳﺴﺘﺨﺪم ﺧﻮارزﻣﻴﺔ اﻟﺘﺪاول اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﺗﻤﺎم املﻌﺮﻓﺔ‪ ،‬أو ﻗﺪ ﻻ ﻳﻌﺮف ﻣُﺴﺘﺨﺪﻣﻮ وﺳﺎﺋﻞ اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ ﺣﺘﻰ‬
‫أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳُﺴﺘﺨﺪَم‪ ،‬ﻓﻤﺎ ﺑﺎﻟﻚ ﺑﺄن ﻳﻔﻬﻤﻮه‪ .‬وﻣﻦ ﺟﻬﺔ املﱪﻣِﺠني )اﻷﺻﻠﻴني(‪،‬‬
‫ﻧﺤﻮ دﻗﻴﻖ اﻻﺳﺘﺨﺪام »ا ُملﺴﺘﻘﺒﲇ« ﻟﻠﺨﻮارزﻣﻴﺔ اﻟﺘﻲ ﻳُﻄﻮروﻧﻬﺎ‬
‫ﻓﻬﻢ ﻗﺪ ﻻ ﻳﻌﺮﻓﻮن ﻋﲆ ٍ‬
‫أو ﻣﺨﺘﻠﻒ ﻣﺠﺎﻻت اﻟﺘﻄﺒﻴﻖ اﻟﺘﻲ ﻳُﻤﻜﻦ اﺳﺘﺨﺪاﻣﻬﺎ ﻓﻴﻬﺎ‪ ،‬ﻓﻤﺎ ﺑﺎﻟﻚ ﺑﻜ ﱢﻞ اﻟﺘ ِﺒﻌﺎت ﻏري‬
‫ﱢ‬
‫ﺑﻐﺾ اﻟﻨﻈﺮ ﻋﻦ املﺸﻜﻠﺔ‬
‫املﻘﺼﻮدة ﻟﻼﺳﺘﺨﺪام ا ُملﺴﺘﻘﺒﲇ ﻟﻬﺬه اﻟﺨﻮارزﻣﻴﺔ‪ .‬ﻟﺬﻟﻚ‪ ،‬ﺣﺘﻰ‬
‫اﻟﺨﺎﺻﺔ ﺑﺘﻌ ﱡﻠﻢ اﻵﻟﺔ )اﻟﺘﻌﻠﻢ اﻟﻌﻤﻴﻖ(‪ ،‬ﻫﻨﺎك ﻣﺸﻜﻠﺔ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎملﻌﺮﻓﺔ ﻟﺪرﺟﺔ ﱠ‬
‫أن اﻟﻜﺜريﻳﻦ ﻣﻤﱠ ﻦ‬
‫ﻳﺴﺘﺨﺪﻣﻮﻧﻪ ﻻ ﻳﻌﺮﻓﻮن ﻣﺎ ﻳﻔﻌﻠﻮن؛ ﻷﻧﻬﻢ ﻻ ﻳﻌﺮﻓﻮن ﻣﺎ اﻟﺬي ﻳﻔﻌﻠﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫وﻣﺎ ﻫﻲ ﺗﺄﺛرياﺗﻪ‪ ،‬أو ﺣﺘﻰ أﻧﻪ ﻣُﺴﺘﺨﺪَم ﻣﻦ اﻷﺳﺎس‪ .‬وﻫﺬه ً‬
‫ﱡ‬
‫ﻳﺨﺺ ﺟﺎﻧﺐ‬
‫أﻳﻀﺎ ﻣﺸﻜﻠﺔ ﻓﻴﻤﺎ‬
‫املﺴﺌﻮﻟﻴﺔ‪ ،‬وﺑﺎﻟﺘﺎﱄ ﻓﻬﻲ ﻣﺸﻜﻠﺔ أﺧﻼﻗﻴﺔ ﺧﻄرية‪.‬‬
‫ﰲ ﺑﻌﺾ اﻷﺣﻴﺎن‪ ،‬ﻳﺘﻢ ﺗﺴﻠﻴﻂ اﻟﻀﻮء ﻋﲆ ﻫﺬه املﺸﻜﻼت ﰲ ﺳﻴﺎق اﻟﺜﻘﺔ‪ :‬ﻓﻐﻴﺎب‬
‫اﻟﺸﻔﺎﻓﻴﺔ ﻳﺆدي إﱃ ﻏﻴﺎب اﻟﺜﻘﺔ ﰲ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﰲ اﻷﺷﺨﺎص اﻟﺬﻳﻦ ﻳﺴﺘﺨﺪﻣﻮن ﻫﺬه‬
‫‪84‬‬

‫ُ‬
‫ﻻﻣﺴﺌﻮﻟﻴﺔ اﻵﻻت واﻟﻘﺮارات ﻏري ا ُملﱪرة‬

‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬ﻟﺬﻟﻚ ﻳﺴﺄل ﺑﻌﺾ اﻟﺒﺎﺣﺜني ﻛﻴﻒ ﻳُﻤﻜﻨﻨﺎ زﻳﺎدة اﻟﺜﻘﺔ ﰲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫ﻛﻌﺎﻣﻞ ﻣﻦ اﻟﻌﻮاﻣﻞ اﻟﺘﻲ ﻳﻤﻜﻦ أن ﺗﺰﻳﺪ ﻣﻦ‬
‫وﻳُﺤﺪﱢدون اﻟﺸﻔﺎﻓﻴﺔ واﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري‬
‫ٍ‬
‫ً‬
‫ﻓﻀﻼ ﻋﻦ ﺗﺠﻨﱡﺐ اﻟﺘﺤﻴﱡﺰ )‪ (Winikoff 2018‬أو ﺻﻮر اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺮﻋﺒﺔ‬
‫اﻟﺜﻘﺔ‪،‬‬
‫)»ﺗﺮﻣﻴﻨﻴﺘﻮر«( )‪ .(Siau and Wang 2018‬وﻛﻤﺎ ﺳﻨﺮى ﰲ اﻟﻔﺼﻞ اﻟﻘﺎدم‪ ،‬ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﻬﺪف‬
‫ﺳﻴﺎﺳﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫أﻳﻀﺎ إﱃ ﺑﻨﺎء اﻟﺜﻘﺔ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻓﺈن ﻣﺼﻄﻠﺤﺎت ﻣﺜﻞ اﻟﺬﻛﺎء‬
‫ﻳﺠﺐ أن ﻧﺤﺘﻔﻆ‬
‫اﻻﺻﻄﻨﺎﻋﻲ »اﻟﺠﺪﻳﺮ ﺑﺎﻟﺜﻘﺔ« ﻣُﺜرية ﻟﻠﺠﺪل؛ إذ ﺗﺠﻌﻠﻨﺎ ﻧﺘﺴﺎءل ﻫﻞ ِ‬
‫ﺑﻤﺼﻄﻠﺢ »اﻟﺜﻘﺔ« ﻟﻠﺤﺪﻳﺚ ﻋﻦ اﻟﻌﻼﻗﺎت اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬أم ﻳﻤﻜﻦ اﺳﺘﺨﺪاﻣﻪ ﻟﻠﺤﺪﻳﺚ ﻋﻦ اﻵﻻت‬
‫ً‬
‫أﻳﻀﺎ؟ ﺗﻘﻮل ﺟﻮاﻧﺎ ﺑﺮاﻳﺴﻮن )‪ ،(٢٠١٨‬اﻟﺒﺎﺣﺜﺔ ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬إن اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺲ ﺷﻴﺌًﺎ ﻳﻤﻜﻦ اﻟﻮﺛﻮق ﺑﻪ وﻟﻜﻨﻪ ﻣﺠﻤﻮﻋﺔ ﻣﻦ ﺗﻘﻨﻴﺎت ﺗﻄﻮﻳﺮ اﻟﱪاﻣﺞ؛ وﻣﻦ‬
‫ﺛَﻢ ﻓﻬﻲ ﺗﻌﺘﻘﺪ أن ﻣﺼﻄﻠﺢ »اﻟﺜﻘﺔ« ﻳﺠﺐ أن ﻳ َ‬
‫ُﺤﺘﻔﻆ ﺑﻪ ﻟﻠﺤﺪﻳﺚ ﻋﻦ اﻟﺒﴩ وﻣﺆﺳﺴﺎﺗﻬﻢ‬
‫ٍ‬
‫ﺗﺴﺎؤﻻت ﺣﻮل ﻧﻮع‬
‫اﻻﺟﺘﻤﺎﻋﻴﺔ‪ .‬وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻳُﺜري ﻣﻮﺿﻮع اﻟﺸﻔﺎﻓﻴﺔ واﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري‬
‫املﺠﺘﻤﻊ اﻟﺬي ﻧﺮﻏﺐ ﰲ اﻟﻌﻴﺶ ﻓﻴﻪ‪ .‬ﻓﻬﻨﺎ ﻻ ﻳﻜﻤُﻦ اﻟﺨﻄﺮ ﰲ ﻣﺠﺮد ﺗﻼﻋُ ﺐ اﻟﺮأﺳﻤﺎﻟﻴني أو‬
‫اﻟﻨﺨﺐ اﻟﺘﻜﻨﻮﻗﺮاﻃﻴﺔ وﻫﻴﻤﻨﺘﻬﻢ‪ ،‬ﻣﻤﺎ ﻳﺨﻠﻖ ﻣﺠﺘﻤﻌً ﺎ ﻳُﻌﺎﻧﻲ ﻣﻦ اﻻﻧﻘﺴﺎم إﱃ ﺣ ﱟﺪ ﻛﺒري‪ .‬وإﻧﻤﺎ‬
‫ﻣﺠﺘﻤﻊ ﻋﺎﱄ اﻟﺘﻘﻨﻴﺔ‪،‬‬
‫ﻳﺘﻤﺜﻞ اﻟﺨﻄﺮ اﻷﻛﱪ ورﺑﻤﺎ اﻷﻋﻤﻖ اﻟﺬي ﻳَﺤﻴﻖ ﺑﻨﺎ ﰲ أن ﻧﻌﻴﺶ ﰲ‬
‫ٍ‬
‫ﻣﺠﺘﻤﻊ ﻻ ﺗﻌﻮد ﻓﻴﻪ ﺣﺘﻰ ﻫﺬه اﻟﻨﺨﺐ ﻗﺎدر ًة ﻋﲆ ﻣﻌﺮﻓﺔ ﻣﺎ ﺗﻔﻌﻠﻪ‪ ،‬ﻣﺠﺘﻤﻊ ﻻ ﻳﺴﺘﻄﻴﻊ ﻓﻴﻪ‬
‫أﺣ ٌﺪ أن ﻳ ﱢ‬
‫ُﻔﴪ ﻣﺎ ﻳﺤﺪث‪.‬‬
‫ﻛﻤﺎ ﺳﻨﺮى‪ ،‬ﻳﻘﱰح ﺻﺎﻧﻌﻮ اﻟﺴﻴﺎﺳﺎت ﰲ ﺑﻌﺾ اﻷﺣﻴﺎن »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺑﻞ‬
‫ﻟﻠﺘﻔﺴري« و»ﺣﻖ اﻟﺘﻔﺴري«‪ .‬إﻻ إﻧﻨﺎ ﻻ ﻧﺪري إن ﻛﺎن ﻣﻦ ا ُملﻤﻜﻦ أن ﻳﻜﻮن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫ً‬
‫ُﺴﺘﺤﻴﻼ‬
‫ﺷﻔﺎﻓﺎ ﻃﻮال اﻟﻮﻗﺖ‪ .‬ﻳﺒﺪو ﻫﺬا ﺳﻬ َﻞ اﻟﺘﺤﻘﻴﻖ ﰲ اﻷﻧﻈﻤﺔ اﻟﻜﻼﺳﻴﻜﻴﺔ‪ .‬وﻟﻜﻦ إذا ﺑﺪا ﻣ‬
‫ﻣﻦ ﺣﻴﺚ املﺒﺪأ ﴍح ﻛ ﱢﻞ ﺧﻄﻮة ﰲ ﻋﻤﻠﻴﺔ اﺗﺨﺎذ اﻟﻘﺮار وﴍح اﻟﻘﺮارات ا ُملﺘﻌﻠﻘﺔ ﺑﺄﻓﺮاد‬
‫ﻣُﺤﺪﱠدﻳﻦ ﻣﻊ ﺗﻄﺒﻴﻘﺎت ﺗﻌﻠﻢ اﻵﻟﺔ ا ُملﻌﺎﴏة‪ ،‬ﻓﻠﺪَﻳﻨﺎ ﻣﺸﻜﻠﺔ إذَن‪ .‬ﻫﻞ ﻣﻦ املﻤﻜﻦ »ﻓﺘﺢ‬
‫اﻟﺼﻨﺪوق اﻷﺳﻮد«؟ ﻗﺪ ﻳﻜﻮن ﻫﺬا ﺷﻴﺌًﺎ ﺟﻴﺪًا‪ ،‬ﻟﻴﺲ ﻓﻘﻂ ﻟﻸﺧﻼق وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﻟﺘﺤﺴني‬
‫اﻟﻨﻈﺎم )أي‪ ،‬اﻟﻨﻤﻮذج( واﻟﺘﻌ ﱡﻠﻢ ﻣﻨﻪ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إذا ﻛﺎن اﻟﻨﻈﺎم أﻛﺜﺮ ﻗﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري‪،‬‬
‫ٍ‬
‫ﺳﻤﺎت ﻏري ﻣﻼﺋﻤﺔ‪ ،‬ﻋﻨﺪﺋ ٍﺬ ﻳﻤﻜﻦ ﻟﻠﺒﴩ‬
‫وإذا ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳَﺴﺘﺨﺪِم ﻣﺎ ﻧﻌﺘﱪه‬
‫اﻛﺘﺸﺎف ﻫﺬه املﺸﻜﻼت واملﺴﺎﻋﺪة ﰲ اﻟﻘﻀﺎء ﻋﲆ اﻻرﺗﺒﺎﻃﺎت اﻟﺰاﺋﻔﺔ‪ .‬وإذا ﻛﺎن اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻳُﺤﺪد اﺳﱰاﺗﻴﺠﻴﺎت ﺟﺪﻳﺪة ُملﻤﺎرﺳﺔ ﻟﻌﺒ ٍﺔ وﻳﺠﻌﻞ ﻫﺬه اﻻﺳﱰاﺗﻴﺠﻴﺎت أﻛﺜﺮ‬
‫ﺷﻔﺎﻓﻴﺔ ﻟﻠﺒﴩ‪ ،‬ﻋﻨﺪﺋ ٍﺬ ﻳﻤﻜﻦ ﻟﻠﺒﴩ ﺗﻌ ﱡﻠﻤﻬﺎ ﻣﻦ اﻵﻟﺔ ﻟﺘﺤﺴني أداﺋﻬﻢ ﰲ اﻟﻠﻌﺒﺔ‪ .‬وﻫﺬا ﻣُﻔﻴﺪ‬
‫ﻟﻴﺲ ﻓﻘﻂ ﰲ ﻣﺠﺎل اﻷﻟﻌﺎب‪ ،‬وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﰲ ﻣﺠﺎﻻت ِﻣﺜﻞ اﻟﺮﻋﺎﻳﺔ اﻟﺼﺤﻴﺔ واﻟﻌﺪاﻟﺔ اﻟﺠﻨﺎﺋﻴﺔ‬
‫‪85‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫واﻟﻌﻠﻮم‪ .‬ﻟﺬﻟﻚ‪ ،‬ﻳُﺤﺎول ﺑﻌﺾ اﻟﺒﺎﺣﺜني ﺗﻄﻮﻳﺮ ﺗﻘﻨﻴﺎت ﻟﻔﺘﺢ اﻟﺼﻨﺪوق اﻷﺳﻮد )‪Samek,‬‬

‫‪ .(Wiegand, and Müller 2017‬وﻟﻜﻦ إذا ﻟﻢ ﻳﻜﻦ ذﻟﻚ ﻣُﻤﻜﻨًﺎ ﺑﻌ ُﺪ أو ﻛﺎن ﻣُﻤﻜﻨًﺎ ﺑﺪرﺟﺔ‬
‫ﻣﺤﺪودة‪ ،‬ﻓﻜﻴﻒ ﻟﻨﺎ أن ﻧﻤﴤ ﻗﺪﻣً ﺎ؟ ﻫﻞ ﺗﺘﻌ ﱠﻠﻖ املﺸﻜﻠﺔ اﻷﺧﻼﻗﻴﺔ ﻫﻨﺎ ﺑﺎﻻﺧﺘﻴﺎر ﺑني اﻷداء‬
‫وإﻣﻜﺎﻧﻴﺔ اﻟﺘﻔﺴري )‪(Seseri 2018‬؟ وإذا ﻛﺎﻧﺖ ﺗﻜﻠﻔﺔ إﻧﺸﺎء ﻧﻈﺎم ذي أداءٍ ﺟﻴﺪ ﻫﻲ ﻧﻘﺺ‬
‫ﻳﺠﺐ ﻋﻠﻴﻨﺎ اﺳﺘﺨﺪام ﻣﺜﻞ ﻫﺬا اﻟﻨﻈﺎم‪ ،‬أم ﻻ؟ أم ﻳﺠﺐ أن ﻧُﺤﺎول ﺗﺠﻨﱡﺐ‬
‫ﰲ اﻟﺸﻔﺎﻓﻴﺔ‪ ،‬ﻓﻬﻞ ِ‬
‫ﻫﺬه املﺸﻜﻠﺔ واﻟﺒﺤﺚ ﻋﻦ ﺣﻠﻮل ﺗﻘﻨﻴﺔ أﺧﺮى‪ ،‬ﺑﺤﻴﺚ ﺗﻜﻮن ﺣﺘﻰ أﻧﻈﻤﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻷﻛﺜﺮ ﺗﻘﺪﻣً ﺎ ﻗﺎدر ًة ﻋﲆ ﺗﱪﻳﺮ أﻓﻌﺎﻟﻬﺎ ﻟﻠﺒﴩ؟ ﻫﻞ ﻳُﻤﻜﻨﻨﺎ ﺗﺪرﻳﺐ اﻵﻻت ﻋﲆ اﻟﻘﻴﺎم ﺑﺬﻟﻚ؟‬
‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﺣﺘﻰ إذا ﻛﺎﻧﺖ اﻟﺸﻔﺎﻓﻴﺔ ﻣﺮﻏﻮﺑﺔ وﻣُﻤﻜﻨﺔ‪ ،‬ﻓﻘﺪ ﻳﻜﻮن ﻣﻦ اﻟﺼﻌﺐ‬
‫ﺗﺤﻘﻴﻘﻬﺎ ﻋﻤﻠﻴٍّﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬رﺑﻤﺎ ﻻ ﺗﻜﻮن اﻟﴩﻛﺎت اﻟﺨﺎﺻﺔ ﻋﲆ اﺳﺘﻌﺪادٍ ﻟﻠﻜﺸﻒ‬
‫ﻋﻦ ﺧﻮارزﻣﻴﺎﺗﻬﺎ؛ ﻷﻧﻬﺎ ﺗﺮﻏﺐ ﰲ ﺣﻤﺎﻳﺔ ﻣﺼﺎﻟﺤﻬﺎ اﻟﺘﺠﺎرﻳﺔ‪ .‬ﻛﺬﻟﻚ ﻗﺪ ﺗﺤُ ﻮل ﻗﻮاﻧني املﻠﻜﻴﺔ‬
‫اﻟﻔﻜﺮﻳﺔ اﻟﺘﻲ ﺗﺤﻤﻲ ﺗﻠﻚ املﺼﺎﻟﺢ دون ذﻟﻚ‪ .‬وﻛﻤﺎ ﺳﻨﺮى ﰲ ﻓﺼﻮل ﻻﺣﻘﺔ‪ ،‬إذا ﻛﺎن اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﰲ أﻳﺪي اﻟﴩﻛﺎت اﻟﻘﻮﻳﺔ‪ ،‬ﻓﺈن ﻫﺬا ﻳُﺜري اﻟﺴﺆال ﺣﻮل ﻣَ ﻦ ﻳﺼﻨﻊ ﻗﻮاﻧني اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ وﻣَ ﻦ ﻳﺠﺐ أن ﻳﺼﻨﻌﻬﺎ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻳﺠﺐ ﻣﺮاﻋﺎة أن اﻟﺸﻔﺎﻓﻴﺔ واﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ ﻻ ﺗﺘﻌ ﱠﻠﻖ‬
‫ﺑﺎﻟﴬورة ﺑﺎﻟﻜﺸﻒ ﻋﻦ اﻟﺮﻣﻮز اﻟﱪﻣﺠﻴﺔ‪ ،‬وﻫﻲ ﺑﺎﻟﺘﺄﻛﻴﺪ ﻻ ﺗﻘﺘﴫ ﻋﲆ ذﻟﻚ ﻓﺤﺴﺐ‪.‬‬
‫ً‬
‫أﺳﺎﺳﺎ ﺑﺘﻔﺴري اﻟﻘﺮارات ﻟﻠﺒﴩ‪ .‬إﻧﻬﺎ ﻻ ﺗﺘﻌ ﱠﻠﻖ ﰲ املﻘﺎم اﻷول ﺑﺘﻔﺴري »ﻛﻴﻒ‬
‫املﺴﺄﻟﺔ ﺗﺘﻌ ﱠﻠﻖ‬
‫ﱠ‬
‫ً‬
‫ﻣﺴﺌﻮﻻ‬
‫املﺘﻮﻗﻊ ﻣﻨﻪ أن ﻳﻜﻮن‬
‫ﻳﻌﻤﻞ« وإﻧﻤﺎ ﺗﺘﻌ ﱠﻠﻖ ﺑﻜﻴﻒ ﻳُﻤﻜﻨﻨﻲ أﻧﺎ‪ ،‬ﺑﻮﺻﻔﻲ إﻧﺴﺎﻧًﺎ ﻣﻦ‬
‫وﻳﺘﴫﱠف ﺑﻤﺴﺌﻮﻟﻴﺔ‪ ،‬ﺗﻔﺴري ﻗﺮاري‪ .‬وﻳُﻤﻜﻦ أن ﺗﻜﻮن ﻛﻴﻔﻴﺔ ﻋﻤﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪،‬‬
‫وﻛﻴﻔﻴﺔ وﺻﻮﻟﻪ إﱃ ﻫﺬه اﻟﺘﻮﺻﻴﺔ‪ ،‬ﺟﺰءًا ﻣﻦ ذﻟﻚ اﻟﺘﻔﺴري‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻓﺈن اﻟﻜﺸﻒ ﻋﻦ‬
‫ً‬
‫ﻣﻌﺮﻓﺔ ﺣﻮل ﻛﻴﻔﻴﺔ ﻋﻤﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫اﻟﺮﻣﻮز اﻟﱪﻣﺠﻴﺔ ﺑﻤُﻔﺮدﻫﺎ ﻻ ﻳﻌﻄﻲ ﺑﺎﻟﴬورة‬
‫ﻓﻬﺬه املﻌﺮﻓﺔ ِ‬
‫ﺗﻌﺘﻤﺪ ﻋﲆ اﻟﺨﻠﻔﻴﺔ اﻟﺘﻌﻠﻴﻤﻴﺔ ﻟﻺﻧﺴﺎن وﻣﻬﺎراﺗﻪ‪ .‬ﻓﺈذا ﻛﺎن ﻳﻔﺘﻘﺮ إﱃ اﻟﺨﱪة‬
‫ﻧﻮع َ‬
‫آﺧﺮ ﻣﻦ اﻟﺘﻔﺴري‪ .‬وﻫﺬا ﻻ ﻳُﺬ ﱢﻛﺮﻧﺎ ﻓﺤﺴﺐ ﺑﻤﺸﻜﻠﺔ‬
‫اﻟﺘﻘﻨﻴﺔ ذات اﻟﺼﻠﺔ‪ ،‬ﻓﺈﻧﻨﺎ ﻧﺤﺘﺎج إﱃ ٍ‬
‫ﺳﺆال ﺣﻮل ﻧﻮع اﻟﺘﻔﺴري اﻟﺬي ﻧﺤﺘﺎﺟُ ﻪ‪ ،‬ﺛﻢ ﻣﺎﻫﻴﺔ اﻟﺘﻔﺴري ﰲ‬
‫اﻟﺘﻌﻠﻴﻢ وﻟﻜﻨﻪ ﻳﺆدﱢي ﺑﻨﺎ إﱃ‬
‫ٍ‬
‫ﺣ ﱢﺪ ذاﺗﻪ‪.‬‬
‫وﻫﻜﺬا ﺗَﻄ َﺮح ﻗﻀﻴﺔ اﻟﺸﻔﺎﻓﻴﺔ واﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري ً‬
‫أﻳﻀﺎ أﺳﺌﻠﺔ ﻓﻠﺴﻔﻴﺔ وﻋﻠﻤﻴﺔ ﻣُﺜرية‬
‫ﻟﻼﻫﺘﻤﺎم‪ ،‬ﻣﺜﻞ اﻷﺳﺌﻠﺔ املﺘﻌﻠﻘﺔ ﺑﻄﺒﻴﻌﺔ اﻟﺘﻔﺴري )‪ .(Weld and Bansal 2018‬وﻣﻤﺎ ﻳﺘﺄﻟﻒ‬
‫اﻟﺘﻔﺴري اﻟﺠﻴﺪ؟ وﻣﺎ اﻟﻔﺮق ﺑني اﻟﺘﻔﺴريات واﻷﺳﺒﺎب‪ ،‬وﻫﻞ ﻳﻤﻜﻦ ﻟﻶﻻت ﺗﻘﺪﻳﻢ أي ﻣﻨﻬﺎ؟‬
‫وﻛﻴﻒ ﻳﺘﱠﺨﺬ اﻟﺒﴩ اﻟﻘﺮارات ﰲ اﻟﻮاﻗﻊ؟ وﻛﻴﻒ ﻳُﱪﱢرون ﻗﺮاراﺗﻬﻢ؟ ﻫﻨﺎك أﺑﺤﺎث ﺣﻮل ﻫﺬا‬
‫‪86‬‬

‫ُ‬
‫ﻻﻣﺴﺌﻮﻟﻴﺔ اﻵﻻت واﻟﻘﺮارات ﻏري ا ُملﱪرة‬

‫املﻮﺿﻮع ﰲ ﻋﻠﻢ اﻟﻨﻔﺲ املﻌﺮﰲ واﻟﻌﻠﻮم املﻌﺮﻓﻴﺔ‪ ،‬واﻟﺘﻲ ﻳُﻤﻜﻦ اﺳﺘﺨﺪاﻣﻬﺎ ﻟﻠﺘﻔﻜري ﰲ اﻟﺬﻛﺎء‬
‫ً‬
‫ﺳﺒﺒﻴﺔ ﻛﺎﻣﻠﺔ؛‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺑﻞ ﻟﻠﺘﻔﺴري‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻻ ﻳُﻘﺪم اﻟﻨﺎس ﻋﻤﻮﻣً ﺎ ﺳﻼﺳﻞ‬
‫ﺗﻔﺴريات وﻳﺠﻴﺒﻮن ﻋﻤﺎ ﻳﻌﺘﻘﺪون أﻧﻬﺎ ﻣُﻌﺘﻘﺪات اﻟﺸﺨﺺ اﻟﺬي ﻳ ﱢ‬
‫ٍ‬
‫ُﻔﴪ ﻟﻬﻢ‪:‬‬
‫وإﻧﻤﺎ ﻳﺨﺘﺎرون‬
‫اﻟﺘﻔﺴريات اﺟﺘﻤﺎﻋﻴﺔ )‪ .(Miller 2018‬ورﺑﻤﺎ ﻧﺘﻮﻗﻊ ً‬
‫أﻳﻀﺎ أن ﺗﻜﻮن ﺗﻔﺴريات اﻵﻻت ﻣﺨﺘﻠﻔﺔ‬
‫ري ﻣﻦ اﻷﺣﻴﺎن ﺑﺄﻧﻬﺎ ﻧﺘﻴﺠﺔ ﻟﻠﻌﻮاﻃﻒ‪.‬‬
‫ﻋﻦ ﺗﻔﺴريات اﻟﺒﴩ‪ ،‬اﻟﺬﻳﻦ ﻳُﱪرون أﻓﻌﺎﻟﻬﻢ ﰲ ﻛﺜ ٍ‬
‫وﻟﻜﻦ إذا ﻓﻌﻠﻨﺎ ذﻟﻚ‪ ،‬ﻓﻬﻞ ﻳﻌﻨﻲ ﻫﺬا أﻧﻨﺎ ﻧﻌﺘﱪ ﻃﺮﻳﻘﺔ اﺗﺨﺎذ اﻵﻻت ﻟﻠﻘﺮارات أﻓﻀﻞ ﻣﻦ‬
‫ﻃﺮﻳﻘﺔ اﺗﺨﺎذ اﻟﺒﴩ ﻟﻬﺎ )‪ ،(Dignum et al. 2018‬وإذا ﻛﺎن اﻷﻣﺮ ﻛﺬﻟﻚ‪ ،‬ﻓﻬﻞ ﻳﺠﺐ أن‬
‫ﻧﻔﻌﻞ؟ ﻳﺘﺤﺪث ﺑﻌﺾ اﻟﺒﺎﺣﺜني ﻋﻦ اﻻﺳﺘﺪﻻل ً‬
‫ﺑﺪﻻ ﻣﻦ اﻟﺘﻔﺴري‪ .‬ﺑﻞ إن وﻳﻨﻴﻜﻮف )‪(٢٠١٨‬‬
‫ﻳﻄﻠﺐ »اﻻﺳﺘﺪﻻل ﺑﻨﺎءً ﻋﲆ اﻟﻘِ ﻴَﻢ« ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻏريه ﻣﻦ اﻷﻧﻈﻤﺔ ا ُملﺴﺘﻘﻠﺔ‪ ،‬اﻟﺘﻲ‬
‫ﻳﺠﺐ أن ﺗﻜﻮن ﻗﺎدر ًة ﻋﲆ ﺗﻤﺜﻴﻞ اﻟﻘﻴﻢ اﻟﺒﴩﻳﺔ واﻻﺳﺘﺪﻻل ﺑﺎﺳﺘﺨﺪام ﺗﻠﻚ اﻟﻘِ ﻴَﻢ‪ .‬وﻟﻜﻦ ﻫﻞ‬
‫ﻳﻤﻜﻦ ﻟﻶﻟﺔ أن ﺗﻘﻮم ﺑﺎﻻﺳﺘﺪﻻل‪ ،‬وﻛﻴﻒ ﻳﻤﻜﻦ ﻟﻠﻨﻈﺎم اﻟﺘﻜﻨﻮﻟﻮﺟﻲ »اﺳﺘﺨﺪام« اﻟﻘِ ﻴَﻢ أو‬
‫»ﺗﻤﺜﻴﻠﻬﺎ« ﻣﻦ اﻷﺳﺎس؟ أي ﻧﻮع ﻣﻦ املﻌﺮﻓﺔ ﻳﻤﺘﻠﻜﻬﺎ ﻫﺬا اﻟﻨﻈﺎم؟ وﻫﻞ ﻳﻤﺘﻠﻚ ﻣﻌﺮﻓﺔ ﻣﻦ‬
‫اﻷﺳﺎس؟ وﻫﻞ ﻳﺴﺘﻄﻴﻊ اﻟﻔﻬﻢ ﻣﻦ اﻷﺳﺎس؟ وﻛﻤﺎ ﻳﺴﺄل ﺑﻮدﻳﻨﺠﺘﻮن )‪ ،(٢٠١٧‬ﻫﻞ ﻳﻤﻜﻦ‬
‫ﻟﻠﺒﴩ أن ﻳ ﱢ‬
‫ﺑﺸﻜﻞ ﻛﺎﻣﻞ ﻋﻦ ﻗﻴ َِﻤﻬﻢ اﻟﺠﻮﻫﺮﻳﺔ؟‬
‫ُﻌﱪوا‬
‫ٍ‬
‫ﻣﺜﻞ ﻫﺬه املﺸﻜﻼت ﻣُﺜرية ﻟﻼﻫﺘﻤﺎم ﻣﻦ ﻣﻨﻈﻮر اﻟﻔﻼﺳﻔﺔ‪ ،‬وﻟﻜﻨﻬﺎ ً‬
‫ٍ‬
‫ﺻﻠﺔ‬
‫أﻳﻀﺎ ذات‬
‫ﻣﺒﺎﴍة ﺑﺎﻷﺧﻼﻗﻴﺎت‪ ،‬ﻛﻤﺎ أﻧﻬﺎ واﻗﻌﻴﺔ وﻋﻤﻠﻴﺔ ﻟﻠﻐﺎﻳﺔ‪ .‬وﻛﻤﺎ ﻳﻘﻮل ﻛﺎﺳﺘﻴﻠﻔﻴﺘﴚ )‪:(٢٠١٦‬‬
‫إن ﻓﺘﺢ »اﻟﺼﻨﺪوق اﻷﺳﻮد« ﻣﺸﻜﻠﺔ ﰲ اﻟﻌﺎ َﻟﻢ اﻟﺤﻘﻴﻘﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﺠﺐ ﻋﲆ اﻟﺒﻨﻮك‬
‫أن ﺗُ ﱢ‬
‫ﻗﺮض ﻣﺎ؛ وﻳﺠﺐ ﻋﲆ اﻟﻘﻀﺎة ﺗﻔﺴري ﺳﺒﺐ إﺻﺪار اﻷواﻣﺮ ﺑﺤﺒﺲ‬
‫ﻔﴪ ﺳﺒﺐ رﻓﺾ ٍ‬
‫ﺷﺨﺺ ﻣﺎ )ﻣﺮ ًة أﺧﺮى(‪ .‬إن ﺗﻔﺴري اﻟﻘﺮارات ﻟﻴﺲ ﻓﻘﻂ ﺟﺰءًا ﻣﻦ ﻃﺒﻴﻌﺔ اﻟﺒﴩ ﻋﻨﺪﻣﺎ‬
‫ٍ‬
‫ً‬
‫ﻳﺘﻮاﺻﻠﻮن )‪ ،(Goebel et al. 2018‬ﺑﻞ ﻫﻮ أﻳﻀﺎ ﻣﻄﻠﺐ أﺧﻼﻗﻲ‪ .‬إن اﻟﻘﺪرة ﻋﲆ اﻟﺘﻔﺴري‬
‫ﴍط ﴐوري ﻟﻠﺴﻠﻮك واﺗﺨﺎذ اﻟﻘﺮارات ﺑﺸﻜﻞ ﻣﺴﺌﻮل وﻗﺎﺑﻞ ﻟﻠﻤﺴﺎءﻟﺔ‪ .‬وﻳﺒﺪو أﻧﻪ‬
‫ﴐوري ﻷي ﻣﺠﺘﻤﻊ ﻳﺮﻏﺐ ﰲ اﺣﱰام اﻟﺒﴩ ﺑﻮﺻﻔﻬﻢ أﻓﺮادًا ﻣُﺴﺘﻘﻠني اﺟﺘﻤﺎﻋﻴﱢني ﻳُﺤﺎوﻟﻮن‬
‫ﺑﺸﻜﻞ ﻣﺴﺌﻮل وﰲ اﻟﻮﻗﺖ ﻧﻔﺴﻪ ﻳُﻄﺎﻟﺒﻮن‪ ،‬ﻋﻦ اﺳﺘﺤﻘﺎق‪،‬‬
‫اﻟﺘﴫف واﺗﺨﺎذ اﻟﻘﺮارات‬
‫ٍ‬
‫أﺳﺒﺎب ﻟﻠﻘﺮارات اﻟﺘﻲ ﺗﺆﺛﺮ ﻋﻠﻴﻬﻢ وﺗﻔﺴريات ﻟﻬﺎ‪ .‬وﺳﻮاءٌ أﻛﺎن ﺑﺈﻣﻜﺎن‬
‫ﺑﺎﻟﺤﺼﻮل ﻋﲆ‬
‫ٍ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﻮﻓري ﺗﻠﻚ اﻷﺳﺒﺎب واﻟﺘﻔﺴريات »ﻣﺒﺎﴍ ًة« أم ﻻ‪ ،‬ﻓﺈن اﻟﺒﴩ ﻻ ﺑﺪ أن‬
‫ﻳﻮاﺟﻪ اﻟﺒﺎﺣﺜني‬
‫ﻳﻜﻮﻧﻮا ﻗﺎدرﻳﻦ ﻋﲆ اﻹﺟﺎﺑﺔ ﻋﻨﺪ ﺳﺆاﻟﻬﻢ ﻋﻦ اﻷﺳﺒﺎب‪ .‬إن اﻟﺘﺤﺪي اﻟﺬي‬
‫ِ‬
‫ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻮ ﺿﻤﺎن أﻧﻪ ﰲ ﺣﺎل اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻷﻏﺮاض‬
‫اﺗﺨﺎذ اﻟﻘﺮارات ﻣﻦ اﻷﺳﺎس‪ ،‬ﻓﻴﺠﺐ ﺗﺼﻤﻴﻢ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﺑﺤﻴﺚ ﻳﺘﻤﻜﻦ اﻟﺒﴩ ﻗﺪْر اﻹﻣﻜﺎن‬
‫ﻣﻦ اﻹﺟﺎﺑﺔ ﻋﻨﺪ ﺳﺆاﻟﻬﻢ ﻋﻦ أﺳﺒﺎب اﺗﺨﺎذ ﺗﻠﻚ اﻟﻘﺮارات‪.‬‬
‫‪87‬‬

‫اﻟﻔﺼﻞ اﻟﺘﺎﺳﻊ‬

‫اﻟﺘﺤﻴﺰ وﻣﻌﻨﻰ اﳊﻴﺎة‬
‫اﻟﺘﺤﻴﺰ‬
‫ً‬
‫ﻣﺸﻜﻠﺔ أﺧﺮى ﻣﻦ املﺸﻜﻼت ذات اﻟﺠﻮاﻧﺐ اﻷﺧﻼﻗﻴﺔ واﻻﺟﺘﻤﺎﻋﻴﺔ ﰲ اﻟﻮﻗﺖ‬
‫ﻳُﻌﺪ اﻟﺘﺤﻴﱡﺰ‬
‫ﱠ‬
‫ً‬
‫ﻧﻔﺴﻪ‪ ،‬وﻫﻲ أﻳﻀﺎ ﺗﺘﻌﻠﻖ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺋﻢ ﻋﲆ ﻋِ ﻠﻢ اﻟﺒﻴﺎﻧﺎت ﺑﻌﻴﺪًا ﻋﻦ ﻏريه ﻣﻦ‬
‫ﺗﻘﻨﻴﺎت اﻷﺗﻤﺘﺔ اﻷﺧﺮى‪ .‬ﻋﻨﺪﻣﺎ ﻳﺘﱠﺨﺬ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ — أو ﻋﲆ اﻷﺣﺮى‪ ،‬ﻋﻨﺪﻣﺎ ﻳُﻮﴆ‬
‫ٍ‬
‫ﻣﻨﺼﻔﺔ أو ﻏري ﻋﺎدﻟﺔ ﺗﺠﺎه‬
‫ﺑﺎﺗﺨﺎذ — ﻗﺮارات‪ ،‬ﻗﺪ ﻳُﻈﻬﺮ اﻟﺘﺤﻴﱡﺰ؛ إذ ﻗﺪ ﺗﻜﻮن اﻟﻘﺮارات ﻏري‬
‫أﻓﺮادٍ أو ﻣﺠﻤﻮﻋﺎت ﺑﻌﻴﻨﻬﺎ‪ .‬وﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن اﻟﺘﺤﻴﱡﺰ ﻗﺪ ﻳﻈﻬﺮ ً‬
‫أﻳﻀﺎ ﻋﻨﺪ اﺳﺘﺨﺪام اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻟﺘﻘﻠﻴﺪي — ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻧﻈﺎم ﺧﺒري ﻳَﺴﺘﺨﺪم ﺷﺠﺮة اﺗﺨﺎذ ﻗﺮارات أو‬
‫ً‬
‫ﻣﺮﺗﺒﻄﺔ ﺑﺘﻄﺒﻴﻘﺎت ﺗﻌ ﱡﻠﻢ‬
‫ﻗﺎﻋﺪة ﺑﻴﺎﻧﺎت ﺗﺘﺴﻢ ﺑﺎﻟﺘﺤﻴﺰ — ﻓﺈن ﻗﻀﻴﺔ اﻟﺘﺤﻴﺰ ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﻜﻮن‬
‫اﻵﻟﺔ‪ .‬وﺑﻴﻨﻤﺎ ﻛﺎﻧﺖ ﻣﺸﻜﻼت اﻟﺘﺤﻴﺰ واﻟﺘﻤﻴﻴﺰ ﻣﻮﺟﻮدة داﺋﻤً ﺎ ﰲ املﺠﺘﻤﻊ‪ ،‬إﻻ أن اﻟﻘﻠﻖ ﻳﻜﻤُﻦ‬
‫ُ‬
‫وﺗﻔﺎﻗﻢ آﺛﺎرﻫﺎ‪.‬‬
‫ﰲ أن ﻳﺆدي اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ اﺳﺘﻤﺮار ﻫﺬه املﺸﻜﻼت‬
‫ﺑﻴﻨﻤﺎ ﻛﺎﻧﺖ ﻣﺸﻜﻼت اﻟﺘﺤﻴﱡﺰ واﻟﺘﻤﻴﻴﺰ ﻣﻮﺟﻮد ًة داﺋﻤً ﺎ ﰲ املﺠﺘﻤﻊ‪ ،‬إﻻ أن اﻟﻘﻠﻖ ﻳﻜﻤُﻦ ﰲ أن ﻳﺆدي‬
‫ُ‬
‫وﺗﻔﺎﻗﻢ آﺛﺎرﻫﺎ‪.‬‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ اﺳﺘﻤﺮار ﻫﺬه املﺸﻜﻼت‬

‫ﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﻜﻮن اﻟﺘﺤﻴﱡﺰ ﻏري ﻣﻘﺼﻮد؛ ﻓﺎ ُملﻄﻮﱢرون واملﺴﺘﺨﺪِﻣﻮن‪ ،‬وﻏريﻫﻢ ﻣﻦ أﻃﺮاف‬
‫ري ﻣﻦ اﻷﺣﻴﺎن‪ ،‬آﺛﺎر اﻟﺘﻤﻴﻴﺰ ﺿ ﱠﺪ ﻣﺠﻤﻮﻋﺎت أو‬
‫ﻣُﻌﻨﻴﺔ ﻣﺜﻞ إدارة اﻟﴩﻛﺔ‪ ،‬ﻻ ﻳﺘﻮﻗﻌﻮن‪ ،‬ﰲ ﻛﺜ ٍ‬
‫أﻓﺮادٍ ﻣُﻌﻴﻨني‪ .‬وﻳﻤﻜﻦ أن ﻳﻜﻮن اﻟﺴﺒﺐ ﰲ ذﻟﻚ ﻫﻮ ﻋﺪم ﻓﻬﻤﻬﻢ ﻧﻈﺎم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺑﺸﻜﻞ ٍ‬
‫ﻛﺎف ﺑﻤﺸﻜﻠﺔ اﻟﺘﺤﻴﱡﺰ أو ﺣﺘﻰ ﺑﺘﺤﻴﱡﺰاﺗﻬﻢ اﻟﺸﺨﺼﻴﺔ‪،‬‬
‫ﻛﻤﺎ ﻳﻨﺒﻐﻲ‪ ،‬أو ﻋﺪم وﻋ ِﻴﻬﻢ‬
‫ٍ‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺑﺸﻜﻞ ﻋﺎم ﻋﺪم ﺗﺼﻮﱡرﻫﻢ وﻋﺪم ﺗﻔﻜريﻫﻢ ﺑﻤﺎ ﻓﻴﻪ اﻟﻜﻔﺎﻳﺔ ﰲ اﻟﻌﻮاﻗﺐ ا ُملﺤﺘﻤَ ﻠﺔ ﻏري‬
‫أو‬
‫ٍ‬
‫ُ‬
‫ﺗﻮاﺻﻠﻬﻢ ﻣﻊ ﺑﻌﺾ اﻷﻃﺮاف ذات اﻟﺼﻠﺔ‪ .‬ﻳُﻌَ ﺪ ﻫﺬا أﻣ ًﺮا‬
‫املﻘﺼﻮدة ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﻋﺪم‬
‫إﺷﻜﺎﻟﻴٍّﺎ ﻧﻈ ًﺮا إﱃ أن اﻟﻘﺮارات ا ُملﺘﺤﻴﱢﺰة ﻳﻤﻜﻦ أن ﺗﻜﻮن ﻟﻬﺎ ﻋﻮاﻗﺐ وﺧﻴﻤﺔ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ‬
‫املﺜﺎل‪ ،‬ﻣﻦ ﺣﻴﺚ اﻟﻮﺻﻮل إﱃ املﻮارد واﻟﺘﻤﺘﱡﻊ ﺑﺎﻟﺤﺮﻳﺎت )‪(CDT 2018‬؛ إذ ﻗﺪ ﻻ ﻳﺤﺼﻞ‬
‫اﻷﻓﺮاد ﻋﲆ وﻇﻴﻔﺔ‪ ،‬أو ﻻ ﻳﺘﻤ ﱠﻜﻨﻮن ﻣﻦ اﻟﺤﺼﻮل ﻋﲆ اﺋﺘﻤﺎن‪ ،‬أو ﻗﺪ ﻳﻨﺘﻬﻲ ﺑﻬﻢ اﻟﺤﺎل ﰲ‬
‫ِ‬
‫ﺗﻘﺘﴫ ﻋﲆ اﻷﻓﺮاد ﻓﺤﺴﺐ؛ إذ ﻗﺪ ﺗﺘﺄﺛﺮ‬
‫اﻟﺴﺠﻦ‪ ،‬أو ﺣﺘﻰ ﻳﺘﻌ ﱠﺮﺿﻮن ﻟﻠﻌﻨﻒ‪ .‬وا ُملﻌﺎﻧﺎة ﻻ‬
‫ﺑﺄﴎﻫﺎ ﺑﺎﻟﻘﺮارات ا ُملﺘﺤﻴﺰة‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻨﺪﻣﺎ ﺗُﺼﻨﱠﻒ ﻣﻨﻄﻘﺔ ﻛﺎﻣﻠﺔ ﰲ‬
‫ﻣﺠﺘﻤﻌﺎت‬
‫ِ‬
‫املﺪﻳﻨﺔ أو ﺟﻤﻴﻊ اﻷﺷﺨﺎص ﻣﻤﱠ ﻦ ﻟﻬﻢ ﺧﻠﻔﻴﺔ ﻋﺮﻗﻴﺔ ﻣُﻌﻴﱠﻨﺔ ﺑﻮاﺳﻄﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ‬
‫أﻧﻬﻢ ﻳﺸﻜﻠﻮن ﺧﻄﻮر ًة أﻣﻨﻴﺔ ﻋﺎﻟﻴﺔ‪.‬‬
‫وﻟﻨﻌُ ﺪ ﻣ ﱠﺮة أﺧﺮى إﱃ ﻣﺜﺎل ﺧﻮارزﻣﻴﺔ ﻛﻮﻣﺒﺎس اﻟﺬي ﺗﺤﺪﱠﺛﻨﺎ ﻋﻨﻪ ﰲ اﻟﻔﺼﻞ اﻷول‪،‬‬
‫ﺗﻠﻚ اﻟﺨﻮارزﻣﻴﺔ اﻟﺘﻲ ﺗﺘﻨﺒﱠﺄ ﺑﻤﺪى اﺣﺘﻤﺎﻟﻴﺔ أن ﻳﻘﻮم ا ُملﺪﱠﻋﻰ ﻋﻠﻴﻪ ﺑﺈﻋﺎدة ارﺗﻜﺎب اﻟﺠﺮﻳﻤﺔ‬
‫وﻛﺎن اﻟﻘﻀﺎة ﰲ ﻓﻠﻮرﻳﺪا ﻳﺴﺘﺨﺪﻣﻮﻧﻬﺎ ﰲ اﺗﺨﺎذ ﻗﺮاراﺗﻬﻢ ﺑﺸﺄن إﻣﻜﺎﻧﻴﺔ ﻣﻨﺢ اﻟﺴﺠني‬
‫ﻃﺎ‪ً .‬‬
‫إﻓﺮاﺟً ﺎ ﻣﴩو ً‬
‫ٍ‬
‫ﻟﺪراﺳﺔ أﺟ َﺮﺗْﻬﺎ »ﺑﺮوﺑﺎﺑﻠﻴﻜﺎ«‪ ،‬وﻫﻲ ﻏﺮﻓﺔ إﺧﺒﺎرﻳﺔ ﻋﱪ اﻹﻧﱰﻧﺖ‪،‬‬
‫وﻓﻘﺎ‬
‫ﻛﺎﻧﺖ اﻟﻨﺘﺎﺋﺞ اﻹﻳﺠﺎﺑﻴﺔ اﻟﻜﺎذﺑﺔ ﻟﻠﺨﻮارزﻣﻴﺔ )ا ُملﺪﱠﻋﻰ ﻋﻠﻴﻬﻢ اﻟﺬﻳﻦ ﺗﻮﻗﻌﺖ اﻟﺨﻮارزﻣﻴﺔ أن‬
‫ُﻔﺮط إﱃ اﻷﺷﺨﺎص‬
‫ﻳُﻌﻴﺪوا ارﺗﻜﺎب اﻟﺠﺮاﺋﻢ وﻟﻜﻨﻬﻢ ﰲ اﻟﻮاﻗﻊ ﻟﻢ ﻳﻔﻌﻠﻮا( ﺗﻤﻴﻞ‬
‫ٍ‬
‫ﺑﺸﻜﻞ ﻣ ِ‬
‫ﻣﻦ ذوي اﻟﺒﴩة اﻟﺴﻤﺮاء‪ ،‬وﻛﺎﻧﺖ اﻟﻨﺘﺎﺋﺞ اﻟﺴﻠﺒﻴﺔ اﻟﻜﺎذﺑﺔ )ا ُملﺪﱠﻋﻰ ﻋﻠﻴﻬﻢ اﻟﺬﻳﻦ ﺗﻮﻗﻌﺖ‬
‫ﺑﺸﻜﻞ ﻣُﻔﺮط إﱃ‬
‫اﻟﺨﻮارزﻣﻴﺔ أﻻ ﻳُﻌﻴﺪوا ارﺗﻜﺎب اﻟﺠﺮاﺋﻢ وﻟﻜﻨﻬﻢ ﰲ اﻟﻮاﻗﻊ ﻓﻌﻠﻮا( ﺗﻤﻴﻞ‬
‫ٍ‬
‫اﻷﺷﺨﺎص ذوي اﻟﺒﴩة اﻟﺒﻴﻀﺎء )‪ .(Fry 2018‬وﻣِﻦ ﺛَﻢ رأى ﱠ‬
‫اﻟﻨﻘﺎد أن ﻫﻨﺎك ﺗﺤﻴﱡ ًﺰا ﺿﺪ‬
‫ا ُملﺪﱠﻋﻰ ﻋﻠﻴﻬﻢ ﻣﻦ ذوي اﻟﺒﴩة اﻟﺴﻤﺮاء‪ .‬ﻣﺜﺎل َ‬
‫آﺧﺮ ﻋﲆ ذﻟﻚ ﻫﻮ أداة »ﺑﺮﻳﺪﺑﻮل«‪ ،‬وﻫﻲ‬
‫أداة ﻟﻠﺘﻨﺒﱡﺆ ﺑﺎﻟﺠﺮاﺋﻢ وﻗﺪ اﺳﺘُﺨﺪِﻣَ ﺖ ﰲ اﻟﻮﻻﻳﺎت املﺘﺤﺪة ﱡ‬
‫ٍ‬
‫ﺟﺮﻳﻤﺔ‬
‫ﻟﺘﻮﻗﻊ اﺣﺘﻤﺎﻟﻴﺔ ﺣﺪوث‬
‫ﰲ ﻣﻨﺎﻃﻖ ﻣُﻌﻴﱠﻨﺔ ﻣﻦ املﺪن وﻟﻠﺘﻮﺻﻴﺔ ﺑﺘﺨﺼﻴﺺ ﻣﻮارد اﻟﴩﻃﺔ )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬أﻳﻦ‬
‫ﱡ‬
‫اﻟﺘﻮﻗﻌﺎت‪.‬‬
‫ﻳﺠﺐ أن ﻳُﺠﺮي ﺿﺒﺎط اﻟﴩﻃﺔ ﻋﻤﻠﻴﺎت اﻟﺘﻔﺘﻴﺶ واﻟﺘﺠﻮال( اﺳﺘﻨﺎدًا إﱃ ﻫﺬه‬
‫وﺗﺮﻛﺰت املﺨﺎوف ﰲ ﻫﺬا اﻟﺼﺪد ﰲ أن ﻳﻜﻮن اﻟﻨﻈﺎم ﻣُﺘﺤﻴ ًﺰا ﺿﺪ اﻷﺣﻴﺎء اﻟﻔﻘرية وأﺣﻴﺎء‬
‫ا ُملﻠﻮﱠﻧني أو أن ﺗﺆدي املﺮاﻗﺒﺔ اﻷﻣﻨﻴﺔ ا ُملﻔﺮﻃﺔ إﱃ ﻛﴪ اﻟﺜﻘﺔ ﺑني اﻟﻨﺎس ﰲ ﺗﻠﻚ املﻨﺎﻃﻖ‪ ،‬ﻣﻤﺎ‬
‫ﱠ‬
‫ﺗﺘﺤﻘﻖ ذاﺗﻴٍّﺎ )‪.(Kelleher and Tierney 2018‬‬
‫ﻳُﺤَ ﻮﱢل ﺗﻮﻗﻊ ﺣﺪوث اﻟﺠﺮﻳﻤﺔ إﱃ ﻧﺒﻮء ٍة‬
‫ﻳﻘﺘﴫ ﻋﲆ اﻟﻌﺪاﻟﺔ اﻟﺠﻨﺎﺋﻴﺔ أو ا ُملﺮاﻗﺒﺔ اﻷﻣﻨﻴﺔ؛ ﺑﻞ ﻳُﻤﻜﻦ أن ﻳﻌﻨﻲ ً‬
‫ِ‬
‫أﻳﻀﺎ‪،‬‬
‫وﻟﻜﻦ اﻟﺘﺤﻴﱡﺰ ﻻ‬
‫ٍ‬
‫ﻟﺘﺤﻴﺰات ﺿﺪﱠﻫﻢ إذا ﺻﻨﱠﻔﻬﻢ اﻟﺬﻛﺎء‬
‫ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗﻌ ﱡﺮض ﻣُﺴﺘﺨﺪﻣﻲ ﺧﺪﻣﺎت اﻹﻧﱰﻧﺖ‬
‫ً‬
‫ﺗﺼﻨﻴﻔﺎ ﺳﻴﺌًﺎ‪.‬‬
‫اﻻﺻﻄﻨﺎﻋﻲ‬
‫‪90‬‬

‫اﻟﺘﺤﻴﺰ وﻣﻌﻨﻰ اﻟﺤﻴﺎة‬

‫ﻗﺪ ﻳﻨﺸﺄ اﻟﺘﺤﻴﱡﺰ ﺑﻌﺪﱠة ﻃ ُﺮ ٍق ﰲ ﺟﻤﻴﻊ ﻣﺮاﺣﻞ اﻟﺘﺼﻤﻴﻢ واﻻﺧﺘﺒﺎر واﻟﺘﻄﺒﻴﻖ‪ .‬وإذا ﻣﺎ‬
‫ر ﱠﻛﺰﻧﺎ ﻋﲆ ﻣﺮﺣﻠﺔ اﻟﺘﺼﻤﻴﻢ‪ ،‬ﻓﺴﻨﺠﺪ أن اﻟﺘﱠﺤﻴﱡﺰ ﻗﺪ ﻳﻈﻬﺮ ﰲ اﺧﺘﻴﺎر ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت‬
‫اﻟﺘﻲ ﺳﻴﺘﻢ اﻟﺘﺪرﻳﺐ ﻋﻠﻴﻬﺎ؛ وﰲ ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﺳﻴﺘﻢ اﻟﺘﺪرﻳﺐ ﻋﻠﻴﻬﺎ ﻧﻔﺴﻬﺎ‪،‬‬
‫واﻟﺘﻲ ﻗﺪ ﺗﻜﻮن ﻏري ﻣُﻤﺜﻠﺔ أو ﻏري ﻛﺎﻣﻠﺔ‪ ،‬وﰲ اﻟﺨﻮارزﻣﻴﺔ‪ ،‬وﰲ ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ‬
‫ﻳﺘﻢ إدﺧﺎﻟﻬﺎ إﱃ اﻟﺨﻮارزﻣﻴﺔ ﺑﻌﺪ ﺗﺪرﻳﺒﻬﺎ‪ ،‬وﰲ اﻟﻘﺮارات اﻟﻘﺎﺋﻤﺔ ﻋﲆ اﻻرﺗﺒﺎﻃﺎت اﻟﺰاﺋﻔﺔ‬
‫َ‬
‫اﻷوﺳﻊ‪ .‬ﻋﲆ‬
‫)اﻧﻈﺮ اﻟﻔﺼﻞ اﻟﺴﺎﺑﻖ(‪ ،‬وﰲ املﺠﻤﻮﻋﺔ اﻟﺘﻲ ﺗُﻨﺸﺊ اﻟﺨﻮارزﻣﻴﺔ‪ ،‬وﰲ املﺠﺘﻤﻊ‬
‫رﺟﺎل‬
‫ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﻻ ﺗﻜﻮن ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت ﻣُﻤﺜﱢﻠﺔ ﻟﻠﺴﻜﺎن )ﻛﺄن ﺗﻜﻮن ﻣَ ﺒﻨﻴﺔ ﻋﲆ‬
‫ٍ‬
‫أﻣﺮﻳﻜﻴني ﺑﻴﺾ( وﻟﻜﻨﻬﺎ ﺗُﺴﺘﺨﺪَم ﻟﻠﺘﻨﺒﺆ ﻣﻊ اﻟﺴﻜﺎن ﻛﻜ ﱟﻞ )اﻟﺮﺟﺎل واﻟﻨﺴﺎء ﻣﻦ ﺧﻠﻔﻴﺎت‬
‫ﻋﺮﻗﻴﺔ ﻣُﺘﻨﻮﱢﻋﺔ(‪ .‬ﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ أن ﻳﻜﻮن اﻟﺘﺤﻴﱡﺰ ﻣُﺘﻌ ﱢﻠ ًﻘﺎ ﺑﺎﻻﺧﺘﻼﻓﺎت ﺑني اﻟﺒﻠﺪان‪ .‬ﻓﻜﺜري ﻣﻦ‬
‫اﻟﺸﺒﻜﺎت اﻟﻌﺼﺒﻴﺔ اﻟﻌﻤﻴﻘﺔ ا ُملﺴﺘﺨﺪﻣﺔ ﰲ اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻟﺼﻮر ﺗُﺪ ﱠرب ﻋﲆ ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت‬
‫ا ُملﺤَ ﺪﱠدة »إﻳﻤﺪﺟﻨﺖ« ‪ ،ImageNet‬اﻟﺘﻲ ﺗﺤﺘﻮي ﻋﲆ ﻛﻤﻴ ٍﺔ ﻏري ﻣﺘﻜﺎﻓﺌﺔ ﻣﻦ اﻟﺒﻴﺎﻧﺎت ﻣﻦ‬
‫ري ﻣﻦ‬
‫اﻟﻮﻻﻳﺎت املﺘﺤﺪة‪ ،‬ﰲ ﺣني أن ﺑﻠﺪان ﻣﺜﻞ اﻟﺼني واﻟﻬﻨﺪ‪ ،‬اﻟ ﱠﻠﺘني ﺗُﻤﺜﻼن ﺟﺰءًا أﻛﱪ ﺑﻜﺜ ٍ‬
‫ﺳﻜﺎن اﻟﻌﺎﻟﻢ‪ ،‬ﺗُﺴﻬﻤﺎن ﺑﻨﺴﺒﺔ ﺻﻐرية ﻓﻘﻂ )‪ .(Zou and Schiebinger 2018‬وﻫﺬا ﻗﺪ‬
‫وﺑﺸﻜﻞ ﻋﺎم‪ ،‬ﻳﻤﻜﻦ أن ﺗﻜﻮن ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت‬
‫ﻳﺆدي إﱃ ﺗﺤﻴﱡﺰ ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت ﺛﻘﺎﻓﻴٍّﺎ‪.‬‬
‫ٍ‬
‫ٍ‬
‫ﻛﺎﻣﻠﺔ أو ذات ﺟﻮدة ردﻳﺌﺔ‪ ،‬ﻣﻤﺎ ﻗﺪ ﻳﺆدي إﱃ وﺟﻮد ﺗﺤﻴﱡﺰ‪ .‬ﻛﺬﻟﻚ ﻗﺪ ﻳﻜﻮن اﻟﺘﻨﺒﱡﺆ ﻣَ ﺒﻨﻴٍّﺎ‬
‫ﻏريَ‬
‫ﻋﲆ ﻗﺪ ٍْر ﺿﺌﻴﻞ ﻣﻦ اﻟﺒﻴﺎﻧﺎت‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﰲ ﺣﺎﻟﺔ اﻟﺘﻨﺒﱡﺆ ﺑﺠﺮاﺋﻢ اﻟﻘﺘﻞ‪ :‬ﺣﻴﺚ ﻻ ﻳُﻮﺟَ ﺪ‬
‫ﻛﻤﺜﺎل آﺧﺮ‪ ،‬ﻳﺸﻌﺮ‬
‫ﻫﺬا اﻟﻜﻢ اﻟﻜﺒري ﻣﻦ ﺟﺮاﺋﻢ اﻟﻘﺘﻞ‪ ،‬ﻣﻤﺎ ﻳﺠﻌﻞ اﻟﺘﻌﻤﻴﻢ أﻣ ًﺮا إﺷﻜﺎﻟﻴٍّﺎ‪.‬‬
‫ٍ‬
‫ﺑﻌﺾ اﻟﺒﺎﺣِ ﺜني ﺑﺎﻟﻘﻠﻖ إزاء ﻧﻘﺺ اﻟﺘﻨﻮﱡع ﰲ ﻓِ ﺮق ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت؛‬
‫رﺟﺎﻻ ِﺑ ً‬
‫ً‬
‫ﻴﻀﺎ ﻣﻦ اﻟﺒﻠﺪان اﻟﻐﺮﺑﻴﺔ‬
‫ﺣﻴﺚ ﻳﻜﻮن ﻣﻌﻈﻢ ﻋﻠﻤﺎء اﻟﻜﻤﺒﻴﻮﺗﺮ وﻣﻬﻨﺪﳼ اﻟﻜﻤﺒﻴﻮﺗﺮ‬
‫ﺗﱰاوَح أﻋﻤﺎرﻫﻢ ﻣﺎ ﺑني ‪ ٢٠‬ﻋﺎﻣً ﺎ و‪ ٤٠‬ﻋﺎﻣً ﺎ‪ ،‬وﻗﺪ ﺗﻨﻌﻜﺲ ﺗﺠﺎرﺑﻬﻢ اﻟﺸﺨﺼﻴﺔ وآراؤﻫﻢ‪،‬‬
‫وﺑﺎﻟﺘﺄﻛﻴﺪ ﺗﺤﻴﱡﺰاﺗﻬﻢ ﰲ اﻟﻌﻤﻠﻴﺔ‪ ،‬وﻫﻮ ﻣﺎ ﻗﺪ ﻳﺆﺛﺮ ﺳﻠﺒًﺎ ﻋﲆ اﻷﺷﺨﺎص اﻟﺬﻳﻦ ﻻ ﺗﻨﻄﺒﻖ‬
‫ﻋﻠﻴﻬﻢ ﻫﺬه اﻷوﺻﺎف‪ ،‬ﻣﺜﻞ اﻟﻨﺴﺎء‪ ،‬واﻷﺷﺨﺎص ذوي اﻹﻋﺎﻗﺔ‪ ،‬وﻛﺒﺎر اﻟﺴﻦ‪ ،‬واﻷﺷﺨﺎص‬
‫ا ُملﻠﻮﻧني‪ ،‬واﻷﺷﺨﺎص ﻣﻦ اﻟﺒﻠﺪان اﻟﻨﺎﻣﻴﺔ‪.‬‬
‫ﻗﺪ ﺗﻜﻮن اﻟﺒﻴﺎﻧﺎت ﻣُﺘﺤﻴﺰة ً‬
‫ٍ‬
‫ٍ‬
‫ُﻤﺎرﺳﺔ‬
‫ﻣﺠﻤﻮﻋﺎت ﻣُﻌﻴﻨﺔ؛ ﻷن ﻫﻨﺎك ﺗَﺤﻴﱡ ًﺰا ﰲ ﻣ‬
‫أﻳﻀﺎ ﺿﺪ‬
‫ﺑﺸﻜﻞ ﻋﺎم‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺛﻤﺔ ادﻋﺎءات ﺑﺄن ﻣﺠﺎل‬
‫ﺑﺸﻜﻞ ﺧﺎص أو ﰲ املﺠﺘﻤﻊ‬
‫ﻣﻌﻴﻨﺔ‬
‫ٍ‬
‫ٍ‬
‫ٍ‬
‫ﺑﻴﺎﻧﺎت ﻣﻦ املﺮﴇ اﻟﺬﻛﻮر‪ ،‬وﺑﺎﻟﺘﺎﱄ ﻓﺈﻧﻪ ﻣُﺘﺤﻴﱢﺰ‪ ،‬ﻛﺬﻟﻚ‬
‫ﺑﺸﻜﻞ رﺋﻴﴘ‬
‫اﻟﻄﺐ ﻳَﺴﺘﺨﺪِم‬
‫ٍ‬
‫ﺑﺸﻜﻞ أوﺳﻊ‪ .‬إذا ﻛﺎﻧﺖ‬
‫ﻫﻨﺎك اﻟﺘﺤﻴﱡﺰ ﺿﺪ اﻷﺷﺨﺎص ا ُملﻠﻮﱠﻧني وﻫﻮ ﻳُﻌﺘﱪ ﺳﺎﺋﺪًا ﰲ املﺠﺘﻤﻊ‬
‫ٍ‬
‫اﻟﺨﻮارزﻣﻴﺔ ﺗﺴﺘﺨﺪِم ﻣﺜﻞ ﻫﺬه اﻟﺒﻴﺎﻧﺎت‪ ،‬ﻓﺈن اﻟﻨﺘﺎﺋﺞ ﺳﺘﻜﻮن ً‬
‫أﻳﻀﺎ ﻣُﺘﺤﻴﺰة‪ .‬وﻛﻤﺎ ورد‬
‫‪91‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﰲ ﻣﻘﺎل ﻣﺠﻠﺔ »ﻧﻴﺘﴩ« اﻻﻓﺘﺘﺎﺣﻲ ﻋﺎم ‪ :٢٠١٦‬اﻟﺘﺤﻴﱡﺰ ﰲ املﺪﺧﻼت ﻳﺆدي إﱃ ﺗﺤﻴﱡﺰ ﰲ‬
‫ا ُملﺨﺮﺟﺎت‪ .‬وﻗﺪ ﺗﺒني ً‬
‫ِ‬
‫ِ‬
‫ﺳﻤﺎت اﻟﺘﺤﻴﱡﺰ ﻣﻦ ﺧﻼل‬
‫ﻳﻜﺘﺴﺐ‬
‫أﻳﻀﺎ أن ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ﻳﻤﻜﻦ أن‬
‫اﺳﺘﺨﺪام اﻟﺒﻴﺎﻧﺎت اﻟﻨﺼﻴﱠﺔ ﻣﻦ ﺷﺒﻜﺔ اﻟﻮﻳﺐ اﻟﻌﺎملﻴﺔ‪ ،‬ﺣﻴﺚ ﺗﻌﻜﺲ ﻫﺬه اﻟﺒﻴﺎﻧﺎت اﻟﻠﻐﻮﻳﺔ‬
‫اﻟﺜﻘﺎﻓﺔ اﻹﻧﺴﺎﻧﻴﺔ اﻟﻴﻮﻣﻴﺔ‪ ،‬ﺑﻤﺎ ﻓﻴﻬﺎ ﻣﻦ ﺗﺤﻴﺰات )‪Caliskan, Bryson, and Narayanan‬‬
‫‪ .(2017‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﺗﺤﺘﻮي ﻣﺘﻮن اﻟﻠﻐﺎت ﻧﻔﺴﻬﺎ ﻋﲆ ﺗﺤﻴﺰات ﺟﻨﺴﻴﺔ‪ .‬وا ُملﺜري‬
‫ﻟﻠﻘﻠﻖ ﰲ ﻫﺬه اﻟﺤﺎﻟﺔ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ رﺑﻤﺎ ﻳﺴﺎﻋﺪ ﰲ اﺳﺘﻤﺮار ﻫﺬه اﻟﺘﺤﻴﱡﺰات‪ ،‬ﻣﻤﺎ‬
‫ﺑﺸﻜﻞ أﻛﱪ اﻟﺠﻤﺎﻋﺎت اﻟﺘﻲ ﻛﺎﻧﺖ ﺗُﻌﺎﻧﻲ ﻣﻦ اﻟﺘﻬﻤﻴﺶ داﺋﻤً ﺎ‪ .‬ﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ أن ﻳﻈﻬﺮ‬
‫ﻳﴬﱡ‬
‫ٍ‬
‫اﻟﺘﺤﻴﺰ إذا ﻛﺎن ﻫﻨﺎك ارﺗﺒﺎط وﻟﻜﻦ ﻻ ﻳﻮﺟﺪ ﺳﺒﺐ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﻣﺠﺎل اﻟﻌﺪاﻟﺔ‬
‫اﻟﺠﻨﺎﺋﻴﺔ ﻣﺮة أﺧﺮى‪ :‬ﻗﺪ ﺗﺴﺘﻨﺘﺞ اﻟﺨﻮارزﻣﻴﺔ أﻧﻪ إذا ﻛﺎن أﺣﺪ واﻟﺪَي ا ُملﺪﱠﻋﻰ ﻋﻠﻴﻪ ﻗﺪ أُودِع‬
‫اﻟﺴﺠﻦ‪ ،‬ﻓﺈن ﻫﺬا ا ُملﺪﱠﻋﻰ ﻋﻠﻴﻪ ﻣﻦ ا ُملﺮﺟﱠ ﺢ أن ﻳُﻮدَع اﻟﺴﺠﻦ ً‬
‫أﻳﻀﺎ‪ .‬ﺣﺘﻰ ﻟﻮ ﻛﺎن ﻫﺬا اﻻرﺗﺒﺎط‬
‫ﻗﺎﺋﻤً ﺎ وﺣﺘﻰ ﻟﻮ ﻛﺎن اﻻﺳﺘﻨﺘﺎج ﺗﻨﺒﺆﻳٍّﺎ‪ ،‬ﻳﺒﺪو أﻧﻪ ﻣﻦ ﻏري اﻟﻌﺪل أن ﻳﺤﺼﻞ ﻫﺬا ا ُملﺪﱠﻋﻰ‬
‫ﻋﻠﻴﻪ ﻋﲆ ﻋﻘﻮﺑﺔ أﺷﺪ؛ ﻧﻈ ًﺮا إﱃ ﻋﺪم وﺟﻮد ﻋﻼﻗﺔ ﺳﺒﺒﻴﺔ )‪.(House of Commons 2018‬‬
‫وأﺧريًا‪ ،‬ﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ أن ﻳﻨﺸﺄ اﻟﺘﺤﻴﱡﺰ ﺑﺴﺒﺐ أن ﺻﺎﻧﻌﻲ اﻟﻘﺮارات ﻣﻦ اﻟﺒﴩ ﻳﺜﻘﻮن ﰲ دﻗﺔ‬
‫ﺗﻮﺻﻴﺎت اﻟﺨﻮارزﻣﻴﺎت أﻛﺜﺮ ﻣﻤﺎ ﻳﻨﺒﻐﻲ )‪ (CDT 2018‬وﻳﺘﺠﺎﻫﻠﻮن املﻌﻠﻮﻣﺎت اﻷﺧﺮى أو‬
‫ﻻ ِ‬
‫ﻳﻌﺘﻤﺪون ﻋﲆ ﺣُ ﻜﻤﻬﻢ اﻟﺸﺨﴢ ﺑﻤﺎ ﻓﻴﻪ اﻟﻜﻔﺎﻳﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﻳﻌﺘﻤﺪ اﻟﻘﺎﴈ‬
‫اﻋﺘﻤﺎدًا ﻛﻠﻴٍّﺎ ﻋﲆ اﻟﺨﻮارزﻣﻴﺔ وﻻ ﻳﺄﺧﺬ ﰲ اﻋﺘﺒﺎره اﻟﻌﻨﺎﴏ اﻷﺧﺮى‪ .‬وﻛﻤﺎ ﻫﻮ اﻟﺤﺎل داﺋﻤً ﺎ‬
‫ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻏريه ﻣﻦ ﺗﻘﻨﻴﺎت اﻷﺗﻤﺘﺔ‪ ،‬ﺗﻠﻌﺐ اﻟﻘﺮارات واﻟﺘﻔﺴريات اﻟﺒﴩﻳﺔ‬
‫دو ًرا ﻣﻬﻤٍّ ﺎ‪ ،‬وﻫﻨﺎك داﺋﻤً ﺎ ﺧﻄﺮ اﻻﻋﺘﻤﺎد اﻟﺰاﺋﺪ ﻋﲆ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻟﻴﺲ ِﻣﻦ اﻟﻮاﺿﺢ ﻣﺎ إذا ﻛﺎن ﻣﻦ ا ُملﻤﻜﻦ ﺗﺠﻨﱡﺐ اﻟﺘﺤﻴﱡﺰ ﻣﻦ اﻷﺳﺎس‪ ،‬أو‬
‫ﺣﺘﻰ ﻣﺎ إذا ﻛﺎن ﻳﺠﺐ ﺗﺠﻨﱡﺒﻪ‪ ،‬ﻓﺈذا ﻛﺎن ﻣﻦ اﻟﻮاﺟﺐ ﺗﺠﻨﱡﺒﻪ‪ ،‬ﻓﻤﺎ اﻟﺘﻜﻠﻔﺔ اﻟﺘﻲ ﻳﻤﻜﻦ ﺗﺤﻤﱡ ﻠﻬﺎ‬
‫ﰲ ﺳﺒﻴﻞ ذﻟﻚ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إذا ﻛﺎن ﺗﻐﻴري ﺧﻮارزﻣﻴﺔ ﺗﻌﻠﻢ اﻵﻟﺔ ﻟﺘﻘﻠﻴﻞ اﺣﺘﻤﺎﻻت‬
‫اﻟﺘﺤﻴﱡﺰ ﺳﻴﻜﻮن ﻋﲆ ﺣﺴﺎب ﺟﻌﻞ ﺗﻮﻗﻌﺎﺗﻬﺎ أﻗﻞ ﱠ‬
‫دﻗﺔ‪ ،‬ﻓﻬﻞ ﻳﺠﺐ ﻋﻠﻴﻨﺎ ﺗﻐﻴريﻫﺎ؟ ﻗﺪ ﻧُﻀﻄﺮ‬
‫إﱃ اﻻﺧﺘﻴﺎر ﻣﺎ ﺑني ﻓﻌﺎﻟﻴﺔ اﻟﺨﻮارزﻣﻴﺔ ﻣﻦ ﻧﺎﺣﻴﺔ وﻣﻜﺎﻓﺤﺔ اﻟﺘﺤﻴﱡﺰ ﻣﻦ ﻧﺎﺣﻴ ٍﺔ أﺧﺮى‪ .‬ﻫﻨﺎك‬
‫ً‬
‫أﻳﻀﺎ ﻣﺸﻜﻠﺔ ﰲ أﻧﻪ إذا ﺗ ﱠﻢ ﺗﺠﺎﻫﻞ ﺳﻤﺎت ﻣُﻌﻴﻨﺔ أو ﺗﺠﺎﻫﻠﻬﺎ ﻣﺜﻞ اﻟﻌﺮق‪ ،‬ﻓﺈن أﻧﻈﻤﺔ ﺗﻌ ﱡﻠﻢ‬
‫اﻵﻟﺔ ﻗﺪ ﺗُﺤﺪد ﻣﺎ ﻳﻌﺮف ﺑﻤﺆﴍات ﻫﺬه اﻟﺴﻤﺎت‪ ،‬ﻣﻤﺎ ﻳﺆدي ً‬
‫أﻳﻀﺎ إﱃ اﻟﺘﺤﻴﱡﺰ‪ .‬ﻋﲆ ﺳﺒﻴﻞ‬
‫املﺜﺎل‪ ،‬ﰲ ﺣﺎﻟﺔ اﻟﻌِ ﺮق‪ ،‬ﻗﺪ ﻳﻜﻮن ﻣﻦ ا ُملﻤﻜﻦ أن ﺗﺨﺘﺎر اﻟﺨﻮارزﻣﻴﺔ ﻣُﺘﻐريات أﺧﺮى ﻣﺮﺗﺒﻄﺔ‬
‫ﺑﺎﻟﻌِ ﺮق ﻣﺜﻞ اﻟﺮﻣﺰ اﻟﱪﻳﺪي‪ .‬وﻫﻞ ﻣﻦ املﻤﻜﻦ وﺟﻮد ﺧﻮارزﻣﻴﺔ ﺧﺎﻟﻴﺔ ﺗﻤﺎﻣً ﺎ ﻣﻦ اﻟﺘﺤﻴﺰ؟ ﻻ‬
‫ﻳُﻮﺟَ ﺪ ﺗﻮاﻓﻖ ﺑني اﻟﻔﻼﺳﻔﺔ أو ﺣﺘﻰ ﰲ املﺠﺘﻤﻊ ﺑﺸﺄن اﻟﻌﺪاﻟﺔ اﻟﻜﺎﻣﻠﺔ أو اﻹﻧﺼﺎف اﻟﻜﺎﻣﻞ‪.‬‬
‫‪92‬‬

‫اﻟﺘﺤﻴﺰ وﻣﻌﻨﻰ اﻟﺤﻴﺎة‬

‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬وﻛﻤﺎ أﴍْ ﻧﺎ ﰲ اﻟﻔﺼﻞ اﻟﺴﺎﺑﻖ‪ ،‬ﻓﺈن ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت ا ُملﺴﺘﺨﺪﻣﺔ ﻣﻦ ﻗِ ﺒﻞ‬
‫اﻟﺨﻮارزﻣﻴﺎت ﻫﻲ ﺗﺠﺮﻳﺪات ﻋﻦ اﻟﻮاﻗﻊ وﻫﻲ ﻧﺘﺎج اﺧﺘﻴﺎرات ﺑﴩﻳﺔ‪ ،‬وﻣﻦ ﺛ َ ﱠﻢ ﻓﻬﻲ ﻻ ﺗﻜﻮن‬
‫ﱠ‬
‫ﻳﺘﻮﻏﻞ اﻟﺘﺤﻴﺰ ﰲ ﻋﺎ َﻟﻤﻨﺎ وﻣُﺠﺘﻤﻌﺎﺗﻨﺎ؛‬
‫ﻣُﺤﺎﻳﺪة أﺑﺪًا )‪.(Kelleher and Tierney 2018‬‬
‫وﺑﺎﻟﺘﺎﱄ‪ ،‬ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أﻧﻪ ﻳﻤﻜﻦ اﻟﻘﻴﺎم ﺑﺎﻟﻜﺜري وﻳﺠﺐ اﻟﻘﻴﺎم ﺑﺎﻟﻜﺜري ﻟﺘﻘﻠﻴﻞ اﻟﺘﺤﻴﺰ‪ ،‬ﻓﺈن‬
‫ﻧﻤﺎذج اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻦ ﺗﺨﻠ َﻮ ﺗﻤﺎﻣً ﺎ ﻣﻦ اﻟﺘﺤﻴﺰ )‪.(Digital Europe 2018‬‬
‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻳﺒﺪو ﺑﺎﻟﺘﺄﻛﻴﺪ أن اﻟﺨﻮارزﻣﻴﺎت ا ُملﺴﺘﺨﺪﻣﺔ ﰲ اﺗﺨﺎذ اﻟﻘﺮار داﺋﻤً ﺎ ﻣﺎ‬
‫ﺗﻜﻮن ﻣُﺘﺤﻴﺰة ﻣﻦ ﻣﻨﻄﻠﻖ ﻛﻮﻧﻬﺎ ﺗﻤﻴﻴﺰﻳﺔ؛ إذ إﻧﻬﺎ ﻣُﺼﻤﻤﺔ ﻟﻠﺘﻤﻴﻴﺰ ﺑني ﻣُﺨﺘﻠﻒ اﻻﺣﺘﻤﺎﻻت‪.‬‬
‫ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﻋﻤﻠﻴﺔ اﻟﺘﻮﻇﻴﻒ‪ ،‬ﻳُﻔﱰَض أن ﻳﻜﻮن ﻓﺤﺺ ﱢ‬
‫اﻟﺴ َري اﻟﺬاﺗﻴﺔ ذا ﻃﺎﺑﻊ‬
‫ﻣُﺘﺤﻴﺰ وﺗﻤﻴﻴﺰي ﺗﺠﺎه ِﺳﻤﺎت املﺮﺷﺤني اﻟﺘﻲ ﺗُﻨﺎﺳﺐ اﻟﻮﻇﻴﻔﺔ‪ .‬وﻳﻜﻤُﻦ اﻟﺴﺆال اﻷﺧﻼﻗﻲ‬
‫واﻟﺴﻴﺎﳼ ﻓﻴﻤﺎ إذا ﻛﺎن ﻫﻨﺎك ﺗﻤﻴﻴﺰ ﻣُﻌني ﻏري ﻣﻨﺼﻒ وﻏري ﻋﺎدل‪ .‬وﻟﻜﻦ ﻣﺮة أﺧﺮى‪،‬‬
‫ﺗﺨﺘﻠﻒ وﺟﻬﺎت اﻟﻨﻈﺮ ﺑﺸﺄن ﻣﺎ ﻫﻮ ﻣﻨﺼﻒ وﻣﺎ ﻫﻮ ﻋﺎدل‪ .‬وﻫﺬا ﻳﺠﻌﻞ ﻗﻀﻴﺔ اﻟﺘﺤﻴﱡﺰ‬
‫ً‬
‫ﺗﻘﻨﻴﺔ وﻟﻜﻨﻬﺎ ً‬
‫أﻳﻀﺎ ﻣﺮﺗﺒﻄﺔ ﺑﺎملﻨﺎﻗﺸﺎت اﻟﺴﻴﺎﺳﻴﺔ ﺣﻮل اﻹﻧﺼﺎف واﻟﻌﺪاﻟﺔ‪ .‬ﻋﲆ‬
‫ﻟﻴﺴﺖ ﻓﻘﻂ‬
‫ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻫﻞ ﻣﻦ اﻟﻌﺪل ﻣُﻤﺎرﺳﺔ اﻟﺘﻤﻴﻴﺰ اﻹﻳﺠﺎﺑﻲ أو اﻟﺘﺪاﺑري اﻹﻳﺠﺎﺑﻴﺔ‪ ،‬اﻟﺘﻲ ﺗُﺤﺎول‬
‫ﻳﺠﺐ‬
‫ﻣﺤﻮ أﺛﺮ اﻟﺘﺤﻴﱡﺰ ﻋﻦ ﻃﺮﻳﻖ اﻟﺘﺤﻴﺰ اﻹﻳﺠﺎﺑﻲ ﻣﻊ اﻷﻓﺮاد أو اﻟﺠﻤﺎﻋﺎت املﺤﺮوﻣﺔ؟ ﻫﻞ ِ‬
‫أن ﺗﻜﻮن اﻟﻌﺪاﻟﺔ ﻋﻤﻴﺎء وﻣﺤﺎﻳﺪة — وﺑﺎﻟﺘﺎﱄ ﻫﻞ ﻳﺠﺐ أن ﺗﻜﻮن اﻟﺨﻮارزﻣﻴﺎت ﻋﻤﻴﺎء إزاء‬
‫اﻟﻌِ ﺮق‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل — أم أن اﻟﻌﺪاﻟﺔ ﺗﻌﻨﻲ ﺗﻤﻴﻴﺰ أوﻟﺌﻚ ا َملﺤﺮوﻣني ﺑﺎﻟﻔﻌﻞ ﻣﻦ أي‬
‫ﻣﻴﺰات‪ ،‬ﻣﻤﺎ ِ‬
‫ﻳﺼﻞ ﺑﻨﺎ ﰲ اﻟﻨﻬﺎﻳﺔ إﱃ ﻧﻮع ﻣﻦ اﻟﺘﺤﻴﱡﺰ واﻟﺘﻤﻴﻴﺰ )اﻟﺘﺼﺤﻴﺤﻲ(؟ وﻫﻞ ﻳﺠﺐ‬
‫ﻋﲆ اﻟﺴﻴﺎﺳﺔ ﰲ اﻟﺴﻴﺎق اﻟﺪﻳﻤﻘﺮاﻃﻲ أن ﺗُﻌﻄﻲ اﻷوﻟﻮﻳﺔ ﻟﺤﻤﺎﻳﺔ ﻣﺼﺎﻟﺢ اﻷﻏﻠﺒﻴﺔ أم ﺗﺮﻛﺰ‬
‫ﻋﲆ ﺗﻌﺰﻳﺰ ﻣﺼﺎﻟﺢ اﻷﻗﻠﻴﺔ‪ ،‬ﺣﺘﻰ وإن ﻛﺎﻧﺖ أﻗﻠﻴﺔ ﻣﺤﺮوﻣﺔ ﻗﺪﻳﻤً ﺎ أو ﺣﺎﻟﻴٍّﺎ؟‬
‫ﻳﺠﺐ أن ﺗﻜﻮن اﻟﻌﺪاﻟﺔ ﻋﻤﻴﺎء وﻣُﺤﺎﻳﺪة أم أن اﻟﻌﺪاﻟﺔ ﺗﻌﻨﻲ ﺗﻤﻴﻴﺰ أوﻟﺌﻚ املﺤﺮوﻣني ﺑﺎﻟﻔﻌﻞ ﻣﻦ‬
‫ﻫﻞ ِ‬
‫أي ﻣﻴﺰات؟‬

‫وﻫﺬا ﻳﻘﻮدﻧﺎ إﱃ اﻟﺴﺆال ﺣﻮل اﻹﺟﺮاءات‪ .‬ﺣﺘﻰ إذا اﺗﻔﻘﻨﺎ ﻋﲆ وﺟﻮد ﺗﺤﻴﱡﺰ‪ ،‬ﻓﻬﻨﺎك‬
‫ﻃﺮق ﻣﺨﺘﻠﻔﺔ ﻟﻠﺘﻌﺎﻣُﻞ ﻣﻊ املﺸﻜﻠﺔ‪ .‬وﺗﺸﻤﻞ ﻫﺬه اﻟﻄﺮق اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ وﻛﺬﻟﻚ اﻹﺟﺮاءات‬
‫ﻳﺠﺐ ﻋﻠﻴﻨﺎ اﺗﺨﺎذﻫﺎ؛‬
‫ا ُملﺠﺘﻤﻌﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ واﻟﺘﻌﻠﻴﻢ‪ .‬وﺛﻤﺔ ﺧﻼف ﺣﻮل اﻹﺟﺮاءات اﻟﺘﻲ ِ‬
‫إذ إﻧﻬﺎ ﺗﻌﺘﻤﺪ ﻣﺮة أﺧﺮى ﻋﲆ ﻣﻔﻬﻮﻣﻨﺎ ﻟﻠﻌﺪاﻟﺔ واﻹﻧﺼﺎف‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗُﺜري ﻗﻀﻴﺔ‬
‫ً‬
‫ﻳﺠﺐ أن ﻧﻘﺒﻞ اﻟﻌﺎﻟﻢ ﻛﻤﺎ ﻫﻮ أم‬
‫اﻟﺘﺪاﺑري اﻹﻳﺠﺎﺑﻴﺔ‬
‫ﻗﻀﻴﺔ أﻛﺜﺮ ﻋﻤﻮﻣﻴﺔ ﺣﻮل ﻣﺎ إذا ﻛﻨﱠﺎ ِ‬
‫‪93‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻣﻦ ﺷﺄﻧﻬﺎ ﺗﺠﻨﱡﺐ اﺳﺘﻤﺮار‬
‫ﻧﺤﻮ ﻓﻌﱠ ﺎل‬
‫أﻧﻨﺎ ﻳﺠﺐ أن ﻧُﺸ ﱢﻜﻞ ﻋﺎملﻨﺎ ا ُملﺴﺘﻘﺒﲇ ﻋﲆ ٍ‬
‫ﻳﺠﺐ أن ﻧﺴﺘﺨﺪِم ﻣﺠﻤﻮﻋﺔ‬
‫اﻟﻈﻠﻢ اﻟﺬي ﻛﺎن ﻣُﺴﺘﴩﻳًﺎ ﰲ املﺎﴈ‪ .‬ﺑﻌﺾ اﻟﻨﺎس ﻳ َﺮون أﻧﻨﺎ ِ‬
‫ٍ‬
‫ﺑﻴﺎﻧﺎت ﺗﻌﻜﺲ اﻟﻌﺎﻟﻢ اﻟﻮاﻗﻌﻲ‪ .‬وﻗﺪ ﺗُﻤﺜﻞ اﻟﺒﻴﺎﻧﺎت اﻟﺘﺤﻴﺰات املﻮﺟﻮدة ﰲ املﺠﺘﻤﻊ وﻗﺪ‬
‫ﺗُﻨﺸﺊ اﻟﺨﻮارزﻣﻴﺔ ﻧﻤﻮذﺟً ﺎ ﻣﻦ اﻟﺘﺤﻴﱡﺰات املﻮﺟﻮدة ﻟﺪى اﻟﻨﺎس اﻵن‪ ،‬وﻟﻜﻦ ﻫﺬه ﻟﻴﺴﺖ‬
‫ً‬
‫ﻣﺸﻜﻠﺔ ﻳﺠﺐ أن ﻳﻘﻠﻖ ﺑﺸﺄﻧﻬﺎ ا ُملﻄﻮرون‪ .‬ﺑﻴﻨﻤﺎ ﻳﺮى آﺧﺮون أن ﻣﺜﻞ ﻫﺬه املﺠﻤﻮﻋﺔ ﻣﻦ‬
‫ﻗﺮون ﻣﻦ اﻟﺘﺤﻴﺰ‪ ،‬وأن ﻫﺬا اﻟﺘﺤﻴﱡﺰ واﻟﺘﻤﻴﻴﺰ ﻏري ﻋﺎدل‬
‫اﻟﺒﻴﺎﻧﺎت ﻣﻮﺟﻮدة ﻓﻘﻂ ﺑﺴﺒﺐ‬
‫ٍ‬
‫ﻳﺠﺐ ﺗﻐﻴري ﺗﻠﻚ املﺠﻤﻮﻋﺔ ﻣﻦ اﻟﺒﻴﺎﻧﺎت أو اﻟﺨﻮارزﻣﻴﺔ ﻣﻦ أﺟﻞ ﺗﻌﺰﻳﺰ‬
‫وﻇﺎﻟِﻢ‪ ،‬وﻋﻠﻴﻪ ﻓﺈﻧﻪ ِ‬
‫اﻟﺘﺪاﺑري اﻹﻳﺠﺎﺑﻴﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ اﺳﺘﺠﺎﺑ ٍﺔ إﱃ ﻧﺘﺎﺋﺞ ﺧﻮارزﻣﻴﺔ اﻟﺒﺤﺚ ﰲ ﺟﻮﺟﻞ اﻟﺘﻲ‬
‫ٍ‬
‫ﺑﺒﺴﺎﻃﺔ‬
‫ﺗﺒﺪو ﻣُﺘﺤﻴﺰ ًة ﺿﺪ أﺳﺎﺗﺬة اﻟﺮﻳﺎﺿﻴﺎت اﻹﻧﺎث‪ ،‬ﻳﻤﻜﻦ ﻟﻠﻤﺮء أن ﻳﻘﻮل إن ﻫﺬا ﻳﻌﻜﺲ‬
‫ﺣﻘﻴﻘﺔ اﻟﻌﺎﻟﻢ )وأن ﻫﺬا ﻫﻮ ﺑﺎﻟﻀﺒﻂ ﻣﺎ ﻳﺠﺐ أن ﺗﻔﻌﻠﻪ ﺧﻮارزﻣﻴﺔ اﻟﺒﺤﺚ(؛ أو ﻳﻤﻜﻦ أن‬
‫ً‬
‫أوﻟﻮﻳﺔ ﻟﺼﻮر أﺳﺎﺗﺬة اﻟﺮﻳﺎﺿﻴﺎت اﻹﻧﺎث ﻣﻦ أﺟﻞ ﺗﻐﻴري اﻟﺘﺼﻮر‬
‫ﻧﺠﻌﻞ اﻟﺨﻮارزﻣﻴﺔ ﺗُﻌﻄﻲ‬
‫ورﺑﻤﺎ ﺗﻐﻴري اﻟﻌﺎ َﻟﻢ )‪ .(Fry 2018‬وﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ أن ﻧُﺤﺎول إﻧﺸﺎء ﻓِ َﺮق ﺗﻄﻮﻳﺮ ﺗﻜﻮن أﻛﺜﺮ‬
‫ﺑﺸﻜﻞ أﻓﻀﻞ اﻟﻔﺌﺎت اﻟﺘﻲ ِﻣﻦ املﺤﺘﻤَ ﻞ‬
‫ﺗﻨﻮﻋً ﺎ ﻣﻦ ﺣﻴﺚ اﻟﺨﻠﻔﻴﺔ واﻟﺮأي واﻟﺘﺠﺮﺑﺔ‪ ،‬وﺗُﻤﺜﻞ‬
‫ٍ‬
‫أن ﺗﺘﺄﺛﺮ ﺑﺎﻟﺨﻮارزﻣﻴﺔ )‪.(House of Commons 2018‬‬
‫ﻟﻦ ﻳﺼﺢ اﻟﺮأي اﻟﻘﺎﺋﻞ ﺑﺄﻧﻬﺎ ﺗﻌﻜﺲ اﻟﻮاﻗﻊ إذا ﻛﺎﻧﺖ ﻣﺠﻤﻮﻋﺔ اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﺳﻴﺘﻢ‬
‫ٍ‬
‫ﺑﻴﺎﻧﺎت ﻗﺪﻳﻤﺔ ﻻ ﺗﻌﻜﺲ اﻟﻮﺿﻊ‬
‫اﻟﺘﺪرﻳﺐ ﻋﻠﻴﻬﺎ ﻻ ﺗﻌﻜﺲ اﻟﻌﺎﻟﻢ اﻟﻮاﻗﻌﻲ وﺗﺤﺘﻮي ﻋﲆ‬
‫اﻟﺤﺎﱄ‪ .‬ﻛﻤﺎ أن اﻟﻘﺮارات املﺒﻨﻴﺔ ﻋﲆ ﻫﺬه اﻟﺒﻴﺎﻧﺎت ﺗﺴﺎﻋﺪ ﺑﺎﻟﻔﻌﻞ ﰲ اﺳﺘﻤﺮار اﻟﺘﻤﻴﻴﺰ‬
‫اﻟﺬي ﻛﺎن ﻣﻮﺟﻮدًا ﰲ املﺎﴈ ً‬
‫ﺑﺪﻻ ﻣﻦ اﻻﺳﺘﻌﺪاد ﻟﻠﻤﺴﺘﻘﺒﻞ‪ .‬وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﺛﻤﱠ ﺔ اﻋﱰاض‬
‫َ‬
‫آﺧﺮ ﻋﲆ اﻟﺮأي اﻟﻘﺎﺋﻞ ﺑﺄﻧﻬﺎ ﺗﻌﻜﺲ اﻟﻮاﻗﻊ وﻫﻮ أﻧﻪ ﺣﺘﻰ إذا ﻛﺎن اﻟﻨﻤﻮذج ﻳﻌﻜﺲ اﻟﻌﺎﻟﻢ‬
‫وأﴐار أﺧﺮى ﻗﺪ ﺗﻘﻊ ﻋﲆ أﻓﺮادٍ أو‬
‫اﻟﻮاﻗﻌﻲ‪ ،‬ﻓﺈن ﻫﺬا ﻳﻤﻜﻦ أن ﻳﺆدي إﱃ ﺗﺪاﺑري ﺗﻤﻴﻴﺰﻳﺔ‬
‫ٍ‬
‫ﻣﺠﻤﻮﻋﺎت ﺑﻌﻴﻨﻬﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﺗﺮﻓﺾ ﴍﻛﺎت اﻻﺋﺘﻤﺎن ﻣﻨﺢَ‬
‫ﻗﺮوض إﱃ ا ُملﺘﻘﺪﻣني‬
‫ٍ‬
‫ﻋﲆ أﺳﺎس ﻣﺤ ﱢﻞ اﻹﻗﺎﻣﺔ‪ ،‬أو ﻗﺪ ﺗﻔﺮض املﻮاﻗﻊ اﻹﻟﻜﱰوﻧﻴﺔ رﺳﻮﻣً ﺎ أﻛﱪ ﻋﲆ ﺑﻌﺾ اﻟﻌﻤﻼء‬
‫ً‬
‫ﻣﻘﺎرﻧﺔ ﺑﻐريﻫﻢ اﺳﺘﻨﺎدًا إﱃ ﻣﻠﻔﺎت اﻟﻌﻤﻼء اﻟﺘﻌﺮﻳﻔﻴﺔ اﻟﺘﻲ أﻧﺸﺄﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﻛﺬﻟﻚ ﻳﻤﻜﻦ أن ﺗﺘﺒﻊ املﻠﻔﺎت اﻟﺘﻌﺮﻳﻔﻴﺔ اﻷﻓﺮاد ﻋﱪ اﻟﻨﻄﺎﻗﺎت املﺨﺘﻠﻔﺔ )‪Kelleher and‬‬
‫ﺸﻜﻞ‬
‫‪ .(Tierney 2018‬وﻳﻤﻜﻦ أن ﺗﺮﺑﻂ وﻇﻴﻔﺔ اﻹﻛﻤﺎل اﻟﺘﻠﻘﺎﺋﻲ اﻟﺒﺴﻴﻄﺔ ﰲ ﻇﺎﻫﺮﻫﺎ ِﺑ ٍ‬
‫ٍ‬
‫ﺑﺠﺮﻳﻤﺔ ﻣﺎ )اﻷﻣﺮ اﻟﺬي ﻗﺪ ﻳﺆدﱢي إﱃ ﻋﻮاﻗﺐ وﺧﻴﻤﺔ(‪ ،‬ﺣﺘﻰ إذا ﻛﺎﻧﺖ ﺧﻮارزﻣﻴﺔ‬
‫ﺧﻄﺄٍ اﺳﻤَ ﻚ‬
‫ﺑﺸﻜﻞ ﺻﺤﻴﺢ؛ ﺑﻤﻌﻨﻰ أن ﻣﻌﻈﻢ اﻟﻨﺎس ﻳُﺮﻳﺪون‬
‫اﻟﺒﺤﺚ اﻟﻜﺎﻣﻨﺔ وراءﻫﺎ ﺗﻌﻜﺲ اﻟﻌﺎ َﻟﻢ‬
‫ٍ‬
‫اﻟﺒﺤﺚ ﻋﻦ اﺳﻢ املﺠﺮم وﻟﻴﺲ ﻋﻦ اﺳﻤﻚ‪ .‬وﺛﻤﱠ ﺔ ﻣﺜﺎل آﺧﺮ ﻋﲆ اﻟﺘﺤﻴﱡﺰ‪ ،‬وﻟﻜﻨﻪ رﺑﻤﺎ ﻟﻴﺲ‬
‫‪94‬‬

‫اﻟﺘﺤﻴﺰ وﻣﻌﻨﻰ اﻟﺤﻴﺎة‬

‫واﺿﺤً ﺎ ﺑﺎﻟﻘﺪْر ﻧﻔﺴﻪ‪ :‬ﻓﻨﻈﺎم اﺳﱰﺟﺎع املﻮﺳﻴﻘﻰ ا ُملﺴﺘﺨﺪَم ﰲ ﺧﺪﻣﺎت ﻣﺜﻞ »ﺳﺒﻮﺗﻴﻔﺎي«‪،‬‬
‫ٍ‬
‫ﺗﻮﺻﻴﺎت ﺑﻨﺎءً ﻋﲆ اﻟﺴﻠﻮك اﻟﺤﺎﱄ )املﺴﺎرات املﻮﺳﻴﻘﻴﺔ اﻟﺘﻲ ﻳﻨﻘﺮ ﻋﻠﻴﻬﺎ ﻣﻌﻈﻢ‬
‫اﻟﺬي ﻳﻘﺪﱢم‬
‫اﻟﻨﺎس(‪ ،‬ﻗﺪ ﻳﺘﺤﻴﱠﺰ ﺿﺪ املﻮﺳﻴﻘﻰ واملﻮﺳﻴﻘﻴني اﻟﺬﻳﻦ ﻫﻢ أﻗ ﱡﻞ ﺷﻴﻮﻋً ﺎ‪ .‬وﺣﺘﻰ إذا ﻛﺎن اﻟﻨﻈﺎم‬
‫َ‬
‫اﻟﻌﻴﺶ‬
‫وﺿﻊ ﻻ ﻳﺴﺘﻄﻴﻊ ﻓﻴﻪ ﺑﻌﺾ املﻮﺳﻴﻘﻴني‬
‫ﻳﻌﻜﺲ اﻟﻌﺎ َﻟ َﻢ اﻟﻮاﻗﻌﻲ‪ ،‬ﻓﺈن ﻫﺬا ﻳﺆدي إﱃ‬
‫ٍ‬
‫ﻣﻦ ﻣﻮﺳﻴﻘﺎﻫﻢ وﻳﺠﻌﻞ ﺑﻌﺾ املﺠﺘﻤﻌﺎت ﺗﺸﻌُ ﺮ ﺑﻌﺪم اﻟﺘﻘﺪﻳﺮ وﻋﺪم اﻻﺣﱰام‪.‬‬
‫ﺣني أن ﻫﺬه ﺣﺎﻻت واﺿﺤﺔ ﻣﻦ اﻟﺘﻤﻴﻴﺰ اﻟﺬي ﻳﻨﻄﻮي ﻋﲆ ﻣﺸﻜﻼت‪ ،‬إﻻ‬
‫ﻣﺮة أﺧﺮى‪ ،‬ﰲ ِ‬
‫ً‬
‫ﻳﺠﺐ أن ﻧﺴﺄل داﺋﻤً ﺎ‪ :‬ﻫﻞ ﻳﻤﻜﻦ أن ﻳﻜﻮن اﻟﺘﻤﻴﻴﺰ ﰲ ٍ‬
‫ﻋﺎدﻻ أم ﻻ؟ وإذا ﻛﺎن‬
‫ﺣﺎﻟﺔ ﻣُﻌﻴﻨﺔ‬
‫أﻧﻨﺎ ِ‬
‫ﻏري ﻋﺎدل‪ ،‬ﻓﻤﺎ اﻹﺟﺮاء اﻟﺬي ﺳﻴُﺘﱠ َﺨﺬ ﺣﻴﺎﻟﻪ وﻣَ ﻦ اﻟﺬي ﺳﻴﺘﺨِ ﺬُه؟ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﺎ اﻟﺬي‬
‫ﻳُﻤﻜﻦ أن ﻳﻔﻌﻠﻪ ﻋﻠﻤﺎء اﻟﻜﻤﺒﻴﻮﺗﺮ ﺣﻴﺎﻟﻪ؟ ﻫﻞ ﻳﺠﺐ أن ﻳﺠﻌﻠﻮا ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت اﻟﺘﻲ ﻳﺘﻢ‬
‫ٍ‬
‫ﺑﻴﺎﻧﺎت وﻣﺠﻤﻮﻋﺎت ﺑﻴﺎﻧﺎت »ﻣﺜﺎﻟﻴﺔ« ﻛﻤﺎ اﻗﱰح‬
‫اﻟﺘﺪرﻳﺐ ﻋﻠﻴﻬﺎ أﻛﺜﺮ ﺗﻨﻮﻋً ﺎ‪ ،‬ورﺑﻤﺎ ﻳُﻨﺸﺌﻮن‬
‫إرﻳﻚ ﻫﻮرﻓﻴﺘﺰ ﻣﻦ ﴍﻛﺔ ﻣﺎﻳﻜﺮوﺳﻮﻓﺖ )‪(Surur 2017‬؟ أم ﻳﺠﺐ أن ﺗﻌﻜﺲ ﻣﺠﻤﻮﻋﺎت‬
‫اﻟﺒﻴﺎﻧﺎت اﻟﻌﺎ َﻟﻢ؟ ﻫﻞ ﻳﺠﺐ ﻋﲆ املﻄﻮﱢرﻳﻦ ﺗﻀﻤني اﻟﺘﻤﻴﻴﺰ اﻹﻳﺠﺎﺑﻲ ﰲ ﺧﻮارزﻣﻴﺎﺗﻬﻢ‪ ،‬أم ﻳﺠﺐ‬
‫ﻋﻠﻴﻬﻢ إﻧﺸﺎء ﺧﻮارزﻣﻴﺎت »ﻋﻤﻴﺎء«؟ إن ﻛﻴﻔﻴﺔ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﺘﺤﻴﺰ ﰲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻟﻴﺴﺖ ﻣﺴﺄﻟﺔ ﺗﻘﻨﻴﺔ ﻓﺤﺴﺐ؛ ﺑﻞ ﻫﻲ ﻣﺴﺄﻟﺔ ﺳﻴﺎﺳﻴﺔ وﻓﻠﺴﻔﻴﺔ‪ .‬إن املﺴﺄﻟﺔ ﺗﺘﻌ ﱠﻠﻖ ﺑﻨﻮع‬
‫اﻟﻮاﺟﺐ ﻋﻠﻴﻨﺎ أن ﻧُﺤﺎول ﺗﻐﻴريه‪ ،‬وإذا ﻛﺎن اﻷﻣﺮ‬
‫املﺠﺘﻤﻊ واﻟﻌﺎ َﻟﻢ اﻟﺬي ﻧﺮﻳﺪه‪ ،‬وإذا ﻛﺎن ﻣﻦ‬
‫ِ‬
‫ﻛﺬﻟﻚ‪ ،‬ﻓﻤﺎ ﻫﻲ اﻟﻄﺮق املﻘﺒﻮﻟﺔ واﻟﻌﺎدﻟﺔ ﻟﺘﻐﻴريه‪ .‬إﻧﻬﺎ ً‬
‫أﻳﻀﺎ ﻣﺴﺄﻟﺔ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻟﺒﴩ ﺑﻘﺪْر ﻣﺎ‬
‫ﺗﺘﻌﻠﻖ ﺑﺎﻵﻻت‪ :‬ﻫﻞ ﻧﻌﺘﻘﺪ أن اﺗﺨﺎذ اﻟﻘﺮارات اﻟﺒﴩﻳﺔ ﻋﺎدل وﻣﻨﺼﻒ‪ ،‬وإذا ﻟﻢ ﻳﻜﻦ اﻷﻣﺮ‬
‫ﻛﺬﻟﻚ‪ ،‬ﻓﻤﺎ دور اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ رﺑﻤﺎ ﻳُﻤﻜﻦ أن ﻳُﻌ ﱢﻠﻤﻨﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺷﻴﺌًﺎ ﻋﻦ‬
‫اﻟﺒﴩ وﻣﺠﺘﻤﻌﺎﺗﻬﻢ ﻣﻦ ﺧﻼل اﻟﻜﺸﻒ ﻋﻦ ﺗَﺤﻴﱡﺰاﺗﻨﺎ‪ .‬وﻗﺪ ﺗﻜﺸﻒ ﻣﻨﺎﻗﺸﺔ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻻﺧﺘﻼل اﻟﻜﺒري ﰲ ﻣﻮازﻳﻦ اﻟﻘﻮى اﻻﺟﺘﻤﺎﻋﻴﺔ واملﺆﺳﺴﻴﺔ‪.‬‬
‫وﻫﻜﺬا ِ‬
‫ﺗﺼﻞ املﻨﺎﻗﺸﺎت ﺣﻮل أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ ﻋُ ﻤﻖ ﻗﻀﺎﻳﺎ ﻣﺠﺘﻤﻌﻴﺔ‬
‫ٍ‬
‫وﺳﻴﺎﺳﻴﺔ ﱠ‬
‫ﺑﺄﺳﺌﻠﺔ ﻓﻠﺴﻔﻴﺔ ﺣﻮل اﻟﻌﺪاﻟﺔ واﻹﻧﺼﺎف‪ ،‬وأﺳﺌﻠﺔ ﻓﻠﺴﻔﻴﺔ وﻋِﻠﻤﻴﺔ‬
‫ﺣﺴﺎﺳﺔ ﺗﺮﺗﺒﻂ‬
‫ﺣﻮل اﻟﺒﴩ وﻣﺠﺘﻤﻌﺎﺗﻬﻢ‪ .‬واﺣﺪة ﻣﻦ ﻫﺬه اﻟﻘﻀﺎﻳﺎ ﻫﻲ ﻣُﺴﺘﻘﺒﻞ اﻟﻌﻤﻞ‪.‬‬
‫ﻣﺴﺘﻘﺒﻞ اﻟﻌﻤﻞ وﻣﻌﻨﻰ اﻟﺤﻴﺎة‬
‫ﱠ‬
‫املﺘﻮﻗﻊ أن ﺗُﺤﻮﱢل اﻷﺗﻤﺘﺔ اﻟﺘﻲ ﺗﻌﺘﻤﺪ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻗﺘﺼﺎداﺗﻨﺎ وﻣﺠﺘﻤﻌﺎﺗﻨﺎ‬
‫ِﻣﻦ‬
‫ً‬
‫ٍ‬
‫ﻓﻀﻼ ﻋﻦ ﻣﺴﺘﻘﺒﻞ اﻟﺤﻴﺎة‬
‫ﺗﺴﺎؤﻻت ﺣﻮل ﻣُﺴﺘﻘﺒﻞ اﻟﻌﻤﻞ وﻣﻌﻨﺎه‪،‬‬
‫ﺑﺸﻜﻞ ﺟﺬري‪ ،‬ﻣﻤﺎ ﻳُﺜري‬
‫ٍ‬
‫اﻟﺒﴩﻳﺔ وﻣﻌﻨﺎﻫﺎ‪.‬‬
‫‪95‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ً‬
‫أوﻻ‪ ،‬ﻫﻨﺎك ﻣَ ﺨﺎوف ﻣﻦ أن ﻳﺆدي اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ ﺗﺪﻣري اﻟﻮﻇﺎﺋﻒ‪ ،‬اﻷﻣﺮ اﻟﺬي‬
‫ﻗﺪ ﻳﺆدي إﱃ اﻟﺒﻄﺎﻟﺔ اﻟﺸﺎﻣﻠﺔ‪ .‬وﻫﻨﺎك ً‬
‫أﻳﻀﺎ ﺳﺆال ﺣﻮل ﻧﻮع اﻟﻮﻇﺎﺋﻒ اﻟﺘﻲ ﻳﺴﺘﻄﻴﻊ اﻟﺬﻛﺎء‬
‫ِ‬
‫ﺳﺘﻘﺘﴫ ﻋﲆ وﻇﺎﺋﻒ ذوي اﻟﻴﺎﻗﺎت اﻟﺰرﻗﺎء )اﻟﻌﻤﺎﻟﺔ اﻟﻴﺪوﻳﺔ(‪،‬‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﺗﻮ ﱢﻟﻴﻬﺎ‪ :‬وﻫﻞ‬
‫ﻛﻤﺎ ﻳُﻄ َﻠﻖ ﻋﻠﻴﻬﺎ‪ ،‬أم أن ﻫﻨﺎك وﻇﺎﺋﻒ أﺧﺮى ﻳﻤﻜﻦ أن ﱠ‬
‫ﻳﺘﻮﻻﻫﺎ؟ ﻳﺘﻨﺒﱠﺄ ﺗﻘﺮﻳﺮ ﺷﻬري ﻟﻜ ﱟﻞ ﻣﻦ‬
‫ﺑﻨﻴﺪﻳﻜﺖ ﻓﺮي وﻣﺎﻳﻜﻞ أوزﺑﻮرن )‪ (٢٠١٣‬ﺑﺄن ‪ ٤٧‬ﰲ املﺎﺋﺔ ﻣﻦ ﺟﻤﻴﻊ اﻟﻮﻇﺎﺋﻒ ﰲ اﻟﻮﻻﻳﺎت‬
‫املﺘﺤﺪة ﻳُﻤﻜﻦ أﺗﻤﺘَﺘُﻬﺎ‪ .‬وﺗﺤﻤﻞ ﺗﻘﺎرﻳﺮ أﺧﺮى أرﻗﺎﻣً ﺎ أﻗ ﱠﻞ إﺛﺎر ًة ﻟﻠﺠﺪل‪ ،‬وﻟﻜﻦ ﻣﻌﻈﻤﻬﺎ ﻳﺘﻨﺒﺄ‬
‫ﺑﺄن ﻓﻘﺪان اﻟﻮﻇﺎﺋﻒ ﺳﻴﻜﻮن ﻛﺒريًا‪ .‬وﻳﺘﻔﻖ اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﻜﺘﱠﺎب ﻋﲆ أن اﻻﻗﺘﺼﺎد ﻗﺪ ﺗﺄﺛﺮ‬
‫ﺑﺸﻜﻞ ﻛﺒري )‪ ،(Brynjolfsson and McAffee 2014‬ﺑﻤﺎ ﰲ ذﻟﻚ اﻟﺘﻐريات‬
‫وﺳﻴﻈ ﱡﻞ ﻳﺘﺄﺛﺮ‬
‫ٍ‬
‫املﻠﺤﻮﻇﺔ اﻟﺘﻲ ﺣﺪﺛﺖ ﰲ اﻟﺘﻮﻇﻴﻒ اﻵن واﻟﺘﻲ ﺳﺘﺤﺪث ﰲ املﺴﺘﻘﺒﻞ‪ .‬وﻣﻦ ا ُمل ﱠ‬
‫ﺘﻮﻗﻊ أن ﻳﺆدي‬
‫ِ‬
‫اﻟﻌﺎﻣﻠني‪ ،‬ﻟﻴﺲ‬
‫ﻓﻘﺪان اﻟﻮﻇﺎﺋﻒ ﺑﺴﺒﺐ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ اﻟﺘﺄﺛري ﻋﲆ ﺟﻤﻴﻊ أﻧﻮاع‬
‫ﺑﺸﻜﻞ ﻣﺘﺰاﻳﺪ ﻋﲆ أداء‬
‫ﻓﻘﻂ ذوي اﻟﻴﺎﻗﺎت اﻟﺰرﻗﺎء‪ ،‬ﺣﻴﺚ أﺻﺒﺢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﺎد ًرا‬
‫ٍ‬
‫املﻬﺎم ا َملﻌﺮﻓﻴﺔ ا ُملﻌﻘﺪة‪ .‬إذا ﻛﺎن ﻫﺬا ﺻﺤﻴﺤً ﺎ‪ ،‬ﻓﻜﻴﻒ ﻳُﻤﻜﻨﻨﺎ أن ﻧُﻌِ ﱠﺪ اﻷﺟﻴﺎل اﻟﺠﺪﻳﺪة ﻟﻬﺬا‬
‫املﺴﺘﻘﺒﻞ؟ ﻣﺎذا ﻳﺠﺐ أن ﻳﺘﻌ ﱠﻠﻤﻮا؟ وﻣﺎذا ﻳﺠﺐ أن ﻳﻔﻌﻠﻮا؟ وﻣﺎذا ﻟﻮ ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻳُﻔﻴﺪ ﺑﻌﺾ اﻷﺷﺨﺎص أﻛﺜﺮ ﻣﻦ ﻏريﻫﻢ؟‬
‫ﺑﻬﺬا اﻟﺴﺆال اﻷﺧري‪ ،‬ﻧﻌﻮد ﻣﺮ ًة أﺧﺮى إﱃ ﻗﻀﺎﻳﺎ اﻟﻌﺪاﻟﺔ واﻹﻧﺼﺎف‪ ،‬اﻟﺘﻲ ﺷﻐﻠﺖ‬
‫ﱢ‬
‫ﺳﻴﻮﺳﻊ‬
‫ﺗﻔﻜري اﻟﻔﻼﺳﻔﺔ اﻟﺴﻴﺎﺳﻴني ﻟﻌﺼﻮر‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إذا ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫ﻋﺎدﻻ‪ ،‬ﻓﻤﺎ اﻟﺬي ﻳﻤﻜﻦ‬
‫اﻟﻔﺠﻮة ﺑني اﻷﺛﺮﻳﺎء واﻟﻔﻘﺮاء‪ ،‬ﻓﻬﻞ ﻫﺬا أﻣﺮ ﻋﺎدل؟ وإذا ﻟﻢ ﻳﻜﻦ‬
‫اﻟﻘﻴﺎم ﺑﻪ ﺣﻴﺎل ذﻟﻚ؟ ﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ ﺻﻴﺎﻏﺔ املﺸﻜﻠﺔ ﻣﻦ ﺣﻴﺚ ﻋﺪم املﺴﺎواة )ﻫﻞ ﺳﻴﺰﻳﺪ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ ﻋﺪم املﺴﺎواة ﰲ املﺠﺘﻤﻌﺎت وﰲ اﻟﻌﺎﻟﻢ؟( أو ﻣﻦ ﺣﻴﺚ اﻟﺘﻌ ﱡﺮض إﱃ‬
‫اﻟﺘﺄﺛريات اﻟﺴﻠﺒﻴﺔ‪ :‬ﻫﻞ ﺳﻴﺤﻈﻰ أﺻﺤﺎب اﻟﻮﻇﺎﺋﻒ واﻷﺛﺮﻳﺎء وا ُملﺘﻌ ﱠﻠﻤﻮن ﰲ اﻟﺪول املﺘﻘﺪﻣﺔ‬
‫ﺗﻜﻨﻮﻟﻮﺟﻴٍّﺎ ﺑﻔﻮاﺋﺪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﻴﻨﻤﺎ ﺳﻴﻜﻮ ُن اﻟﻌﺎﻃﻠﻮن ﻋﻦ اﻟﻌﻤﻞ واﻟﻔﻘﺮاء واﻷﻗﻞ‬
‫ً‬
‫ﻋﺮﺿﺔ ﻟﺘﺄﺛرياﺗﻪ اﻟﺴﻠﺒﻴﺔ )‪(Jansen et al. 2018‬؟ وﻟﻠﺘﻌﺎﻣُﻞ‬
‫ﺗﻌﻠﻴﻤً ﺎ ﰲ اﻟﺪول اﻟﻨﺎﻣﻴﺔ أﻛﺜﺮ‬
‫ﻣﻊ ﻗﻀﻴﺔ أﺧﻼﻗﻴﺔ وﺳﻴﺎﺳﻴﺔ أﺧﺮى أﻛﺜﺮ ﺣﺪاﺛﺔ‪ :‬ﻣﺎذا ﻋﻦ اﻟﻌﺪاﻟﺔ اﻟﺒﻴﺌﻴﺔ؟ ﻣﺎ ﻫﻮ ﺗﺄﺛري‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ اﻟﺒﻴﺌﺔ وﻋﻼﻗﺘﻨﺎ ﺑﺎﻟﺒﻴﺌﺔ؟ ﻣﺎذا ﻳﻌﻨﻲ »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺴﺘﺪام«؟‬
‫ﻫﻨﺎك ً‬
‫أﻳﻀﺎ ﺳﺆال ﺣﻮل ﻣﺎ إذا ﻛﺎﻧﺖ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺳﻴﺎﺳﺎﺗﻪ ﻣُﺮﺗﺒﻄﺔ ﺑ ِﻘﻴَﻢ‬
‫اﻟﺒﴩ وﻣﺼﺎﻟﺤﻬﻢ ﻓﻘﻂ أم ﻻ‪) .‬اﻧﻈﺮ اﻟﻔﺼﻞ اﻟﺜﺎﻧﻲ ﻋﴩ‪(.‬‬

‫‪96‬‬

‫اﻟﺘﺤﻴﺰ وﻣﻌﻨﻰ اﻟﺤﻴﺎة‬
‫ﱠ‬
‫ﺑﺸﻜﻞ‬
‫املﺘﻮﻗﻊ أن ﺗُﺤﻮﱢل اﻷﺗﻤﺘﺔ اﻟﺘﻲ ﺗﻌﺘﻤﺪ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻗﺘﺼﺎداﺗﻨﺎ وﻣُﺠﺘﻤﻌﺎﺗﻨﺎ‬
‫ﻣﻦ‬
‫ٍ‬
‫ً‬
‫ﺟﺬري‪ ،‬ﻣﻤﺎ ﻳُﺜري أﺳﺌﻠﺔ ﺣﻮل ﻣُﺴﺘﻘﺒﻞ اﻟﻌﻤﻞ وﻣﻌﻨﺎه‪ ،‬ﻓﻀﻼ ﻋﻦ ﻣُﺴﺘﻘﺒﻞ اﻟﺤﻴﺎة اﻟﺒﴩﻳﺔ وﻣﻌﻨﺎﻫﺎ‪.‬‬

‫ﺛﻤﺔ ﺳﺆال َ‬
‫ﺗﻔﱰض‬
‫آﺧﺮ ذو ﻃﺎﺑﻊ وﺟﻮدي ﻳﺘﻌ ﱠﻠﻖ ﺑﻤﻌﻨﻰ اﻟﻌﻤﻞ واﻟﺤﻴﺎة اﻟﺒﴩﻳﺔ‪ِ .‬‬
‫املﺨﺎوف ﻣﻦ ﻓﻘﺪان اﻟﻮﻇﺎﺋﻒ أن اﻟﻌﻤﻞ ﻫﻮ اﻟﻘﻴﻤﺔ اﻟﻮﺣﻴﺪة واملﺼﺪر اﻟﻮﺣﻴﺪ ﻟﻠﺪﺧﻞ واملﻌﻨﻰ‪.‬‬
‫وﻟﻜﻦ إذا ﻛﺎﻧﺖ اﻟﻮﻇﺎﺋﻒ ﻫﻲ اﻟﴚء اﻟﻮﺣﻴﺪ ذو اﻟﻘﻴﻤﺔ‪ ،‬ﻓﺮﺑﻤﺎ ﻋﻠﻴﻨﺎ ﻋﻨﺪﺋ ٍﺬ ﺧﻠﻖ املﺰﻳﺪ ﻣﻦ‬
‫اﻷﻣﺮاض اﻟﻌﻘﻠﻴﺔ‪ ،‬ورﻓﻊ ﻣُﻌﺪل اﻟﺘﺪﺧني‪ ،‬وزﻳﺎدة ﻣﻌﺪﻻت اﻟﺴﻤﻨﺔ؛ ﻷن ﻫﺬه املﺸﻜﻼت ﻫﻲ‬
‫اﻟﺘﻲ ﺗﺆدي إﱃ ﺧﻠﻖ وﻇﺎﺋﻒ‪ 1 .‬وﻧﺤﻦ ﻻ ﻧُﺮﻳﺪ ذﻟﻚ‪ .‬إذَن ﻓﻤﻦ اﻟﻮاﺿﺢ أﻧﻨﺎ ﻧﺆﻣﻦ ﺑﺄن ﻫﻨﺎك‬
‫ﻗﻴﻤً ﺎ أﺧﺮى أﻫﻢ ﻣﻦ ﺧﻠﻖ اﻟﻮﻇﺎﺋﻒ ﰲ ﺣ ﱢﺪ ذاﺗﻪ‪ .‬وملﺎذا ﻧﻌﺘﻤﺪ ﻋﲆ اﻟﻮﻇﺎﺋﻒ ﻟﺘﺤﻘﻴﻖ اﻟﺪﺧﻞ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻣﺨﺘﻠﻔﺔ‪ .‬ﻳُﻤﻜﻨﻨﺎ أن ﻧﻔﺼﻞ ﺑني‬
‫واملﻌﻨﻰ؟ ﻳُﻤﻜﻨﻨﺎ ﺗﻨﻈﻴﻢ ﻣﺠﺘﻤﻌﺎﺗﻨﺎ واﻗﺘﺼﺎداﺗﻨﺎ‬
‫ً‬
‫ً‬
‫ودﺧﻼ‪ .‬ﻓﻬﻨﺎك اﻟﻜﺜريون ﻳﻘﻮﻣﻮن ﺑﺎﻟﻌﻤﻞ‬
‫»ﻋﻤﻼ«‬
‫اﻟﻌﻤﻞ واﻟﺪﺧﻞ‪ ،‬أو ﺑﺎﻷﺣﺮى ﻣﺎ ﻧﻌﺘﱪه‬
‫ُ‬
‫ً‬
‫ﱢ‬
‫ﻣﺠﱠ ﺎﻧًﺎ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﰲ املﻨﺰل ورﻋﺎﻳﺔ اﻷﻃﻔﺎل واملﺴﻨني‪ .‬ﻓﻠﻤﺎذا ﻻ ﻳُﻌﺘﱪ ﻫﺬا »ﻋﻤﻼ«؟‬
‫ً‬
‫ﻗﻴﻤﺔ وأﻫﻤﻴﺔ ﻣﻦ ﻏريه ﻣﻦ اﻷﻋﻤﺎل؟ وملﺎذا‬
‫وملﺎذا ﻳﻜﻮن اﻟﻘﻴﺎم ﺑﺬﻟﻚ اﻟﻨﻮع ﻣﻦ اﻟﻌﻤﻞ أﻗ ﱠﻞ‬
‫ﻻ ﻧﺠﻌﻠﻪ ﻣﺼﺪ ًرا ﻟﻠﺪﺧﻞ؟ ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻳﻌﺘﻘﺪ ﺑﻌﺾ اﻷﺷﺨﺎص أن اﻷﺗﻤﺘﺔ ﻳُﻤﻜﻦ أن‬
‫ً‬
‫ﻣﺘﻌﺔ وإﺑﺪاﻋً ﺎ‪ ،‬ﻟﻴﺲ‬
‫ﺗُﺘﻴﺢ ﻟﻨﺎ املﺰﻳﺪ ﻣﻦ اﻟﺮﻓﺎﻫﻴﺔ واﻟﺮاﺣﺔ‪ .‬رﺑﻤﺎ ﻳُﻤﻜﻨﻨﺎ اﻟﻘﻴﺎم ﺑﺄﺷﻴﺎء أﻛﺜﺮ‬
‫ﺑﺎﻟﴬورة ﰲ ﺷﻜﻞ وﻇﻴﻔﺔ‪ .‬ﻳُﻤﻜﻨﻨﺎ‪ ،‬ﺑﻌﺒﺎر ٍة أﺧﺮى‪ ،‬اﻻﻋﱰاض ﻋﲆ ﻓﻜﺮة أن اﻟﺤﻴﺎة ذات‬
‫ﻋﻤﻞ ﻣﺪﻓﻮع اﻷﺟﺮ وﻣُﻨﻈﻢ ﻣ ً‬
‫املﻌﻨﻰ ﻫﻲ ﻓﻘﻂ ﺣﻴﺎة ﺗُ َ‬
‫ُﺴﺒﻘﺎ ﻣﻦ ﻗِ ﺒﻞ اﻵﺧﺮﻳﻦ أو‬
‫ﻘﴣ ﰲ أداء ٍ‬
‫ﻋﻤﻞ ﻳﺘﻢ ﰲ إﻃﺎر ﻣﺎ ﻳُﻄ َﻠﻖ ﻋﻠﻴﻪ »اﻟﺘﻮﻇﻴﻒ اﻟﺬاﺗﻲ«‪ .‬رﺑﻤﺎ ﻳُﻤﻜﻨﻨﺎ ﻓﺮض ﺗﺪاﺑري ﻣُﻌﻴﱠﻨﺔ ﻣﺜﻞ‬
‫ﺗﺤﺪﻳﺪ »دﺧﻞ أﺳﺎﳼ« ﻟﻨﺴﻤﺢ ﻟﻠﺠﻤﻴﻊ ﺑﻔﻌﻞ ﻣﺎ ﻳ َﺮوﻧﻪ ذا ﻣﻌﻨًﻰ وﻗﻴﻤﺔ‪ .‬وﺑﺎﻟﺘﺎﱄ‪ ،‬ردٍّا ﻋﲆ‬
‫ﻣﺸﻜﻠﺔ ﻣُﺴﺘﻘﺒﻞ اﻟﻌﻤﻞ‪ ،‬ﻳُﻤﻜﻨﻨﺎ أن ﻧُﻔﻜﺮ ﻓﻴﻤﺎ ﻳﺠﻌﻞ اﻟﻌﻤﻞ ذا ﻣﻌﻨًﻰ‪ ،‬وﰲ ﻧﻮع اﻟﻌﻤﻞ اﻟﺬي‬
‫ﻳﻨﺒﻐﻲ ﻟﻠﺒﴩ ﻋﻤﻠُﻪ )أو ﺑﺎﻷﺣﺮى ﻳُﺴﻤَ ﺢ ﻟﻬﻢ ﺑﻌﻤﻠﻪ(‪ ،‬وﰲ ﻛﻴﻔﻴﺔ إﻋﺎدة ﺗﻨﻈﻴﻢ ﻣﺠﺘﻤﻌﺎﺗﻨﺎ‬
‫واﻗﺘﺼﺎداﺗﻨﺎ ﺑﺤﻴﺚ ﻻ ﻳﺮﺗﺒﻂ اﻟﺪﺧﻞ ﺑﺎﻟﻮﻇﺎﺋﻒ واﻟﺘﻮﻇﻴﻒ‪.‬‬
‫ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ ﻛ ﱢﻞ ﻣﺎ ﻗﻴﻞ‪ ،‬ﻓﺈن اﻷﻓﻜﺎر اﻟﻴﻮﺗﻮﺑﻴﺔ ﺣﻮل املﺠﺘﻤﻌﺎت ا ُمل ﱠ‬
‫ﺮﻓﻬﺔ وﻏريﻫﺎ‬
‫ﱠ‬
‫ٍ‬
‫ﻣﻮﺟﺎت ﻣﻦ‬
‫ﺗﺘﺤﻘﻖ ﺣﺘﻰ اﻵن‪ .‬ﻟﻘﺪ ﺷﻬﺪﻧﺎ ﺑﺎﻟﻔﻌﻞ ﻋﺪة‬
‫ﻣﻦ اﻟﺠﻨﺎن ﻣﺎ ﺑﻌﺪ اﻟﺼﻨﺎﻋﻴﺔ ﻟﻢ‬
‫اﻷﺗﻤﺘﺔ ﺑﺪءًا ﻣﻦ اﻟﻘﺮن اﻟﺘﺎﺳﻊ ﻋﴩ ﺣﺘﻰ اﻵن‪ ،‬وﻟﻜﻦ إﱃ أي ﻣﺪًى ﺣ ﱠﺮ َرﺗﻨﺎ اﻵﻻت وأﻋﺘﻘﺖ‬
‫ً‬
‫ﺑﻌﺾ اﻷﻋﻤﺎل ا ُملﻀﺠﺮة واﻟﺨﻄرية‪ ،‬وﻟﻜﻨﻬﺎ اﺳﺘُﺨﺪِﻣﺖ ً‬
‫َ‬
‫أﻳﻀﺎ‬
‫ﻧﻴﺎﺑﺔ ﻋﻨﺎ‬
‫رﻗﺎﺑﻨﺎ؟ رﺑﻤﺎ ﺗﻮ ﱠﻟﺖ‬
‫ﻟﻼﺳﺘﻐﻼل وﻟﻢ ﺗُ ﱢ‬
‫ﺑﺸﻜﻞ ﺟﺬري اﻟﻬﻴﻜﻞ اﻟﻬ َﺮﻣﻲ ﻟﻠﻤﺠﺘﻤﻊ‪ .‬وﻗﺪ اﺳﺘﻔﺎد ﺑﻌﺾ اﻟﻨﺎس‬
‫ﻐري‬
‫ٍ‬
‫‪97‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﺳﺘﻔﺎد ًة ﻫﺎﺋﻠﺔ ﻣﻦ اﻷﺗﻤﺘﺔ‪ ،‬ﺑﻴﻨﻤﺎ ﻟﻢ ﻳﻔﻌﻞ آﺧﺮون‪ .‬ورﺑﻤﺎ ﺗﻜﻮن اﻷوﻫﺎم ﺣﻮل ﻋﺪم وﺟﻮد‬
‫ً‬
‫ﻓﻀﻼ ﻋﻦ‬
‫وﻇﺎﺋﻒ ﻫﻲ رﻓﺎﻫﻴﺔ ﻣﺤﻔﻮﻇﺔ ﻓﻘﻂ ﻷوﻟﺌﻚ اﻟﺬﻳﻦ ﻛﺎﻧﻮا ﰲ ﺟﺎﻧﺐ ا ُملﺴﺘﻔﻴﺪﻳﻦ‪.‬‬
‫ذﻟﻚ‪ ،‬ﻫﻞ ﺣ ﱠﺮ َرﺗﻨﺎ اﻵﻻت ﻟﻨﻌﻴﺶ ﺣﻴﺎ ًة ذات ﻣﻌﻨًﻰ أﻛﺜﺮ ﻣﻦ ذي ﻗﺒﻞ؟ أم أﻧﻬﺎ ﺗُﻬﺪﱢد إﻣﻜﺎﻧﻴﺔ‬
‫ﻫﺬه اﻟﺤﻴﺎة ﻧﻔﺴﻬﺎ؟ ﻫﺬا ﻧﻘﺎش ﻃﻮﻳﻞ وﻻ ﺗُﻮﺟَ ﺪ إﺟﺎﺑﺎت ﺳﻬﻠﺔ ﻋﻦ ﻫﺬه اﻷﺳﺌﻠﺔ‪ ،‬وﻟﻜﻦ‬
‫املﺨﺎوف اﻟﺘﻲ ﻟﺪﻳﻨﺎ ﺗُﻌﺪ أﺳﺒﺎﺑًﺎ وﺟﻴﻬﺔ ﻷن ﻧﺘﺸ ﱠﻜﻚ ﻋﲆ اﻷﻗﻞ ﰲ اﻟﻌﺎﻟﻢ اﻟﺠﺪﻳﺪ اﻟﺠﻤﻴﻞ‬
‫ُ‬
‫ﻧﺒﻮءات اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫اﻟﺬي رﺳﻤَ ﺘْﻪ ﻟﻨﺎ‬
‫ً‬
‫اﺳﺘﻐﻼﻻ ﻳﺠﺐ‬
‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻗﺪ ﻻ ﻳﻜﻮن اﻟﻌﻤﻞ ﺑﺎﻟﴬورة ﺷﻘﺎءً ﻳﺠﺐ ﺗﺠﻨﱡﺒُﻪ أو‬
‫ﻣﻘﺎوﻣﺘﻪ؛ ﻓﺜﻤﺔ وﺟﻬﺔ ﻧﻈﺮ أﺧﺮى ﺗُﺸري إﱃ أن اﻟﻌﻤﻞ ﻟﻪ ﻗﻴﻤﺔ ﰲ ﺣ ﱢﺪ ذاﺗﻪ‪ ،‬وأﻧﻪ ﻳﻤﻨﺢ‬
‫اﻟﻌﺎﻣﻞ ً‬
‫ُ‬
‫اﻟﺘﻮاﺻﻞ اﻻﺟﺘﻤﺎﻋﻲ ﻣﻊ اﻵﺧﺮﻳﻦ‪ ،‬واﻻﻧﺘﻤﺎء‬
‫ﻫﺪﻓﺎ وﻣﻌﻨًﻰ‪ ،‬وأن ﻟﻪ ﻓﻮاﺋﺪ ﻣُﺘﻨﻮﻋﺔ ﻣﺜﻞ‬
‫إﱃ ﳾءٍ أﻛﱪ‪ ،‬واﻟﺘﻤﺘﱡﻊ ﺑﺎﻟﺼﺤﺔ‪ ،‬واﻟﺤﺼﻮل ﻋﲆ ُﻓ َﺮص ملﻤﺎرﺳﺔ املﺴﺌﻮﻟﻴﺔ )‪Boddington‬‬
‫‪ .(2016‬ﻓﺈذا ﻛﺎن ﻫﺬا ﻫﻮ اﻟﺤﺎل‪ ،‬ﻓﻠﺮﺑﻤﺎ ﻛﺎن ﻋﻠﻴﻨﺎ أن ﻧﺤﺘﻔِ ﻆ ﺑﺎﻟﻌﻤﻞ ﻟﻠﺒﴩ؛ أو ﻋﲆ‬
‫اﻷﻗﻞ ﺑﺒﻌﺾ أﻧﻮاع اﻟﻌﻤﻞ‪ ،‬ﻛﺎﻟﻌﻤﻞ ذي ا َملﻐﺰى اﻟﺬي ﻳ ﱢ‬
‫ً‬
‫ﻓﺮﺻﺎ ﻟﺘﺤﻘﻴﻖ ﻫﺬه اﻟﻔﻮاﺋﺪ‪ .‬أو‬
‫ُﻮﻓﺮ‬
‫رﺑﻤﺎ ﻋﻠﻴﻨﺎ أن ﻧﺤﺘﻔِ ﻆ ﻋﲆ اﻷﻗﻞ ﺑﺒﻌﺾ املﻬﺎم‪ .‬وﻟﻴﺲ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن ُ‬
‫ﻳﺄﺧﺬ‬
‫ﱠ‬
‫ﻳﺘﻮﱃ ﺑﻌﺾ املﻬﺎم ذات اﻟﻘﻴﻤﺔ اﻷﻗﻞ‪ .‬وﻳُﻤﻜﻨﻨﺎ‬
‫ﻋﲆ ﻋﺎﺗﻘﻪ وﻇﺎﺋﻒ ﺑﺄﻛﻤﻠﻬﺎ‪ ،‬وﻟﻜﻦ ﻳُﻤﻜﻦ أن‬
‫أن ﻧﺘﻌﺎون ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳُﻤﻜﻨﻨﺎ اﺧﺘﻴﺎر ﻋﺪم ﺗﻔﻮﻳﺾ اﻟﻌﻤﻞ‬
‫ﻳﻘﱰﺣﻪ ﺑﻮﺳﱰوم( أو ﻳُﻤﻜﻨﻨﺎ اﺧﺘﻴﺎر اﻟﺘﻌﺎون‬
‫اﻹﺑﺪاﻋﻲ إﱃ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ )وﻫﻮ ﻣﺎ ِ‬
‫ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻠﻘﻴﺎم ﺑﺄﺷﻴﺎء إﺑﺪاﻋﻴﺔ‪ .‬ﻣﺎ ﻳُﺜري اﻟﻘﻠﻖ ﰲ ﻫﺬا اﻟﺼﺪد ﻫﻮ أﻧﻪ إذا‬
‫ﺳﺘﺘﻮﱃ اﻟﻘﻴﺎم ﺑﻜ ﱢﻞ ﻣﺎ ﻧﻘﻮم ﺑﻪ ﰲ ﺣﻴﺎﺗﻨﺎ اﻵن‪ ،‬ﻓﻠﻦ ﱠ‬
‫ﱠ‬
‫ﻳﺘﺒﻘﻰ ﻟﻨﺎ ﳾء ﻧﻘﻮم‬
‫ﻛﺎﻧﺖ اﻵﻻت‬
‫ً‬
‫ﺑﻪ‪ ،‬وﺳﻨﺠﺪ ﺣﻴﺎﺗﻨﺎ ﺑﻼ ﻣﻌﻨﻰ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻓﻨﺤﻦ ﻧﻘﻮل »إذا«؛ وﻳﺠﺐ أن ﻧﻀﻊ ﰲ اﻋﺘﺒﺎرﻧﺎ‬
‫اﻟﺸ ﱠﻚ ﻓﻴﻤﺎ ﻳﻤﻜﻦ أن ﻳﻘﻮم ﺑﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ )اﻧﻈﺮ اﻟﻔﺼﻞ اﻟﺜﺎﻟﺚ( وﺣﻘﻴﻘﺔ أن اﻟﻌﺪﻳﺪ‬
‫ً‬
‫»ﻋﻤﻼ« وﻟﻜﻨﻬﺎ ذات ﻣﻐ ًﺰى ﻛﺒري‪ ،‬وﺑﺎﻟﺘﺎﱄ ﻓﺈﻧﻨﺎ ﺳﻨﺤﺘﻔﻆ ﻋﲆ اﻷرﺟﺢ‬
‫ﻣﻦ أﻧﺸﻄﺘﻨﺎ ﻟﻴﺴﺖ‬
‫ﺑﺎﻟﻜﺜري ﻟﻨﻘﻮم ﺑﻪ‪ .‬ﻋﲆ ﻫﺬا‪ ،‬ﻳُﻤﻜﻨﻨﺎ أن ﻧﻘﻮل إن اﻟﺴﺆال اﻵن ﻟﻴﺲ ﻣﺎذا ﺳﻴﻔﻌﻞ اﻟﺒﴩ‬
‫ﱠ‬
‫ﺗﺘﻮﱃ اﻵﻻت اﻟﻘﻴﺎم ﺑﺠﻤﻴﻊ أﻋﻤﺎﻟﻬﻢ وأﻧﺸﻄﺘﻬﻢ‪ ،‬وﻟﻜﻦ أي املﻬﺎم ﻧﺮﻳﺪ أو ﻧﺤﺘﺎج إﱃ‬
‫ﻋﻨﺪﻣﺎ‬
‫اﻻﺣﺘﻔﺎظ ﺑﻬﺎ ﻟﻠﺒﴩ‪ ،‬وﻣﺎ ﻫﻲ اﻷدوار اﻟﺘﻲ ﻳﻤﻜﻦ أن ﱠ‬
‫ﻳﺘﻮﻻﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬إن ﻛﺎن‬
‫ﺳﻴﺘﻮﱃ أي أدوار‪ ،‬ﻟﺪﻋﻤﻨﺎ ﰲ ﻫﺬه املﻬﺎ ﱢم ﺑﻄﺮق أﺧﻼﻗﻴﺔ وﻣﻘﺒﻮﻟﺔ اﺟﺘﻤﺎﻋﻴٍّﺎ‪.‬‬
‫ﺧﺘﺎﻣً ﺎ‪ ،‬ﺗﺪﻋﻮﻧﺎ أﺧﻼﻗﻴﱠﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ اﻟﺘﻔﻜري ﰲ ﻣﺎﻫﻴﺔ املﺠﺘﻤﻊ َ‬
‫اﻟﺨ ﱢري‬
‫واﻟﻌﺎدل‪ ،‬وﻣﺎﻫﻴﺔ اﻟﺤﻴﺎة اﻟﺒﴩﻳﺔ ذات املﻌﻨﻰ‪ ،‬وﻣﺎﻫﻴﺔ اﻟﺪور اﻟﺬي ﺗﻀﻄﻠِﻊ ﺑﻪ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫واﻟﺬي ﻳﻤﻜﻦ أن ﺗﻀﻄﻠﻊ ﺑﻪ ﻓﻴﻤﺎ ﻳﺘﻌ ﱠﻠﻖ ﺑﻜ ﱢﻞ ذﻟﻚ‪ .‬وﻳﻤﻜﻦ أن ﺗﻜﻮن اﻟﻔﻠﺴﻔﺔ‪ ،‬ﺑﻤﺎ ﻓﻴﻬﺎ‬
‫‪98‬‬

‫اﻟﺘﺤﻴﺰ وﻣﻌﻨﻰ اﻟﺤﻴﺎة‬

‫اﻟﻔﻠﺴﻔﺔ اﻟﻘﺪﻳﻤﺔ‪ ،‬ﻣﺼﺪر إﻟﻬﺎ ٍم ﻟﻠﺘﻔﻜري ﰲ ﺗﻘﻨﻴﺎت اﻟﻴﻮم واملﺸﻜﻼت اﻟﺘﻲ ﺗﺠﻠِﺒﻬﺎ ﺑﺎﻟﻔﻌﻞ‬
‫واﻟﺘﻲ ﻳُﺤﺘﻤَ ﻞ أن ﺗﺠﻠﺒﻬﺎ ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ وا ُملﺠﺘﻤﻌﻴﺔ‪ .‬ﻓﺈذا ﻛﺎن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻳُﺜري ﻫﺬه اﻷﺳﺌﻠﺔ اﻟﻘﺪﻳﻤﺔ ﺣﻮل اﻟﺤﻴﺎة اﻟﺠﻴﺪة ذات املﻌﻨﻰ‪ ،‬ﻓﻠﺪَﻳﻨﺎ ﻣﺼﺎدر ﰲ ﻣﺨﺘﻠﻒ‬
‫اﻟﺘﻘﺎﻟﻴﺪ اﻟﻔﻠﺴﻔﻴﺔ واﻟﺪﻳﻨﻴﺔ ﻳﻤﻜﻦ أن ﺗُﺴﺎﻋﺪﻧﺎ ﰲ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ ﻫﺬه اﻷﺳﺌﻠﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫ﻛﻤﺎ اﻗﱰﺣﺖ ﺷﺎﻧﻮن ﻓﺎﻟﻮر )‪ ،(٢٠١٦‬ﻓﺈن ﺗﻘﻠﻴﺪ أﺧﻼﻗﻴﺎت اﻟﻔﻀﻴﻠﺔ اﻟﺬي وﺿﻌﻪ أرﺳﻄﻮ‬
‫وﻛﻮﻧﻔﻮﺷﻴﻮس وﻓﻼﺳﻔﺔ ﻗﺪﻣﺎء آﺧﺮون رﺑﻤﺎ ﻣﺎ زال ﻳﺴﺘﻄﻴﻊ أن ﻳُﺴﺎﻋﺪﻧﺎ اﻟﻴﻮم ﻟﻠﺘﻔﻜري ﰲ‬
‫ﻣﻌﻨﻰ ازدﻫﺎر اﻹﻧﺴﺎن وﻛﻴﻒ ﻳﻨﺒﻐﻲ أن ﻳﻜﻮن ﰲ ﻋﴫ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬وﺑﻌﺒﺎر ٍة أﺧﺮى‪ ،‬ﻗﺪ‬
‫ﺗُﻮﺟَ ﺪ ﻟﺪﻳﻨﺎ ﺑﺎﻟﻔﻌﻞ إﺟﺎﺑﺎت ﻋﻦ ﻫﺬه اﻷﺳﺌﻠﺔ‪ ،‬وﻟﻜﻦ ﻋﻠﻴﻨﺎ اﻟﻘﻴﺎم ﺑﺒﻌﺾ اﻟﻌﻤﻞ ﻟﻠﺘﻔﻜري ﰲ‬
‫ﻣﻌﻨﻰ اﻟﺤﻴﺎة اﻟﺠﻴﺪة ﰲ ﺳﻴﺎق اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺤﺪﻳﺜﺔ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﻮاﺟﻪ ﻓﻜﺮة ﺗﻄﻮﻳﺮ »أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻠﺤﻴﺎة اﻟﺠﻴﺪة«‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﺗُ ِ‬
‫ﺑﺸﻜﻞ ﻋﺎ ﱟم ﻋﺪة ﻣﺸﻜﻼت‪ .‬ﺗﺘﻤﺜﱠﻞ املﺸﻜﻠﺔ‬
‫وأﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻠﻌﺎ َﻟﻢ اﻟﻮاﻗﻌﻲ‬
‫ٍ‬
‫اﻷوﱃ ﰲ اﻟﴪﻋﺔ‪ .‬ﻳﻔﱰض ﻧﻤﻮذج أﺧﻼﻗﻴﺎت اﻟﻔﻀﻴﻠﺔ اﻟﺬي ورﺛﺘﻪ اﻟﻔﻠﺴﻔﺔ اﻟﻐﺮﺑﻴﺔ ﻣﻦ‬
‫ٍ‬
‫ﻳﺘﻐري ﺑﺒﻂءٍ وﻻ ﱠ‬
‫أرﺳﻄﻮ ﻣﺠﺘﻤﻌً ﺎ ﱠ‬
‫ﺑﴪﻋﺔ ﻛﺒرية‪ ،‬وﻳﻤﺘﻠﻚ ﻓﻴﻪ اﻟﻨﺎس‬
‫ﺗﺘﻐري ﻓﻴﻪ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫وﻗﺘًﺎ ﻟﺘﻌ ﱡﻠﻢ اﻟﺤﻜﻤﺔ اﻟﻌﻤﻠﻴﺔ؛ وﻟﺬا‪ ،‬ﻓﺈﻧﻪ ﻣﻦ ﻏري اﻟﻮاﺿﺢ ﻛﻴﻒ ﻳﻤﻜﻦ اﺳﺘﺨﺪاﻣﻪ ﻟﻠﺘﻌﺎﻣُﻞ‬
‫ﱡ‬
‫اﻟﺘﻐري )‪ (Boddington 2016‬وﻣﻊ اﻟﺘﻄﻮﱡر اﻟﴪﻳﻊ ﻟﻠﺘﻘﻨﻴﺎت ﻣﺜﻞ اﻟﺬﻛﺎء‬
‫ﻣﺠﺘﻤﻊ ﴎﻳﻊ‬
‫ﻣﻊ‬
‫ٍ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻫﻞ ﻣﺎ زال ﻟﺪَﻳﻨﺎ اﻟﻮﻗﺖ اﻟﻜﺎﰲ ﻟﻼﺳﺘﺠﺎﺑﺔ وﻟﺘﻄﻮﻳﺮ اﻟﺤﻜﻤﺔ اﻟﻌﻤﻠﻴﺔ وﻧﻘﻠﻬﺎ‬
‫ﻓﻴﻤﺎ ﻳﺘﻌ ﱠﻠﻖ ﺑﺎﺳﺘﺨﺪام ﺗﻘﻨﻴﺎت ﻣﺜﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ ﻫﻞ ﺗﺄﺗﻲ اﻷﺧﻼﻗﻴﺎت ﺑﻌﺪ ﻓﻮات‬
‫اﻷوان؟ ﻋﻨﺪﻣﺎ ُ‬
‫ﺗﻨﴩ ﺑﻮﻣﺔ ِﻣﻴﻨريﻓﺎ ﺟﻨﺎﺣَ ﻴﻬﺎ )اﻟﺘﻲ ﺗﺮﻣﺰ ﻟﻠﺤﻜﻤﺔ ﻋﻨﺪ اﻟﻴﻮﻧﺎن(‪ ،‬رﺑﻤﺎ ﻳﻜﻮن‬
‫ﺷﻜﻞ اﻟﻌﺎﻟﻢ ﻗﺪ ﱠ‬
‫ﺗﻐري ﺗﻤﺎﻣً ﺎ وﻟﻢ ﻳﻌُ ﺪ ﺑﺎﻹﻣﻜﺎن اﻟﺘﻌﺮف ﻋﻠﻴﻪ‪ .‬ﻓﻤﺎ ﻫﻮ دور ﻣﺜﻞ ﻫﺬه‬
‫اﻷﺧﻼﻗﻴﺎت‪ ،‬وﻣﺎذا ﻳﻨﺒﻐﻲ أن ﻳﻜﻮن دورﻫﺎ ﰲ ﺳﻴﺎق اﻟﺘﻄﻮﱡرات اﻟﺘﻲ ﺗﺤﺪث ﰲ اﻟﻌﺎﻟﻢ‬
‫اﻟﻮاﻗﻌﻲ؟‬
‫أﻣﺎ املﺸﻜﻠﺔ اﻟﺜﺎﻧﻴﺔ‪ ،‬ﻓﻨﻈ ًﺮا إﱃ ﺗﻨﻮع وﺗﻌﺪد وﺟﻬﺎت اﻟﻨﻈﺮ ﰲ ﻫﺬا اﻷﻣﺮ داﺧﻞ‬
‫املﺠﺘﻤﻌﺎت‪ ،‬واﻻﺧﺘﻼﻓﺎت اﻟﺜﻘﺎﻓﻴﺔ ﺑني املﺠﺘﻤﻌﺎت‪ ،‬ﻓﺈن اﻷﺳﺌﻠﺔ اﻟﺨﺎﺻﺔ ﺑﻤﺎﻫﻴﺔ اﻟﺤﻴﺎة‬
‫ﻧﺤﻮ ﻣﺨﺘﻠﻒ ﰲ‬
‫اﻟﺠﻴﺪة ذات املﻌﻨﻰ ﰲ ﻇﻞ وﺟﻮد اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻳﻤﻜﻦ اﻹﺟﺎﺑﺔ ﻋﻨﻬﺎ ﻋﲆ ٍ‬
‫اﻷﻣﺎﻛﻦ واﻟﺴﻴﺎﻗﺎت ا ُملﺨﺘﻠﻔﺔ‪ ،‬وﻫﻲ ﺗﺨﻀﻊ‪ ،‬ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻟﻌﻤﻠﻴﺔ إﱃ ﻛﻞ أﻧﻮاع اﻟﻌﻤﻠﻴﺎت‬
‫اﻟﺴﻴﺎﺳﻴﺔ اﻟﺘﻲ ﻗﺪ ﺗﻨﺘﻬﻲ أو ﻻ ﺗﻨﺘﻬﻲ ﺑﺎﻟﺘﻮاﻓﻖ‪ .‬واﻻﻋﱰاف ﺑﻬﺬا اﻟﺘﻨﻮﱡع واﻟﺘﻌﺪﱡد ﻗﺪ ﻳﺆدي‬
‫إﱃ ﻧﻬﺞ ﻳﻤﻴﻞ إﱃ اﻟﺘﻌﺪﱡدﻳﺔ‪ .‬ﻛﻤﺎ ﻳﻤﻜﻦ أن ﻳﺄﺧﺬ ﺷﻜﻞ اﻟﻨﺴﺒﻴﺔ‪ .‬وﻗﺪ أﺛﺎرت اﻟﻔﻠﺴﻔﺔ‬
‫ً‬
‫ﺧﺎﺻﺔ ﻣﺎ ﻳُﻌ َﺮف ﺑﻤﺪرﺳﺔ ﻣﺎ ﺑﻌﺪ اﻟﺤﺪاﺛﺔ‪ ،‬اﻟﻜﺜري‬
‫وﻧﻈﺮﻳﺔ املﺠﺘﻤﻊ ﰲ اﻟﻘﺮن اﻟﻌﴩﻳﻦ‪،‬‬
‫‪99‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺳﻴﺎق ﺟﻐﺮاﰲ‬
‫ﻣﻦ اﻟﺸﻜﻮك ﺣﻮل اﻹﺟﺎﺑﺎت اﻟﺘﻲ ﻳُﺰﻋَ ﻢ ﻛﻮﻧﻬﺎ ﻋﺎملﻴﺔ ﰲ ﺣني أﻧﻬﺎ ﻧﺸﺄت ﻣﻦ‬
‫ٍ‬
‫وﺗﺎرﻳﺨﻲ وﺛﻘﺎﰲ ﻣُﻌني )ﻣﻦ »اﻟﻐﺮب«‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل( وأﻧﻬﺎ ﻣﺮﺗﺒﻄﺔ ﺑﻤﺼﺎﻟﺢ وﻋﻼﻗﺎت‬
‫ُ‬
‫اﻟﺘﻮاﻓﻖ ﻣﻦ‬
‫ﻗﻮة ﻣُﻌﻴﻨﺔ‪ .‬ﻛﻤﺎ أﺛريت ﺷﻜﻮك ﺣﻮل ﻣﺎ إذا ﻛﺎﻧﺖ اﻟﺴﻴﺎﺳﺔ ﻳﺠﺐ أن ﺗﻬﺪُف إﱃ‬
‫ُ‬
‫اﻟﺘﻮاﻓﻖ‬
‫اﻷﺳﺎس )اﻧﻈﺮ أﻋﻤﺎل ﺷﺎﻧﺘﺎل ﻣﻮف‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﻮف ‪(٢٠١٣‬؛ وﻣﺎ إذا ﻛﺎن‬
‫ﻣﺮﻏﻮﺑًﺎ ﻓﻴﻪ داﺋﻤً ﺎ‪ ،‬أم أن اﻟﴫاع اﻟﴩس ﺣﻮل ﻣﺴﺘﻘﺒﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻤﻜﻦ أن‬
‫ﻳﻜﻮن ﻟﻪ ﺑﻌﺾ اﻟﻔﻮاﺋﺪ؟ وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻫﻨﺎك ﻣﺸﻜﻠﺔ أﺧﺮى ﺗﺘﻌﻠﻖ ﺑﺎﻟﻬﻴﻤﻨﺔ‪ :‬ﻓﺎﻟﺘﻔﻜري‬
‫ﰲ اﻷﺧﻼﻗﻴﺎت ﰲ اﻟﻌﺎﻟﻢ اﻟﺤﻘﻴﻘﻲ ﻳﻌﻨﻲ اﻟﺘﻔﻜري ﻟﻴﺲ ﻓﻘﻂ ﻓﻴﻤﺎ ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ ﻓﻴﻤﺎ‬
‫ﻳﺘﻌﻠﻖ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﻓﻴﻤَ ﻦ ﺳﻴﻘﺮر‪ ،‬وﻣَ ﻦ ﻳﺠﺐ ﻋﻠﻴﻪ أن ﻳﻘﺮر‪ ،‬ﻣﺴﺘﻘﺒﻞ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺑﺎﻟﺘﺎﱄ ﻣﺴﺘﻘﺒﻞ ﻣﺠﺘﻤﻌﻨﺎ‪ .‬ودﻋﻮﻧﺎ ﻧﻔﻜﺮ ﻣﻌً ﺎ ﻣﺮة أﺧﺮى ﰲ ﻗﻀﺎﻳﺎ‬
‫اﻟﺤﻜﻢ اﻟﺸﻤﻮﱄ وﻫﻴﻤﻨﺔ اﻟﴩﻛﺎت اﻟﻜﺒرية‪ .‬وإذا رﻓﻀﻨﺎ اﻟﺤﻜﻢ اﻟﺸﻤﻮﱄ واﻟﺒﻠﻮﺗﻮﻗﺮاﻃﻴﺔ‬
‫)ﺣﻜﻢ اﻷﺛﺮﻳﺎء(‪ ،‬ﻓﻤﺎذا ﻳﻌﻨﻲ اﺗﺨﺎذ ﻗﺮار دﻳﻤﻮﻗﺮاﻃﻲ ﺑﺸﺄن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ ﻣﺎ ﻫﻮ‬
‫ﻧﻮع املﻌﺮﻓﺔ املﺘﻌﻠﻖ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬي ﻳﺤﺘﺎﺟﻪ اﻟﺴﻴﺎﺳﻴﻮن واملﻮاﻃﻨﻮن؟ إذا ﻛﺎن‬
‫ﻫﻨﺎك َﻓﻬﻢ ﺿﻌﻴﻒ ﻟﻠﻐﺎﻳﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻣﺸﻜﻼﺗﻪ املﺤﺘﻤَ ﻠﺔ‪ ،‬ﻓﺈﻧﻨﺎ ﻧﻮاﺟﻪ ﺧﻄﺮ‬
‫اﻟﺘﻜﻨﻮﻗﺮاﻃﻴﺔ أو ﺑﺒﺴﺎﻃﺔ ﻋﺪم وﺟﻮد ﺳﻴﺎﺳﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ اﻹﻃﻼق‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻛﻤﺎ ﻳُﻮﺿﺢ اﻟﻔﺼﻞ اﻟﺘﺎﱄ‪ ،‬ﻳﺒﺪو أن واﺣﺪة ﻋﲆ اﻷﻗﻞ ﻣﻦ اﻟﻌﻤﻠﻴﺎت اﻟﺴﻴﺎﺳﻴﺔ‬
‫املﺘﻌﻠﻘﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺘﻲ ﻇﻬﺮت ﻣﺆﺧ ًﺮا ﺟﺎءت ﰲ اﻟﻮﻗﺖ املﻨﺎﺳﺐ‪ .‬وﺗﻠﻚ ﻫﻲ ﺻﻨﻊ‬
‫ﻈﻬﺮ‬
‫ﺳﻴﺎﺳﺎت ﺧﺎﺻﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻫﻲ ﻋﻤﻠﻴﺔ اﺳﺘﺒﺎﻗﻴﺔ‪ ،‬وﺗﻬﺪف إﱃ اﻟﺘﻮاﻓﻖ‪ ،‬وﺗُ ِ‬
‫ً‬
‫درﺟﺔ ﻣﺘﺰاﻳﺪة ﻣﻦ اﻟﺘﻘﺎرب‪ ،‬وﻳﺒﺪو أﻧﻬﺎ ﺗﻠﺘﺰم ﺑﻨﻮع ﻣﻦ اﻟﻌﺎملﻴﺔ ﺑﻼ ﺧﺠﻞ‪ ،‬وﺗﻌﺘﻤﺪ ﻋﲆ‬
‫املﻌﺮﻓﺔ اﻟﺨﺒرية‪ ،‬وﺗﺰﻋﻢ — وﻟﻮ ﻋﲆ اﻷﻗﻞ ﺷﻔﻬﻴٍّﺎ — اﺣﱰام ﻣﺒﺎدئ اﻟﺪﻳﻤﻘﺮاﻃﻴﺔ‪ ،‬وﺧﺪﻣﺔ‬
‫اﻟﺼﺎﻟﺢ اﻟﻌﺎم واملﺼﻠﺤﺔ اﻟﻌﺎﻣﺔ‪ ،‬وﻣﺸﺎرﻛﺔ ﺟﻤﻴﻊ اﻷﻃﺮاف ا َملﻌﻨﻴﺔ‪.‬‬

‫‪100‬‬

‫اﻟﻔﺼﻞ اﻟﻌﺎﴍ‬

‫اﻟﺴﻴﺎﺳﺎت اﳌﻘﱰﺣﺔ‬

‫ﱠ‬
‫ﻳﺘﻌني ﻋﲆ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت اﻹﺟﺎﺑﺔ ﻋﻨﻬﺎ‬
‫ﻣﺎ ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ وأﺳﺌﻠﺔ أﺧﺮى‬
‫ﻧﻈ ًﺮا إﱃ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ ا ُملﺮﺗﺒﻄﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻓﺈﻧﻪ ﻣﻦ اﻟﻮاﺿﺢ أن ﺷﻴﺌًﺎ ﻣﺎ‬
‫ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ‪ .‬وﻟﺬا‪ ،‬ﺗﺘﻀﻤﱠ ﻦ ﻣﻌﻈﻢ ﻣﺒﺎدرات اﻟﺴﻴﺎﺳﺎت ا ُملﺘﻌﻠﻘﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﺟﺪﻳﺮ ﺑﺎﻟﺬﻛﺮ أن ﻫﻨﺎك اﻟﻜﺜري ﻣﻦ املﺒﺎدرات ﰲ ﻫﺬا املﺠﺎل‬
‫ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ‪ ،‬وﻣﺎ املﺴﺎر اﻟﺬي‬
‫ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻟﻴﺲ ﻣﻦ اﻟﻮاﺿﺢ ﺑﺎﻟﻀﺒﻂ ﻣﺎ ِ‬
‫ﻳﺠﺐ اﺗﱢﺨﺎذه‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻟﻴﺲ واﺿﺤً ﺎ ﻛﻴﻔﻴﺔ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ ﻣﺸﻜﻠﺔ اﻟﺸﻔﺎﻓﻴﺔ أو اﻟﺘﺤﻴﱡﺰ‪،‬‬
‫ﻧﻈ ًﺮا إﱃ اﻟﺘﻘﻨﻴﺎت ﻧﻔﺴﻬﺎ‪ ،‬واﻟﺘﺤﻴﱡﺰ اﻟﺬي ﻳُﻌﺎﻧﻲ ﻣﻨﻪ املﺠﺘﻤﻊ ﺑﺎﻟﻔﻌﻞ‪ ،‬واﻵراء ا ُملﺘﺒﺎﻳﻨﺔ ﺣﻮل‬
‫اﻟﻌﺪاﻟﺔ واﻹﻧﺼﺎف‪ .‬وﻫﻨﺎك ً‬
‫أﻳﻀﺎ اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﺘﺪاﺑري ا ُملﻤﻜﻦ اﺗﺨﺎذﻫﺎ‪ :‬إذ ﻳﻤﻜﻦ أن ﺗﻌﻨﻲ‬
‫اﻟﺴﻴﺎﺳﺔ اﻟﺘﻨﻈﻴ َﻢ ﻣﻦ ﺧﻼل إﺻﺪار اﻟﻘﻮاﻧني واﻟﻠﻮاﺋﺢ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﻷﻧﻈﻤﺔ اﻟﻘﺎﻧﻮﻧﻴﺔ‪،‬‬
‫وﻟﻜﻦ ﻫﻨﺎك ً‬
‫أﻳﻀﺎ اﺳﱰاﺗﻴﺠﻴﺎت أﺧﺮى ﻗﺪ ﺗﻜﻮن ﻣُﺘﺼﻠﺔ أو ﻏري ﻣﺘﺼﻠﺔ ﺑﺎﻷﻧﻈﻤﺔ اﻟﻘﺎﻧﻮﻧﻴﺔ‪،‬‬
‫ﻣﺜﻞ اﻟﺘﺪاﺑري اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‪ ،‬وﻗﻮاﻋﺪ اﻷﺧﻼق‪ ،‬واﻟﺘﻌﻠﻴﻢ‪ .‬وﻻ ﻳﻘﺘﴫ اﻟﺘﻨﻈﻴﻢ ﻋﲆ اﻟﻘﻮاﻧني وﻟﻜﻨﻪ‬
‫أﻳﻀﺎ ﻣﻌﺎﻳري ﻣﺜﻞ ﻣﻌﺎﻳري اﻵﻳﺰو‪ .‬وﻋﻼوة ﻋﲆ ذﻟﻚ‪ ،‬ﻫﻨﺎك ً‬
‫ﻳﺘﻀﻤﻦ ً‬
‫أﻳﻀﺎ أﻧﻮاع أﺧﺮى ﻣﻦ‬
‫اﻷﺳﺌﻠﺔ اﻟﺘﻲ ﱠ‬
‫ﻳﺠﺐ اﻟﻘﻴﺎم‬
‫ﻳﺘﻌني اﻹﺟﺎﺑﺔ ﻋﻨﻬﺎ ﰲ اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ؛ ﻓﺎﻷﻣﺮ ﻟﻴﺲ ﻓﻘﻂ ﻣﺎ ِ‬
‫ﺑﻪ‪ ،‬وﻟﻜﻦ ً‬
‫ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ‪ ،‬وﻣﺎ ِﻣﻘﺪار ﻣﺎ ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ‪،‬‬
‫أﻳﻀﺎ ملﺎذا ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ‪ ،‬وﻣﺘﻰ ِ‬
‫وﻣَ ﻦ ﻳﺠﺐ ﻋﻠﻴﻪ أن ﻳﻘﻮم ﺑﻪ‪ ،‬وﻣﺎ ﻫﻲ ﻃﺒﻴﻌﺔ املﺸﻜﻠﺔ وﻣَ ﺪاﻫﺎ ودرﺟﺔ ﺧﻄﻮرﺗﻬﺎ وإﻟﺤﺎﺣﻬﺎ‪.‬‬
‫ً‬
‫أوﻻ‪ :‬ﻣﻦ ا ُملﻬﻢ ﺗﱪﻳﺮ اﻟﺘﺪاﺑري املﻘﱰﺣﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﺗﺴﺘﻨِﺪ اﻟﺴﻴﺎﺳﺔ ا ُملﻘﱰﺣﺔ‬
‫ِ‬
‫ﺗﻌﺘﻤﺪ ﻋﲆ‬
‫اﻗﱰاح ﺑﺎﻟﺘﻘﻠﻴﻞ ﻣﻦ اﺗﺨﺎذ اﻟﻘﺮارات اﻟﺘﻲ‬
‫إﱃ ﻣﺒﺎدئ ﺣﻘﻮق اﻹﻧﺴﺎن ﻟﺘﱪﻳﺮ‬
‫ٍ‬
‫ﺧﻮارزﻣﻴﺎت ﻣُﺘﺤﻴﺰة‪ .‬ﺛﺎﻧﻴًﺎ‪ :‬اﺳﺘﺠﺎﺑﺔ إﱃ اﻟﺘﻄﻮﱡر اﻟﺘﻜﻨﻮﻟﻮﺟﻲ‪ ،‬ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗﺄﺗﻲ اﻟﺴﻴﺎﺳﺔ‬
‫ﺑﻌﺪ ﻓﻮات اﻷوان‪ ،‬ﻋﻨﺪﻣﺎ ﺗﻜﻮن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻗﺪ ﱠ‬
‫ﺗﻮﻏﻠﺖ ﺑﺎﻟﻔﻌﻞ ﰲ املﺠﺘﻤﻊ ودﺧﻠﺖ ﰲ ﻛ ﱢﻞ‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﳾء‪ً .‬‬
‫ِ‬
‫ﻳﻜﺘﻤﻞ ﺗﻄﻮﻳﺮ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫ﺑﺪﻻ ﻣﻦ ذﻟﻚ‪ ،‬ﻳﻤﻜﻦ أن ﻧُﺤﺎول وﺿﻊ ﺳﻴﺎﺳﺔ ﻗﺒﻞ أن‬
‫ﱡ‬
‫ﻳﺨﺺ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻳﻤﻜﻦ اﻟﻘﻮل إن ﻫﺬا ﻣﺎ زال ﻣُﻤﻜﻨًﺎ‪،‬‬
‫وﻳﺒﺪأ اﺳﺘﺨﺪاﻣﻬﺎ‪ .‬وﻓﻴﻤﺎ‬
‫إﱃ ﺣ ﱟﺪ ﻣﺎ‪ ،‬ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن اﻟﻜﺜري ﻣﻦ اﻷﻧﻈﻤﺔ املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻮﺟﻮدة‬
‫ﺑﺎﻟﻔﻌﻞ ﺣﻮﻟﻨﺎ‪ .‬واﻟﺒُﻌﺪ اﻟﺰﻣﻨﻲ ﻣُﻬﻢ ً‬
‫أﻳﻀﺎ ﻓﻴﻤﺎ ﻳﺘﻌ ﱠﻠﻖ ﺑﺎﻟﻨﻄﺎق اﻟﺰﻣﻨﻲ ﻟﻠﺴﻴﺎﺳﺔ‪ :‬ﻫﻞ ﻫﻲ‬
‫ﻣ ﱠ‬
‫ُﺨﺼﺼﺔ ﻓﻘﻂ ﻟﻠﺴﻨﻮات اﻟﺨﻤﺲ أو اﻟﻌﴩ املﻘﺒﻠﺔ‪ ،‬أم ﺗﻬﺪف إﱃ أن ﺗُﻜﻮﱢن إﻃﺎر ﻋﻤﻞ‬
‫ﻋﲆ املﺪى اﻟﺒﻌﻴﺪ؟ ﻫﻨﺎ ﻋﻠﻴﻨﺎ أن ﻧﺨﺘﺎر‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻤﻜﻦ ﺗﺠﺎﻫﻞ اﻟﺘﻨﺒﺆات ﻋﲆ‬
‫املﺪى اﻟﺒﻌﻴﺪ واﻟﱰﻛﻴﺰ ﻋﲆ املﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ‪ ،‬ﻛﻤﺎ ﺗﻔﻌﻞ ﻣﻌﻈﻢ اﻟﺴﻴﺎﺳﺎت ا ُملﻘﱰﺣﺔ‪ ،‬أو‬
‫ﻳﻤﻜﻦ ﻃﺮح رؤﻳﺔ ملﺴﺘﻘﺒﻞ اﻹﻧﺴﺎﻧﻴﺔ‪ .‬ﺛﺎﻟﺜًﺎ‪ :‬ﻻ ﻳﺘﻔﻖ اﻟﺠﻤﻴﻊ ﻋﲆ أن ﺣ ﱠﻞ املﺸﻜﻼت ﻳﺘﻄ ﱠﻠﺐ‬
‫اﻟﻜﺜري ﻣﻦ اﻟﺘﺪاﺑري اﻟﺠﺪﻳﺪة‪ .‬ﻳﺰﻋﻢ ﺑﻌﺾ اﻷﺷﺨﺎص واملﺆﺳﺴﺎت أن اﻟﺘﴩﻳﻌﺎت اﻟﺤﺎﻟﻴﺔ‬
‫ﻛﺎﻓﻴﺔ ﻟﻠﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻓﺈذا ﻛﺎن ﻫﺬا ﻫﻮ اﻟﺤﺎل‪ ،‬ﻓﺈﻧﻪ ﻳﺒﺪو أن ا ُملﴩﱢﻋني‬
‫ﻟﻴﺴﻮا ﰲ ﺣﺎﺟﺔ إﱃ اﻟﻘﻴﺎم ﺑﺎﻟﻜﺜري‪ ،‬ﰲ ﺣني أن اﻟﺬﻳﻦ ﻳُﻔﴪون اﻟﻘﺎﻧﻮن واﻟﺬﻳﻦ ﻳُﻄﻮﱢرون‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻢ ﻣَ ﻦ ﻳﺤﺘﺎﺟﻮن إﱃ اﻟﻌﻤﻞ اﻟﺪءوب‪ .‬وﻳﻌﺘﻘﺪ َ‬
‫آﺧﺮون أﻧﻪ ﻳﺠﺐ أن ﻧُﻌﻴﺪ‬
‫اﻟﺘﻔﻜري ﰲ ﺟﻮﻫﺮ املﺠﺘﻤﻊ وﻣﺆﺳﺴﺎﺗﻪ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ أﻧﻈﻤﺘﻨﺎ اﻟﻘﺎﻧﻮﻧﻴﺔ‪ ،‬ﻣﻦ أﺟﻞ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ‬
‫املﺸﻜﻼت اﻷﺳﺎﺳﻴﺔ وإﻋﺪاد أﺟﻴﺎل ا ُملﺴﺘﻘﺒﻞ‪ .‬راﺑﻌً ﺎ‪ :‬ﻳﺠﺐ أن ﱢ‬
‫ﺗﻮﺿﺢ اﻟﺴﻴﺎﺳﺔ املﻘﱰﺣﺔ ﻣَ ﻦ‬
‫ﻳﻘﺘﴫ ﻫﺬا ﻋﲆ ٍ‬
‫ِ‬
‫ﺟﻬﺔ واﺣﺪة وإﻧﻤﺎ أﻛﺜﺮ ﻣﻦ ﺟﻬﺔ‪.‬‬
‫اﻟﺬي ﻳﺠﺐ أن ﻳﺘﺨﺬ اﻹﺟﺮاءات‪ .‬وﻗﺪ ﻻ‬
‫ً‬
‫ﺳﺆاﻻ ﺣﻮل ﻛﻴﻔﻴﺔ ﺗﻮزﻳﻊ‬
‫ﻋﻤﻞ ﺗﻜﻨﻮﻟﻮﺟﻲ‪ .‬وﻳُﺜري ﻫﺬا‬
‫ﻓﻜﻤﺎ رأﻳﻨﺎ‪ ،‬ﻳﺸﱰك اﻟﻜﺜريون ﰲ أي ٍ‬
‫ً‬
‫أﺳﺎﺳﺎ ﻫﻲ املﺴﺌﻮﻟﺔ ﻋﻦ اﺗﺨﺎذ إﺟﺮاءات‪،‬‬
‫املﺴﺌﻮﻟﻴﺔ ﻋﻦ اﻟﺴﻴﺎﺳﺔ واﻟﺘﻐﻴري‪ :‬ﻫﻞ اﻟﺤﻜﻮﻣﺎت‬
‫ٍ‬
‫إﺟﺮاءات ﺧﺎﺻﺔ ﺑﻬﺎ ﻟﻀﻤﺎن‬
‫أم ﻳﺠﺐ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﲆ اﻟﴩﻛﺎت واﻟﺼﻨﺎﻋﺔ اﺗﺨﺎذ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻷﺧﻼﻗﻲ؟ وﻓﻴﻤﺎ ﻳﺘﻌ ﱠﻠﻖ ﺑﺎﻟﴩﻛﺎت‪ ،‬ﻫﻞ ﻳﺠﺐ ﻣﺨﺎﻃﺒﺔ اﻟﴩﻛﺎت اﻟﻜﺒرية‬
‫ﻓﻘﻂ أم ً‬
‫واملﺘﻮﺳﻄﺔ اﻟﺤﺠﻢ؟ وﻣﺎ ﻫﻮ دور اﻟﻌﻠﻤﺎء )ا ُمل ﱢ‬
‫ﱢ‬
‫ﺨﺘﺼني‬
‫أﻳﻀﺎ اﻟﴩﻛﺎت اﻟﺼﻐرية‬
‫ﺑﺎﻟﻜﻤﺒﻴﻮﺗﺮ( واملﻬﻨﺪﺳني اﻷﻓﺮاد؟ وﻣﺎ ﻫﻮ دور املﻮاﻃﻨني؟‬
‫ً‬
‫ﺧﺎﻣﺴﺎ‪ :‬ﺗﻌﺘﻤﺪ اﻹﺟﺎﺑﺔ ﻋﻤﺎ ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ وﻣﻘﺪار ﻣﺎ ﻳﺠﺐ اﻟﻘﻴﺎم ﺑﻪ‪ ،‬وﻋﻦ أﺳﺌﻠﺔ‬
‫أﺧﺮى‪ ،‬ﻋﲆ ﻛﻴﻔﻴﺔ ﺗﻌﺮﻳﻒ ﻃﺒﻴﻌﺔ املﺸﻜﻠﺔ ﻧﻔﺴﻬﺎ وﻣَ ﺪاﻫﺎ ودرﺟﺔ ﺧﻄﻮرﺗﻬﺎ وإﻟﺤﺎﺣﻬﺎ‪.‬‬
‫ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻫﻨﺎك اﺗﺠﺎه ﰲ ﺳﻴﺎﺳﺎت اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ )وﰲ اﻟﻮاﻗﻊ‪ ،‬ﰲ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء‬
‫ٍ‬
‫ﻣﺸﻜﻼت ﺟﺪﻳﺪة ﰲ ﻛ ﱢﻞ ﻣﻜﺎن‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻛﻤﺎ رأﻳﻨﺎ ﰲ اﻟﻔﺼﻞ اﻟﺴﺎﺑﻖ‪،‬‬
‫اﻻﺻﻄﻨﺎﻋﻲ( ﻟﺮؤﻳﺔ‬
‫ﻓﺎﻟﻌﺪﻳﺪ ﻣﻦ املﺸﻜﻼت ﻗﺪ ﻻ ﺗﻜﻮن ﺣﻜ ًﺮا ﻋﲆ اﻟﺘﻘﻨﻴﺎت اﻟﺠﺪﻳﺪة‪ ،‬وﻟﻜﻨﻬﺎ رﺑﻤﺎ ﺗﻜﻮن ﻣﻮﺟﻮد ًة‬
‫وﻗﺖ ﻃﻮﻳﻞ‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻛﻤﺎ أﻇﻬﺮ اﻟﻨﻘﺎش ﺣﻮل اﻟﺘﺤﻴﱡﺰ‪ِ ،‬‬
‫ﻣﻨﺬ ٍ‬
‫ﻳﻌﺘﻤﺪ ﻣﺎ ﻧﻘﱰح اﻟﻘﻴﺎم‬
‫ﺑﻪ ﻋﲆ ﻛﻴﻔﻴﺔ ﺗﻌﺮﻳﻒ املﺸﻜﻠﺔ‪ :‬ﻫﻞ ﻫﻲ ﻣﺸﻜﻠﺔ ﺧﺎﺻﺔ ﺑﺎﻟﻌﺪاﻟﺔ‪ ،‬وإذا ﻛﺎﻧﺖ ﻛﺬﻟﻚ‪ ،‬ﻓﻤﺎ ﻫﻮ‬
‫‪102‬‬

‫اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ‬

‫ﻧﻮع اﻟﻌﺪاﻟﺔ ا ُملﻬﺪﱠدة؟ ﺳﻴﺸﻜﻞ ﺗﻌﺮﻳﻒ املﺸﻜﻠﺔ اﻟﺘﺪاﺑري اﻟﺘﻲ ﻧﻘﱰﺣﻬﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إذا‬
‫ٍ‬
‫ﺗﻌﺮﻳﻒ ﻣُﻌني ﻟﻠﻤﺸﻜﻠﺔ‪ .‬وأﺧريًا‪ ،‬ﻳﻠﻌﺐ‬
‫ﻗﺪﱠﻣﻨﺎ ﺗﺪاﺑري ﻟﻠﻌﻤﻞ اﻹﻳﺠﺎﺑﻲ‪ ،‬ﻓﺈن ﻫﺬا ﻳﺴﺘﻨﺪ إﱃ‬
‫ً‬
‫أﻳﻀﺎ ﺗﻌﺮﻳﻒ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ دو ًرا ﰲ ﺗﺤﺪﻳﺪ اﻟﺴﻴﺎﺳﺔ املﻘﱰﺣﺔ وﻧﻄﺎﻗﻬﺎ‪ ،‬وﻗﺪ ﻛﺎن ﻫﺬا‬
‫َ‬
‫ﺴﺘﺤﺴﻦ‬
‫اﻟﺘﻌﺮﻳﻒ داﺋﻤً ﺎ ﻣُﺜريًا ﻟﻠﺠﺪل واﻟﻨﻘﺎﺷﺎت‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻫﻞ ِﻣﻦ ا ُملﻤﻜﻦ وﻣﻦ ا ُمل‬
‫أن ﻧُﻤﻴﺰ ﺑﻮﺿﻮح ﺑني اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺨﻮارزﻣﻴﺎت اﻟﺬﻛﻴﺔ ا ُملﺴﺘﻘﻠﺔ‪ ،‬أو ﺑني اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ وﺗﻘﻨﻴﺎت اﻷﺗﻤﺘﺔ؟ ﺟﻤﻴﻊ ﻫﺬه اﻷﺳﺌﻠﺔ ﺗﺠﻌﻞ ﻣﻦ ﺻﻨﻊ اﻟﺴﻴﺎﺳﺎت ا ُملﺘﻌﻠﻘﺔ‬
‫ﺑﺸﻜﻞ ﻛﺒري‪ .‬وﺑﺎﻟﻔﻌﻞ‪ ،‬ﻧﺠﺪ اﻟﻌﺪﻳﺪ ﻣﻦ اﻻﺧﺘﻼﻓﺎت‬
‫ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أﻣ ًﺮا ﻗﺪ ﻳُﺜري اﻟﺠﺪل‬
‫ٍ‬
‫واﻟﺠﺪاﻻت‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل ﺣﻮل ﻣﺪى اﻟﺤﺎﺟﺔ إﱃ ﺗﴩﻳﻌﺎت ﺟﺪﻳﺪة‪ ،‬وﺣﻮل املﺒﺎدئ اﻟﺘﻲ‬
‫ﻳﺠﺐ اﻻﺳﺘﻨﺎد إﻟﻴﻬﺎ ﺑﺎﻟﻀﺒﻂ ﻟﺘﱪﻳﺮ اﻟﺘﺪاﺑري‪ ،‬وﺣﻮل ﻣﺴﺄﻟﺔ ﻣﺎ إذا ﻛﺎن ﻳﻨﺒﻐﻲ ﺗﺤﻘﻴﻖ ﺗﻮازن‬
‫ﺑني أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻻﻋﺘﺒﺎرات اﻷﺧﺮى )ﻣﺜﻞ ﺗﻨﺎﻓﺴﻴﺔ اﻟﴩﻛﺎت واﻻﻗﺘﺼﺎد(‪.‬‬
‫ً‬
‫درﺟﺔ ﻣﻠﺤﻮﻇﺔ ﻣﻦ اﻟﺘﻘﺎ ُرب‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬إذا ﻓ ﱠﻜﺮﻧﺎ ﰲ وﺛﺎﺋﻖ اﻟﺴﻴﺎﺳﺔ اﻟﻔﻌﻠﻴﺔ‪ ،‬ﻓﺴﻨﺠﺪ‬
‫املﺒﺎدئ اﻷﺧﻼﻗﻴﺔ واﻟﺘﱪﻳﺮات‬
‫ﻟﻘﺪ أدﱠى اﻹﺣﺴﺎس اﻟﻮاﺳﻊ اﻻﻧﺘﺸﺎر ﺑﴬورة وأﻫﻤﻴﺔ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﺘﺤﺪﻳﺎت اﻷﺧﻼﻗﻴﺔ‬
‫وا ُملﺠﺘﻤﻌﻴﺔ اﻟﺘﻲ أﺛﺎرﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ َﺳﻴﻞ ﻣﻦ املﺒﺎدرات ووﺛﺎﺋﻖ اﻟﺴﻴﺎﺳﺎت اﻟﺘﻲ‬
‫ﻻ ﺗُﻌ ﱢﺮف ﻓﻘﻂ ﺑﻌﺾ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ ا ُملﺮﺗﺒﻄﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻟﻜﻨﻬﺎ ﺗﻬﺪف‬
‫أﻳﻀﺎ إﱃ ﺗﻮﻓري ﺗﻮﺟﻴﻬﺎت ِﻣﻌﻴﺎرﻳﺔ ﻟﻠﺴﻴﺎﺳﺎت‪ .‬وﻗﺪ ُ‬
‫ً‬
‫اﻗﱰﺣﺖ ﺳﻴﺎﺳﺎت ﺧﺎﺻﺔ ﺑﺎﻟﺬﻛﺎء‬
‫ﻋﻨﴫ أﺧﻼﻗﻲ ﻣﻦ ﻗِ ﺒﻞ ﻣﺠﻤﻮﻋﺔ ﻣﺘﻨﻮﻋﺔ ﻣﻦ اﻟﺠﻬﺎت‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﺗﺸﺘﻤﻞ ﻋﲆ‬
‫ٍ‬
‫اﻟﺤﻜﻮﻣﺎت واﻟﻬﻴﺌﺎت اﻟﺤﻜﻮﻣﻴﺔ ﻣﺜﻞ اﻟﻠﺠﺎن اﻟﻮﻃﻨﻴﺔ ﻟﻸﺧﻼﻗﻴﺎت‪ ،‬وﴍﻛﺎت اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫ﻣﺜﻞ ﺟﻮﺟﻞ‪ ،‬واملﻬﻨﺪﺳني وﻣﻨﻈﻤﺎﺗﻬﻢ املﻬﻨﻴﺔ ﻣﺜﻞ ﻣﻌﻬﺪ ﻣﻬﻨﺪﳼ اﻟﻜﻬﺮﺑﺎء واﻹﻟﻜﱰوﻧﻴﺎت‪،‬‬
‫واﻟﻬﻴﺌﺎت اﻟﺤﻜﻮﻣﻴﺔ اﻟﺪوﻟﻴﺔ ﻣﺜﻞ اﻻﺗﺤﺎد اﻷوروﺑﻲ‪ ،‬واﻟﺠﻬﺎت ﻏري اﻟﺤﻜﻮﻣﻴﺔ وﻏري اﻟﻬﺎدﻓﺔ‬
‫ﻟﻠﺮﺑﺢ‪ ،‬واﻟﺒﺎﺣﺜني‪.‬‬
‫ﻟﻘﺪ أدﱠى اﻹﺣﺴﺎس اﻟﻮاﺳﻊ اﻻﻧﺘﺸﺎر ﺑﴬورة وأﻫﻤﻴﺔ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﺘﺤﺪﻳﺎت اﻷﺧﻼﻗﻴﺔ وا ُملﺠﺘﻤﻌﻴﺔ‬
‫اﻟﺘﻲ أﺛﺎرﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﱃ َﺳﻴﻞ ﻣﻦ املﺒﺎدرات ووﺛﺎﺋﻖ اﻟﺴﻴﺎﺳﺎت‪.‬‬

‫إذا راﺟﻌﻨﺎ ﺑﻌﺾ املﺒﺎدرات واملﻘﱰﺣﺎت اﻟﺤﺪﻳﺜﺔ‪ ،‬ﱠ‬
‫ﻳﺘﺒني أن ﻣﻌﻈﻢ اﻟﻮﺛﺎﺋﻖ ﺗﺒﺪأ ﺑﺘﱪﻳﺮ‬
‫اﻟﺴﻴﺎﺳﺔ ﻣﻦ ﺧﻼل ﺗﻮﺿﻴﺢ املﺒﺎدئ‪ ،‬ﺛﻢ ﺗُﻘﺪم ﺑﻌﺾ اﻟﺘﻮﺻﻴﺎت ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﺎملﺸﻜﻼت‬
‫‪103‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻷﺧﻼﻗﻴﺔ ا ُملﺤﺪدة‪ .‬وﻛﻤﺎ ﺳﻨﺮى‪ ،‬ﻫﺬه املﺸﻜﻼت واملﺒﺎدئ ﺷﺪﻳﺪة اﻟﺘﺸﺎﺑُﻪ‪ .‬وﰲ ﻛﺜري ﻣﻦ‬
‫ِ‬
‫ﺗﻌﺘﻤﺪ املﺒﺎدرات ﻋﲆ ﻣﺒﺎدئ أﺧﻼﻗﻴﺔ ﻋﺎﻣﺔ وﻣﺒﺎدئ ﻣﻦ ﻗﺎﻧﻮن أﺧﻼﻗﻴﺎت املﻬﻨﺔ‪.‬‬
‫اﻟﺤﺎﻻت‪،‬‬
‫راﺟﻊ ﻣﻌﻜﻢ ﺑﻌﺾ املﻘﱰﺣﺎت‪.‬‬
‫ﻓﺪﻋﻮﻧﻲ أ ُ ِ‬
‫ﺗﺮﻓﺾ ﻣﻌﻈﻢ املﻘﱰﺣﺎت ﺳﻴﻨﺎرﻳﻮ اﻟﺨﻴﺎل اﻟﻌﻠﻤﻲ اﻟﺬي ﺗﺴﺘﻮﱄ ﻓﻴﻪ اﻵﻻت اﻟﻔﺎﺋﻘﺔ‬
‫ﱠ‬
‫وﺗﺘﻮﱃ ﻓﻴﻪ اﻟﺴﻴﻄﺮة‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﻓﱰة رﺋﺎﺳﺔ أوﺑﺎﻣﺎ‪،‬‬
‫اﻟﺬﱠﻛﺎء ﻋﲆ زﻣﺎم اﻷﻣﻮر‬
‫ﻧﴩت ﺣﻜﻮﻣﺔ اﻟﻮﻻﻳﺎت املﺘﺤﺪة ﺗﻘﺮﻳ ًﺮا ﺑﻌﻨﻮان »اﻻﺳﺘﻌﺪاد ُملﺴﺘﻘﺒﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ«‪،‬‬
‫ً‬
‫ﴏاﺣﺔ ﻋﲆ أن املﺨﺎوف اﻟﻄﻮﻳﻠﺔ اﻷﻣﺪ ﺑﺸﺄن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻔﺎﺋﻖ اﻟﻌﺎم‬
‫ﺗﺆ ﱢﻛﺪ ﻓﻴﻪ‬
‫»ﻳﺠﺐ أﻻ ﻳﻜﻮن ﻟﻬﺎ ﺗﺄﺛري ﻛﺒري ﻋﲆ اﻟﺴﻴﺎﺳﺔ اﻟﺤﺎﻟﻴﺔ« )املﻜﺘﺐ اﻟﺘﻨﻔﻴﺬي ﻟﻠﺮﺋﻴﺲ ‪،٢٠١٦‬‬
‫وﺑﺪﻻ ﻣﻦ ذﻟﻚ‪ ،‬ﻳﺘﻨﺎول اﻟﺘﻘﺮﻳﺮ ا ُملﺸﻜﻼت اﻟﺤﺎﻟﻴﺔ وا ُمل ﱠ‬
‫ً‬
‫ﺘﻮﻗﻌﺔ ﰲ املﺴﺘﻘﺒﻞ اﻟﻘﺮﻳﺐ اﻟﺘﻲ‬
‫‪.(٨‬‬
‫ﻳُﺜريﻫﺎ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ ،‬ﻣﺜﻞ اﻟﺘﺤﻴﱡﺰ وﻣﺸﻜﻠﺔ أﻧﻪ ﺣﺘﻰ ا ُملﻄﻮرون ﻗﺪ ﻻ ﻳﻔﻬﻤﻮن ﻧﻈﺎﻣﻬﻢ ﺑﻤﺎ ﻓﻴﻪ‬
‫اﻟﻜﻔﺎﻳﺔ ﻟﺘﺠﻨﱡﺐ ﻣﺜﻞ ﻫﺬه اﻟﻌﻮاﻗﺐ‪ .‬وﻳﺆ ﱢﻛﺪ اﻟﺘﻘﺮﻳﺮ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣُﻔﻴﺪ ﻟﻼﺑﺘﻜﺎر‬
‫واﻟﻨﻤﻮ اﻻﻗﺘﺼﺎدي وﻳُﺸﺪﱢد ﻋﲆ اﻟﺮﻗﺎﺑﺔ اﻟﺬاﺗﻴﺔ‪ ،‬وﻟﻜﻨﻪ ﻳﻘﻮل إن ﺣﻜﻮﻣﺔ اﻟﻮﻻﻳﺎت املﺘﺤﺪة‬
‫ﻟﺰم اﻷﻣﺮ‪.‬‬
‫ﻳُﻤﻜﻨﻬﺎ ﻣﺮاﻗﺒﺔ ﺳﻼﻣﺔ اﻟﺘﻄﺒﻴﻘﺎت وﻋﺪاﻟﺘﻬﺎ‪ ،‬وﺗﻌﺪﻳﻞ اﻷﻃﺮ اﻟﻘﺎﻧﻮﻧﻴﺔ إذا ِ‬
‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﺗﻤﻠﻚ اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﺪول اﻷوروﺑﻴﺔ ﺣﺎﻟﻴٍّﺎ اﺳﱰاﺗﻴﺠﻴﺎت ﻟﻠﺬﻛﺎء‬
‫ً‬
‫ﻫﺪﻓﺎ‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﺗﺘﻀﻤﱠ ﻦ ﻋﻨﴫًا أﺧﻼﻗﻴٍّﺎ‪ .‬وﻳُﻌﺪ »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺑﻞ ﻟﻠﺘﻔﺴري«‬
‫ﻣﺸﱰ ًﻛﺎ ﺑني اﻟﻌﺪﻳﺪ ﻣﻦ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‪ .‬ﻳﻘﻮل ﻣﺠﻠﺲ ﻋﻤﻮم املﻤﻠﻜﺔ املﺘﺤﺪة )‪(٢٠١٨‬‬
‫إن اﻟﺸﻔﺎﻓﻴﺔ وﺣﻖ اﻟﺘﻔﺴري أﻣﻮر أﺳﺎﺳﻴﺔ ﻟﻨﺘﻤ ﱠﻜﻦ ﻣﻦ ﻣﺴﺎءﻟﺔ اﻟﺨﻮارزﻣﻴﺎت‪ ،‬وﻳﺠﺐ‬
‫ﻋﲆ اﻟﺼﻨﺎﻋﺎت واﻟﺠﻬﺎت اﻟﺘﴩﻳﻌﻴﺔ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ ﻣﺴﺄﻟﺔ اﺗﺨﺎذ اﻟﻘﺮارات ا ُملﺘﺤﻴﺰة ﻣﻦ‬
‫ﻗِ ﺒﻞ اﻟﺨﻮارزﻣﻴﺎت‪ .‬ﻛﺬﻟﻚ ﺗﻔﺤﺺ ﻟﺠﻨﺔ ﻣﺠﻠﺲ ﻟﻮردات املﻤﻠﻜﺔ املﺘﺤﺪة املﺨﺘﺎرة ا َملﻌﻨﻴﺔ‬
‫ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺘﺪاﻋِ ﻴﺎت اﻷﺧﻼﻗﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﰲ ﻓﺮﻧﺴﺎ‪ ،‬ﻳﻘﱰح ﺗﻘﺮﻳﺮ‬
‫ُ‬
‫ﺗﻔﺎﻗﻢ ﻣﺸﻜﻼت‬
‫ﻓﻴﻼﻧﻲ اﻟﻌﻤﻞ ﻧﺤﻮ ﺗﻄﻮﻳﺮ »ذﻛﺎء اﺻﻄﻨﺎﻋﻲ ذي ﻣﻌﻨًﻰ« ﻻ ﻳﺆدي إﱃ‬
‫اﻹﻗﺼﺎء‪ ،‬أو ﻳﺰﻳﺪ ﻣﻦ اﻟﺘﻔﺎوت اﻻﺟﺘﻤﺎﻋﻲ‪ ،‬أو ﻳﺆدي إﱃ ﻣﺠﺘﻤﻊ ﺗﺤ ُﻜﻤﻨﺎ ﻓﻴﻪ ﺧﻮارزﻣﻴﺎت‬
‫ً‬
‫»ﺻﻨﺎدﻳﻖ ﺳﻮداء«؛ إذ ﻳﺠﺐ أن ﻳﻜﻮن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫وﺻﺪﻳﻘﺎ ﻟﻠﺒﻴﺌﺔ‬
‫ﻗﺎﺑﻼ ﻟﻠﺘﻔﺴري‬
‫ً‬
‫ﻣﺠﻠﺴﺎ اﺳﺘﺸﺎرﻳٍّﺎ وﻃﻨﻴٍّﺎ ﻣﻌﻨﻴٍّﺎ ﺑﺎﻟﺮوﺑﻮﺗﺎت‬
‫)‪ .(Villani 2018‬ﻛﻤﺎ أﻧﺸﺄت اﻟﻨﻤﺴﺎ ﻣﺆﺧ ًﺮا‬
‫ٍ‬
‫ٍ‬
‫ﻟﺴﻴﺎﺳﺔ ﺗﺴﺘﻨِﺪ إﱃ ﺣﻘﻮق اﻹﻧﺴﺎن‪ ،‬واﻟﻌﺪاﻟﺔ‬
‫ﺗﻮﺻﻴﺎت‬
‫واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ 1 ،‬واﻟﺬي ﻗﺪﱠم‬
‫واﻹﻧﺼﺎف‪ ،‬واﻹﴍاك واﻟﺘﻀﺎﻣُﻦ‪ ،‬واﻟﺪﻳﻤﻘﺮاﻃﻴﺔ واملﺸﺎرﻛﺔ‪ ،‬وﻋﺪم اﻟﺘﻤﻴﻴﺰ‪ ،‬واملﺴﺌﻮﻟﻴﺔ‪،‬‬
‫ﻗﺎﺑﻞ ﻟﻠﺘﻔﺴري‬
‫و ِﻗﻴَﻢ أﺧﺮى ﺷﺒﻴﻬﺔ‪ .‬ﻛﻤﺎ ﺗُﻮﴆ ورﻗﺘﻬﺎ اﻟﺒﻴﻀﺎء ﺑﺘﻄﻮﻳﺮ ذﻛﺎء اﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ً‬
‫ﴏاﺣﺔ إن املﺴﺌﻮﻟﻴﺔ ﺗﻈ ﱡﻞ ﻋﲆ ﻋﺎﺗﻖ اﻟﺒﴩ؛ وﻻ ﻳﻤﻜﻦ أن ﻳﻜﻮن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫وﺗﻘﻮل‬
‫‪104‬‬

‫اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ‬

‫ً‬
‫ﻣﺴﺌﻮﻻ أﺧﻼﻗﻴٍّﺎ )‪ .(ACRAI 2018‬ﻛﺬﻟﻚ‪ ،‬ﻓﺈن اﻟﻬﻴﺌﺎت واملﺆﺗﻤﺮات اﻟﺪوﻟﻴﺔ ﻧﺸﻄﺔ ﻟﻠﻐﺎﻳﺔ‪.‬‬
‫ﻓﻘﺪ ﻧﴩ املﺆﺗﻤﺮ اﻟﺪوﱄ ُملﻔﻮﴈ ﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت واﻟﺨﺼﻮﺻﻴﺔ إﻋﻼﻧًﺎ ﺑﺸﺄن اﻷﺧﻼﻗﻴﺎت‬
‫وﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت ﰲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻳﺘﻀﻤﱠ ﻦ ﻣﺒﺎدئ اﻟﻌﺪاﻟﺔ‪ ،‬واملﺴﺎءﻟﺔ‪ ،‬واﻟﺸﻔﺎﻓﻴﺔ‬
‫واﻟﻔﻬﻢ‪ ،‬واﻟﺘﺼﻤﻴﻢ املﺴﺌﻮل‪ ،‬واﻟﺨﺼﻮﺻﻴﺔ ا ُملﺘﻀﻤﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ )ﻣﻔﻬﻮم ﻳُﻄﺎﻟﺐ ﺑﻤﺮاﻋﺎة‬
‫اﻟﺨﺼﻮﺻﻴﺔ ﰲ ﺟﻤﻴﻊ ﻣﺮاﺣﻞ ﻋﻤﻠﻴﺔ اﻟﻬﻨﺪﺳﺔ(‪ ،‬وﺗﻤﻜني اﻷﻓﺮاد‪ ،‬واﻟﺤ ﱢﺪ ﻣﻦ اﻟﺘﺤﻴﺰ أو‬
‫اﻟﺘﻤﻴﻴﺰ وﺗﺨﻔﻴﻒ آﺛﺎرﻫﻤﺎ )‪.(ICDPPC 2018‬‬
‫ﻳﻀﻊ ﺑﻌﺾ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت ﻫﺪﻓﻬﻢ ﰲ إﻃﺎر »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺠﺪﻳﺮ ﺑﺎﻟﺜﻘﺔ«‪.‬‬
‫ﻓﻌﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗﺆ ﱢﻛﺪ ا ُملﻔﻮﺿﻴﺔ اﻷوروﺑﻴﺔ‪ ،‬اﻟﺘﻲ ﺗُﻌَ ﺪ ﺑﻼ ﺷ ﱟﻚ واﺣﺪة ﻣﻦ أﺑﺮز اﻟﻬﻴﺌﺎت‬
‫اﻟﻌﺎملﻴﺔ ﰲ ﻣﺠﺎل ُ‬
‫ﺻﻨﻊ ﺳﻴﺎﺳﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻋﲆ أﻫﻤﻴﺔ ﻫﺬا املﺼﻄﻠﺢ‪ .‬وﰲ أﺑﺮﻳﻞ‬
‫َ‬
‫ٍ‬
‫ﻣﺠﻤﻮﻋﺔ‬
‫ﻓﺮﻳﻖ ﺧﱪاء رﻓﻴﻊ املﺴﺘﻮى ﻣﻌ ِﻨﻴٍّﺎ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻮﺿﻊ‬
‫‪ ،٢٠١٨‬أﻧﺸﺄت‬
‫ﺟﺪﻳﺪة ﻣﻦ إرﺷﺎدات اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؛ وﰲ دﻳﺴﻤﱪ ‪ ،٢٠١٨‬أﺻﺪر اﻟﻔﺮﻳﻖ ﻣُﺴﻮدة‬
‫ﻧﻬﺞ ﰲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺘﻤﺤﻮر ﺣﻮل‬
‫وﺛﻴﻘﺔ ﻋﻤﻞ ﺗﺘﻀﻤﱠ ﻦ إرﺷﺎدات أﺧﻼﻗﻴﺔ ﺗﺪﻋﻮ إﱃ ٍ‬
‫اﻹﻧﺴﺎن‪ ،‬وإﱃ ﺗﻄﻮﻳﺮ ذﻛﺎءٍ اﺻﻄﻨﺎﻋﻲ ﺟﺪﻳﺮ ﺑﺎﻟﺜﻘﺔ‪ ،‬ﻳﺤﱰم اﻟﺤﻘﻮق اﻷﺳﺎﺳﻴﺔ واملﺒﺎدئ‬
‫اﻷﺧﻼﻗﻴﺔ‪ .‬وﻛﺎﻧﺖ اﻟﺤﻘﻮق املﺬﻛﻮرة ﻫﻲ ﻛﺮاﻣﺔ اﻹﻧﺴﺎن‪ ،‬وﺣﺮﻳﺔ اﻟﻔﺮد‪ ،‬واﺣﱰام اﻟﺪﻳﻤﻘﺮاﻃﻴﺔ‪،‬‬
‫واﻟﻌﺪاﻟﺔ‪ ،‬وﺳﻴﺎدة اﻟﻘﺎﻧﻮن‪ ،‬وﺣﻘﻮق املﻮاﻃﻦ‪ .‬أﻣﺎ املﺒﺎدئ اﻷﺧﻼﻗﻴﺔ‪ ،‬ﻓﻬﻲ اﻹﺣﺴﺎن )ﻓﻌﻞ‬
‫اﻟﺨري( وﻋﺪم إﻟﺤﺎق اﻷذى‪ ،‬واﻻﺳﺘﻘﻼل )اﻟﺤﻔﺎظ ﻋﲆ وﻛﺎﻟﺔ اﻹﻧﺴﺎن(‪ ،‬واﻟﻌﺪاﻟﺔ )أن ﺗﻜﻮن‬
‫ً‬
‫ﻋﺎدﻻ(‪ ،‬واﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري )ﺷﻔﺎﻓﻴﺔ اﻟﺘﻨﻔﻴﺬ(‪ .‬ﻫﺬه املﺒﺎدئ ﻣﺄﻟﻮﻓﺔ ﻣﻦ ﻣﺠﺎل أﺧﻼﻗﻴﺎت ﻋﻠﻢ‬
‫ٍ‬
‫ﺗﻔﺴريات ﺗﺴﻠﻂ اﻟﻀﻮء ﻋﲆ‬
‫اﻷﺣﻴﺎء‪ ،‬وﻟﻜﻦ اﻟﻮﺛﻴﻘﺔ ﺗُﻀﻴﻒ إﻟﻴﻬﺎ اﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري‪ ،‬وﺗﺘﻀﻤﱠ ﻦ‬
‫املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ اﻟﺨﺎﺻﺔ اﻟﺘﻲ ﻳُﺜريﻫﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳ ﱢ‬
‫ُﻔﴪ ﻣﺒﺪأ‬
‫ﻋﺪم إﻟﺤﺎق اﻷذى ﻋﲆ املﻄﺎﻟﺒﺔ ﺑﺄن ﺧﻮارزﻣﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺠﺐ أن ﺗﺘﺠﻨﱠﺐ اﻟﺘﻤﻴﻴﺰ‪،‬‬
‫وﻳﺠﺐ أن ﺗﺤﻤﻲ اﻟﻔﺌﺎت اﻟﻀﻌﻴﻔﺔ ﻣﺜﻞ اﻷﻃﻔﺎل واملﻬﺎﺟﺮﻳﻦ‪.‬‬
‫واﻟﺘﻼﻋُ ﺐ‪ ،‬واﻟﺘﻮﺟﻴﻪ اﻟﺴﻠﺒﻲ‪ِ ،‬‬
‫ﱢ‬
‫أﻣﺎ ﻣﺒﺪأ اﻟﻌﺪاﻟﺔ‪ ،‬ﻓﻴ ﱠ‬
‫وﻣﻨﻔﺬﻳﻪ‬
‫ُﻔﴪ ﻋﲆ أﻧﻪ ﻳﺘﻀﻤﻦ ﻣﻄﺎﻟﺒﺔ ﻣﻄﻮري اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺑﻀﻤﺎن اﺣﺘﻔﺎظ اﻷﻓﺮاد واملﺠﻤﻮﻋﺎت اﻷﻗﻠﻴﺔ ﺑﺎﻟﺘﺤ ﱡﺮر ﻣﻦ اﻟﺘﺤﻴﱡﺰ‪ .‬وﻳﻔﴪ ﻣﺒﺪأ اﻟﻘﺎﺑﻠﻴﺔ‬
‫ﻟﻠﺘﻔﺴري ﻋﲆ أﻧﻪ ﻳُﻄﺎﻟﺐ ﺑﺄن ﺗﻜﻮن أﻧﻈﻤﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﺎﺑﻠﺔ ﻟﻠﺘﺪﻗﻴﻖ و»ﻣﻔﻬﻮﻣﺔ‬
‫ﻣﻦ ﻗِ ﺒﻞ اﻟﺒﴩ ﻋﲆ اﺧﺘﻼف ﻣﺴﺘﻮﻳﺎت ﻓﻬﻤﻬﻢ وﺧﱪﺗﻬﻢ« )‪European Commission AI‬‬
‫ﱟ‬
‫ﺧﺎص‬
‫ﺑﺸﻜﻞ‬
‫‪ .(HLEG 2018, 10‬وﺗُﺤﺪﱢد اﻟﻨﺴﺨﺔ اﻟﻨﻬﺎﺋﻴﺔ‪ ،‬اﻟﺘﻲ ﺻﺪرت ﰲ أﺑﺮﻳﻞ ‪،٢٠١٩‬‬
‫ٍ‬
‫أن ﻗﺎﺑﻠﻴﺔ اﻟﺘﻔﺴري ﻻ ﺗﺘﻌ ﱠﻠﻖ ﻓﻘﻂ ﺑﺘﻔﺴري اﻟﻌﻤﻠﻴﺔ اﻟﺘﻘﻨﻴﺔ وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﺑﺎﻟﻘﺮارات اﻟﺒﴩﻳﺔ‬
‫ذات ﱢ‬
‫اﻟﺼﻠﺔ ﺑﻬﺎ )‪.(European Commission AI HLEG 2019, 18‬‬
‫‪105‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﰲ ٍ‬
‫وﻗﺖ ﺳﺎﺑﻖ‪ ،‬أﺻﺪرت ﻫﻴﺌﺔ اﺳﺘﺸﺎرﻳﺔ أﺧﺮى ﺗﺎﺑﻌﺔ إﱃ اﻻﺗﺤﺎد اﻷوروﺑﻲ‪ ،‬وﻫﻲ‬
‫املﺠﻤﻮﻋﺔ اﻷوروﺑﻴﺔ ا َملﻌﻨﻴﺔ ﺑﺎﻷﺧﻼﻗﻴﺎت ﰲ اﻟﻌﻠﻮم واﻟﺘﻘﻨﻴﺎت اﻟﺠﺪﻳﺪة ﺑﻴﺎﻧًﺎ ﺣﻮل اﻟﺬﻛﺎء‬
‫ً‬
‫ﻣﻘﱰﺣﺔ ﻣﺒﺎدئ اﻟﻜﺮاﻣﺔ اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬واﻻﺳﺘﻘﻼل‪،‬‬
‫اﻻﺻﻄﻨﺎﻋﻲ واﻟﺮوﺑﻮﺗﺎت واﻷﻧﻈﻤﺔ ا ُملﺴﺘﻘﻠﺔ‪،‬‬
‫واملﺴﺌﻮﻟﻴﺔ‪ ،‬واﻟﻌﺪاﻟﺔ‪ ،‬واملﺴﺎواة‪ ،‬واﻟﺘﻀﺎﻣُﻦ‪ ،‬واﻟﺪﻳﻤﻘﺮاﻃﻴﺔ‪ ،‬وﺳﻴﺎدة اﻟﻘﺎﻧﻮن واملﺴﺎءﻟﺔ‪،‬‬
‫واﻷﻣﺎن واﻟﺴﻼﻣﺔ‪ ،‬وﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت واﻟﺨﺼﻮﺻﻴﺔ‪ ،‬واﻻﺳﺘﺪاﻣﺔ‪ .‬وﻳُﻘﺎل إن ﻣﺒﺪأ اﻟﻜﺮاﻣﺔ‬
‫اﻹﻧﺴﺎﻧﻴﺔ ﻳﻘﺘﴤ إﻋﻼم اﻷﻓﺮاد ﺑﻤﺎ إذا ﻛﺎﻧﻮا ﻳﺘﻔﺎﻋﻠﻮن ﻣَ ﻊ آﻟﺔ أم ﻣ َﻊ إﻧﺴﺎن آﺧﺮ )‪EGE‬‬
‫‪ .(2018‬ﻛﺬﻟﻚ ﻋﻠﻴﻚ ﻣﻼﺣﻈﺔ أن اﻻﺗﺤﺎد اﻷوروﺑﻲ ﻟﺪَﻳﻪ ﺑﺎﻟﻔﻌﻞ ﺗﴩﻳﻌﺎت ﻗﺎﺋﻤﺔ ﺗﺘﻌﻠﻖ‬
‫ﺑﺘﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﺳﺘﺨﺪاﻣﻪ‪ .‬وﺗﻬﺪف ﻻﺋﺤﺔ ﺣِ ﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت اﻟﻌﺎﻣﺔ‪ ،‬اﻟﺘﻲ‬
‫اﻋﺘُ ِﻤﺪت ﰲ ﻣﺎﻳﻮ ‪ ،٢٠١٨‬إﱃ ﺣﻤﺎﻳﺔ ﺟﻤﻴﻊ ﻣﻮاﻃﻨﻲ اﻻﺗﺤﺎد اﻷوروﺑﻲ وﺗﻤﻜﻴﻨﻬﻢ ﻓﻴﻤﺎ ﻳﺘﻌ ﱠﻠﻖ‬
‫ﺑﺨﺼﻮﺻﻴﺔ اﻟﺒﻴﺎﻧﺎت‪ .‬وﺗﺘﻀﻤﱠ ﻦ ﻣﺒﺎدئ ﻣﺜﻞ ﺣﻖ اﻟﻔﺮد ﰲ ﻧﺴﻴﺎن ﺑﻴﺎﻧﺎﺗﻪ )ﻳﻤﻜﻦ ﻟﻠﻔﺮد أن‬
‫ﻳﻄﻠﺐ ﻣﺴﺢ ﺑﻴﺎﻧﺎﺗﻪ اﻟﺸﺨﺼﻴﺔ ووﻗﻒ ﻣﻌﺎﻟﺠﺔ ﺗﻠﻚ اﻟﺒﻴﺎﻧﺎت ﰲ املﺴﺘﻘﺒﻞ( وا ْﻟﺨﺼﻮﺻﻴﺔ‬
‫ا ُملﺘﻀﻤﱠ ﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ‪ .‬ﻛﻤﺎ ﺗﻤﻨﺢ اﻷﻓﺮاد املﻌﻨﻴني ﺣﻖ اﻟﻮﺻﻮل إﱃ »ﻣﻌﻠﻮﻣﺎت ذات ﻣﻌﻨﻰ‬
‫ﺣﻮل املﻨﻄﻖ ا ُملﻀﻤﱠ ﻦ« ﰲ اﺗﺨﺎذ اﻟﻘﺮارات املﺆﺗﻤﺘﺔ وﻣﻌﻠﻮﻣﺎت ﺣﻮل »اﻟﻌﻮاﻗﺐ ا ُمل ﱠ‬
‫ﺘﻮﻗﻌﺔ«‬
‫ﻟِﻤﺜﻞ ﻫﺬه املﻌﺎﻟﺠﺔ )اﻟﱪملﺎن اﻷوروﺑﻲ وﻣﺠﻠﺲ اﻻﺗﺤﺎد اﻷوروﺑﻲ ‪ .(٢٠١٦‬اﻻﺧﺘﻼف ﻋﻦ‬
‫وﺛﺎﺋﻖ اﻟﺴﻴﺎﺳﺔ ﻫﻮ أن ﻫﺬه املﺒﺎدئ املﺬﻛﻮرة ﻫﻨﺎ ﺗُﻌﺪ ﻣُﺘﻄﻠﺒﺎت ﻗﺎﻧﻮﻧﻴﺔ‪ .‬إﻧﻬﺎ ﺑﻤﺜﺎﺑﺔ ﺗﴩﻳﻊ‬
‫ﱠ‬
‫املﺆﺳﺴﺎت اﻟﺘﻲ ﺗﻨﺘﻬﻚ ﻻﺋﺤﺔ ﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت اﻟﻌﺎﻣﺔ ﻳُﻤﻜﻦ ﺗﻐﺮﻳﻤﻬﺎ‪.‬‬
‫ﻣﻔﺮوض؛ ﺑﻤﻌﻨﻰ أن‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﺛﻤﺔ ﺗﺴﺎؤل ﻣﻄﺮوح ﻋﻤﺎ إذا ﻛﺎﻧﺖ أﺣﻜﺎم ﻻﺋﺤﺔ ﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت اﻟﻌﺎﻣﱠ ﺔ ﺗﻜﺎﻓﺊ‬
‫اﻟﺤﻖ اﻟﻜﺎﻣﻞ ﰲ ﺗﻔﺴري اﻟﻘﺮار )‪ ،(Digital Europe 2018‬وﺑﺸﻜﻞ ﻋﺎم‪ ،‬إذا ﻛﺎﻧﺖ ﱢ‬
‫ﺗﻮﻓﺮ‬
‫ﺣﻤﺎﻳﺔ ﻛﺎﻓﻴﺔ ﺿﺪ ﻣﺨﺎﻃﺮ اﺗﺨﺎذ اﻟﻘﺮار املﺆﺗﻤﺖ )‪Wachter, Mittelstadt, and Floridi‬‬
‫‪ .(2017‬ﺗﻮﻓﺮ ﻻﺋﺤﺔ ﺣﻤﺎﻳﺔ اﻟﺒﻴﺎﻧﺎت اﻟﻌﺎﻣﱠ ﺔ اﻟﺤﻖ ﰲ اﻹﻋﻼم ﺑﺎﺗﺨﺎذ اﻟﻘﺮار املﺆﺗﻤﺖ وﻟﻜﻦ‬
‫ﻗﺮار ﺑﻌَ ﻴﻨﻪ‪ .‬وﻫﺬه ً‬
‫أﻳﻀﺎ ﻣﺸﻜﻠﺔ ﻓﻴﻤﺎ‬
‫ﻳﺒﺪو أﻧﻬﺎ ﻻ ﺗُﻄﺎﻟِﺐ ﺑﺘﻔﺴري اﻷﺳﺎس املﻨﻄﻘﻲ ﻷي ٍ‬
‫ٌ‬
‫دراﺳﺔ أﺟﺮاﻫﺎ ﻣﺠﻠﺲ أوروﺑﺎ‪،‬‬
‫ﻳﺘﻌ ﱠﻠﻖ ﺑﺎﺗﺨﺎذ اﻟﻘﺮار ﰲ املﺠﺎل اﻟﻘﺎﻧﻮﻧﻲ‪ .‬وﻗﺪ ﻃﺎﻟﺒﺖ‬
‫ٍ‬
‫ٍ‬
‫ﻣﺤﺎﻛﻤﺔ ﻋﺎدﻟﺔ‬
‫ﻟﺠﻨﺔ ﻣﻦ ﺧﱪاء ﺣﻘﻮق اﻹﻧﺴﺎن‪ ،‬ﺑﺄن ﻳﻜﻮن ﻟﻸﻓﺮاد اﻟﺤﻖ ﰲ‬
‫اﺳﺘﻨﺎدًا إﱃ ﻋﻤﻞ‬
‫وإﺟﺮاءات ﻗﺎﻧﻮﻧﻴﺔ ﺳﻠﻴﻤﺔ ﺑﴩوط ﻳُﻤﻜﻨﻬﻢ ﻓﻬﻤﻬﺎ )‪.(Yeung 2018‬‬
‫ﺗُﻌَ ﺪ املﻨﺎﻗﺸﺎت اﻟﻘﺎﻧﻮﻧﻴﺔ ذات أﻫﻤﻴ ٍﺔ ﺑﺎﻟﻄﺒﻊ ﰲ املﻨﺎﻗﺸﺎت ا ُملﺘﻌﻠﻘﺔ ﺑﺄﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ وﺳﻴﺎﺳﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻗﺪ ﻧﺎﻗﺶ ﺗريﻧﺮ )‪ (٢٠١٩‬املﻘﺎرﻧﺎت ﺑﺎﻟﺤﻴﻮاﻧﺎت‬
‫)ﻛﻴﻒ ﻋﻮﻣِﻠﺖ وﺗُﻌﺎﻣﻞ ﰲ اﻟﻘﺎﻧﻮن وﻣﺎ إذا ﻛﺎﻧﺖ ﺗﺘﻤﺘﱠﻊ ﺑﺤﻘﻮق( وراﺟﻊ ﻋﺪدًا ﻣﻦ اﻟﺼﻜﻮك‬
‫اﻟﻘﺎﻧﻮﻧﻴﺔ ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﻤﺎ ﻳﻤﻜﻦ أن ﺗﻌﻨﻲ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻨﺪ وﻗﻮع‬
‫‪106‬‬

‫اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ‬

‫ٌ‬
‫ﺷﺨﺺ ﻣﺎ ﻣُﻠﺘﺰﻣً ﺎ ﺑﻮاﺟﺐ اﻟﺮﻋﺎﻳﺔ ﻟﺘﺠﻨﱡﺐ‬
‫اﻟﴬر‪ ،‬ﻓﺈن ﻣﺴﺄﻟﺔ اﻹﻫﻤﺎل ﺗﺘﻌﻠﻖ ﺑﻤﺎ إذا ﻛﺎن‬
‫وﻗﻮع ﴐر‪ ،‬ﺣﺘﻰ إذا ﻟﻢ ﻳﻜﻦ اﻟﴬَ ر اﻟﻮاﻗﻊ ﻣﻘﺼﻮدًا‪ .‬ﻳﻤﻜﻦ أن ﻳﻨﻄﺒﻖ ذﻟﻚ ﻋﲆ ﻣُﺼﻤﻢ أو‬
‫ﻣُﺪرب اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻟﻜﻦ ﻣﺎ ﻣﺪى ﺳﻬﻮﻟﺔ اﻟﺘﻨﺒﺆ ﺑﻌﻮاﻗﺐ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ أﻣﺎ‬
‫اﻟﻘﺎﻧﻮن اﻟﺠﻨﺎﺋﻲ‪ ،‬ﻓﻌﲆ اﻟﻌﻜﺲ ﻣﻦ ذﻟﻚ‪ ،‬ﻓﻬﻮ ﻳﺘﻄ ﱠﻠﺐ ِﻧﻴﱠﺔ إﻳﻘﺎع اﻟﴬر‪ .‬وﻟﻜﻦ ﻫﺬا ﻏﺎﻟﺒًﺎ‬
‫ﻟﻴﺲ اﻟﺤﺎل ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻣﻦ ﻧﺎﺣﻴ ٍﺔ أﺧﺮى‪ ،‬ﻻ ﺗﺘﻌﻠﻖ املﺴﺌﻮﻟﻴﺔ ﻋﻦ املﻨﺘَﺞ ﺑﺨﻄﺄ‬
‫ٍ‬
‫ﺗﻌﻮﻳﻀﺎت ﻋﻦ اﻷﴐار‪،‬‬
‫ﺗﻔﺮض ﻋﲆ اﻟﴩﻛﺔ اﻟﺘﻲ أﻧﺘﺠﺖ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ دﻓﻊ‬
‫اﻷﻓﺮاد وﻟﻜﻨﻬﺎ ِ‬
‫ﱢ‬
‫ﺑﻐﺾ اﻟﻨﻈﺮ ﻋﻦ اﻟﺨﻄﺄ‪ .‬وﻳﻤﻜﻦ أن ﻳﻜﻮن ﻫﺬا أﺣﺪ اﻟﺤﻠﻮل ا ُملﻤﻜﻨﺔ ﻟﻠﻤﺴﺌﻮﻟﻴﺔ اﻟﻘﺎﻧﻮﻧﻴﺔ ﻋﻦ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻛﺬﻟﻚ ﺗﺘﱠﺼﻞ ﻗﻮاﻧني املﻠﻜﻴﺔ اﻟﻔﻜﺮﻳﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻣﺜﻞ ﺣﻘﻮق‬
‫اﻟﻄﺒﻊ واﻟﻨﴩ وﺑﺮاءات اﻻﺧﱰاع‪ ،‬وﻗﺪ ﺑﺪأت ﻣﻨﺎﻗﺸﺎت ﺣﻮل »اﻟﺸﺨﺼﻴﺔ اﻻﻋﺘﺒﺎرﻳﺔ« ﻟﻠﺬﻛﺎء‬
‫ً‬
‫اﻓﱰاﺿﺎ ﻗﺎﻧﻮﻧﻴٍّﺎ وﻟﻜﻨﻪ ذرﻳﻌﺔ ﺗُﻄﺒﱠﻖ ﺣﺎﻟﻴٍّﺎ ﻋﲆ اﻟﴩﻛﺎت وﻣﺨﺘﻠﻒ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻫﻮ ﻣﺎ ﻳُﻌﺪ‬
‫ا ُملﻨﻈﻤﺎت‪ .‬ﻓﻬﻞ ﻳﺠﺐ أن ﻳُﻄﺒﱠﻖ ً‬
‫ﻗﺮار ﻣُﺜري ﻟﻠﺠﺪل ﰲ ﻋﺎم‬
‫أﻳﻀﺎ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ ﰲ ٍ‬
‫‪ ،٢٠١٧‬اﻗﱰح اﻟﱪملﺎن اﻷوروﺑﻲ أن ﻣﻨﺢ اﻟﺮوﺑﻮﺗﺎت اﻟﺬاﺗﻴﺔ اﻟﺘﺸﻐﻴﻞ اﻷﻛﺜﺮ ﺗﻄﻮ ًرا ﻣﻨﺰﻟﺔ‬
‫اﻷﺷﺨﺎص اﻹﻟﻜﱰوﻧﻴني ﻫﻮ ﺣ ﱞﻞ ﻗﺎﻧﻮﻧﻲ ﻣُﻤﻜﻦ ﻟﻘﻀﻴﺔ املﺴﺌﻮﻟﻴﺔ اﻟﻘﺎﻧﻮﻧﻴﺔ؛ وﻫﺬه اﻟﻔﻜﺮة ﻟﻢ‬
‫ﻳﺘﻢ اﻻﻋﱰاف ﺑﻬﺎ ﻣﻦ ﻗِ ﺒﻞ املﻔﻮﺿﻴﺔ اﻷوروﺑﻴﺔ ﰲ اﺳﱰاﺗﻴﺠﻴﺘﻬﺎ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ‪ 2‬ﰲ ﻋﺎم‬
‫ً‬
‫ﺣﻘﻮق وﺷﺨﺼﻴﺔ ﻟﻶﻻت‪،‬‬
‫اﻋﱰاﺿﺎ ﺣﺎزﻣً ﺎ ﻋﲆ ﻓﻜﺮة إﻋﻄﺎء‬
‫‪ .٢٠١٨‬ﻛﺬﻟﻚ اﻋﱰض آﺧﺮون‬
‫ٍ‬
‫ﻣُﺠﺎدِ ﻟني‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺑﺄﻧﻪ ﺳﻴُﺼﺒﺢ ﻣﻦ اﻟﺼﻌﺐ‪ ،‬إن ﻟﻢ ﻳ ُﻜﻦ ﻣﻦ ا ُملﺴﺘﺤﻴﻞ‪ ،‬ﻣﺤﺎﺳﺒﺔ‬
‫ﻷﻏﺮاض ذاﺗﻴﺔ )‪Bryson,‬‬
‫ﺷﺨﺺ ﻷن اﻟﻨﺎس ﺳﻴﺴﻌَ ﻮن إﱃ اﺳﺘﻐﻼل ﻫﺬه اﻟﻔﻜﺮة‬
‫أي‬
‫ٍ‬
‫ٍ‬
‫ً‬
‫‪ .(Diamantis, and Grant 2017‬ﻛﺎن ﻫﻨﺎك أﻳﻀﺎ اﻟﺤﺎﻟﺔ اﻟﺸﻬرية ﻟﺼﻮﻓﻴﺎ‪ ،‬اﻟﺮوﺑﻮت‬
‫اﻟﺬي ﻣﻨﺤﺘﻪ اﻟﺴﻌﻮدﻳﺔ »اﻟﺠﻨﺴﻴﺔ« ﰲ ﻋﺎم ‪ .٢٠١٧‬ﺗُﺜري ﻣﺜﻞ ﻫﺬه اﻟﺤﺎﻟﺔ ﻣﺠﺪدًا ﻣﺴﺄﻟﺔ‬
‫املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ ﻟﻠﺮوﺑﻮﺗﺎت واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ )اﻧﻈﺮ اﻟﻔﺼﻞ اﻟﺮاﺑﻊ(‪.‬‬
‫اﻗﱰﺣَ ﺖ ً‬
‫أﻳﻀﺎ ﺳﻴﺎﺳﺎت ذﻛﺎء اﺻﻄﻨﺎﻋﻲ ﺧﺎرج ﻧﻄﺎق أﻣﺮﻳﻜﺎ اﻟﺸﻤﺎﻟﻴﺔ وأوروﺑﺎ‪.‬‬
‫ُِ‬
‫ﻓﺎﻟﺼني‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻟﺪَﻳﻬﺎ اﺳﱰاﺗﻴﺠﻴﺔ وﻃﻨﻴﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﺗُﻘﺮ ُﺧﻄﺘﻬﺎ‬
‫اﻟﺘﻨﻤﻮﻳﺔ ﺑﺄن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻮ ﺗﻜﻨﻮﻟﻮﺟﻴﺎ ﻫﺪﱠاﻣﺔ ﻳﻤﻜﻦ أن ﺗﴬﱠ ﺑﺎﻻﺳﺘﻘﺮار اﻻﺟﺘﻤﺎﻋﻲ‪،‬‬
‫وﺗﻨﺘﻬﻚ اﻟﺨﺼﻮﺻﻴﺔ اﻟﺸﺨﺼﻴﺔ‪ ،‬وﺗﺨﻠﻖ‬
‫وﺗﺆﺛﺮ ﻋﲆ اﻟﻘﺎﻧﻮن واﻷﺧﻼﻗﻴﺎت اﻻﺟﺘﻤﺎﻋﻴﺔ‪،‬‬
‫ِ‬
‫ُ‬
‫ﻣﺨﺎﻃﺮ أﻣﻨﻴﺔ؛ وﻣِﻦ ﺛَﻢ ﺗُﻮﴆ اﻟﺨﻄﺔ ﺑﺘﻌﺰﻳﺰ اﻟﻮﻗﺎﻳﺔ املﺴﺘﻘﺒﻠﻴﺔ وﺗﻘﻠﻴﻞ املﺨﺎﻃﺮ املﺤﺘﻤَ ﻠﺔ‬
‫)ﻣﺠﻠﺲ اﻟﺪوﻟﺔ اﻟﺼﻴﻨﻲ ‪ .(٢٠١٧‬وﺗﺮوي ﺑﻌﺾ اﻟﺠﻬﺎت اﻟﻔﺎﻋﻠﺔ ﰲ اﻟﻐﺮب ﴎدﻳﺔ ﻣﻨﺎﻓﺴﺔ‪:‬‬
‫َ‬
‫ﺣﺮب ﻋﺎملﻴﺔ‬
‫ﻧﻘﱰب ﻣﻦ اﻧﺪﻻع‬
‫ﻳﺨﺸﻮن أن ﺗﺘﺠﺎوزﻫﻢ اﻟﺼني أو ﺣﺘﻰ ﻓﻜﺮة أﻧﻨﺎ‬
‫إﻧﻬﻢ‬
‫ِ‬
‫ٍ‬
‫ﺟﺪﻳﺪة‪ .‬ﺑﻴﻨﻤﺎ ﻳُﺤﺎول آﺧﺮون اﻟﺘﻌ ﱡﻠﻢ ﻣﻦ اﺳﱰاﺗﻴﺠﻴﺔ اﻟﺼني‪ .‬وﻗﺪ ﻳﺘﺴﺎءل اﻟﺒﺎﺣﺜﻮن ً‬
‫أﻳﻀﺎ‬
‫‪107‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻋﻦ ﻛﻴﻔﻴﺔ ﺗﻌﺎﻣُﻞ اﻟﺜﻘﺎﻓﺎت املﺨﺘﻠﻔﺔ ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﻄ ُﺮق ﻣﺨﺘﻠﻔﺔ‪ .‬وﻳﻤﻜﻦ أن‬
‫ﻳُﺴﻬﻢ اﻟﺒﺤﺚ ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻧﻔﺴﻪ ﰲ ﺑﻨﺎء وﺟﻬﺔ ﻧﻈﺮ ﻣﻘﺎرﻧﺔ ﻋﺎﺑﺮة ﻟﻠﺜﻘﺎﻓﺎت‬
‫ﺑﺸﺄن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻨﺪﻣﺎ ﻳُﺬﻛﺮﻧﺎ ﺑﺎﻟﻔﺮوق ﺑني اﻟﺜﻘﺎﻓﺎت‬
‫اﻟﻔﺮدﻳﺔ واﻟﺠﻤﺎﻋﻴﺔ ﻓﻴﻤﺎ ﻳﺘﻌ ﱠﻠﻖ ﺑﺎ ُملﻌﻀﻼت اﻷﺧﻼﻗﻴﺔ )‪ .(Awad et al. 2018‬وﻳﻤﻜﻦ أن ﻳُﺜري‬
‫ٍ‬
‫ﻣﺸﻜﻼت ﻷﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إذا ﻛﺎﻧﺖ ﺗﻬﺪف إﱃ أن ﺗﻜﻮن ﻋﺎملﻴﺔ‪ .‬وﻳﻤﻜﻦ‬
‫ﻫﺬا‬
‫ً‬
‫أﻳﻀﺎ اﺳﺘﻜﺸﺎف ﻛﻴﻒ ﺗﺨﺘﻠﻒ اﻟﴪدﻳﺎت ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ اﻟﺼني أو اﻟﻴﺎﺑﺎن‪ ،‬ﻋﲆ‬
‫ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻋﻦ اﻟﴪدﻳﺎت اﻟﻐﺮﺑﻴﺔ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ اﻻﺧﺘﻼﻓﺎت اﻟﺜﻘﺎﻓﻴﺔ‪ ،‬ﱠ‬
‫ﻳﺘﺒني أن‬
‫ٍ‬
‫ﺑﺪرﺟﺔ ﻛﺒرية وﻣﻠﺤﻮﻇﺔ‪ .‬ﻓﺒﻴﻨﻤﺎ ﺗﺆﻛﺪ ﺧﻄﺔ‬
‫ﺳﻴﺎﺳﺎت أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣُﺘﺸﺎﺑﻬﺔ‬
‫اﻟﺼني أﻛﺜﺮ ﻋﲆ اﻻﺳﺘﻘﺮار اﻻﺟﺘﻤﺎﻋﻲ واﻟﺼﺎﻟﺢ اﻟﻌﺎم اﻟﺠﻤﺎﻋﻲ‪ ،‬إﻻ أن املﺨﺎﻃﺮ اﻷﺧﻼﻗﻴﺔ‬
‫ا ُملﺤﺪدة واملﺒﺎدئ املﺬﻛﻮرة ﻟﻴﺴﺖ ﻣﺨﺘﻠﻔﺔ ﻛﺜريًا ﻋﻦ ﺗﻠﻚ ا ُملﻘﱰﺣﺔ ﻣﻦ ﻗِ ﺒﻞ اﻟﺪول اﻟﻐﺮﺑﻴﺔ‪.‬‬

‫ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ اﻻﺧﺘﻼﻓﺎت اﻟﺜﻘﺎﻓﻴﺔ‪ ،‬ﱠ‬
‫ﻳﺘﺒني أن ﺳﻴﺎﺳﺎت أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺘﺸﺎﺑﻬﺔ ﺑﺪرﺟﺔ‬
‫ﻛﺒرية وﻣﻠﺤﻮﻇﺔ‪.‬‬

‫ً‬
‫ﺳﺎﺑﻘﺎ‪ ،‬ﺳﻴﺎﺳﺔ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺴﺖ ﻣﻘﺼﻮر ًة‬
‫وﻟﻜﻦ‪ ،‬ﻛﻤﺎ ذﻛﺮﻧﺎ‬
‫ﻋﲆ اﻟﺤﻜﻮﻣﺎت وﻟﺠﺎﻧﻬﺎ وﻫﻴﺌﺎﺗﻬﺎ ﻓﻘﻂ‪ .‬ﻓﻘﺪ أﺧﺬ اﻷﻛﺎدﻳﻤﻴﻮن ً‬
‫أﻳﻀﺎ زﻣﺎم املﺒﺎدرة‪ .‬ﻋﲆ‬
‫اﻗﱰحَ إﻋﻼن ﻣﻮﻧﱰﻳﺎل ﺑﺸﺄن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ املﺴﺌﻮل ﻣﻦ ﻗِ ﺒﻞ ﺟﺎﻣﻌﺔ‬
‫ﺳﺒﻴﻞ املﺜﺎل‪ِ ُ ،‬‬
‫ﻣﻮﻧﱰﻳﺎل وﺷﻤﻞ اﺳﺘﺸﺎرة املﻮاﻃﻨني واﻟﺨﱪاء وﻏريﻫﻢ ﻣﻦ أﺻﺤﺎب اﻟﺸﺄن‪ .‬وﻳﻘﻮل اﻹﻋﻼن‬
‫إن ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺠﺐ أن ﻳُﻌ ﱢﺰز رﻓﺎه ﺟﻤﻴﻊ املﺨﻠﻮﻗﺎت اﻟﺤﻴﺔ وﻳُﻌ ﱢﺰز اﺳﺘﻘﻼل‬
‫اﻟﺒﴩ‪ ،‬وﻳﻘﴤ ﻋﲆ ﺟﻤﻴﻊ أﻧﻮاع اﻟﺘﻤﻴﻴﺰ‪ ،‬وﻳﺤﱰم اﻟﺨﺼﻮﺻﻴﺔ اﻟﺸﺨﺼﻴﺔ‪ ،‬وﻳَﺤﻤﻴﻨﺎ‬
‫ﻣﻦ اﻟﺪﻋﺎﻳﺔ واﻟﺘﻼﻋُ ﺐ‪ ،‬وﻳُﻌ ﱢﺰز اﻟﻨﻘﺎش اﻟﺪﻳﻤﻘﺮاﻃﻲ‪ ،‬وﻳﺠﻌﻞ ﻣﺨﺘﻠﻒ اﻟﺠﻬﺎت اﻟﻔﺎﻋﻠﺔ‬
‫ﻣَ ﺴﺌﻮﻟني ﻋﻦ ﻣﻜﺎﻓﺤﺔ ﻣﺨﺎﻃﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ )‪.(Université de Montréal 2017‬‬
‫وﻗﺪ اﻗﱰح ﺑﺎﺣﺜﻮن آﺧﺮون ﻣﺒﺎدئ اﻹﺣﺴﺎن‪ ،‬وﻋﺪم اﻟﺘﺴﺒﱡﺐ ﰲ اﻷذى‪ ،‬واﻻﺳﺘﻘﻼل‪ ،‬واﻟﻌﺪاﻟﺔ‪،‬‬
‫وﻗﺎﺑﻠﻴﺔ اﻟﺘﻔﺴري )‪ .(Floridi et al. 2018‬وﺗﻌﻤﻞ اﻟﺠﺎﻣﻌﺎت ﻣﺜﻞ ﻛﺎﻣﱪﻳﺪج وﺳﺘﺎﻧﻔﻮرد‬
‫ﻋﲆ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻏﺎﻟﺒًﺎ ﻣﻦ وﺟﻬﺔ ﻧﻈﺮ اﻷﺧﻼق اﻟﺘﻄﺒﻴﻘﻴﺔ‪ .‬وﻛﺬﻟﻚ ﻳﺆدي‬
‫أﻳﻀﺎ ً‬
‫اﻟﻌﺎﻣﻠﻮن ﰲ ﻣﺠﺎل اﻷﺧﻼق املﻬﻨﻴﺔ ً‬
‫اﻷﺷﺨﺎص ِ‬
‫ﻋﻤﻼ ﻣُﻔﻴﺪًا‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪم ﻣﺮﻛﺰ‬
‫ً‬
‫ﻣﺠﻤﻮﻋﺔ ﻣﻦ اﻟﻨﻈﺮﻳﺎت اﻷﺧﻼﻗﻴﺔ ﻛﺄدا ٍة‬
‫ﻣﺎرﻛﻮﻻ ﻟﻸﺧﻼق اﻟﺘﻄﺒﻴﻘﻴﺔ ﰲ ﺟﺎﻣﻌﺔ ﺳﺎﻧﺘﺎ ﻛﻼرا‬
‫‪108‬‬

‫اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ‬

‫ُملﻤﺎرﺳﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ واﻟﻬﻨﺪﺳﺔ‪ ،‬واﻟﺘﻲ ﻗﺪ ﺗُﻔﻴﺪ ً‬
‫أﻳﻀﺎ ﰲ إﺛﺮاء أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﺑﺎملﻌﻠﻮﻣﺎت‪ 3 .‬ﻛﻤﺎ أﺑﺪى ﻓﻼﺳﻔﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻫﺘﻤﺎﻣً ﺎ ﻛﺒريًا ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺆﺧ ًﺮا‪.‬‬
‫ﻧﺠﺪ ً‬
‫أﻳﻀﺎ ﻣُﺒﺎدرات ﺑﺸﺄن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﻋﺎﻟﻢ اﻟﴩﻛﺎت‪ .‬ﻋﲆ ﺳﺒﻴﻞ‬
‫املﺜﺎل‪ ،‬ﻳﺪﺧﻞ ﰲ اﻟﴩاﻛﺔ ﺑﺸﺄن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﴍﻛﺎت ﻣﺜﻞ دﻳﺐ ﻣﺎﻳﻨﺪ‪ ،‬وآي ﺑﻲ إم‪،‬‬
‫وإﻧﺘﻞ‪ ،‬وأﻣﺎزون‪ ،‬وأﺑﻞ‪ ،‬وﺳﻮﻧﻲ‪ ،‬وﻓﻴﺴﺒﻮك‪ 4 .‬وﺗﺪرك اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﴩﻛﺎت اﻟﺤﺎﺟﺔ إﱃ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻷﺧﻼﻗﻲ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻧﴩت ﺟﻮﺟﻞ ﻣﺒﺎدئ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪:‬‬
‫ﺗﻘﺪﻳﻢ ﻓﺎﺋﺪة اﺟﺘﻤﺎﻋﻴﺔ‪ ،‬وﺗﺠﻨﱡﺐ اﻟﺘﺴﺒﱡﺐ ﰲ اﻟﺘﺤﻴﱡﺰ ﻏري اﻟﻌﺎدل أو ﺗﻌﺰﻳﺰه‪ ،‬وﻓﺮض اﻟﺴﻼﻣﺔ‪،‬‬
‫واﻟﺤﻔﺎظ ﻋﲆ ﺗﺤﻤﱡ ﻞ املﺴﺌﻮﻟﻴﺔ‪ ،‬واﻟﺤﻔﺎظ ﻋﲆ ﺗﺼﻤﻴﻢ اﻟﺨﺼﻮﺻﻴﺔ‪ ،‬وﺗﻌﺰﻳﺰ اﻟﺘﻤﻴﱡﺰ اﻟﻌﻠﻤﻲ‪،‬‬
‫وﺗﻘﻴﻴﺪ اﻟﺘﻄﺒﻴﻘﺎت اﻟﺘﻲ ﻳُﺤﺘَﻤﻞ ﻛﻮﻧﻬﺎ ﺿﺎرة أو ﻣُﺴﻴﺌﺔ ﻣﺜﻞ اﻷﺳﻠﺤﺔ أو اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫ﺗﻨﺘﻬﻚ ﻣﺒﺎدئ اﻟﻘﺎﻧﻮن اﻟﺪوﱄ وﺣﻘﻮق اﻹﻧﺴﺎن‪ 5 .‬وﺗﺘﺤﺪﱠث ﴍﻛﺔ ﻣﺎﻳﻜﺮوﺳﻮﻓﺖ ﻋﻦ‬
‫اﻟﺘﻲ ِ‬
‫ﻓﻜﺮة »اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ أﺟﻞ اﻟﺨري« وﺗﻘﱰح ﻣﺒﺎدئ اﻟﻌﺪاﻟﺔ‪ ،‬واملﻮﺛﻮﻗﻴﺔ واﻟﺴﻼﻣﺔ‪،‬‬
‫واﻟﺨﺼﻮﺻﻴﺔ واﻷﻣﺎن‪ ،‬واﻟﺘﻀﻤني‪ ،‬واﻟﺸﻔﺎﻓﻴﺔ‪ ،‬واملﺴﺎءﻟﺔ‪ 6 .‬ﻛﻤﺎ اﻗﱰﺣﺖ ﴍﻛﺔ أﻛﺴﻨﺘﴩ‬
‫ﻣﺒﺎدئ ﻋﺎملﻴﺔ ﻷﺧﻼﻗﻴﺎت اﻟﺒﻴﺎﻧﺎت‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ اﺣﱰام اﻷﺷﺨﺎص اﻟﻜﺎﻣﻨﺔ وراء اﻟﺒﻴﺎﻧﺎت‪،‬‬
‫واﻟﺨﺼﻮﺻﻴﺔ‪ ،‬واﻟﺘﻀﻤني‪ ،‬واﻟﺸﻔﺎﻓﻴﺔ‪ 7 .‬وﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن وﺛﺎﺋﻖ اﻟﴩﻛﺎت ﺗَﻤﻴﻞ إﱃ اﻟﱰﻛﻴﺰ‬
‫ﻋﲆ اﻟﺮﻗﺎﺑﺔ اﻟﺬاﺗﻴﺔ‪ ،‬ﻓﺈن ﺑﻌﺾ اﻟﴩﻛﺎت ﺗﻌﱰف ﺑﴬورة اﻟﻠﻮاﺋﺢ اﻟﺘﻨﻈﻴﻤﻴﺔ اﻟﺨﺎرﺟﻴﺔ‪.‬‬
‫وﻗﺪ ﻗﺎل ﺗﻴﻢ ﻛﻮك اﻟﺮﺋﻴﺲ اﻟﺘﻨﻔﻴﺬي ﻟﴩﻛﺔ أﺑﻞ إن اﻟﻠﻮاﺋﺢ اﻟﺘﻨﻈﻴﻤﻴﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‪ ،‬ﻋﲆ‬
‫ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻟﻀﻤﺎن اﻟﺨﺼﻮﺻﻴﺔ أﻣﺮ ﻻ ﻏِ ﻨﻰ ﻋﻨﻪ ﻷن اﻟﺴﻮق اﻟﺤﺮة اﻟﺘﻲ ﻻ ﺗﺨﻀﻊ ﻟﺮﻗﺎﺑﺔ‬
‫ﺣﻜﻮﻣﻴﺔ ﻻ ﺗُﻔﻴﺪ ﰲ ﻫﺬه اﻟﺤﺎﻟﺔ‪ 8 .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻫﻨﺎك ﺟﺪل ﺣﻮل ﻣﺎ إذا ﻛﺎن ﻫﺬا ﻳﺘﻄﻠﺐ ﻟﻮاﺋﺢ‬
‫ﺗﻨﻈﻴﻤﻴﺔ ﺟﺪﻳﺪة‪ .‬وﻳﺪﻋﻢ اﻟﺒﻌﺾ ﻣﺴﺎر اﻟﻠﻮاﺋﺢ اﻟﺘﻨﻈﻴﻤﻴﺔ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ اﻟﻘﻮاﻧني اﻟﺠﺪﻳﺪة‪.‬‬
‫ﻓﻘﺪ ﻗﺪﻣﺖ وﻻﻳﺔ ﻛﺎﻟﻴﻔﻮرﻧﻴﺎ ﺑﺎﻟﻔﻌﻞ ﻣﴩوع ﻗﺎﻧﻮن ﻳﻄﺎﻟﺐ ﺑﺎﻟﻜﺸﻒ ﻋﻦ اﻟﺮوﺑﻮﺗﺎت‪:‬‬
‫ﺑﻄﺮﻳﻘﺔ ﺗُﻀ ﱢﻠﻞ ﱠ‬
‫ٍ‬
‫اﻟﺸﺨﺺ اﻵﺧﺮ ﺣﻮل ﻫﻮﻳﺘﻪ اﻻﺻﻄﻨﺎﻋﻴﺔ أﻣﺮ ﻏري‬
‫ﻓﺈن اﺳﺘﺨﺪام اﻟﺮوﺑﻮت‬
‫ً‬
‫ﻣﻮﻗﻔﺎ أﻛﺜﺮ ﺗﺤﻔ ً‬
‫ﻈﺎ‪ .‬ﻓﻘﺪ ﺟﺎدﻟﺖ ﴍﻛﺔ دﻳﺠﻴﺘﺎل ﻳﻮروب‬
‫ﻗﺎﻧﻮﻧﻲ‪ 9 .‬وﺗﺘﺨﺬ ﴍﻛﺎت أﺧﺮى‬
‫)‪ ،(٢٠١٨‬اﻟﺘﻲ ﺗﻤﺜﻞ اﻟﺼﻨﺎﻋﺔ اﻟﺮﻗﻤﻴﺔ ﰲ أوروﺑﺎ‪ ،‬ﺑﺄن اﻹﻃﺎر اﻟﻘﺎﻧﻮﻧﻲ اﻟﺤﺎﱄ ﻣُﺠﻬﱠ ﺰ ملﻌﺎﻟﺠﺔ‬
‫املﺸﻜﻼت املﺘﻌﻠﻘﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﻤﺎ ﻓﻴﻬﺎ اﻟﺘﺤﻴﱡﺰ واﻟﺘﻤﻴﻴﺰ‪ ،‬وﻟﻜﻦ ﻟﺒﻨﺎء اﻟﺜﻘﺔ‪ ،‬ﻓﺈن‬
‫اﻟﺸﻔﺎﻓﻴﺔ واﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري أﻣﺮان ﻏﺎﻳﺔ ﰲ اﻷﻫﻤﻴﺔ‪ :‬ﻳﺠﺐ أن ﻳﻔﻬﻢ اﻷﻓﺮاد واﻟﴩﻛﺎت ﻣﺘﻰ‬
‫ٍ‬
‫ٍ‬
‫ﻣﻌﻠﻮﻣﺎت ذات‬
‫ﺑﺤﺎﺟﺔ إﱃ ﺗﻮﻓري‬
‫وﻛﻴﻒ ﺗُﺴﺘﺨﺪَم اﻟﺨﻮارزﻣﻴﺎت ﰲ اﺗﺨﺎذ اﻟﻘﺮارات‪ ،‬وﻧﺤﻦ‬
‫ﻣﻌﻨﻰ وﺗﻴﺴري ﻋﻤﻠﻴﺔ ﺗﻔﺴري اﻟﻘﺮارات اﻟﺨﻮارزﻣﻴﺔ‪.‬‬
‫ﺗﻠﻌﺐ اﻟﺠﻬﺎت ﻏري اﻟﻬﺎدﻓﺔ إﱃ اﻟﺮﺑﺢ دو ًرا ً‬
‫أﻳﻀﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗﻄﺮح اﻟﺤﻤﻠﺔ‬
‫اﻟﺪوﻟﻴﺔ ﻟﻮﻗﻒ اﻟﺮوﺑﻮﺗﺎت اﻟﻘﺎﺗﻠﺔ اﻟﻌﺪﻳﺪ ﻣﻦ اﻷﺳﺌﻠﺔ اﻷﺧﻼﻗﻴﺔ ﺑﺸﺄن اﻟﺘﻄﺒﻴﻘﺎت اﻟﻌﺴﻜﺮﻳﺔ‬
‫‪109‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ‪ 10‬وﻣﻦ ﺟﺎﻧﺐ دُﻋﺎة ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬ﺗُﻮﺟَ ﺪ ﻣﺒﺎدئ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻟﺘﻲ اﺗﱠﻔﻖ ﻋﻠﻴﻬﺎ املﺸﺎرﻛﻮن اﻷﻛﺎدﻳﻤﻴﻮن واﻟﺼﻨﺎﻋﻴﻮن ﰲ ﻣﺆﺗﻤﺮ أﺳﻴﻠﻮﻣﺎر‪ ،‬وﻫﻮ ﻣﺆﺗﻤﺮ‬
‫ﻋﻘﺪه »ﻣﻌﻬﺪ ﻣﺴﺘﻘﺒﻞ اﻟﺤﻴﺎة« )ﻣﺎﻛﺲ ﺗﻴﺠﻤﺎرك وآﺧﺮون(‪ .‬وﻛﺎن اﻟﻬﺪف اﻟﻌﺎم ﻫﻮ‬
‫اﻟﺤﺮص ﻋﲆ أن ﻳﻈﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻔﻴﺪًا‪ ،‬واﺣﱰام املﺒﺎدئ واﻟﻘﻴﻢ اﻷﺧﻼﻗﻴﺔ ﻣﺜﻞ‬
‫اﻟﺴﻼﻣﺔ واﻟﺸﻔﺎﻓﻴﺔ واملﺴﺌﻮﻟﻴﺔ‪ ،‬وﺗﻮﺟﻴﻪ اﻟﻘﻴﻢ‪ ،‬واﻟﺨﺼﻮﺻﻴﺔ‪ ،‬واﻟﺘﺤﻜﻢ اﻟﺒﴩي‪ 11 .‬ﻫﻨﺎك‬
‫ً‬
‫أﻳﻀﺎ ﻣﻨﻈﻤﺎت ﻣﻬﻨﻴﺔ ﺗﻌﻤﻞ ﰲ ﻣﺠﺎل ﺳﻴﺎﺳﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻓﻘﺪ ﻃﺮح ﻣﻌﻬﺪ ﻣﻬﻨﺪﳼ‬
‫اﻟﻜﻬﺮﺑﺎء واﻹﻟﻜﱰوﻧﻴﺎت‪ ،‬اﻟﺬي ﻳﺰﻋﻢ أﻧﻪ أﻛﱪ ﻣﻨﻈﻤﺔ ﻣﻬﻨﻴﺔ ﻓﻨﻴﺔ ﰲ اﻟﻌﺎﻟﻢ‪ ،‬ﻣﺒﺎدر ًة ﻋﺎملﻴﺔ‬
‫ٍ‬
‫ﻣﻨﺎﻗﺸﺎت ﺑني اﻟﺨﱪاء‪ ،‬أﺛﻤﺮت املﺒﺎدرة ﻋﻦ‬
‫ﺣﻮل أﺧﻼﻗﻴﺎت اﻷﻧﻈﻤﺔ اﻟﺬﻛﻴﺔ وا ُملﺴﺘﻘﻠﺔ‪ .‬وﺑﻌﺪ‬
‫ٍ‬
‫وﺛﻴﻘﺔ ﺗﺘﻀﻤﱠ ﻦ رؤﻳﺔ ﻟ »ﺗﺼﻤﻴ ٍﻢ ﻣﻮﺟﱠ ﻪ أﺧﻼﻗﻴٍّﺎ«‪ ،‬ﺗﻘﱰح أن ﻳﻜﻮن ﺗﺼﻤﻴﻢ ﻫﺬه اﻟﺘﻘﻨﻴﺎت‬
‫وﺗﻄﻮﻳﺮﻫﺎ وﺗﻨﻔﻴﺬﻫﺎ ﻣﻮﺟﻬً ﺎ ﺑﻮاﺳﻄﺔ املﺒﺎدئ اﻟﻌﺎﻣﺔ ﻟﺤﻘﻮق اﻹﻧﺴﺎن واﻟﺮﻓﺎه واملﺴﺎءﻟﺔ‬
‫واﻟﺸﻔﺎﻓﻴﺔ واﻟﺘﻮﻋﻴﺔ ﺑﺸﺄن ﺳﻮء ْ‬
‫اﻻﺳﺘِﺨﺪام‪ .‬وﻳﻤﻜﻦ أن ﻳﻜﻮن ﺗﻀﻤني اﻷﺧﻼق ﰲ املﻌﺎﻳري‬
‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ اﻟﻌﺎملﻴﺔ وﺳﻴﻠﺔ ﻓﻌﱠ ﺎﻟﺔ ﻟﻠﻤﺴﺎﻫﻤﺔ ﰲ ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻷﺧﻼﻗﻲ‪.‬‬
‫اﻟﺤﻠﻮل اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ وﻣﺴﺄﻟﺔ اﻷﺳﺎﻟﻴﺐ واﻟﺘﻨﻔﻴﺬ‬
‫ﺗﺒني املﺒﺎدرة اﻟﻌﺎملﻴﺔ اﻟﺘﻲ ﻃﺮﺣﻬﺎ ﻣﻌﻬﺪ ﻣﻬﻨﺪﳼ اﻟﻜﻬﺮﺑﺎء واﻹﻟﻜﱰوﻧﻴﺎت أﻧﻪ ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ‬
‫ﺑﺎﻟﺘﺪاﺑري‪ ،‬ﺗُﺮﻛﺰ ﺑﻌﺾ وﺛﺎﺋﻖ اﻟﺴﻴﺎﺳﺎت ﻋﲆ اﻟﺤﻠﻮل اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻛﻤﺎ‬
‫ذﻛﺮﻧﺎ ﰲ اﻟﻔﺼﻞ اﻟﺴﺎﺑﻖ‪ ،‬دﻋﺎ ﺑﻌﺾ اﻟﺒﺎﺣِ ﺜني إﱃ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺑﻞ ﻟﻠﺘﻔﺴري‪ ،‬إﱃ‬
‫ﻓﺘﺢ اﻟﺼﻨﺪوق اﻷﺳﻮد‪ .‬وﻫﻨﺎك أﺳﺒﺎب وﺟﻴﻬﺔ ﻟﻠﺮﻏﺒﺔ ﰲ ﻓِ ﻌﻞ ذﻟﻚ؛ إذ إن ﺗﻔﺴري املﻨﻄﻖ وراء‬
‫اﻟﻘﺮار اﻟﺬي ﻳُﺘﱠ َﺨﺬ ﻟﻴﺲ ﻣﻄﻠﺒًﺎ أﺧﻼﻗﻴٍّﺎ ﻓﻘﻂ وﻟﻜﻨﻪ ً‬
‫أﻳﻀﺎ ﺟﺎﻧﺐ ﻣُﻬﻢ ﻣﻦ اﻟﺬﻛﺎء اﻟﺒﴩي‬
‫)‪ .(Samek, Wiegand, and Müller 2017‬إذَن ﻓﺎﻟﻔﻜﺮة وراء اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺑﻞ‬
‫ﱠ‬
‫اﻟﺸﻔﺎف ﻫﻲ أن ﻳﻜﻮن ﻣﻦ اﻟﺴﻬﻞ ﻓﻬﻢ أﻓﻌﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻗﺮاراﺗﻪ‪.‬‬
‫ﻟﻠﺘﻔﺴري أو‬
‫وﻛﻤﺎ رأﻳﻨﺎ‪ ،‬ﻓﺈن ﻫﺬه اﻟﻔﻜﺮة ﻣﻦ اﻟﺼﻌﺐ ﺗﻨﻔﻴﺬﻫﺎ ﰲ ﺣﺎﻟﺔ ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ اﻟﺬي ﻳﺴﺘﺨﺪم‬
‫اﻟﺸﺒﻜﺎت اﻟﻌﺼﺒﻴﺔ )‪ .(Goebel et al. 2018‬وﻟﻜﻦ ﻳﻤﻜﻦ ﻟﻠﺴﻴﺎﺳﺎت ﺑﺎﻟﻄﺒﻊ دﻋﻢ اﻟﺒﺤﺚ‬
‫ﰲ ﻫﺬا اﻻﺗﺠﺎه‪.‬‬
‫ﺑﺸﻜﻞ ﻋﺎم‪ ،‬ﻓﺈن ﻓﻜﺮة ﺗﻀﻤني اﻷﺧﻼق ﰲ ﺗﺼﻤﻴﻢ اﻟﺘﻘﻨﻴﺎت اﻟﺠﺪﻳﺪة ﻫﻲ ﻓﻜﺮة راﺋﻌﺔ‪.‬‬
‫ﱠ‬
‫اﻟﺤﺴﺎس‬
‫وﻳﻤﻜﻦ أن ﺗُﺴﺎﻋﺪﻧﺎ اﻷﻓﻜﺎر ﻣﺜﻞ اﻷﺧﻼﻗﻴﺎت ا ُملﺘﻀﻤﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ أو اﻟﺘﺼﻤﻴﻢ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﺗﺆدي إﱃ ﻣﺰﻳ ٍﺪ‬
‫ﻟﻠﻘﻴﻢ‪ ،‬اﻟﺘﻲ ﻟﻬﺎ ﺗﺎرﻳﺨﻬﺎ اﻟﺨﺎص‪ 12 ،‬ﰲ ﺗﺼﻤﻴﻢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻣﻦ املﺴﺎءﻟﺔ واملﺴﺌﻮﻟﻴﺔ واﻟﺸﻔﺎﻓﻴﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻤﻜﻦ أن ﺗﻨﻄﻮي اﻷﺧﻼﻗﻴﺎت ا ُملﺘﻀﻤﱠ ﻨﺔ‬
‫‪110‬‬

‫اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ‬

‫ﰲ اﻟﺘﺼﻤﻴﻢ ﻋﲆ ﺿﻤﺎن اﻟﺘﺘﺒﱡﻊ ﰲ ﺟﻤﻴﻊ املﺮاﺣﻞ )‪ ،(Dignum et al. 2018‬ﻣﻤﺎ ﻳُﺴﻬﻢ ﰲ‬
‫إﻣﻜﺎﻧﻴﺔ ﻣﺴﺎءﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻳﻤﻜﻦ ﺗﺤﻘﻴﻖ ﻓﻜﺮة اﻟﺘﺘﺒﻊ ﺣﺮﻓﻴٍّﺎ‪ ،‬ﺑﻤﻌﻨﻰ ﺗﺴﺠﻴﻞ‬
‫ﺑﻴﺎﻧﺎت ﺣﻮل ﺳﻠﻮك اﻟﻨﻈﺎم‪ .‬وﻗﺪ ﻃﺎﻟﺐ وﻳﻨﻔﻴﻠﺪ وﺟريوﺗﻜﺎ )‪ (٢٠١٧‬ﺑﺘﻨﻔﻴﺬ »ﺻﻨﺪوق‬
‫أﺳﻮد أﺧﻼﻗﻲ« ﰲ اﻟﺮوﺑﻮﺗﺎت واﻷﻧﻈﻤﺔ ا ُملﺴﺘﻘ ﱠﻠﺔ‪ ،‬ﻟﻴُﺴﺠﻞ ﻣﺎ ﻳﻔﻌﻠﻪ اﻟﺮوﺑﻮت )اﻟﺒﻴﺎﻧﺎت ﻣﻦ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﺗُﺸﺒﻪ اﻟﺼﻨﺪوق اﻷﺳﻮد‬
‫اﻷﺟﻬﺰة اﻻﺳﺘﺸﻌﺎرﻳﺔ وﻣِﻦ اﻟﻮﺿﻊ »اﻟﺪاﺧﲇ« ﻟﻠﻨﻈﺎم(‬
‫ا ُملﺜﺒﱠﺖ ﰲ اﻟﻄﺎﺋﺮات‪ .‬وﻳﻤﻜﻦ ﺗﻄﺒﻴﻖ ﻫﺬه اﻟﻔﻜﺮة ً‬
‫أﻳﻀﺎ ﰲ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺴﺘﻘﻞ‪:‬‬
‫ﻓﻌﻨﺪﻣﺎ ﻳﺤﺪث ﺧﻄﺄ ﻣﺎ‪ ،‬ﻗﺪ ﺗُﺴﺎﻋﺪﻧﺎ ﻣﺜﻞ ﻫﺬه اﻟﺒﻴﺎﻧﺎت ﰲ ﺗﻔﺴري ﻣﺎ ﺣﺪث ﺑﺎﻟﻀﺒﻂ‪.‬‬
‫وﻫﺬا ﺑﺪَوره ﻗﺪ ﻳُﺴﺎﻋﺪ ﰲ اﻟﺘﺤﻠﻴﻞ اﻷﺧﻼﻗﻲ واﻟﻘﺎﻧﻮﻧﻲ ﻟﻠﺤﺎﻟﺔ‪ .‬وﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻛﻤﺎ‬
‫ﻳﻘﻮل اﻟﺒﺎﺣﺜﻮن‪ ،‬وﻫﻢ ﻣ ﱡ‬
‫ُﺤﻘﻮن ﰲ ﻗﻮﻟﻬﻢ‪ ،‬ﻳُﻤﻜﻨﻨﺎ أن ﻧﺘﻌ ﱠﻠﻢ ﺷﻴﺌًﺎ ﻣﻦ ﺻﻨﺎﻋﺔ اﻟﻄﺎﺋﺮات‪،‬‬
‫ﱡ‬
‫ﻟﻠﺘﺤﻘﻖ ﻣﻦ اﻟﺴﻼﻣﺔ وﻋﻤﻠﻴﺎت ﻣﺮﺋﻴﺔ‬
‫اﻟﺘﻲ ﺗﺨﻀﻊ إﱃ ﺗﻨﻈﻴ ٍﻢ ﺻﺎرم وﻟﺪَﻳﻬﺎ ﻋﻤﻠﻴﺎت دﻗﻴﻘﺔ‬
‫ﻟﻠﺘﺤﻘﻴﻖ ﰲ اﻟﺤﻮادث‪ .‬ﻓﻬﻞ ﻳُﻤﻜﻦ ﺗﺜﺒﻴﺖ ِﺑﻨﻴﺔ أﺳﺎﺳﻴﺔ ﻣُﻤﺎﺛﻠﺔ ﺗﻀﻤﻦ اﻟﺘﻨﻈﻴﻢ واﻟﺴﻼﻣﺔ ﰲ‬
‫ﺣﺎﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟ وﻟﻠﻤُﻘﺎرﻧﺔ ﺑﻤﺠﺎل َ‬
‫آﺧﺮ ﻣﻦ ﻣﺠﺎﻻت وﺳﺎﺋﻞ اﻟﻨﻘﻞ‪ ،‬ﻗﺪ اﻗﱰﺣﺖ‬
‫ﺻﻨﺎﻋﺔ اﻟﺴﻴﺎرات ً‬
‫أﻳﻀﺎ ﺷﻬﺎد ًة أو ﻧﻮﻋً ﺎ ﻣﻦ »رﺧﺼﺔ اﻟﻘﻴﺎدة« ﻟﻠﻤﺮﻛﺒﺎت اﻟﺬاﺗﻴﺔ اﻟﺘﺸﻐﻴﻞ‬
‫‪13‬‬
‫املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬ﻳﺬﻫﺐ ﺑﻌﺾ اﻟﺒﺎﺣﺜني إﱃ أﺑﻌَ َﺪ ﻣﻦ ذﻟﻚ وﻳﻬﺪﻓﻮن إﱃ‬
‫ٍ‬
‫إﻧﺸﺎء ٍ‬
‫ُﺤﺎوﻟﺔ ﻟﺘﺤﻘﻴﻖ »أﺧﻼﻗﻴﺎت اﻵﻟﺔ« ﺑﻤﻌﻨﻰ أن ﺗﺴﺘﻄﻴﻊ اﻵﻻت ﻧﻔﺴﻬﺎ‬
‫آﻻت أﺧﻼﻗﻴﺔ‪ ،‬ﰲ ﻣ‬
‫ٍ‬
‫ﻳﺠﺐ اﻻﺣﺘﻔﺎظ ﺑﻬﺬه‬
‫اﺗﺨﺎذ‬
‫ﻗﺮارات أﺧﻼﻗﻴﺔ‪ .‬وﻳُﺠﺎدل آﺧﺮون ﺑﺄن ﻫﺬه ﻓﻜﺮة ﺧﻄرية وأﻧﻪ ِ‬
‫اﻟﻘﺪرة َ‬
‫ﻟﻠﺒﴩ‪ ،‬وأﻧﻪ ﻣﻦ ا ُملﺴﺘﺤﻴﻞ ﺧﻠﻖ آﻻت ﺗﺘﻤﺘﱠﻊ ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﻜﺎﻣﻠﺔ‪ ،‬وﻻ ﺣﺎﺟﺔ‬
‫إﱃ أن ﺗﺘﻤﺘﱠﻊ اﻵﻻت ﺑﺎﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﻜﺎﻣﻠﺔ‪ ،‬وﻳﻜﻔﻲ أن ﺗﻜﻮن اﻵﻻت ً‬
‫آﻣﻨﺔ وﻣﻠﺘﺰﻣﺔ‬
‫ﺑﺎﻟﻘﺎﻧﻮن )‪ ،(Yampolskiy 2013‬أو ﻗﺪ ﺗُ َ‬
‫ﻨﺸﺄ أﺷﻜﺎل ﻣﻦ »اﻷﺧﻼق اﻟﻮﻇﻴﻔﻴﺔ« )‪Wallach‬‬
‫‪ (and Allen 2009‬اﻟﺘﻲ ﻻ ﺗﻜﺎﻓﺊ اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﻜﺎﻣﻠﺔ‪ ،‬وﻟﻜﻨﻬﺎ ﻣﻊ ذﻟﻚ ﺗﺠﻌﻞ اﻵﻟﺔ‬
‫ﻣ ً‬
‫ُﺮاﻋﻴﺔ ﻧﺴﺒﻴٍّﺎ ﻟﻘﻮاﻋﺪ اﻷﺧﻼق‪ .‬ﺗُﻌﺪ ﻫﺬه املﻨﺎﻗﺸﺔ‪ ،‬اﻟﺘﻲ ﺗﺘﻌ ﱠﻠﻖ ﻣﺠﺪدًا ﺑﻤﻮﺿﻮع املﻜﺎﻧﺔ‬
‫اﻷﺧﻼﻗﻴﺔ‪ ،‬ذات ِﺻﻠﺔ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﺣﺎﻟﺔ اﻟﺴﻴﺎرات اﻟﺬاﺗﻴﺔ اﻟﻘﻴﺎدة‪ :‬وإﱃ أي ﻣﺪًى‬
‫َ‬
‫ُﺴﺘﺤﺴﻦ ﺗﻀﻤني اﻟﻘﻮاﻋﺪ اﻷﺧﻼﻗﻴﺔ ﰲ ﻫﺬه اﻟﺴﻴﺎرات‪ ،‬وﻣﺎ ﻧﻮع ﻫﺬه‬
‫ﻳﺘﻌني وﻳﻤﻜﻦ وﻳ‬
‫اﻟﻘﻮاﻋﺪ اﻷﺧﻼﻗﻴﺔ وﻛﻴﻒ ﻳُﻤﻜﻦ ﺗﻨﻔﻴﺬﻫﺎ ﺗﻘﻨﻴٍّﺎ؟‬
‫ﱠ‬
‫اﻟﺤﺴﺎس ﻟﻠﻘِ ﻴَﻢ‪ ،‬ﰲ‬
‫ﻳﻤﻜﻦ أن ﺗُﺴﺎﻋﺪﻧﺎ اﻷﻓﻜﺎر ﻣﺜﻞ اﻷﺧﻼﻗﻴﺎت ا ُملﺘﻀﻤﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ أو اﻟﺘﺼﻤﻴﻢ‬
‫إﻧﺸﺎء اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﻄﺮﻳﻘﺔٍ ﺗﺆدي إﱃ ﻣﺰﻳ ٍﺪ ﻣﻦ املﺴﺎءﻟﺔ واملﺴﺌﻮﻟﻴﺔ واﻟﺸﻔﺎﻓﻴﺔ‪.‬‬

‫‪111‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻳﻤﻴﻞ ﺻﺎﻧﻌﻮ اﻟﺴﻴﺎﺳﺎت إﱃ دﻋﻢ اﻟﻌﺪﻳﺪ ﻣﻦ ﻫﺬه اﻻﺗﺠﺎﻫﺎت ﰲ اﻟﺒﺤﺚ واﻻﺑﺘﻜﺎر ﰲ‬
‫وﺑﺸﻜﻞ ﻋﺎم‪ ،‬ﺗﻀﻤني‬
‫ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻣﺜﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺑﻞ ﻟﻠﺘﻔﺴري‬
‫ٍ‬
‫اﻷﺧﻼق ﰲ اﻟﺘﺼﻤﻴﻢ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إﱃ ﺟﺎﻧﺐ اﻷﺳﺎﻟﻴﺐ ﻏري اﻟﺘﻘﻨﻴﺔ ﻣﺜﻞ اﻟﻠﻮاﺋﺢ‬
‫اﻟﺘﻨﻈﻴﻤﻴﺔ‪ ،‬ووﺿﻊ املﻌﺎﻳري‪ ،‬واﻟﺘﻌﻠﻴﻢ‪ ،‬وﺣﻮار اﻷﻃﺮاف ا َملﻌﻨﻴﺔ وﻓِﺮق اﻟﺘﺼﻤﻴﻢ اﻟﺸﺎﻣﻠﺔ‪،‬‬
‫ذﻛﺮ ﺗﻘﺮﻳﺮ ﻓﺮﻳﻖ اﻟﺨﱪاء اﻟﺮﻓﻴﻊ املﺴﺘﻮى ﻋﺪدًا ﻣﻦ اﻷﺳﺎﻟﻴﺐ اﻟﺘﻘﻨﻴﺔ وﻣﻨﻬﺎ ﺗﻀﻤني‬
‫اﻟﻘﻮاﻋﺪ اﻷﺧﻼﻗﻴﺔ وﺳﻴﺎدة اﻟﻘﺎﻧﻮن ﰲ اﻟﺘﺼﻤﻴﻢ‪ ،‬وﻫﻴﺎﻛﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺠﺪﻳﺮ ﺑﺎﻟﺜﻘﺔ‪،‬‬
‫واﻻﺧﺘﺒﺎر واﻟﺘﺤﻘﻖ‪ ،‬واﻟﺘﺘﺒﻊ واﻟﺘﺪﻗﻴﻖ‪ ،‬واﻟﺘﻔﺴري‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳُﻤﻜﻦ أن ﺗﺸﺘﻤﻞ‬
‫اﻷﺧﻼﻗﻴﺎت ا ُملﻀﻤﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ ﻋﲆ اﻟﺨﺼﻮﺻﻴﺔ ا ُملﻀﻤﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ‪ .‬وﻳُﺸري اﻟﺘﻘﺮﻳﺮ‬
‫ً‬
‫أﻳﻀﺎ إﱃ ﺑﻌﺾ اﻟﻄﺮق اﻟﺘﻲ ﻳُﻤﻜﻦ ﺑﻬﺎ ﺗﻨﻔﻴﺬ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺠﺪﻳﺮ ﺑﺎﻟﺜﻘﺔ‪ ،‬ﻣﺜﻞ اﻟﺘﺘﺒﱡﻊ‬
‫ٍ‬
‫ﻳﺠﺐ‬
‫ﻛﻄﺮﻳﻘﺔ ﻟﻠﻤﺴﺎﻫﻤﺔ ﰲ اﻟﺸﻔﺎﻓﻴﺔ‪ :‬وﰲ ﺣﺎﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺴﺘﻨﺪ إﱃ ﻗﻮاﻋﺪ ِ‬
‫ﺗﻮﺿﻴﺢ ﻛﻴﻔﻴﺔ ﺑﻨﺎء اﻟﻨﻤﻮذج‪ ،‬وﰲ ﺣﺎﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺴﺘﻨِﺪ إﱃ اﻟﺘﻌ ﱡﻠﻢ ﻳﺠﺐ ﺗﻮﺿﻴﺢ‬
‫وﺳﻴﻠﺔ ﺗﺪرﻳﺐ اﻟﺨﻮارزﻣﻴﺔ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ ﻛﻴﻔﻴﺔ ﺟﻤﻊ اﻟﺒﻴﺎﻧﺎت واﺧﺘﻴﺎرﻫﺎ‪ .‬وﻣﻦ ا ُملﻔﱰَض أن‬
‫ﻳﻀﻤَ ﻦ ﻫﺬا أن ﻳﻜﻮن ﻧﻈﺎم اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫ﻗﺎﺑﻼ ﻟﻠﺘﺪﻗﻴﻖ‪ ،‬وﻻ ﺳﻴﱠﻤﺎ ﰲ املﻮاﻗﻒ اﻟﺨﻄرية‬
‫)‪.(European Commission AI HLEG 2019‬‬
‫َ‬
‫ﺣﺎﺳﻤﺔ اﻷﻫﻤﻴﺔ‪ :‬ﺣﻴﺚ إن ﺗﺤﺪﻳﺪ ﻋﺪدٍ ﻣﻦ املﺒﺎدئ‬
‫ﺗُﻌ ﱡﺪ ﻣَ ﺴﺄﻟﺔ اﻷﺳﺎﻟﻴﺐ واﻟﺘﻨﻔﻴﺬ‬
‫اﻷﺧﻼﻗﻴﺔ ﳾء‪ ،‬واﻛﺘﺸﺎف ﻃﺮﻳﻘﺔ ﺗﻨﻔﻴﺬ ﻫﺬه املﺒﺎدئ ﻋﻤﻠﻴٍّﺎ ﳾءٌ ﻣﺨﺘﻠﻒ ﺗﻤﺎﻣً ﺎ‪ .‬وﺣﺘﻰ‬
‫املﻔﺎﻫﻴﻢ ﻣﺜﻞ اﻟﺨﺼﻮﺻﻴﺔ ا ُملﻀﻤﱠ ﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ‪ ،‬اﻟﺘﻲ ﻳُﻔﱰض أن ﺗﻜﻮن أﻗﺮب إﱃ ﻋﻤﻠﻴﺔ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻣﺠﺮدة وﻋﺎﻣﺔ؛ وﻣِﻦ ﺛَﻢ ﻓﺈﻧﻨﺎ ﻣﺎ زﻟﻨﺎ ﻻ ﻧﺪري‬
‫اﻟﺘﻄﻮﻳﺮ واﻟﻬﻨﺪﺳﺔ‪ ،‬ﻓﻐﺎﻟﺒًﺎ ﻣﺎ ﺗُﺼﺎغ‬
‫ٍ‬
‫ملﻨﺎﻗﺸﺔ ﻣﻮﺟﺰة ﺣﻮل ﺑﻌﺾ‬
‫ﺑﺎﻟﺘﺤﺪﻳﺪ ﻣﺎ ﻳﻨﺒﻐﻲ أن ﻧﻔﻌﻠﻪ‪ .‬وﻳﻘﻮدﻧﺎ ﻫﺬا إﱃ اﻟﻔﺼﻞ اﻟﺘﺎﱄ‬
‫ِ‬
‫ﺳﻴﺎﺳﺎت أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﺗﻮاﺟﻪ‬
‫اﻟﺘﺤﺪﻳﺎت اﻟﺘﻲ ِ‬

‫‪112‬‬

‫اﻟﻔﺼﻞ اﻟﺤﺎدي ﻋﴩ‬

‫اﻟﺘﺤﺪﻳﺎت اﻟﺘﻲ ُﺗﻮاﺟﻪ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‬

‫اﻷﺧﻼﻗﻴﺎت اﻻﺳﺘﺒﺎﻗﻴﺔ‪ :‬اﻻﺑﺘﻜﺎر املﺴﺌﻮل وﺗﻀﻤني اﻟﻘِ ﻴَﻢ ﰲ اﻟﺘﺼﻤﻴﻢ‬
‫ﺗﻮاﺟﻪ اﻟﻌﺪﻳﺪ ﻣﻦ‬
‫رﺑﻤﺎ ﻻ ﻳُﺪﻫﺸﻨﺎ أن ﻧﻌﺮف أن ﺳﻴﺎﺳﺎت أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ِ‬
‫ً‬
‫رؤﻳﺔ اﺳﺘﺒﺎﻗﻴﺔ ﻷﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء‬
‫اﻟﺘﺤﺪﱢﻳﺎت‪ .‬وﻗﺪ رأﻳﻨﺎ أن ﺑﻌﺾ اﻟﺴﻴﺎﺳﺎت ا ُملﻘﱰﺣﺔ ﺗﺆﻳﺪ‬
‫ٍ‬
‫ﺑﺤﺎﺟﺔ إﱃ ﻣﺮاﻋﺎة اﻷﺧﻼق ﰲ املﺮﺣﻠﺔ ا ُملﺒﻜﺮة ﻣﻦ ﺗﻄﻮﻳﺮ ﺗﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫اﻻﺻﻄﻨﺎﻋﻲ؛ ﺑﻤﻌﻨﻰ أﻧﻨﺎ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﺗﻜﻤُﻦ اﻟﻔﻜﺮة ﰲ ﺗﺠﻨﱡﺐ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ وا ُملﺠﺘﻤﻌﻴﺔ اﻟﺘﻲ ﻳﺨﻠﻘﻬﺎ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺘﻲ ﺳﻴﻜﻮن ﻣﻦ اﻟﺼﻌﺐ اﻟﺘﻌﺎﻣُﻞ ﻣﻌﻬﺎ ﺑﻤﺠﺮد ﺣﺪوﺛﻬﺎ‪ .‬وﻳﺘﻤﺎﳽ‬
‫ﻫﺬا ﻣﻊ أﻓﻜﺎر اﻻﺑﺘﻜﺎر املﺴﺌﻮل‪ ،‬وﺗﻀﻤني اﻟﻘِ ﻴَﻢ ﰲ اﻟﺘﺼﻤﻴﻢ‪ ،‬وﻏريﻫﺎ ﻣﻦ اﻷﻓﻜﺎر ا ُملﺸﺎﺑﻬﺔ‬
‫اﻗﱰﺣﺖ ﻋﲆ ﻣﺪار اﻟﺴﻨﻮات اﻷﺧرية‪ .‬وﻫﺬا ﻳُﺤﻮﱢل املﺸﻜﻠﺔ ﻣﻦ ﻣﻌﺎﻟﺠﺔ اﻵﺛﺎر اﻟﺴﻠﺒﻴﺔ‬
‫اﻟﺘﻲ ُ ِ‬
‫ﻟﻠﺘﻘﻨﻴﺎت ا ُملﺴﺘﺨﺪﻣﺔ ﻋﲆ ﻧﻄﺎق واﺳﻊ ﺑﺎﻟﻔﻌﻞ إﱃ ﺗﺤﻤﻞ املﺴﺌﻮﻟﻴﺔ ﺗﺠﺎه اﻟﺘﻘﻨﻴﺎت اﻟﺘﻲ ﻳﺘﻢ‬
‫ﺗﻄﻮﻳﺮﻫﺎ اﻟﻴﻮم‪.‬‬
‫ﱠ‬
‫ﻧﺘﻮﻗﻊ اﻟﻌﻮاﻗِ ﺐ ﻏري املﻘﺼﻮدة ﻟﻠﺘﻘﻨﻴﺎت اﻟﺠﺪﻳﺪة ﰲ‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻟﻴﺲ ﻣﻦ اﻟﺴﻬﻞ أن‬
‫ﻣﺮﺣﻠﺔ اﻟﺘﺼﻤﻴﻢ‪ .‬إﺣﺪى اﻟﻄﺮق ﻟﺘﺨﻔﻴﻒ ﻫﺬه املﺸﻜﻠﺔ ﻫﻮ ﺑﻨﺎء ﺳﻴﻨﺎرﻳﻮﻫﺎت ﺣﻮل اﻟﻌﻮاﻗﺐ‬
‫اﻷﺧﻼﻗﻴﺔ ا ُملﺴﺘﻘﺒﻠﻴﺔ‪ .‬وﻫﻨﺎك أﺳﺎﻟﻴﺐ ﻣُﺨﺘﻠﻔﺔ ملﻤﺎرﺳﺔ اﻷﺧﻼﻗﻴﺎت ﰲ اﻟﺒﺤﺚ واﻻﺑﺘﻜﺎر‬
‫)‪ ،(Reijers et al. 2018‬إﺣﺪاﻫﺎ ﻟﻴﺴﺖ ﻓﻘﻂ دراﺳﺔ ﺗﺄﺛري ﴎدﻳﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ً‬
‫اﻟﺤﺎﻟﻴﺔ وﺗﻘﻴﻴﻤﻬﺎ )‪ (Royal Society, 2018‬وﻟﻜﻦ ً‬
‫واﻗﻌﻴﺔ‬
‫أﻳﻀﺎ ﺧﻠﻖ ﴎدﻳﺎت ﺟﺪﻳﺪة أﻛﺜﺮ‬
‫ﺣﻮل ﺗﻄﺒﻴﻘﺎت ﻣُﻌﻴﻨﺔ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﻨﻬﺞ ا ُملﻮﺟﱠ ﻪ ﻟﻠﻤُﻤﺎرﺳﺔ واﻟﻨﻬﺞ اﻟﺘﺼﺎﻋﺪي‪ :‬ﻛﻴﻒ ﻧﱰﺟﻤﻬﻤﺎ ﻋﻤﻠﻴٍّﺎ؟‬
‫اﻻﺑﺘﻜﺎر املﺴﺌﻮل ﻻ ﻳﺘﻌﻠﻖ ﻓﻘﻂ ﺑﺘﻀﻤني اﻷﺧﻼﻗﻴﺎت ﰲ اﻟﺘﺼﻤﻴﻢ‪ ،‬وﻟﻜﻨﻪ ﻳﺘﻄ ﱠﻠﺐ ً‬
‫أﻳﻀﺎ‬
‫ﻣﺮاﻋﺎة آراء ﻣُﺨﺘﻠﻒ اﻷﻃﺮاف ا َملﻌﻨﻴﺔ وﻣﺼﺎﻟﺤﻬﻢ‪ .‬وﺗﻨﻄﻮي اﻟﺤﻮﻛﻤﺔ اﻟﺸﺎﻣﻠﺔ ﻋﲆ إﴍاك‬
‫ﻧﻘﺎش ﻋﺎم‪ ،‬واﻟﺘﺪﺧﻞ ا ُملﺠﺘﻤﻌﻲ ا ُملﺒﻜﺮ ﰲ ﻣﺮﺣﻠﺔ‬
‫ﻧﻄﺎق واﺳﻊ ﻣﻦ اﻷﻃﺮاف ا َملﻌﻨﻴﺔ‪ ،‬وإﺟﺮاء ٍ‬
‫ٍ‬
‫ً‬
‫اﻟﺒﺤﺚ واﻻﺑﺘﻜﺎر )‪ .(Von Schomberg 2011‬وﻫﺬا ﻗﺪ ﻳﻌﻨﻲ‪ ،‬ﻣﺜﻼ‪ ،‬ﺗﻨﻈﻴﻢ ﻣﺠﻤﻮﻋﺎت‬
‫ﻧﻘﺎش ﻣﺮﻛﺰة واﺳﺘﺨﺪام ﺗﻘﻨﻴﺎت أﺧﺮى ملﻌﺮﻓﺔ رأي اﻟﻨﺎس ﰲ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪.‬‬
‫ٍ‬
‫ﻳﺘﻌﺎرض ﻫﺬا اﻟﻨﻬﺞ اﻟﺘﺼﺎﻋﺪي ﰲ اﻻﺑﺘﻜﺎر املﺴﺌﻮل ﻣﻊ ﻧﻬﺞ اﻷﺧﻼﻗﻴﺎت اﻟﺘﻄﺒﻴﻘﻴﺔ‬
‫اﻟﺬي ﻳﺘﺒﻌﻪ ﻣﻌﻈﻢ وﺛﺎﺋﻖ اﻟﺴﻴﺎﺳﺎت‪ ،‬واﻟﺬي ﻳﻤﻴﻞ ﰲ اﻟﻐﺎﻟﺐ إﱃ أن ﻳﻜﻮن ﻧﻬﺠً ﺎ ﺗﻨﺎزﻟﻴٍّﺎ‬
‫وﻣﺠﺮدًا‪ً .‬‬
‫أوﻻ‪ ،‬ﻳﺘﻢ إﻧﺸﺎء اﻟﺴﻴﺎﺳﺎت ﻏﺎﻟﺒًﺎ ﻣﻦ ﻗِ ﺒﻞ ﺧﱪاء‪ ،‬دون أن ﻳﺸﺎرك ﻓﻴﻬﺎ ﻧﻄﺎق‬
‫واﺳﻊ ﻣﻦ اﻷﻃﺮاف ا َملﻌﻨﻴﺔ‪ .‬ﺛﺎﻧﻴًﺎ‪ ،‬ﺣﺘﻰ إذا أﻳﱠﺪت ﻫﺬه اﻟﺴﻴﺎﺳﺎت ﻣﺒﺎدئ ﻣﺜﻞ اﻷﺧﻼﻗﻴﺎت‬
‫ا ُملﻀﻤﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ‪ ،‬ﻓﺈﻧﻬﺎ ﺗﻈ ﱡﻞ ﺷﺪﻳﺪة اﻟﻐﻤﻮض ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﻤﺎ ﻳَﻌﻨﻴﻪ ﺗﻄﺒﻴﻖ ﻫﺬه‬
‫ﺟﴪ ﺑني‬
‫املﺒﺎدئ ﻋﻤﻠﻴٍّﺎ‪ .‬وﻹﻧﺠﺎح ﺳﻴﺎﺳﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻳﻈ ﱡﻞ اﻟﺘﺤﺪي ﻛﺒريًا ﻟﺒﻨﺎء‬
‫ٍ‬
‫املﺒﺎدئ اﻷﺧﻼﻗﻴﺔ واﻟﻘﺎﻧﻮﻧﻴﺔ ا ُملﺠﺮدة واﻟﻌﺎﻟﻴﺔ املﺴﺘﻮى ﻣﻦ ﻧﺎﺣﻴﺔ‪ ،‬وﺑني ﻣُﻤﺎرﺳﺎت ﺗﻄﻮﻳﺮ‬
‫ٍ‬
‫ﺳﻴﺎﻗﺎت ﻣُﻌﻴﻨﺔ‪ ،‬واﻟﺘﻘﻨﻴﺎت‪ ،‬وأﺻﻮات أوﻟﺌﻚ اﻟﺬﻳﻦ ﻳﺸﺎرﻛﻮن‬
‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ واﺳﺘﺨﺪاﻣﻬﺎ ﰲ‬
‫ﰲ ﻫﺬه املﻤﺎرﺳﺎت وﻳﻌﻤﻠﻮن ﰲ ﻫﺬه اﻟﺴﻴﺎﻗﺎت ﻣﻦ ﻧﺎﺣﻴﺔ أﺧﺮى‪ .‬ﻳُﱰك ﺑﻨﺎء ﻫﺬا اﻟﺠﴪ َملﻦ‬
‫ﺗُﻮﺟﱠ ﻪ إﻟﻴﻬﻢ ﻫﺬه اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ‪ .‬ﻓﻬﻞ ﻳُﻤﻜﻨﻨﺎ اﻟﻘﻴﺎم ﺑﺎملﺰﻳﺪ ﰲ املﺮﺣﻠﺔ اﻷوﱃ ﻣﻦ ﺻﻨﻊ‬
‫ﻳﺠﺐ ﻋﻠﻴﻨﺎ ذﻟﻚ؟ ﻧﺤﺘﺎج ﻋﲆ اﻷﻗﻞ إﱃ املﺰﻳﺪ ﻣﻦ اﻟﻌﻤﻞ ﻋﲆ اﻷﺳﺎﻟﻴﺐ‬
‫اﻟﺴﻴﺎﺳﺎت‪ ،‬وﻫﻞ ِ‬
‫ﱠ‬
‫واملﺆﺳﺴﺎت اﻟﺘﻲ ﻧﺤﺘﺎﺟﻬﺎ ﻟﺠﻌﻞ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﻨﺠﺢ ﻋﻤﻠﻴٍّﺎ‪.‬‬
‫واﻹﺟﺮاءات‬
‫وﻳﺠﺐ ﻋﻠﻴﻨﺎ أن ﻧُﻮﱄ املﺰﻳﺪ ﻣﻦ اﻻﻫﺘﻤﺎم ﻟﻠﻌﻤﻠﻴﺔ‪.‬‬
‫اﻻﺑﺘﻜﺎر املﺴﺌﻮل ﻻ ﻳﺘﻌﻠﻖ ﻓﻘﻂ ﺑﺘﻀﻤني اﻷﺧﻼﻗﻴﺎت ﰲ اﻟﺘﺼﻤﻴﻢ‪ ،‬وﻟﻜﻨﻪ ﻳﺘﻄﻠﺐ ً‬
‫أﻳﻀﺎ ﻣﺮاﻋﺎة آراء‬
‫ﻣﺨﺘﻠﻒ اﻷﻃﺮاف ا َملﻌﻨﻴﺔ وﻣﺼﺎﻟﺤﻬﻢ‪.‬‬

‫ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﺎﻟﺴﺆال ﻋﻤﱠ ﻦ ﻳﺸﺎرك ﰲ وﺿﻊ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻓﺈﻧﻨﺎ‬
‫ﻧﺤﺘﺎج إﱃ ﺗﻄﺒﻴﻖ ﻧﻬﺞ ﺗﺼﺎﻋﺪي إﱃ ﺟﺎﻧﺐ اﻟﻨﻬﺞ اﻟﺘﻨﺎزﱄ‪ ،‬ﺑﻤﻌﻨﻰ اﻻﺳﺘﻤﺎع أﻛﺜﺮ إﱃ‬
‫اﻟﺒﺎﺣﺜني واملﻬﻨﻴني اﻟﺬﻳﻦ ﻳﺘﻌﺎﻣﻠﻮن ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﻤﻠﻴٍّﺎ‪ ،‬ﺑﻞ وإﱃ اﻷﺷﺨﺎص اﻟﺬﻳﻦ‬
‫ﻣﻦ ا ُملﺤﺘﻤَ ﻞ أن ﻳﴬﱠ ﻫﻢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬إذا ﻛﻨﺎ ﻧﺆﻳﺪ ﻣﺒﺪأ اﻟﺪﻳﻤﻘﺮاﻃﻴﺔ وإذا ﻛﺎن‬
‫‪114‬‬

‫اﻟﺘﺤﺪﻳﺎت اﻟﺘﻲ ﺗُﻮاﺟﻪ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‬

‫ﻫﺬا املﻔﻬﻮم ﻳﺸﻤﻞ اﻟﺘﻀﻤني واملﺸﺎرﻛﺔ ﰲ ﺻﻨﻊ اﻟﻘﺮار ﺑﺸﺄن ﻣُﺴﺘﻘﺒﻞ ﻣﺠﺘﻤﻌﺎﺗﻨﺎ‪ ،‬ﻓﺈن‬
‫ﺳﻤﺎع ﺻﻮت اﻷﻃﺮاف ا َملﻌﻨﻴﺔ ﻟﻴﺲ أﻣ ًﺮا اﺧﺘﻴﺎرﻳٍّﺎ وﻟﻜﻨﻪ إﻟﺰاﻣﻲ ﻣﻦ اﻟﻨﺎﺣﻴﺘَني اﻷﺧﻼﻗﻴﺔ‬
‫واﻟﺴﻴﺎﺳﻴﺔ‪ .‬ﺑﻴﻨﻤﺎ ﻳﺸﺎرك ﺑﻌﺾ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت ﰲ ﻧﻮع ﻣﻦ اﻟﺘﺸﺎور ﻣﻊ اﻷﻃﺮاف‬
‫املﻌﻨﻴﺔ )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻟﺪى املﻔﻮﺿﻴﺔ اﻷوروﺑﻴﺔ ﺗﺤﺎﻟﻒ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺨﺎص‬
‫ﺑﻬﺎ(‪ 1 ،‬ﻻ ﻳﺰال ﻣﻦ املﺸﻜﻮك ﻓﻴﻪ ﻣﺎ إذا ﻛﺎﻧﺖ ﻣﺜﻞ ﻫﺬه اﻟﺠﻬﻮد ﺗﺼﻞ ٍّ‬
‫ﺣﻘﺎ إﱃ ا ُملﻄﻮرﻳﻦ‬
‫ﱠ‬
‫ﺳﻴﺘﻌني ﻋﻠﻴﻬﻢ ﺗﺤﻤﱡ ﻞ‬
‫واملﺴﺘﺨﺪﻣني اﻟﻨﻬﺎﺋﻴني ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬واﻷﻫﻢ ﻣﻦ ذﻟﻚ‪ ،‬إﱃ أوﻟﺌﻚ اﻟﺬﻳﻦ‬
‫ﻣﻌﻈﻢ املﺨﺎﻃﺮ واﻟﺘﻌﺎﻳﺶ ﻣﻊ آﺛﺎرﻫﺎ اﻟﺴﻠﺒﻴﺔ‪ .‬ﻓﻬﻞ ُ‬
‫ﺻﻨﻊ اﻟﻘﺮار واﻟﺴﻴﺎﺳﺎت اﻟﺨﺎﺻﺔ‬
‫ٍّ‬
‫ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أﻣ ٌﺮ دﻳﻤﻘﺮاﻃﻲ ﻳﻨﻄﻮي ﻋﲆ ﻣﺸﺎرﻛﺔ ﺣﻘﺎ؟‬
‫إن ﻣﻔﻬﻮم اﻟﺪﻳﻤﻘﺮاﻃﻴﺔ ﻣُﻬﺪﱠد ً‬
‫ﱠ‬
‫أﻳﻀﺎ ﺑﺤﻘﻴﻘﺔ ﺗﺮﻛﺰ اﻟﺴﻠﻄﺔ ﰲ أﻳﺪي ﻋﺪدٍ ﺻﻐري ﻧﺴﺒﻴٍّﺎ‬
‫ﻣﻦ اﻟﴩﻛﺎت اﻟﻜﺒرية‪ .‬وﻳﺮى ﺑﻮل ﻧﻴﻤﻴﺘﺰ )‪ (٢٠١٨‬أن ﺗﺮا ُﻛﻢ اﻟﺴﻠﻄﺔ اﻟﺮﻗﻤﻴﺔ ﰲ أﻳﺪي‬
‫ﴍﻛﺎت ﻗﻠﻴﻠﺔ ﻳﻨﻄﻮي ﻋﲆ إﺷﻜﺎﻟﻴﺔ‪ :‬إذا ﻣﺎرﺳﺖ ﺣﻔﻨﺔ ﻣﻦ اﻟﴩﻛﺎت ُﺳﻠﻄﺘﻬﺎ ﻟﻴﺲ ﻓﻘﻂ‬
‫ﻋﲆ اﻷﻓﺮاد — ﻣﻦ ﺧﻼل ﺗﻜﻮﻳﻦ ﱠ‬
‫ﺎت ﺗﻌﺮﻳﻔﻴﺔ ﻋﻨﺎ — وﻟﻜﻦ ً‬
‫ﻣﻠﻔ ٍ‬
‫أﻳﻀﺎ ﻋﲆ اﻟﺒﻨﻴﺔ اﻷﺳﺎﺳﻴﺔ‬
‫ﻟﻠﺪﻳﻤﻘﺮاﻃﻴﺔ‪ ،‬ﻓﺈن ﻫﺬه اﻟﴩﻛﺎت‪ ،‬ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ ﻧﻮاﻳﺎﻫﺎ اﻟﺤﺴﻨﺔ ﻟﻠﻤﺴﺎﻫﻤﺔ ﰲ اﻟﺬﻛﺎء‬
‫ﻋﻘﺒﺎت أﻣﺎﻣﻪ‪ .‬وﻟﺬﻟﻚ‪ِ ،‬‬
‫ٍ‬
‫ﻓﻤﻦ اﻟﴬوري وﺿﻊ ﻟﻮاﺋﺢ‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻷﺧﻼﻗﻲ‪ ،‬ﺳﻮف ﺗﻀﻊ‬
‫ﺗﻨﻈﻴﻤﻴﺔ وﺣﺪود ﻟﺤﻤﺎﻳﺔ املﺼﻠﺤﺔ اﻟﻌﺎﻣﺔ‪ ،‬وﻟﻀﻤﺎن أن ﻫﺬه اﻟﴩﻛﺎت ﻟﻦ ﺗُﺸﻜﻞ اﻟﻘﻮاﻋﺪ‬
‫ﺑﻤﻔﺮدﻫﺎ‪ .‬وأﺷﺎر ﻣﻮراي ﺷﺎﻧﺎﻫﺎن إﱃ أن »ا َملﻴﻞ إﱃ ﺗﺮ ﱡﻛﺰ اﻟﺴﻠﻄﺔ واﻟﺜﺮوة واملﻮارد ﰲ أﻳﺪي‬
‫ﻋﺪد ﻗﻠﻴﻞ ﻳﺘﱠﺴﻢ ﺑﺎﻻﺳﺘﺪاﻣﺔ اﻟﺬاﺗﻴﺔ« )‪ ،(١٦٦ ،٢٠١٥‬ﻣﻤﺎ ﻳﺠﻌﻞ ﻣﻦ اﻟﺼﻌﺐ ﺗﺤﻘﻴﻖ‬
‫ً‬
‫إﻧﺼﺎﻓﺎ‪ .‬ﻛﻤﺎ أﻧﻪ ﻳﺠﻌﻞ اﻷﻓﺮاد ﻋُ ﺮﺿﺔ ﻟﺠﻤﻴﻊ أﻧﻮاع املﺨﺎﻃﺮ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ‬
‫ﻣﺠﺘﻤﻊ أﻛﺜﺮ‬
‫ٍ‬
‫اﻻﺳﺘﻐﻼل واﻧﺘﻬﺎﻛﺎت اﻟﺨﺼﻮﺻﻴﺔ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﺎ ﺗُﺴﻤﱢ ﻴﻪ دراﺳﺔ أﺟﺮاﻫﺎ املﺠﻠﺲ‬
‫اﻷوروﺑﻲ »اﻟﺘﺄﺛري ا ُملﺮوﱢع ﻹﻋﺎدة اﺳﺘﺨﺪام اﻟﺒﻴﺎﻧﺎت« )‪.(Yeung 2018, 33‬‬
‫إذا ﻗﺎرﻧﱠﺎ اﻟﻮﺿﻊ ﻣﻊ ﺳﻴﺎﺳﺔ اﻟﺒﻴﺌﺔ‪ ،‬ﻳُﻤﻜﻦ أن ﻧﻜﻮن ﻣُﺘﺸﺎﺋﻤني ً‬
‫أﻳﻀﺎ ﺑﺸﺄن إﻣﻜﺎﻧﻴﺔ أن‬
‫ُ‬
‫ﻓﻠﻨﺄﺧﺬ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ‬
‫ﺗﺘﱠﺨﺬ اﻟﺒﻠﺪان إﺟﺮاءً ﻓﻌﱠ ًﺎﻻ وﺗﻌﺎوﻧﻴٍّﺎ ﺑﺸﺄن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﱡ‬
‫ﺑﺘﻐري املﻨﺎخ ﰲ اﻟﻮﻻﻳﺎت املﺘﺤﺪة‪ ،‬ﺣﻴﺚ ﻳﺘﻢ ﰲ ﺑﻌﺾ‬
‫املﺜﺎل‪ ،‬اﻟﻌﻤﻠﻴﺎت اﻟﺴﻴﺎﺳﻴﺔ ا ُملﺘﻌﻠﻘﺔ‬
‫ﱡ‬
‫وﺗﻐري املﻨﺎخ ﻧﻔﺴﻬﺎ‪ ،‬وﺣﻴﺚ ﺗﻌﻤﻞ ﺑﻌﺾ اﻟﻘﻮى‬
‫اﻷﺣﻴﺎن إﻧﻜﺎر ﻣﺸﻜﻠﺔ اﻻﺣﱰار اﻟﻌﺎملﻲ‬
‫اﻟﺴﻴﺎﺳﻴﺔ ذات اﻟﻨﻔﻮذ ﺿﺪ اﺗﺨﺎذ أي إﺟﺮاءٍ ﺣﻴﺎل ذﻟﻚ‪ ،‬أو اﻟﻨﺠﺎح املﺤﺪود ﻟﻠﻐﺎﻳﺔ ملﺆﺗﻤﺮات‬
‫ﱡ‬
‫ﻳﻮاﺟﻪ أوﻟﺌﻚ اﻟﺬﻳﻦ‬
‫ﺗﻐري املﻨﺎخ اﻟﺪوﻟﻴﺔ ﰲ اﻻﺗﻔﺎق ﻋﲆ ﺳﻴﺎﺳﺔ ﻣﻨﺎﺧﻴﺔ ﻣﺸﱰﻛﺔ وﻓﻌﱠ ﺎﻟﺔ‪ .‬وﻗﺪ ِ‬
‫ﻳﺴﻌﻮن إﱃ اﺗﺨﺎذ إﺟﺮاءٍ ﻋﺎ َﻟﻤﻲ ﰲ ﻇﻞ املﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ واملﺠﺘﻤﻌﻴﺔ اﻟﺘﻲ أﺛﺎرﻫﺎ اﻟﺬﻛﺎءُ‬
‫‪115‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻻﺻﻄﻨﺎﻋﻲ ﺻﻌﻮﺑﺎت ﻣُﻤﺎﺛﻠﺔ‪ .‬ﻓﻐﺎﻟﺒًﺎ ﻣﺎ ﺗﺘﻔﻮق املﺼﺎﻟﺢ اﻷﺧﺮى ﻋﲆ املﺼﻠﺤﺔ اﻟﻌﺎﻣﺔ‪،‬‬
‫وﻫﻨﺎك ﻧﺪرة ﰲ اﻟﺴﻴﺎﺳﺎت اﻟﺤﻜﻮﻣﻴﺔ اﻟﺪوﻟﻴﺔ اﻟﺨﺎﺻﺔ ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺮﻗﻤﻴﺔ اﻟﺠﺪﻳﺪة‪ ،‬ﺑﻤﺎ‬
‫ﻓﻴﻬﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻫﻨﺎك اﺳﺘﺜﻨﺎءٌ واﺣﺪ ﻟﺬﻟﻚ وﻫﻮ اﻻﻫﺘﻤﺎم اﻟﻌﺎملﻲ ﺑﺤﻈﺮ‬
‫اﻷﺳﻠﺤﺔ اﻟﻘﺎﺗﻠﺔ اﻟﺬاﺗﻴﺔ اﻟﺘﺸﻐﻴﻞ‪ ،‬اﻟﺘﻲ ﺗﺤﺘﻮي ً‬
‫أﻳﻀﺎ ﻋﲆ ﺟﺎﻧﺐ ذﻛﺎء اﺻﻄﻨﺎﻋﻲ‪ .‬وﻟﻜﻦ ﻫﺬا‬
‫ﻻ ﻳﺰال اﺳﺘﺜﻨﺎءً‪ ،‬وﻻ ﻳﺤﻈﻰ ً‬
‫أﻳﻀﺎ ﺑﺪﻋﻢ ﺟﻤﻴﻊ اﻟﺒﻠﺪان )ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﺎ زال ﻣﻮﺿﻊ‬
‫ﺟﺪل ﰲ اﻟﻮﻻﻳﺎت املﺘﺤﺪة(‪.‬‬
‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ورﻏﻢ ﺣﺴﻦ اﻟﻨﻴﺔ‪ ،‬ﻓﺈن ﻟﻜ ﱟﻞ ﻣﻦ أﺧﻼﻗﻴﺎت اﻟﺘﺼﻤﻴﻢ واﻻﺑﺘﻜﺎر املﺴﺌﻮل‬
‫ﻗﻴﻮدﻫﻤﺎ اﻟﺨﺎﺻﺔ‪ً .‬‬
‫أوﻻ‪ ،‬ﺗﻔﱰض أﺳﺎﻟﻴﺐ ﻣﺜﻞ اﻟﺘﺼﻤﻴﻢ اﻟﺤﺴﺎس ﻟﻠﻘِ ﻴَﻢ أﻧﻪ ﻳُﻤﻜﻨﻨﺎ اﻟﺘﻌﺒري‬
‫ﻋﻦ ﻗِ ﻴَﻤﻨﺎ‪ ،‬وﺗﻔﱰض ﺟﻬﻮد ﺑﻨﺎء اﻵﻻت اﻷﺧﻼﻗﻴﺔ أﻧﻨﺎ ﻳﻤﻜﻦ أن ﻧ ُ ﱢ‬
‫ﺑﺸﻜﻞ ﻛﺎﻣﻞ ﻋﻦ‬
‫ﻌﱪ‬
‫ٍ‬
‫ﺑﻮﺿﻮح‬
‫أﺧﻼﻗﻴﺎﺗﻨﺎ‪ .‬وﻟﻜﻦ ﻫﺬا ﻻ ﻳﺤﺪث ﺑﺎﻟﴬورة داﺋﻤً ﺎ؛ إذ إﻧﻨﺎ ﻗﺪ ﻻ ﻧﺴﺘﻄﻴﻊ اﻟﺘﻔﻜري‬
‫ٍ‬
‫وﻻ اﻟﺘﻌﺒري ﻋﻦ أﺧﻼﻗﻴﺎﺗﻨﺎ اﻟﻴﻮﻣﻴﺔ‪ .‬ﻓﻔﻲ ﺑﻌﺾ اﻷﺣﻴﺎن‪ ،‬ﻧﺴﺘﺠﻴﺐ إﱃ ﻣﺸﻜﻼت أﺧﻼﻗﻴﺔ‬
‫ٍ‬
‫ﺑﻄﺮﻳﻘﺔ ﻣُﻌﻴﻨﺔ دون أن ﻧﺘﻤﻜﻦ ﻣﻦ ﺗﱪﻳﺮ اﺳﺘﺠﺎﺑﺘﻨﺎ ﺑﺸﻜﻞ ﻛﺎﻣﻞ )‪.(Boddington 2017‬‬
‫ﺷﻜﻞ ﻣﻦ‬
‫وﻛﻤﺎ ﻗﺎل ﻓﻴﺘﺠﻨﺸﺘﺎﻳﻦ‪ :‬أﺧﻼﻗﻴﺎﺗﻨﺎ ﻟﻴﺴﺖ ﻓﻘﻂ ﻣﺘﺠﺴﺪة وﻟﻜﻨﻬﺎ ﻣُﻀﻤﱠ ﻨﺔ ﰲ‬
‫ٍ‬
‫ﻧﺤﻮ ﻋﻤﻴﻖ ﺑﻄﺮﻳﻘﺔ ﻗﻴﺎﻣﻨﺎ ﺑﺎﻷﻓﻌﺎل ﻛﻜﺎﺋﻨﺎت ﻣﺘﺠﺴﺪة‬
‫أﺷﻜﺎل اﻟﺤﻴﺎة‪ .‬إﻧﻬﺎ ﻣﺘﺼﻠﺔ ﻋﲆ ٍ‬
‫واﺟﺘﻤﺎﻋﻴﺔ‪ ،‬وﻛﻤﺠﺘﻤﻌﺎت وﺛﻘﺎﻓﺎت‪ .‬وﻫﺬا ﻳﻔﺮض ﺣﺪودًا ﻋﲆ ﻣﴩوع اﻟﺘﻌﺒري اﻟﻜﺎﻣﻞ ﻋﻦ‬
‫اﻷﺧﻼق واﻟﺘﻔﻜري اﻷﺧﻼﻗﻲ‪ .‬وﻳﻤﺜﻞ ً‬
‫أﻳﻀﺎ ﻣﺸﻜﻠﺔ ملﴩوع ﺗﻄﻮﻳﺮ اﻵﻻت اﻷﺧﻼﻗﻴﺔ‪ ،‬وﻳﺘﺤﺪى‬
‫اﻻﻓﱰاﺿﺎت اﻟﺘﻲ ﺗﻘﻮل إن اﻷﺧﻼق واﻟﺪﻳﻤﻘﺮاﻃﻴﺔ ﻳﻤﻜﻦ ﻣﻨﺎﻗﺸﺘﻬﻤﺎ واﻟﺘﻌﺒري ﻋﻨﻬﻤﺎ ﺑﺎﻟﻜﺎﻣﻞ‪.‬‬
‫ﻛﻤﺎ ﻳﺨﻠﻖ ﻣﺸﻜﻠﺔ ﻟﺼﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت اﻟﺬﻳﻦ ﻳﻌﺘﻘﺪون أن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻳﻤﻜﻦ اﻟﺘﻌﺎﻣﻞ ﻣﻌﻬﺎ ﺗﻤﺎﻣً ﺎ ﻣﻦ ﺧﻼل ﻗﺎﺋﻤﺔ ﻣﻦ املﺒﺎدئ أو ﻣﻦ ﺧﻼل أﺳﺎﻟﻴﺐ ﻗﺎﻧﻮﻧﻴﺔ وﺗﻘﻨﻴﺔ‬
‫ٍ‬
‫ﺑﺤﺎﺟﺔ إﱃ أﺳﺎﻟﻴﺐ وإﺟﺮاءات وﻋﻤﻠﻴﺎت‪ .‬وﻟﻜﻦ ﻛﻞ ﻫﺬا ﻟﻴﺲ ﻛﺎﻓﻴًﺎ؛‬
‫ﻣُﺤﺪﱠدة‪ .‬ﻧﺤﻦ ﺑﺎﻟﺘﺄﻛﻴﺪ‬
‫ﻓﺎﻷﺧﻼﻗﻴﺎت ﻻ ﺗﻌﻤﻞ ﻣﺜﻞ اﻵﻟﺔ‪ ،‬وﻛﺬﻟﻚ اﻟﺴﻴﺎﺳﺔ واﻻﺑﺘﻜﺎر املﺴﺌﻮل‪.‬‬
‫ﺛﺎﻧﻴًﺎ‪ ،‬ﻳُﻤﻜﻦ أن ﻳﻜﻮن ﻫﺬان اﻟﻨﻬﺠﺎن ً‬
‫ﻋﺎﺋﻘﺎ أﻣﺎم اﻷﺧﻼﻗﻴﺎت ﻋﻨﺪﻣﺎ ﻳﻜﻮن ﻣﻦ اﻟﻮاﺟﺐ‬
‫أﺧﻼﻗﻴٍّﺎ إﻳﻘﺎف ﺗﻄﻮﻳﺮ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬ﻓﻐﺎﻟﺒًﺎ ﻣﺎ ﺗﻜﻮن وﻇﻴﻔﺘﻬﻤﺎ ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻟﻌﻤﻠﻴﺔ ﻫﻲ‬
‫ﺗﻴﺴري ﻋﻤﻠﻴﺔ اﻻﺑﺘﻜﺎر‪ ،‬وﺗﻌﺰﻳﺰ ﺗﺤﻘﻴﻖ اﻷرﺑﺎح‪ ،‬وﺿﻤﺎن ﻗﺒﻮل اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬وﻗﺪ ﻻ ﻳﻜﻮن‬
‫ﻫﺬا ﺑﺎﻟﴬورة ﺳﻴﺌًﺎ‪ .‬وﻟﻜﻦ ﻣﺎذا ﻟﻮ ﻛﺎﻧﺖ املﺒﺎدئ اﻷﺧﻼﻗﻴﺔ ﺗُﺸري إﱃ أﻧﻪ ﻳﺠﺐ إﻳﻘﺎف أو‬
‫ﺗﻌﻠﻴﻖ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬أو ﺗﻄﺒﻴﻖ ﻣ ﱠ‬
‫ُﻌني ﻣﻦ ﺗﻄﺒﻴﻘﺎﺗﻬﺎ؟ اﻋﺘﱪ ﻛﺮوﻓﻮرد وﻛﺎﻟﻮ )‪ (٢٠١٦‬أن‬
‫ﱠ‬
‫اﻟﺤﺴﺎس ﻟﻠﻘِ ﻴَﻢ واﻻﺑﺘﻜﺎر املﺴﺌﻮل ﺗﻌﺘﻤﺪان ﻋﲆ اﻓﱰاض أن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫أداﺗَﻲ اﻟﺘﺼﻤﻴﻢ‬
‫ﺳﻴﺠﺮي ﺗﻄﻮﻳﺮﻫﺎ؛ وﺗَﻘ ﱡﻞ ﻓﻌﺎﻟﻴﱠﺘﻬﻤﺎ ﻋﻨﺪﻣﺎ ﻳﺘﻌﻠﻖ اﻷﻣﺮ ﺑﺎﺗﺨﺎذ ﻗﺮار ﺣﻮل ﻣﺎ إذا ﻛﺎن ﻳﺠﺐ‬
‫‪116‬‬

‫اﻟﺘﺤﺪﻳﺎت اﻟﺘﻲ ﺗُﻮاﺟﻪ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‬

‫إﻧﺸﺎء ﻫﺬه اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻣﻦ اﻷﺳﺎس‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ ﺣﺎﻟﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺘﻘﺪﱢم‬
‫ﻣﺜﻞ ﺗﻄﺒﻴﻘﺎت ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ اﻟﺠﺪﻳﺪة‪ ،‬رﺑﻤﺎ ﺗﻜﻮن ﻫﺬه اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻻ ﺗﺰال ﻏري ﺟﺪﻳﺮة ﺑﺎﻟﺜﻘﺔ‬
‫أو ﻟﻬﺎ ﻋﻴﻮب أﺧﻼﻗﻴﺔ ﺧﻄرية‪ ،‬وأن ﺑﻌﺾ ﺗﻄﺒﻴﻘﺎﺗﻬﺎ ﻋﲆ اﻷﻗﻞ ﻗﺪ ﻳﺘﻮﺟﺐ ﻋﺪم اﺳﺘﺨﺪاﻣﻬﺎ‬
‫)ﺑﻌﺪ(‪ .‬وﺳﻮاء أﻛﺎن وﻗﻒ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻫﻮ اﻟﺤﻞ اﻷﻓﻀﻞ داﺋﻤً ﺎ أم ﻻ‪ ،‬ﻓﺈن اﻟﻘﻀﻴﺔ ﻫﻲ أﻧﻨﺎ‬
‫ﻳﺠﺐ ﻋﲆ اﻷﻗﻞ أن ﻧﺘﻤﺘﱠﻊ ﺑﺎﻟﺤﻖ ﰲ ﻃﺮح اﻟﺴﺆال وﺗﻘﺮﻳﺮ ﻣﺎ ﻳﻨﺒﻐﻲ ﻓﻌﻠﻪ‪ .‬ﻓﺈذا ﻛﺎن ﻫﺬا‬
‫ِ‬
‫اﻟﺤﻖ ﻏﺎﺋﺒًﺎ‪ ،‬ﻓﺴﻮف ﻳﻈ ﱡﻞ اﻻﺑﺘﻜﺎر املﺴﺌﻮل ﺳﺘﺎ ًرا ﻧُﺨﻔﻲ وراءه ﻣﻮاﺻﻠﺔ اﻟﻌﻤﻞ ﻛﺎملﻌﺘﺎد‪.‬‬
‫ﻧﺤﻮ أﺧﻼﻗﻴﺎت إﻳﺠﺎﺑﻴﺔ‬
‫ﺑﺸﻜﻞ ﻋﺎ ﱟم ﻻ ﺗﺘﻌ ﱠﻠﻖ‬
‫ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ ﻛ ﱢﻞ ﻣﺎ ﻗﻴﻞ‪ ،‬ﻓﺈن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ٌ‬
‫ﻋﺎﺋﻖ آﺧﺮ ﻳَﺤُ ﻮل دون ﻣُﻤﺎرﺳﺔ‬
‫ﺑﺎﻟﴬورة ﺑﻤﻨﻊ اﻷﺷﻴﺎء )‪ .(Boddington 2017‬ﻫﻨﺎك‬
‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﻤﻠﻴٍّﺎ‪ ،‬وﻫﺬا اﻟﻌﺎﺋﻖ ﻫﻮ ﱠ‬
‫أن اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﺠﻬﺎت اﻟﻔﺎﻋﻠﺔ ﰲ‬
‫ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺜﻞ اﻟﴩﻛﺎت واﻟﺒﺎﺣﺜني اﻟﺘﻘﻨﻴﱢني ﻻ ﻳﺰاﻟﻮن ﻳﻌﺘﱪون اﻷﺧﻼﻗﻴﺎت‬
‫ﺑﺸﻜﻞ ﻛﺎﻣﻞ؛ إذ ﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﺠﺐ ﻋﲆ‬
‫ﻗﻴﻮدًا‪ ،‬أو أﺷﻴﺎءً ﺳﻠﺒﻴﺔ‪ .‬ﻫﺬه اﻟﻔﻜﺮة ﻟﻴﺴﺖ ﻣُﻀﻠﻠﺔ‬
‫ٍ‬
‫اﻷﺧﻼق أن ﺗُﻘﻴﱢﺪ‪ ،‬وﺗَﺤُ ﺪ‪ ،‬وﺗﻘﻮ َل إن ﺷﻴﺌًﺎ ﻣﺎ ﻏري ﻣﻘﺒﻮل‪ .‬وإذا أﺧﺬﻧﺎ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ ﻣَ ﺤﻤﻞ اﻟﺠﺪ ﱠ‬
‫ﻮاﺟﻪ ﺑﻌﺾ اﻟﺘﻨﺎ ُزﻻت‪ ،‬وﻻ ﺳﻴﱠﻤﺎ ﻋﲆ‬
‫وﻧﻔﺬﻧﺎ ﺗﻮﺻﻴﺎﺗﻬﺎ‪ ،‬ﻓﻘﺪ ﻧ ُ ِ‬
‫املﺪى اﻟﻘﺼري‪ .‬ﻓﻘﺪ ﻳﻜﻮن ﻟﻸﺧﻼﻗﻴﺎت ﺛﻤَ ﻦ ﻻ ﺑﺪ ﻣﻦ دﻓﻌﻪ؛ ﺳﻮاءٌ ﻋﲆ ﻣﺴﺘﻮى املﺎل أو‬
‫اﻟﻮﻗﺖ أو اﻟﻄﺎﻗﺔ‪ .‬وﻣﻊ ذﻟﻚ‪ِ ،‬‬
‫ﻓﻤﻦ ﺧﻼل ﺗﻘﻠﻴﻞ املﺨﺎﻃﺮ‪ ،‬ﺗﺪﻋﻢ اﻷﺧﻼﻗﻴﺎت واﻻﺑﺘﻜﺎر املﺴﺌﻮل‬
‫َ‬
‫اﻟﺘﻨﻤﻴﺔ ا ُملﺴﺘﺪاﻣﺔ ﻟﻸﻋﻤﺎل اﻟﺘﺠﺎرﻳﺔ واملﺠﺘﻤﻊ ﻋﲆ املﺪى اﻟﺒﻌﻴﺪ‪ .‬وﻻ ﻳﺰال ﻫﻨﺎك ﺗَﺤ ﱟﺪ ﰲ‬
‫إﻗﻨﺎع ﺟﻤﻴﻊ اﻟﺠﻬﺎت اﻟﻔﺎﻋﻠﺔ ﰲ ﻣﺠﺎل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﻤَ ﻦ ﻓﻴﻬﻢ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‪،‬‬
‫ﻓﻌﻼ‪ .‬ﻻﺣﻆ ً‬
‫ﺑﺄن ﻫﺬا ﻫﻮ اﻟﺤﺎل ً‬
‫أﻳﻀﺎ أن اﻟﺴﻴﺎﺳﺔ واﻟﻠﻮاﺋﺢ اﻟﺘﻨﻈﻴﻤﻴﱠﺔ ﻻ ﺗﺘﻌ ﱠﻠﻖ ﻓﻘﻂ ﺑﺤﻈﺮ‬
‫ً‬
‫ﺻﻌﻮﺑﺔ وﺗﻌﻘﻴﺪًا؛ ﺑﻞ ﻳُﻤﻜﻦ أن ﺗﻜﻮن داﻋﻤﺔ‪ ،‬ﻣﻦ ﺧﻼل ﺗﻘﺪﻳﻢ‬
‫اﻷﺷﻴﺎء أو ﺑﺠﻌﻠِﻬﺎ أﻛﺜﺮ‬
‫ﺣﻮاﻓﺰ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪.‬‬
‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬إﱃ ﺟﺎﻧﺐ اﻷﺧﻼﻗﻴﺎت اﻟﺴﻠﺒﻴﺔ اﻟﺘﻲ ﺗﻔﺮض ﻗﻴﻮدًا‪ ،‬ﻧﺤﻦ ﰲ ﺣﺎﺟﺔ‬
‫إﱃ ﺗﻮﺿﻴﺢ اﻷﺧﻼﻗﻴﺎت اﻹﻳﺠﺎﺑﻴﺔ وﴍﺣﻬﺎ‪ :‬ﻟﻮﺿﻊ رؤﻳﺔ ﻟﻠﺤﻴﺎة اﻟﺠﻴﺪة واملﺠﺘﻤﻊ اﻟﺠﻴﺪ‪.‬‬
‫وﺑﻴﻨﻤﺎ ﺗﻠﻤﺢ ﺑﻌﺾ املﺒﺎدئ اﻷﺧﻼﻗﻴﺔ املﻘﱰﺣﺔ أﻋﻼه إﱃ ِﻣﺜﻞ ﻫﺬه اﻟﺮؤﻳﺔ‪ ،‬ﻓﻼ ﻳﺰال‬
‫ﺗﻮﺟﻴﻪ املﻨﺎﻗﺸﺔ إﱃ ﻫﺬا اﻻﺗﺠﺎه ﺗﺤﺪﻳٍّﺎ‪ .‬ﻛﻤﺎ ﺳﺒﻖ وذﻛﺮﻧﺎ‪ ،‬ﻻ ﺗﺘﻌ ﱠﻠﻖ املﺴﺎﺋﻞ اﻷﺧﻼﻗﻴﺔ‬
‫اﻟﺨﺎﺻﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻓﺤﺴﺐ؛ ﺑﻞ ﺗﺘﻌ ﱠﻠﻖ ﺑﺤﻴﺎة اﻹﻧﺴﺎن وازدﻫﺎره‪،‬‬
‫وﺗﺘﻌ ﱠﻠﻖ ﺑﻤُﺴﺘﻘﺒﻞ املﺠﺘﻤﻊ‪ ،‬ورﺑﻤﺎ ﺗﺘﻌﻠﻖ ً‬
‫أﻳﻀﺎ ﺑﻐري اﻟﺒﴩ‪ ،‬وﺑﺎﻟﺒﻴﺌﺔ‪ ،‬وﺑﻤُﺴﺘﻘﺒﻞ اﻟﻜﻮﻛﺐ‬
‫‪117‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫)اﻧﻈﺮ اﻟﻔﺼﻞ اﻟﺘﺎﱄ(‪ .‬وﻫﻜﺬا ﺗُﻌﻴﺪﻧﺎ املﻨﺎﻗﺸﺎت ﺣﻮل أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫وﺳﻴﺎﺳﺎﺗﻪ ﻣﻦ ﺟﺪﻳ ٍﺪ إﱃ اﻷﺳﺌﻠﺔ اﻟﻜﺒرية اﻟﺘﻲ ﻳﺠﺐ أن ﻧﻄﺮﺣﻬﺎ ﻋﲆ أﻧﻔﺴﻨﺎ؛ أﻓﺮادًا‪،‬‬
‫ٍ‬
‫ُﺠﺘﻤﻌﺎت‪ ،‬ورﺑﻤﺎ ﺑﴩًا‪ .‬وﻳﻤﻜﻦ ﻟﻠﻔﻼﺳﻔﺔ أن ﻳُﺴﺎﻋﺪوﻧﺎ ﰲ اﻟﺘﻔﻜري ﰲ ﻫﺬه اﻷﺳﺌﻠﺔ‪.‬‬
‫وﻣ‬
‫وﺑﺎﻟﻨﺴﺒﺔ إﱃ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‪ ،‬ﻳﻜﻤُﻦ اﻟﺘﺤﺪﱢي ﰲ ﺗﻄﻮﻳﺮ رؤﻳﺔ واﺳﻌﺔ ﻟﻠﻤُﺴﺘﻘﺒﻞ‬
‫اﻟﺘﻜﻨﻮﻟﻮﺟﻲ ﺗﺘﻀﻤﱠ ﻦ أﻓﻜﺎ ًرا ﺣﻮل ﻣﺎ ﻫﻮ ﻣُﻬﻢ وﻣﺎ ﻫﻮ ذو ﻣﻌﻨًﻰ وﻣﺎ ﻫﻮ ذو ﻗﻴﻤﺔ‪ .‬ﻋﲆ‬
‫ﺑﺸﻜﻞ ﻋﺎ ﱟم ﺗﺘﻌﻤﱠ ﺪ ﺗﺠﺎﻫﻞ ﻣﺜﻞ ﻫﺬه اﻷﺳﺌﻠﺔ وﺗﺮﻛﻬﺎ‬
‫اﻟﺮﻏﻢ ﻣﻦ أن اﻟﺪﻳﻤﻘﺮاﻃﻴﺎت اﻟﻠﻴﱪاﻟﻴﺔ‬
‫ٍ‬
‫ﱠ‬
‫ﺗﺘﺪﺧﻞ ﰲ ﻣﺜﻞ ﻫﺬه املﻮﺿﻮﻋﺎت اﻟﻌﻤﻴﻘﺔ ﻣﺜﻞ ﻣﺎﻫﻴﺔ اﻟﺤﻴﺎة اﻟﺠﻴﺪة وﻣِﻦ ﺛَﻢ‬
‫ﻟﻸﻓﺮاد‪ ،‬وﻻ‬
‫ﻓﻬﻲ »ﺳﻄﺤﻴﺔ« )اﺑﺘﻜﺎر ﺳﻴﺎﳼ أدﱠى إﱃ ﺗﺠﻨﱡﺐ ﺑﻌﺾ أﻧﻮاع اﻟﺤﺮوب ﻋﲆ اﻷﻗﻞ وﺳﺎﻫﻢ‬
‫ﺗﻮاﺟﻬﻨﺎ‪ ،‬ﻓﺈن‬
‫ﰲ اﻻﺳﺘﻘﺮار واﻻزدﻫﺎر(‪ ،‬ﻓﺈﻧﻪ ﰲ ﻇ ﱢﻞ اﻟﺘﺤﺪﱢﻳﺎت اﻷﺧﻼﻗﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ اﻟﺘﻲ‬
‫ِ‬
‫ً‬
‫»ﻋﻤﻘﺎ« ﻳُﻌﺘﱪ ﻣﻦ ﻗﺒﻴﻞ اﻧﻌﺪام املﺴﺌﻮﻟﻴﺔ‪ .‬وﻳﻨﺒﻐﻲ أن ﺗﺘﻌ ﱠﻠﻖ‬
‫ﺗﺠﺎﻫﻞ اﻷﺳﺌﻠﺔ اﻷﺧﻼﻗﻴﺔ اﻷﻛﺜﺮ‬
‫اﻟﺴﻴﺎﺳﺔ ً‬
‫أﻳﻀﺎ‪ ،‬ﺑﻤﺎ ﻓﻴﻬﺎ ﺳﻴﺎﺳﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﺑﺎﻷﺧﻼﻗﻴﺎت اﻹﻳﺠﺎﺑﻴﺔ‪.‬‬
‫ﺑﺸﻜﻞ ﻋﺎم‪ ،‬ﻻ ﺗﺘﻌ ﱠﻠﻖ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻟﴬورة ﺑﻤﻨﻊ اﻷﺷﻴﺎء؛ ﺑﻞ ﻧﺤﻦ ﰲ ﺣﺎﺟﺔ إﱃ‬
‫ٍ‬
‫أﺧﻼﻗﻴﺎت إﻳﺠﺎﺑﻴﺔ‪ :‬ﻟﻮﺿﻊ رؤﻳﺔ ﻟﻠﺤﻴﺎة اﻟﺠﻴﺪة وا ُملﺠﺘﻤﻊ اﻟﺠﻴﺪ‪.‬‬

‫وﻣﻊ ذﻟﻚ‪ ،‬ﻓﺎﻟﺴﺒﻴﻞ إﱃ ذﻟﻚ ﻣﻦ ﻣﻨﻈﻮر ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‪ ،‬ﻟﻴﺲ ﻣﻦ ﺧﻼل اﻟﻌﻤﻞ‬
‫ﱢ‬
‫وﺗﻮﱄ دور املﻠﻚ اﻟﻔﻴﻠﺴﻮف ﻛﻤﺎ ﰲ ﻓﻠﺴﻔﺔ أﻓﻼﻃﻮن‪ ،‬وﻟﻜﻦ ﺑﺎﻟﻌﺜﻮر ﻋﲆ‬
‫ﺑﺸﻜﻞ ﻓﺮدي‬
‫ٍ‬
‫اﻟﺘﻮازن اﻟﺼﺤﻴﺢ ﺑني اﻟﺘﻜﻨﻮﻗﺮاﻃﻴﺔ واﻟﺪﻳﻤﻘﺮاﻃﻴﺔ اﻟﺘﺸﺎرﻛﻴﺔ‪ .‬اﻷﺳﺌﻠﺔ اﻟﺘﻲ ﺗُﻮاﺟﻬﻨﺎ ﻫﻲ‬
‫أﺳﺌﻠﺔ ﺗُﻬﻤﻨﺎ ﺟﻤﻴﻌً ﺎ؛ وﻋﻠﻴﻨﺎ أن ﻧﺘﺸﺎرك ﺟﻤﻴﻌً ﺎ ﰲ اﻹﺟﺎﺑﺔ ﻋﻨﻬﺎ‪ .‬ﻟﺬﻟﻚ‪ ،‬ﻻ ﻳُﻤﻜﻨﻨﺎ ﺗﺮﻛﻬﺎ ﰲ‬
‫أﻳﺪي ٍ‬
‫ﻓﺌﺔ ﻗﻠﻴﻠﺔ ﻣﻦ اﻷﺷﺨﺎص‪ ،‬ﺳﻮاء أﻛﺎﻧﻮا ﰲ اﻟﺤﻜﻮﻣﺔ أم ﰲ اﻟﴩﻛﺎت اﻟﻜﺒرية‪ .‬وﻳُﻌﻴﺪﻧﺎ ﻫﺬا‬
‫إﱃ اﻷﺳﺌﻠﺔ ﺣﻮل ﻛﻴﻔﻴﺔ إﻧﺠﺎح اﻻﺑﺘﻜﺎر املﺴﺌﻮل واملﺸﺎرﻛﺔ ﰲ ﺳﻴﺎﺳﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫املﺸﻜﻠﺔ ﻻ ﺗﺘﻌ ﱠﻠﻖ ﻓﻘﻂ ﺑﺎﻟﺴﻠﻄﺔ؛ إﻧﻬﺎ ﺗﺘﻌ ﱠﻠﻖ ً‬
‫أﻳﻀﺎ ﺑﺎﻟﺨري‪ :‬اﻟﺨري ﻟﻸﻓﺮاد واﻟﺨري ﻟﻠﻤﺠﺘﻤﻊ‪.‬‬
‫إن أﻓﻜﺎرﻧﺎ اﻟﺤﺎﻟﻴﺔ ﺣﻮل اﻟﺤﻴﺎة اﻟﺠﻴﺪة واملﺠﺘﻤﻊ اﻟﺠﻴﺪ — إذا ﻛﻨﺎ ﻗﺎدِ رﻳﻦ ﻋﲆ اﻟﺘﻌﺒري‬
‫ﻧﻘﺎش ﻧﻘﺪي أﻋﻤﻖ ﺑﻜﺜري‪ .‬ودﻋﻮﻧﻲ أﻗﱰح أﻧﻪ ﻗﺪ ﻳﻜﻮن‬
‫ﻋﻨﻬﺎ ﻣﻦ اﻷﺳﺎس — ﻗﺪ ﺗﺤﺘﺎج إﱃ ٍ‬
‫ٍ‬
‫أﻧﻈﻤﺔ ﺳﻴﺎﺳﻴﺔ أﺧﺮى‬
‫ﻣﻦ ا ُملﻔﻴﺪ ﻟﻠﻐﺮب‪ ،‬ﻋﲆ اﻷﻗﻞ أن ﻳﺴﺘﻜﺸﻔﻮا ﺧﻴﺎر ﻣُﺤﺎوﻟﺔ اﻟﺘﻌ ﱡﻠﻢ ﻣﻦ‬
‫ﻏري ﻏﺮﺑﻴﺔ وﺛﻘﺎﻓﺎت ﺳﻴﺎﺳﻴﺔ أﺧﺮى‪ .‬ﻻ ﻳﺠﻮز ﻟﺴﻴﺎﺳﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻔﻌﱠ ﺎﻟﺔ وا ُملﱪرة‬
‫ﺗﺠﻨﱡﺐ املﺸﺎرﻛﺔ ﰲ ﻣﺜﻞ ﻫﺬه اﻟﻨﻘﺎﺷﺎت اﻷﺧﻼﻗﻴﺔ اﻟﻔﻠﺴﻔﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ اﻟﻔﻠﺴﻔﻴﺔ‪.‬‬
‫‪118‬‬

‫اﻟﺘﺤﺪﻳﺎت اﻟﺘﻲ ﺗُﻮاﺟﻪ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‬

‫ُ‬
‫ﱡ‬
‫ﱡ‬
‫اﻟﺘﺨﺼﺼﺎت‬
‫اﻟﺘﺨﺼﺼﺎت وﺗﺠﺎوز‬
‫ﺗﺪاﺧﻞ‬
‫ﻫﻨﺎك ﻋﻮاﺋﻖ أﺧﺮى ﻳﺠﺐ ﺗﺠﺎوزﻫﺎ إذا أردْﻧﺎ ﺟﻌﻞ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أﻛﺜﺮ‬
‫ً‬
‫ﻓﻌﺎﻟﻴﺔ وأردﻧﺎ دﻋﻢ اﻟﺘﻄﻮﻳﺮ املﺴﺌﻮل ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ ،‬ﺗﺠﻨﱡﺒًﺎ ملﺎ ﻳُﺴﻤﻴﻪ اﻟﺒﺎﺣﺜﻮن اﻟﺘﻘﻨﻴﻮن‬
‫»ﺷﺘﺎءَ« اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺠﺪﻳﺪ‪ :‬إﺑﻄﺎء ﻋﻤﻠﻴﺔ ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻻﺳﺘﺜﻤﺎر‬
‫ُ‬
‫ﱡ‬
‫ﱡ‬
‫اﻟﺘﺨﺼﺼﺎت اﻟﻜﺎﰲ‪ .‬ﻣﺎ‬
‫اﻟﺘﺨﺼﺼﺎت وﺗﺠﺎوز‬
‫ﺗﺪاﺧﻞ‬
‫ﻓﻴﻪ‪ .‬أﺣﺪ ﻫﺬه اﻟﻌﻮاﺋﻖ ﻫﻮ ﻧﻘﺺ‬
‫ﻧﻮاﺟﻪ ﻓﺠﻮة ﺷﺎﺳﻌﺔ ﰲ اﻟﺨﻠﻔﻴﺔ واﻟﻔﻬﻢ ﺑني ا ُمل ﱢ‬
‫ﺨﺘﺼني ﰲ اﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ واﻟﻌﻠﻮم‬
‫زﻟﻨﺎ‬
‫ِ‬
‫ٍ‬
‫اﻻﺟﺘﻤﺎﻋﻴﺔ ﻣﻦ ﺟﻬﺔ‪ ،‬وا ُمل ﱢ‬
‫ﺟﻬﺔ أﺧﺮى‪ ،‬داﺧﻞ‬
‫ﺨﺘﺼني ﰲ اﻟﻌﻠﻮم اﻟﻄﺒﻴﻌﻴﺔ واﻟﻬﻨﺪﺳﻴﺔ ﻣﻦ‬
‫ﱠ‬
‫املﺆﺳﴘ ﻟﺴ ﱢﺪ اﻟﻔﺠﻮة اﻟﻮاﺳﻌﺔ‬
‫ا ُملﺠﺘﻤﻊ اﻷﻛﺎدﻳﻤﻲ وﺧﺎرﺟﻪ‪ .‬ﺣﺘﻰ اﻵن‪ ،‬ﻣﺎ زﻟﻨﺎ ﻧﻔﺘﻘﺪ اﻟﺪﻋﻢ‬
‫ﺑني ﻫﺬَﻳﻦ »اﻟﻌﺎ َملني«‪ ،‬ﺳﻮاء ﰲ املﺠﺘﻤﻊ اﻷﻛﺎدﻳﻤﻲ أو ﰲ املﺠﺘﻤﻊ اﻷوﺳﻊ‪ .‬وﻟﻜﻦ إذا ﻛﻨﱠﺎ ﻧُﺮﻳﺪ‬
‫ٍّ‬
‫ﺣﻘﺎ أن ﻧﻤﺘﻠﻚ ﺗﻜﻨﻮﻟﻮﺟﻴﺎ ﻣﺘﻘﺪﻣﺔ أﺧﻼﻗﻴﺔ ﻣﺜﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻷﺧﻼﻗﻲ‪ ،‬ﻓﻴﺠﺐ ﻋﻠﻴﻨﺎ‬
‫أن ﻧُﻘ ﱢﺮب ﺑني ﻫﺆﻻء اﻷﺷﺨﺎص وﺑني ﻫﺬَﻳﻦ اﻟﻌﺎ َملني‪ ،‬ﰲ أﻗﺮب ٍ‬
‫وﻗﺖ ﻣﻤﻜﻦ‪.‬‬
‫ً‬
‫ﻓﻤﺜﻼ‪ ،‬ﻳﺠﺐ أن‬
‫وﻳﺘﻄ ﱠﻠﺐ ﻫﺬا إﺣﺪاث ﺗﻐﻴري ﰲ ﻛﻴﻔﻴﺔ إﺟﺮاء اﻟﺒﺤﺚ واﻟﺘﻄﻮﻳﺮ —‬
‫ﻳُﺸﺎرك ﻓﻴﻪ ﻟﻴﺲ ﻓﻘﻂ اﻷﺷﺨﺎص اﻟﺘﻘﻨﻴﻮن ورﺟﺎل اﻷﻋﻤﺎل وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﻣ ﱡ‬
‫ُﺨﺘﺼﻮن ﰲ‬
‫اﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ — وﻛﺬﻟﻚ ﺗﻐﻴري ﻛﻴﻔﻴﺔ »ﺗﻌﻠﻴﻢ« اﻷﺷﺨﺎص‪ ،‬ﻣﻦ اﻟﺸﺒﺎب وﻏريﻫﻢ‪ .‬ﻳﺠﺐ‬
‫أن ﻧﺤﺮص ﻋﲆ أن ﻳُﺪرك اﻷﺷﺨﺎص اﻟﺬﻳﻦ ﻟﺪَﻳﻬﻢ ﺧﻠﻔﻴﺔ ﰲ اﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ أﻫﻤﻴﺔ اﻟﺘﻔﻜري‬
‫ﰲ اﻟﺘﻘﻨﻴﺎت اﻟﺠﺪﻳﺪة ﻣﺜﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻳُﺤﺎوﻟﻮا اﻛﺘﺴﺎب ﺑﻌﺾ املﻌﺮﻓﺔ ﺣﻮل ﻫﺬه‬
‫ً‬
‫ﺣﺴﺎﺳﻴﺔ‬
‫اﻟﺘﻘﻨﻴﺎت وﻣﺎ ﺗﻘﻮم ﺑﻪ‪ .‬وﻣﻦ ﻧﺎﺣﻴ ٍﺔ أﺧﺮى‪ ،‬ﻳﺠﺐ ﺟﻌﻞ اﻟﻌﻠﻤﺎء واملﻬﻨﺪﺳني أﻛﺜﺮ‬
‫َ‬
‫ِ‬
‫واﺳﺘﺨﺪاﻣﻬﺎ‪ .‬وﻣﻦ ﺛﻢ ﻋﻨﺪﻣﺎ‬
‫ﺗﺠﺎه اﻟﺠﻮاﻧﺐ اﻷﺧﻼﻗﻴﺔ وا ُملﺠﺘﻤﻌﻴﺔ ﻟﺘﻄﻮﻳﺮ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫ﻳﺘﻌ ﱠﻠﻤﻮن اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻳُﺴﺎﻫﻤﻮن ﺑﻌﺪ ذﻟﻚ ﰲ ﺗﻄﻮﻳﺮ ﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﺬﻛﺎء‬
‫ﱡ‬
‫ﻳﻤﺖ ِ‬
‫ﺑﺼ ٍﻠﺔ إﱃ‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻟﺠﺪﻳﺪة‪ ،‬ﻓﺈﻧﻬﻢ ﻟﻦ ﻳ َﺮوا اﻷﺧﻼﻗﻴﺎت ﻣﻮﺿﻮﻋً ﺎ ﻫﺎﻣﺸﻴٍّﺎ ﻻ‬
‫ﻣُﻤﺎرﺳﺎﺗﻬﻢ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ وﻟﻜﻦ ﻳ َﺮوﻧﻬﺎ »ﺟﺰءًا أﺳﺎﺳﻴٍّﺎ« ﻣﻦ ﻫﺬه املﻤﺎرﺳﺎت‪ .‬وﻋﻨﺪﺋﺬٍ‪ ،‬ﰲ‬
‫اﻟﺤﺎﻟﺔ املﺜﺎﻟﻴﺔ‪ ،‬ﺳﺘﻌﻨﻲ »ﻣﻤﺎرﺳﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ« أو »ﻣﻤﺎرﺳﺔ ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت« أن ﻳﺘ ﱠﻢ‬
‫ﻧﻄﺎق أوﺳﻊ‪ ،‬ﻳُﻤﻜﻨﻨﺎ‬
‫ﺗﻀﻤني اﻷﺧﻼﻗﻴﺎت ﺑﺒﺴﺎﻃﺔ ﺑﻮﺻﻔﻬﺎ ﺟﺰءًا أﺳﺎﺳﻴٍّﺎ ﻻ ﻏِ ﻨﻰ ﻋﻨﻪ‪ .‬ﻋﲆ‬
‫ٍ‬
‫ﱡ‬
‫اﻟﺘﺨﺼﺼﺎت‬
‫ﺷﻜﻞ أﻛﺜﺮ ﺗﻨﻮﱡﻋً ﺎ وﺷﻤﻮﻟﻴﺔ ﻣﻦ اﻟﺘﻌﻠﻴﻢ أو اﻟﴪد ﺗﺘﺪاﺧﻞ ﻓﻴﻪ‬
‫أن ﻧﻔﻜﺮ ﰲ‬
‫ٍ‬
‫ً‬
‫وأﻳﻀﺎ ﺑﺎﻟﻮﺳﺎﺋﻂ واﻟﺘﻘﻨﻴﺎت‪ .‬ﺑﻌﺒﺎر ٍة‬
‫ﺟﺬرﻳٍّﺎ ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﺎﻷﺳﺎﻟﻴﺐ واملﻨﺎﻫﺞ‪ ،‬وﺑﺎملﻮﺿﻮﻋﺎت‪،‬‬
‫أﺧﺮى أوﺿﺢ‪ ،‬إذا ﺗﻌ ﱠﻠ َﻢ املﻬﻨﺪﺳﻮن ﻛﻴﻔﻴﺔ اﻟﻌﻤﻞ ﺑﺎﺳﺘﺨﺪام اﻟﻨﺼﻮص وﺗﻌﻠﻢ ا ُملﺨﺘﺼﻮن ﰲ‬
‫اﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ ﻛﻴﻔﻴﺔ اﻟﻌﻤﻞ ﺑﺎﺳﺘﺨﺪام أﺟﻬﺰة اﻟﻜﻤﺒﻴﻮﺗﺮ‪ ،‬ﻓﺴﻴﺰداد اﻷﻣﻞ ﰲ أﺧﻼﻗﻴﺎت‬
‫اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﰲ ﺳﻴﺎﺳﺔ ﺗﺼﻠﺢ ﻟﻠﺘﻨﻔﻴﺬ ﻋﻤﻠﻴٍّﺎ‪.‬‬
‫‪119‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻣﺨﺎﻃﺮ »ﺷﺘﺎء« اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺧﻄﺮ اﻻﺳﺘﺨﺪام اﻟﻼواﻋﻲ‬
‫ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫وﺑﺸﻜﻞ ﻋﺎم‪،‬‬
‫إذا ﻟﻢ ﻳﺒﺪأ ﺗﻨﻔﻴﺬ ﻫﺬه اﻟﺘﻮﺟﻴﻬﺎت ﰲ اﻟﺴﻴﺎﺳﺔ واﻟﺘﻌﻠﻴﻢ ﻋﲆ أرض اﻟﻮاﻗﻊ‪،‬‬
‫ٍ‬
‫ﻮاﺟﻪ ﻓﻘﻂ ﻣﺨﺎﻃﺮ »ﺷﺘﺎء«‬
‫إذا ﻓﺸﻞ ﻣﴩوع اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻷﺧﻼﻗﻲ‪ ،‬ﻓﺈﻧﻨﺎ ﻟﻦ ﻧ ُ ِ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؛ ﺑﻞ إن اﻟﺨﻄﺮ اﻷدﻫﻰ واﻷﻣَ ﱠﺮ ﺳﻴﻜﻤﻦ ﰲ اﻟﻜﺎرﺛﺔ اﻷﺧﻼﻗﻴﺔ واﻻﺟﺘﻤﺎﻋﻴﺔ‬
‫واﻻﻗﺘﺼﺎدﻳﺔ اﻟﺘﻲ ﺳﺘُﻠ ﱡﻢ ﺑﻨﺎ وﺳﻴﺪﻓﻊ ﺛﻤﻨﻬﺎ اﻟﺒﴩ وﻏري اﻟﺒﴩ واﻟﺒﻴﺌﺔ‪ .‬ﻫﺬا ﻻ ﻳﺘﻌﻠﻖ ﺑﺎﻟﺘﻔﺮد‬
‫اﻟﺘﻜﻨﻮﻟﻮﺟﻲ‪ ،‬أو ﺑﺎﻵﻻت اﻟﺘﻲ ﺳﺘﺪﻣﺮ اﻟﻌﺎﻟﻢ‪ ،‬أو ﺑﺴﻴﻨﺎرﻳﻮﻫﺎت ﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ اﻷﺧﺮى ﺣﻮل‬
‫املﺴﺘﻘﺒﻞ اﻟﺒﻌﻴﺪ‪ ،‬وﻟﻜﻨﻪ ﻳﺘﻌﻠﻖ ﺑﺎﻟﺰﻳﺎدة اﻟﺒﻄﻴﺌﺔ وﻟﻜﻦ املﺆﻛﺪة ﰲ ﺗﺮاﻛﻢ املﺨﺎﻃﺮ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‬
‫وﻣﺎ ﻳﻨﺠﻢ ﻋﻨﻬﺎ ﻣﻦ ﺗﻔﺎﻗﻢ اﻟﻀﻌﻒ اﻟﺒﴩي واﻻﺟﺘﻤﺎﻋﻲ واﻻﻗﺘﺼﺎدي واﻟﺒﻴﺌﻲ‪ .‬ﻫﺬه اﻟﺰﻳﺎدة‬
‫ﰲ املﺨﺎﻃﺮ واﻟﻀﻌﻒ ﻣﺮﺗﺒﻄﺔ ﺑﺎملﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ املﺸﺎر إﻟﻴﻬﺎ ﻫﻨﺎ وﰲ اﻟﻔﺼﻮل اﻟﺴﺎﺑﻘﺔ‪،‬‬
‫ﺑﻤﺎ ﻓﻴﻬﺎ اﻻﺳﺘﺨﺪام اﻟﺠﺎﻫﻞ واملﺘﻬﻮر ﻟﺘﻘﻨﻴﺎت اﻷﺗﻤﺘﺔ ا ُملﺘﻘﺪﱢﻣﺔ ﻣﺜﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﺑﺸﻜﻞ ﻋﺎم‪ :‬ﺣﺘﻰ‬
‫إن اﻟﻔﺠﻮة ﰲ اﻟﺘﻌﻠﻴﻢ رﺑﻤﺎ ﺗﺰﻳﺪ ﻣﻦ ﺗﺄﺛري ﻣﺨﺎﻃﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ٍ‬
‫ﻟﻮ ﻟﻢ ﺗﺘﺴﺒﱠﺐ داﺋﻤً ﺎ ﰲ ﻣﺨﺎﻃﺮ ﺟﺪﻳﺪة ﻣﺒﺎﴍة‪ ،‬ﻓﺈﻧﻬﺎ ﺗُﻀﺎﻋﻒ املﺨﺎﻃﺮ املﻮﺟﻮدة ﺑﺎﻟﻔﻌﻞ‬
‫ﻧﺤﻮ اﺳﺘﺜﻨﺎﺋﻲ‪ .‬ﺣﺘﻰ اﻵن‪ ،‬ﻻ ﻳُﻮﺟَ ﺪ ﻣﺎ ﻳُﺴﻤﻰ »رﺧﺼﺔ ﻗﻴﺎدة« ﻻﺳﺘﺨﺪام اﻟﺬﻛﺎء‬
‫ﻋﲆ ٍ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وﻻ ﻳُﻮﺟَ ﺪ ﺗﻌﻠﻴﻢ إﻟﺰاﻣﻲ ﻷﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻠﺒﺎﺣﺜني اﻟﺘﻘﻨﻴﱢني‪،‬‬
‫ورﺟﺎل اﻷﻋﻤﺎل‪ ،‬وﻣﺴﺌﻮﱄ اﻟﺤﻜﻮﻣﺔ وﻏريﻫﻢ ﻣﻦ اﻷﺷﺨﺎص املﺸﺎرﻛني ﰲ اﺑﺘﻜﺎر اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ واﺳﺘﺨﺪاﻣﻪ وﺳﻴﺎﺳﺎﺗﻪ‪ .‬ﻫﻨﺎك اﻟﻜﺜري ﻣﻦ آﻻت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻏري ا ُملﺮوﱠﺿﺔ‬
‫أﺷﺨﺎص ﻻ ﻳﻌﺮﻓﻮن املﺨﺎﻃﺮ واملﺸﻜﻼت اﻷﺧﻼﻗﻴﺔ ا ُملﺮﺗﺒﻄﺔ ﺑﻬﺎ‪ ،‬أو اﻟﺬﻳﻦ ﻗﺪ ﺗﻜﻮن‬
‫ﰲ أﻳﺪي‬
‫ٍ‬
‫ﻟﺪﻳﻬﻢ ﺗﻮﻗﻌﺎت ﺧﻄﺄ ﺑﺸﺄن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬وﻳﻜﻤﻦ اﻟﺨﻄﺮ‪ ،‬ﻣﺮة أﺧﺮى‪ ،‬ﰲ ﻣﻤﺎرﺳﺔ اﻟﺴﻠﻄﺔ‬
‫دون ﻣﻌﺮﻓﺔ و)ﺑﺎﻟﺘﺎﱄ( دون ﻣﺴﺌﻮﻟﻴﺔ؛ واﻷﺳﻮأ ﻣﻦ ذﻟﻚ أن ﻳﺨﻀﻊ اﻵﺧﺮون إﱃ ﻫﺬه‬
‫اﻟﺴﻠﻄﺔ‪ .‬وإذا ﻛﺎن ﻫﻨﺎك ﴍﱞ ﻋﲆ اﻹﻃﻼق‪ ،‬ﻓﺈﻧﻪ ﻳُﻘﻴﻢ ﺣﻴﺜﻤﺎ ﻗﺎﻟﺖ ﻓﻴﻠﺴﻮﻓﺔ اﻟﻘﺮن اﻟﻌﴩﻳﻦ‬
‫ﺣﻨﺔ آرﻧﺖ‪ :‬ﰲ ﻏﻴﺎب اﻟﻮﻋﻲ ﻋﻦ اﻟﻘﺮارات واﻟﻌﻤﻞ اﻟﻴﻮﻣﻲ ا ُملﻤﻞ‪ .‬وﻋﻨﺪﻣﺎ ﻳُﻔﱰَض أن اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻏري ﻣُﺘﺤﻴﺰ وﻳُﺴﺘﺨﺪَم دون َﻓﻬﻢ ملﺎ ﻳﺘﻢ اﻟﻘﻴﺎم ﺑﻪ‪ ،‬ﻓﺈن ﻫﺬا ﻣﻦ ﺷﺄﻧﻪ أن ﻳُﺴﻬﻢ‬
‫ﰲ ﺗﻌﻤﻴﻖ ﻏﻴﺎب اﻟﻮﻋﻲ‪ ،‬ﺛﻢ ﰲ اﻟﻨﻬﺎﻳﺔ‪ ،‬ﰲ اﻟﻔﺴﺎد اﻷﺧﻼﻗﻲ ﻟﻠﻌﺎﻟﻢ‪ .‬وﺗﺴﺘﻄﻴﻊ ﺳﻴﺎﺳﺎت‬
‫اﻟﺘﻌﻠﻴﻢ املﺴﺎﻋَ ﺪة ﰲ اﻟﺘﺨﻔﻴﻒ ﻣﻦ ذﻟﻚ وﺑﺎﻟﺘﺎﱄ املﺴﺎﻫﻤﺔ ﰲ ﺟﻌﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺟﻴﺪًا‬
‫وذا ﻣﻌﻨًﻰ‪.‬‬
‫ﻻ ﺗﺰال ﻫﻨﺎك اﻟﻌﺪﻳﺪ ﻣﻦ اﻷﺳﺌﻠﺔ ا ُملﺰﻋﺠﺔ‪ ،‬ورﺑﻤﺎ املﺆﻟِﻤﺔ إﱃ ﺣ ﱟﺪ ﻣﺎ‪ ،‬اﻟﺘﻲ ﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﺘﻢ‬
‫ﺗﺠﺎﻫﻠﻬﺎ ﰲ املﻨﺎﻗﺸﺎت اﻟﺘﻲ ﺗﺪور ﺣﻮل أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺳﻴﺎﺳﺎﺗﻪ‪ ،‬وﻟﻜﻨﻬﺎ‬
‫‪120‬‬

‫اﻟﺘﺤﺪﻳﺎت اﻟﺘﻲ ﺗُﻮاﺟﻪ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‬

‫ﱡ‬
‫ً‬
‫ً‬
‫ﻛﺎﻣﻼ‪ .‬ﻫﻞ أﺧﻼﻗﻴﺎت‬
‫ﺗﺤﻠﻴﻼ‬
‫ﺗﺴﺘﺤﻖ ﻣﻨﱠﺎ ﻋﲆ اﻷﻗﻞ أن ﻧﺬ ُﻛﺮﻫﺎ ﻫﻨﺎ‪ ،‬ﺣﺘﻰ وإن ﻟﻢ ﻧُﺤﻠﻠﻬﺎ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺘﻌ ﱠﻠﻖ ﻓﻘﻂ ﺑﺨري اﻟﺒﴩ وﻗﻴﻤﺘﻬﻢ‪ ،‬أم إن ﻋﻠﻴﻨﺎ أن ﻧﺮاﻋﻲ ً‬
‫أﻳﻀﺎ ﻗِ ﻴَﻢ ﻏري‬
‫اﻟﺒﴩ وﺧريﻫﻢ وﻣﺼﺎﻟﺤﻬﻢ؟ وﺣﺘﻰ إذا ﻛﺎﻧﺖ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺗﺘﻌﻠﻖ ﺑﺸﻜﻞ‬
‫رﺋﻴﴘ ﺑﺎﻟﺒﴩ‪ ،‬ﻓﻬﻞ ﻳﻤﻜﻦ أن ﺗﻜﻮن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺴﺖ ﺑﺎملﺴﺄﻟﺔ اﻷﻫﻢ‬
‫اﻟﺘﻲ ﱠ‬
‫ﻳﺘﻌني ﻋﲆ اﻟﺒﴩﻳﺔ اﻻﻫﺘﻤﺎم ﺑﻬﺎ؟ ﻳﻘﻮدﻧﺎ ﻫﺬا اﻟﺴﺆال إﱃ اﻟﻔﺼﻞ اﻷﺧري ﻣﻦ اﻟﻜﺘﺎب‪.‬‬

‫‪121‬‬

‫اﻟﻔﺼﻞ اﻟﺜﺎﻧﻲ ﻋﴩ‬

‫ﺗﻐﲑ اﳌﻨﺎخ‪ :‬ﺣﻮل اﻷوﻟﻮﻳﺎت‬
‫ﱢ‬
‫ﲢﺪي ﱡ‬
‫وﺣﻘﺒﺔ اﻟﺘﺄﺛﲑ اﻟﺒﴩي‬

‫ﻳﺠﺐ أن ﺗﻜﻮن أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺤﻮرﻫﺎ اﻹﻧﺴﺎن؟‬
‫ﻫﻞ ِ‬
‫ﻋﲆ اﻟﺮﻏﻢ ﻣﻦ أن اﻟﻌﺪﻳﺪ ﻣﻦ املﺆ ﱠﻟﻔﺎت ا ُملﺘﻌﻠﻘﺔ ﺑﺄﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺴﻴﺎﺳﺎت‬
‫ﺗﺄﺗﻲ ﻋﲆ ذِ ﻛﺮ اﻟﺒﻴﺌﺔ أو اﻟﺘﻨﻤﻴﺔ ا ُملﺴﺘﺪاﻣﺔ‪ ،‬ﻓﺈﻧﻬﺎ ﺗﺆ ﱢﻛﺪ ﻋﲆ اﻟﻘِ ﻴَﻢ اﻹﻧﺴﺎﻧﻴﺔ وﻏﺎﻟﺒًﺎ ﻣﺎ‬
‫ﺗﺘﻤﺤﻮَر ﺣﻮل اﻹﻧﺴﺎن ﺑﻮﺿﻮح‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﺗﻘﻮل اﻹرﺷﺎدات اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ وﺿﻌﻬﺎ‬
‫ﻧﻬﺞ ﻣﺘﻤﺤﻮر‬
‫ﻓﺮﻳﻖ اﻟﺨﱪاء اﻟﺮﻓﻴﻊ ا ُملﺴﺘﻮى‬
‫ﱢ‬
‫املﻌﻨﻲ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إﻧﻪ ﻳﺠﺐ ﺗﺒﻨﱢﻲ ٍ‬
‫ٍ‬
‫ﺑﻤﻜﺎﻧﺔ أﺧﻼﻗﻴﺔ ﻓﺮﻳﺪة وراﺳﺨﺔ‬
‫ﺣﻮل اﻹﻧﺴﺎن ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ »ﻳﺘﻤﺘﻊ ﻓﻴﻪ اﻹﻧﺴﺎن‬
‫ﻟﻬﺎ أوﻟﻮﻳﺔ ﻋﲆ ﺟﻤﻴﻊ اﻷﺻﻌﺪة املﺪﻧﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ واﻻﻗﺘﺼﺎدﻳﺔ واﻻﺟﺘﻤﺎﻋﻴﺔ« )‪European‬‬
‫‪ (Commission AI HLEG 2019, 10‬وﻗﺪ ﺻﺎﻏﺖ اﻟﺠﺎﻣﻌﺎت ﻣﺜﻞ ﺳﺘﺎﻧﻔﻮرد وﻣﻌﻬﺪ‬
‫ﻣﺎﺳﺎﺗﺸﻮﺳﺘﺲ ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﺳﻴﺎﺳﺎت ﺑﺤﺜﻬﺎ ﰲ ﺳﻴﺎق اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺘﻤﺤﻮر ﺣﻮل‬
‫‪1‬‬
‫اﻹﻧﺴﺎن‪.‬‬
‫ﻏﺎﻟﺒًﺎ ﻣﺎ ﻳﺘﻢ ﺗﻌﺮﻳﻒ ﻫﺬا اﻟﺘﻤﺤﻮُر ﺣﻮل اﻹﻧﺴﺎن ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﺑﺈﻋﻄﺎء‬
‫اﻷوﻟﻮﻳﺔ ﻟﺨري اﻹﻧﺴﺎن وﻛﺮاﻣﺘﻪ ﻋﲆ ﺣﺴﺎب ﻣﺎ ﻗﺪ ﺗﺘﻄ ﱠﻠﺒﻪ أو ﺗﻔﻌﻠﻪ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪.‬‬
‫ﻓﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻳﺠﺐ أن ﺗﻌﻮد ﺑﺎﻟﻔﺎﺋﺪة ﻋﲆ اﻟﺒﴩ وأن ﺗﺨﺪﻣﻬﻢ وﻟﻴﺲ اﻟﻌﻜﺲ‪ .‬وﻣﻊ‬
‫ذﻟﻚ‪ ،‬وﻛﻤﺎ رأﻳﻨﺎ ﰲ اﻟﻔﺼﻮل اﻷوﱃ‪ ،‬ﻓﺈن ﻣﺪى ﻣﻨﺎﺳﺒﺔ ﻫﺬا اﻟﱰﻛﻴﺰ ﻋﲆ اﻹﻧﺴﺎن ﰲ أﺧﻼﻗﻴﺎت‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺲ واﺿﺤً ﺎ ﻛﻤﺎ ﻗﺪ ﻳﺒﺪو ﻟﻠﻮﻫﻠﺔ اﻷوﱃ‪ ،‬وﻻ ﺳﻴﱠﻤﺎ إذا أﺧﺬﻧﺎ ﰲ اﻻﻋﺘﺒﺎر‬
‫املﻨﺎﻫﺞ املﺆﻳﺪة ﻟﺘﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ أو ﴎدﻳﺎت املﻨﺎﻓﺴﺔ )ﻣﺎ ﺑني اﻹﻧﺴﺎن واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ(‪.‬‬
‫وﺗﺒني ﻓﻠﺴﻔﺔ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ أن ﻫﻨﺎك املﺰﻳﺪ ﻣﻦ اﻟﻄﺮق — اﻷﻛﺜﺮ ً‬
‫دﻗﺔ وﺗﻌﻘﻴﺪًا — ﻟﺘﺤﺪﻳﺪ‬
‫اﻟﻌﻼﻗﺔ ﺑني اﻟﺒﴩ واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻳُﻌﺪ اﻟﻨﻬﺞ ا ُملﺘﻤﺤﻮر ﺣﻮل اﻹﻧﺴﺎن ﻏري‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫واﺿﺢ ﻋﲆ أﻗﻞ ﺗﻘﺪﻳﺮ‪ ،‬إن ﻟﻢ ﻳ ُﻜﻦ ﻣُﺜريًا ﻟﻠﻤﺸﻜﻼت‪ ،‬ﰲ ﺿﻮء املﻨﺎﻗﺸﺎت اﻟﻔﻠﺴﻔﻴﺔ ﺣﻮل‬
‫اﻟﺒﻴﺌﺔ واﻟﻜﺎﺋﻨﺎت اﻟﺤﻴﺔ اﻷﺧﺮى‪ .‬ﰲ ﻓﻠﺴﻔﺔ اﻟﺒﻴﺌﺔ وأﺧﻼﻗﻴﺎﺗﻬﺎ‪ ،‬ﻫﻨﺎك ﻧﻘﺎش ﻃﻮﻳﻞ ﺣﻮل‬
‫ﻗﻴﻤﺔ ﻏري اﻟﺒﴩ‪ ،‬ﺧﺎﺻﺔ اﻟﻜﺎﺋﻨﺎت اﻟﺤﻴﺔ‪ ،‬وﺣﻮل ﻛﻴﻔﻴﺔ اﺣﱰام ﺗﻠﻚ اﻟﻘﻴﻤﺔ وﻫﺬه اﻟﻜﺎﺋﻨﺎت‪،‬‬
‫ﱡ‬
‫ﻳﺨﺺ أﺧﻼﻗﻴﺎت‬
‫وﺣﻮل املﺸﻜﻼت ا ُملﺤﺘﻤﻠﺔ اﻟﺘﻲ ﻗﺪ ﺗﻨﺸﺄ ﻧﺘﻴﺠﺔ اﺣﱰام ﻗﻴﻤﺔ اﻟﺒﴩ‪ .‬وﻓﻴﻤﺎ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻓﺈن ﻫﺬا ﻳﻌﻨﻲ أن ﻋﻠﻴﻨﺎ ﻋﲆ اﻷﻗﻞ ﻃﺮح اﻟﺴﺆال ﺑﺸﺄن ﺗﺄﺛري اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ اﻟﻜﺎﺋﻨﺎت اﻟﺤﻴﺔ اﻷﺧﺮى واﻟﻨﻈﺮ ﰲ اﺣﺘﻤﺎﻟﻴﺔ وﺟﻮد ﺗﻌﺎ ُرض ﺑني ﻗِ ﻴَﻢ‬
‫وﻣﺼﺎﻟﺢ اﻟﺒﴩ وﻏري اﻟﺒﴩ‪.‬‬
‫ﺗﺤﺪﻳﺪ اﻷوﻟﻮﻳﺎت ﻋﲆ اﻟﻨﺤﻮ اﻟﺼﺤﻴﺢ‬
‫ﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ اﻟﻘﻮل ﺑﻮﺟﻮد ﻣﺸﻜﻼت أﺧﺮى أﻛﺜﺮ ﺧﻄﻮرة ﻣﻦ ﺗﻠﻚ اﻟﺘﻲ ﻳُﺴﺒﺒﻬﺎ اﻟﺬﻛﺎء‬
‫ﺑﺸﻜﻞ ﺻﺤﻴﺢ‪ .‬وﻗﺪ ﻳﻨﺸﺄ ﻫﺬا اﻻﻋﱰاض ﻣﻦ‬
‫اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬وأﻧﻪ ﻣﻦ ا ُملﻬﻢ ﺗﺤﺪﻳﺪ أوﻟﻮﻳﺎﺗﻨﺎ‬
‫ٍ‬
‫ﺗﻐري املﻨﺎخ‪ ،‬اﻟﺘﻲ ﺗُﻌﺪ ً‬
‫اﻟﻨﻈﺮ إﱃ املﺸﻜﻼت اﻟﻌﺎملﻴﺔ ﻣﺜﻞ ﱡ‬
‫وﻓﻘﺎ ﻟﻠﺒﻌﺾ املﺸﻜﻠﺔ اﻷﻫﻢ اﻟﺘﻲ‬
‫ﺗﺤﺘﺎج اﻟﺒﴩﻳﺔ إﱃ اﻟﺘﺼﺪﱢي ﻟﻬﺎ وإﻳﻼﺋﻬﺎ اﻷوﻟﻮﻳﺔ ﻧﻈ ًﺮا إﱃ ﺧﻄﻮرﺗﻬﺎ وﺗﺄﺛريﻫﺎ ا ُملﺤﺘﻤَ ﻞ ﻋﲆ‬
‫اﻟﻜﻮﻛﺐ ٍّ‬
‫ﻛﻼ‪.‬‬
‫واﺿﺢ ﻋﲆ أﻗﻞ ﺗﻘﺪﻳﺮ‪ ،‬إن ﻟﻢ ﻳ ُﻜﻦ ﻣُﺜريًا ﻟﻠﻤﺸﻜﻼت‪ ،‬ﰲ ﺿﻮء‬
‫ﻳُﻌَ ﺪ اﻟﻨﻬﺞ ا ُملﺘﻤﺤﻮر ﺣﻮل اﻹﻧﺴﺎن ﻏري‬
‫ٍ‬
‫املﻨﺎﻗﺸﺎت اﻟﻔﻠﺴﻔﻴﺔ ﺣﻮل اﻟﺒﻴﺌﺔ واﻟﻜﺎﺋﻨﺎت اﻟﺤﻴﺔ اﻷﺧﺮى‪.‬‬

‫ﺑﺎﻟﻨﻈﺮ إﱃ ﺟﺪول أﻋﻤﺎل اﻷﻣﻢ املﺘﺤﺪة ﻟﻠﺘﻨﻤﻴﺔ ا ُملﺴﺘﺪاﻣﺔ ﻟﻌﺎم ‪) ٢٠١٥‬اﻟﺬي ﻳﻄﻠﻖ‬
‫ﻋﻠﻴﻪ أﻫﺪاف اﻟﺘﻨﻤﻴﺔ ا ُملﺴﺘﺪاﻣﺔ( ‪ 2‬وﻧﻈﺮﺗﻪ اﻟﻌﺎﻣﺔ إﱃ اﻟﻘﻀﺎﻳﺎ اﻟﻌﺎملﻴﺔ املﺘﻌﻠﻘﺔ ﺑﻤﺎ وﺻﻔﻪ‬
‫اﻷﻣني اﻟﻌﺎم ﻟﻸﻣﻢ املﺘﺤﺪة ﺑﺎن ﻛﻲ‪-‬ﻣﻮن »اﻹﻧﺴﺎن واﻟﻜﻮﻛﺐ«‪ ،‬ﻧﺮى اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﻘﻀﺎﻳﺎ‬
‫اﻟﻌﺎملﻴﺔ اﻟﺘﻲ ﺗﺘﻄ ﱠﻠﺐ ﻳﻘﻈﺔ أﺧﻼﻗﻴﺔ وﺳﻴﺎﺳﻴﺔ‪ :‬اﻟﺘﻔﺎوت اﻻﺟﺘﻤﺎﻋﻲ ا ُملﺘﺰاﻳﺪ داﺧﻞ اﻟﺒﻠﺪان‬
‫وﻓﻴﻤﺎ ﺑﻴﻨﻬﺎ‪ ،‬واﻟﺤﺮوب واﻟﺘﻄ ﱡﺮف اﻟﻌﻨﻴﻒ‪ ،‬واﻟﻔﻘﺮ وﺳﻮء اﻟﺘﻐﺬﻳﺔ‪ ،‬وﺻﻌﻮﺑﺔ اﻟﻮﺻﻮل إﱃ‬
‫املﻴﺎه اﻟﻌﺬﺑﺔ‪ ،‬وﻧﻘﺺ املﺆﺳﺴﺎت اﻟﻔﻌﺎﻟﺔ واﻟﺪﻳﻤﻘﺮاﻃﻴﺔ‪ ،‬وزﻳﺎدة ﻧِﺴﺒﺔ اﻟﺴﻜﺎن ا ُملﺘﻘﺪﱢﻣني‬
‫ﰲ اﻟﺴﻦ‪ ،‬واﻷﻣﺮاض ا ُملﻌﺪﻳﺔ واﻟﻮﺑﺎﺋﻴﺔ‪ ،‬وﻣﺨﺎﻃﺮ اﻟﻄﺎﻗﺔ اﻟﻨﻮوﻳﺔ‪ ،‬وﻧﻘﺺ اﻟﻔﺮص ﻟﻸﻃﻔﺎل‬
‫َ‬
‫اﻟﺠﻨﺴني وأﺷﻜﺎل اﻟﺘﻤﻴﻴﺰ واﻹﻗﺼﺎء ا ُملﺨﺘﻠﻔﺔ‪ ،‬واﻷزﻣﺎت‬
‫واﻟﺸﺒﺎب‪ ،‬وﻋﺪم املﺴﺎواة ﺑني‬
‫اﻹﻧﺴﺎﻧﻴﺔ وﺟﻤﻴﻊ أﻧﻮاع اﻧﺘﻬﺎﻛﺎت ﺣﻘﻮق اﻹﻧﺴﺎن‪ ،‬وا ُملﺸﻜﻼت املﺘﻌﻠﻘﺔ ﺑﺎﻟﻬﺠﺮة واﻟﻼﺟﺌني‪،‬‬
‫‪124‬‬

‫ﺗﺤﺪﱢي ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ :‬ﺣﻮل اﻷوﻟﻮﻳﺎت وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‬

‫ﱡ‬
‫ﱡ‬
‫ﺑﺘﻐري املﻨﺎخ — ﻣﺜﻞ‬
‫وﺗﻐري املﻨﺎخ واملﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ — اﻟﺘﻲ ﺗﺘﻌ ﱠﻠﻖ ﰲ ﺑﻌﺾ اﻷﺣﻴﺎن‬
‫اﻟﻜﻮارث اﻟﻄﺒﻴﻌﻴﺔ ا ُملﺘﻜ ﱢﺮرة وا ُملﺘﻔﺎﻗﻤﺔ وأﺷﻜﺎل ﺗﺪﻫﻮر اﻟﺒﻴﺌﺔ ﻣﺜﻞ اﻟﺠﻔﺎف وﻓﻘﺪان اﻟﺘﻨﻮع‬
‫اﻟﺒﻴﻮﻟﻮﺟﻲ‪ .‬ﰲ ﺿﻮء ﻫﺬه املﺸﻜﻼت اﻟﻀﺨﻤﺔ‪ ،‬ﻫﻞ ﻳﺠﺐ أن ﻧﻌﺘﱪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫أوﻟﻮﻳﺘﻨﺎ اﻷوﱃ؟ وﻫﻞ ﻳُﺸﺘﱢﺖ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻧﺘﺒﺎﻫﻨﺎ ﻋﻦ ﻗﻀﺎﻳﺎ أﻛﺜﺮ أﻫﻤﻴﺔ؟‬
‫ﻣﻦ ﺟﻬﺔ‪ ،‬ﻳﺒﺪو أن اﻟﱰﻛﻴﺰ ﻋﲆ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻏريه ﻣﻦ املﺸﻜﻼت اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‬
‫ٍ‬
‫ﻣﺸﻜﻼت أﺧﺮى‬
‫ﰲ ﻏري ﻣﺤ ﱢﻠﻪ ﻋﻨﺪﻣﺎ ﻳُﻌﺎﻧﻲ ﻋﺪد ﻫﺎﺋﻞ ﻣﻦ اﻟﺒﴩ وﻳُﻌﺎﻧﻲ اﻟﻌﺎﻟﻢ ﺑﺄﴎه ﻣﻦ‬
‫ﻛﺜرية ﻟﻠﻐﺎﻳﺔ‪ .‬ﻓﻔﻲ ﺣني أن اﻟﻨﺎس ﰲ أﺣﺪ أﻧﺤﺎء اﻟﻌﺎﻟﻢ ﻳُﻜﺎﻓﺤﻮن ﻣﻦ أﺟﻞ اﻟﻮﺻﻮل إﱃ املﻴﺎه‬
‫ٍ‬
‫ﺑﻴﺌﺎت ﻋﻨﻴﻔﺔ‪ ،‬ﻳﻘﻠﻖ آﺧﺮون ﰲ ﺟﺰءٍ آﺧﺮ ﻣﻦ‬
‫اﻟﻌﺬﺑﺔ أو ﻣﻦ أﺟﻞ اﻟﺒﻘﺎء ﻋﲆ ﻗﻴﺪ اﻟﺤﻴﺎة ﰲ‬
‫ً‬
‫اﻟﻌﺎ َﻟﻢ ﺑﺸﺄن ﺧﺼﻮﺻﻴﺘﻬﻢ ﻋﲆ اﻹﻧﱰﻧﺖ وﻳﺘﺨﻴﱠﻠﻮن ﻣُﺴﺘﻘﺒﻼ ﻳُﺤﻘﻖ ﻓﻴﻪ اﻟﺬﻛﺎءُ اﻻﺻﻄﻨﺎﻋﻲ‬
‫اﻟﺬﻛﺎءَ اﻟﻔﺎﺋﻖ‪ .‬ﻣﻦ اﻟﻨﺎﺣﻴﺔ اﻷﺧﻼﻗﻴﺔ‪ ،‬ﻳﺒﺪو أن ﺷﻴﺌًﺎ ﻣُﺮﻳﺒًﺎ ﻳﺤﺪث‪ ،‬ﺷﻴﺌًﺎ ﻳﺘﻌﻠﻖ ﺑﺎﻟﺘﻔﺎوُت‬
‫ﱠ‬
‫َ‬
‫اﻟﻄﺮف ﻋﻦ ﻣﺜﻞ ﻫﺬه‬
‫ﺗﻐﺾ اﻷﺧﻼق واﻟﺴﻴﺎﺳﺎت‬
‫اﻻﺟﺘﻤﺎﻋﻲ واﻟﻈﻠﻢ اﻟﻌﺎ َﻟﻤﻴﱠني‪ .‬ﻳﺠﺐ أﻻ‬
‫املﺸﻜﻼت‪ ،‬اﻟﺘﻲ ﻻ ﺗﺘﻌ ﱠﻠﻖ ﺑﺎﻟﴬورة ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻋﲆ اﻹﻃﻼق‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﰲ‬
‫اﻟﺒﻠﺪان اﻟﻨﺎﻣﻴﺔ‪ ،‬ﻳُﻤﻜﻦ أﺣﻴﺎﻧًﺎ ﻟﻠﺘﻜﻨﻮﻟﻮﺟﻴﺎ ا ُملﻨﺨﻔﻀﺔ اﻟﺘﻜﻠﻔﺔ — وﻟﻴﺲ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ا ُملﺘﻘﺪﻣﺔ‬
‫— املﺴﺎﻋﺪة ﰲ ﺣ ﱢﻞ ﻣُﺸﻜﻼت اﻟﻨﺎس؛ ﻷﻧﻬﻢ ﻳﺴﺘﻄﻴﻌﻮن أن ﻳﺘﺤﻤﱠ ﻠﻮا ﺗﻜﺎﻟﻴﻔﻬﺎ وﻳﺴﺘﻄﻴﻌﻮن‬
‫ﺗﺮﻛﻴﺒﻬﺎ وﺻﻴﺎﻧﺘﻬﺎ‪.‬‬
‫ً‬
‫ٍ‬
‫وأﻳﻀﺎ ﻳﻌﻤﻞ‬
‫ﻣﺸﻜﻼت ﺟﺪﻳﺪة‬
‫ﻣﻦ ﺟﻬﺔ أﺧﺮى‪ ،‬ﻳﻤﻜﻦ أن ﻳُﺴﺒﺐ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ُ‬
‫ﺗﻔﺎﻗﻢ املﺸﻜﻼت اﻟﻘﺎﺋﻤﺔ ﺑﺎﻟﻔﻌﻞ ﰲ ا ُملﺠﺘﻤﻌﺎت وﰲ اﻟﺒﻴﺌﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﺨﴙ‬
‫ﻋﲆ‬
‫اﻟﺒﻌﺾ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺳﻴﻮﺳﻊ اﻟﻔﺠﻮة ﺑني اﻷﻏﻨﻴﺎء واﻟﻔﻘﺮاء‪ ،‬وأﻧﻪ‪ ،‬ﻣﺜﻞ اﻟﻌﺪﻳﺪ‬
‫ﻣﻦ اﻟﺘﻘﻨﻴﺎت اﻟﺮﻗﻤﻴﺔ‪ ،‬ﺳﻴﺰﻳﺪ ﻣﻦ اﺳﺘﻬﻼك اﻟﻄﺎﻗﺔ‪ ،‬وﻳﺨﻠﻖ ﻣﺰﻳﺪًا ﻣﻦ اﻟﻨﻔﺎﻳﺎت‪ .‬ﻣﻦ ﻫﺬا‬
‫املﻨﻈﻮر‪ ،‬ﻓﺈن ﻣﻨﺎﻗﺸﺔ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺘﻌﺎﻣُﻞ ﻣﻌﻬﺎ ﻟﻴﺲ ﺗﺸﺘﻴﺘًﺎ ﻟﻼﻧﺘﺒﺎه‬
‫وﻟﻜﻨﻪ إﺣﺪى اﻟﻄﺮق اﻟﺘﻲ ﻳُﻤﻜﻨﻨﺎ ﻣﻦ ﺧﻼﻟﻬﺎ املﺴﺎﻫﻤﺔ ﰲ ﻣﻌﺎﻟﺠﺔ ﻣﺸﻜﻼت اﻟﻌﺎﻟﻢ‪ ،‬ﺑﻤﺎ‬
‫ﺑﺤﺎﺟﺔ ً‬
‫ٍ‬
‫أﻳﻀﺎ إﱃ إﻳﻼء اﻻﻫﺘﻤﺎم‬
‫ﻓﻴﻬﺎ املﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ‪ .‬وﻣﻦ ﺛَﻢ‪ ،‬ﻳُﻤﻜﻨﻨﺎ أن ﻧﺴﺘﺨﻠﺺ أﻧﻨﺎ‬
‫ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ :‬ﻧﻌﻢ‪ ،‬اﻟﻔﻘﺮ واﻟﺤﺮوب وﻣﺎ إﱃ ذﻟﻚ ﻫﻲ ﻣﺸﻜﻼت ﺧﻄرية‪ ،‬وﻟﻜﻦ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻳُﻤﻜﻦ ً‬
‫أﻳﻀﺎ أن ﻳﺆدﱢي إﱃ — أو ﻳُﺴﺎﻋﺪ ﻋﲆ — ﺗﻔﺎﻗﻢ ﻣﺸﻜﻼت ﺧﻄرية اﻵن وﰲ‬
‫ا ُملﺴﺘﻘﺒﻞ‪ ،‬وﻳﺠﺐ أن ﻳﻜﻮن ﰲ ﻗﺎﺋﻤﺔ املﺸﻜﻼت اﻟﺘﻲ ﺗﺤﺘﺎج ﻣﻨﺎ إﱃ إﻳﺠﺎد اﻟﺤﻠﻮل‪ .‬وﻣﻊ ذﻟﻚ‪،‬‬
‫ﻓﻬﺬا ﻻ ﻳُﺠﻴﺒﻨﺎ ﻋﻦ اﻟﺴﺆال املﺘﻌﻠﻖ ﺑﺎﻷوﻟﻮﻳﺎت؛ وﻫﻮ ﺳﺆال ﻣُﻬﻢ ﻋﲆ ﻣﺴﺘﻮى اﻷﺧﻼﻗﻴﺎت‬
‫واﻟﺴﻴﺎﺳﺔ ﻋﲆ ﺣ ﱟﺪ ﺳﻮاء‪ .‬إن اﻟﻘﻀﻴﺔ ﻻ ﺗﺘﻤﺜﻞ ﰲ وﺟﻮد إﺟﺎﺑﺎت ﺳﻬﻠﺔ ﻋﻦ ذﻟﻚ اﻟﺴﺆال؛ ﺑﻞ‬
‫اﻟﻘﻀﻴﺔ ﻫﻲ أن ﻫﺬا اﻟﺴﺆال ﻻ ﻳُﻄ َﺮح ﺣﺘﻰ ﰲ ﻣﻌﻈﻢ املﺆ ﱠﻟﻔﺎت اﻷﻛﺎدﻳﻤﻴﺔ ووﺛﺎﺋﻖ اﻟﺴﻴﺎﺳﺎت‬
‫ﺣﻮل اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫‪125‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﻓﻔﻲ ﺣني أن اﻟﻨﺎس ﰲ أﺣﺪ أﻧﺤﺎء اﻟﻌﺎﻟﻢ ﻳُﻜﺎﻓﺤﻮن ﻣﻦ أﺟﻞ اﻟﻮﺻﻮل إﱃ املﻴﺎه اﻟﻌﺬﺑﺔ أو ﻣﻦ أﺟﻞ‬
‫ﺑﻴﺌﺎت ﻋﻨﻴﻔﺔ‪ ،‬ﻳﻘﻠﻖ آﺧﺮون ﰲ ﺟﺰء َ‬
‫ٍ‬
‫آﺧﺮ ﻣﻦ اﻟﻌﺎﻟﻢ ﺑﺸﺄن ﺧﺼﻮﺻﻴﺘﻬﻢ ﻋﲆ‬
‫اﻟﺒﻘﺎء ﻋﲆ ﻗﻴﺪ اﻟﺤﻴﺎة ﰲ‬
‫اﻹﻧﱰﻧﺖ‪.‬‬

‫ﱡ‬
‫وﺗﻐري املﻨﺎخ وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫إﺣﺪى أﺻﻌﺐ اﻟﻄﺮق ﻟﻄﺮح اﻟﺴﺆال ا ُملﺘﻌﻠﻖ ﺑﺎﻷوﻟﻮﻳﺎت ﻫﻮ اﻟﺘﻌ ﱡﺮض ملﻨﺎﻗﺸﺔ ﻣﺴﺄﻟﺔ ﱡ‬
‫ﺗﻐري‬
‫املﻨﺎخ واملﻮﺿﻮﻋﺎت ذات اﻟﺼﻠﺔ ﻣﺜﻞ ﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‪» :‬ملﺎذا ﻧﻘﻠﻖ ﺑﺸﺄن اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ إذا ﻛﺎﻧﺖ املﺸﻜﻠﺔ املﻠﺤﱠ ﺔ ﻫﻲ ﱡ‬
‫ﺗﻐري املﻨﺎخ وﻛﻮن ﻣُﺴﺘﻘﺒﻞ اﻟﻜﻮﻛﺐ ﰲ ﺧﻄﺮ؟«‬
‫أو دﻋﻮﻧﺎ ﻧﺴﺘﻌري ﻋﺒﺎر ًة ﻣﻦ اﻟﺜﻘﺎﻓﺔ اﻟﺴﻴﺎﺳﻴﺔ اﻷﻣﺮﻳﻜﻴﺔ‪» :‬إﻧﻪ املﻨﺎخ‪ ،‬أﻳﻬﺎ اﻟﻐﺒﻲ!« وﺳﻮف‬
‫ﱢ‬
‫أوﺿﺢ ﻫﻨﺎ ﻫﺬا اﻟﺘﺤﺪﱢي وأﻧﺎﻗﺶ ﺗﺪاﻋِ ﻴﺎﺗﻪ ﻋﲆ اﻟﺘﻔﻜري ﰲ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫ﰲ ﺣني ﻳﺮﻓﺾ ﺑﻌﺾ املﺘﻄ ﱢﺮﻓني اﻟﻨﺘﺎﺋﺞ اﻟﻌﻠﻤﻴﺔ‪ ،‬ﻳُﻘﺮ اﻟﻌﻠﻤﺎء وﺻﺎﻧﻌﻮ اﻟﺴﻴﺎﺳﺎت‬
‫ً‬
‫ﻣﺸﻜﻠﺔ ﻋﺎملﻴﺔ ﺧﻄرية وﻟﻜﻨﻪ ً‬
‫ﻧﻄﺎق واﺳﻊ ﺑﺄن ﱡ‬
‫أﻳﻀﺎ »أﺣﺪ أﻛﱪ‬
‫ﺗﻐري املﻨﺎخ ﻟﻴﺲ ﻓﻘﻂ‬
‫ﻋﲆ‬
‫ٍ‬
‫اﻟﺘﺤﺪﱢﻳﺎت ﰲ ﻋﴫﻧﺎ«‪ ،‬ﻛﻤﺎ ﻫﻮ ﻣﺬﻛﻮر ﰲ ﱢ‬
‫ﻧﺺ أﻫﺪاف اﻟﺘﻨﻤﻴﺔ ا ُملﺴﺘﺪاﻣﺔ ﻟﻸﻣﻢ املﺘﺤﺪة‪.‬‬
‫ً‬
‫ﻣﺸﻜﻠﺔ ﻣُﺴﺘﻘﺒﻠﻴﺔ‪ :‬ﻓﺪرﺟﺔ اﻟﺤﺮارة اﻟﻌﺎملﻴﺔ وﻣﺴﺘﻮﻳﺎت اﻟﺒﺤﺮ ﺗﺮﺗﻔﻊ ﺑﺎﻟﻔﻌﻞ‪،‬‬
‫وﻫﻮ ﻟﻴﺲ‬
‫ﻣﻤﺎ ﻳﺆﺛﺮ ﻋﲆ اﻟﺒﻠﺪان واملﻨﺎﻃﻖ اﻟﺴﺎﺣﻠﻴﺔ ا ُملﻨﺨﻔﻀﺔ‪ .‬وﻗﺮﻳﺒًﺎ ﺟﺪٍّا ﺳﻮف ﻳُﻀﻄﺮ املﺰﻳﺪ ﻣﻦ‬
‫اﻟﻨﺎس إﱃ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ ﻋﻮاﻗﺐ ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ .‬وﻳﺴﺘﻨﺘﺞ اﻟﻜﺜريون ﻣﻦ ﻫﺬا أﻧﻪ ﻳﺠﺐ ﻋﻠﻴﻨﺎ‬
‫ﺑﺸﻜﻞ ﻋﺎﺟﻞ ﻟﻠﺘﺨﻔﻴﻒ ﻣﻦ ﻣﺨﺎﻃﺮ ﺗﻐري املﻨﺎخ؛ وأﻧﺎ أﻗﻮل »اﻟﺘﺨﻔﻴﻒ« ﻷن‬
‫اﻟﺘﴫﱡف اﻵن‬
‫ٍ‬
‫ﱡ‬
‫اﻟﺘﻮﻗﻒ‪ .‬إن اﻟﻔﻜﺮة ﻫﻲ أن ﻫﺬا ﻟﻴﺲ ﻓﻘﻂ اﻟﻮﻗﺖ‬
‫اﻟﻌﻤﻠﻴﺔ رﺑﻤﺎ ﻗﺪ ﺗﺠﺎوزت ﺑﺎﻟﻔﻌﻞ ﻧﻘﻄﺔ‬
‫املﻨﺎﺳﺐ ﻟﻠﻘﻴﺎم ﺑﴚءٍ وﻟﻜﻦ رﺑﻤﺎ ﻓﺎت اﻷوان ﺑﺎﻟﻔﻌﻞ ﻟﺘﺠﻨﱡﺐ ﺟﻤﻴﻊ اﻟﻌﻮاﻗﺐ‪ .‬وﺑﺎملﻘﺎرﻧﺔ ﻣﻊ‬
‫ﺑﺸﻜﻞ‬
‫ﻣﺨﺎوف ﻣﺆﻳﺪي ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﺑﺸﺄن اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ‪ ،‬ﻓﺈن ﻫﺬه املﺨﺎوف ﻣﺪﻋﻮﻣﺔ‬
‫ٍ‬
‫أﻓﻀﻞ ﺑﺎﻷدﻟﺔ اﻟﻌﻠﻤﻴﺔ وﺣﺎزت دﻋﻤً ﺎ ﻛﺒريًا ﺑني اﻟﻨﱡﺨﺐ ا ُملﺜﻘﻔﺔ ﰲ اﻟﻐﺮب — اﻟﺘﻲ ﺿﺠﺮت‬
‫ﻣﻦ اﻟﻨﺰﻋﺔ اﻟﺸﻜﻴﺔ ﻣﺎ ﺑﻌﺪ اﻟﺤﺪاﺛﻴﺔ وﺳﻴﺎﺳﺎت اﻟﻬﻮﻳﺔ اﻟﺒريوﻗﺮاﻃﻴﺔ — اﻟﺘﻲ ﺗﺮى اﻵن ﺳﺒﺒًﺎ‬
‫ﻟﻠﱰﻛﻴﺰ ﻋﲆ ﻣﺸﻜﻠﺔ ﻳﺒﺪو أﻧﻬﺎ ﺣﻘﻴﻘﻴﺔ ﻟﻠﻐﺎﻳﺔ وواﻗﻌﻴﺔ ﻟﻠﻐﺎﻳﺔ وﻋﺎملﻴﺔ ﻟﻠﻐﺎﻳﺔ‪ :‬ﱡ‬
‫ﺗﻐري املﻨﺎخ‬
‫ﻳﺤﺪث ٍّ‬
‫ﺣﻘﺎ وﻳﺆﺛﺮ ﻋﲆ ﻛ ﱢﻞ ﺷﺨﺺ وﻛﻞ ﳾء ﰲ ﻫﺬا اﻟﻜﻮﻛﺐ‪ .‬وﺗﺪﻋﻮ ﺣﻤﻠﺔ ﺟﺮﻳﺘﺎ ﺛﻮﻧﱪج‬
‫واﻻﻋﺘﺼﺎﻣﺎت املﻨﺎﺧﻴﺔ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬إﱃ ﺗﻮﺟﻴﻪ اﻻﻫﺘﻤﺎم إﱃ أزﻣﺔ املﻨﺎخ‪.‬‬
‫‪126‬‬

‫ﺗﺤﺪﱢي ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ :‬ﺣﻮل اﻷوﻟﻮﻳﺎت وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‬

‫»ملﺎذا ﻧﻘﻠﻖ ﺑﺸﺄن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ إذا ﻛﺎﻧﺖ املﺸﻜﻠﺔ املﻠﺤﱠ ﺔ ﻫﻲ ﱡ‬
‫ﺗﻐري املﻨﺎخ وﻛﻮن ﻣُﺴﺘﻘﺒﻞ اﻟﻜﻮﻛﺐ‬
‫ﰲ ﺧﻄﺮ؟«‬

‫ﻳُﺴﺘﺨﺪَم أﺣﻴﺎﻧًﺎ ﻣﻔﻬﻮم ﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي ﻟﺘﺄﻃري املﺸﻜﻠﺔ‪ .‬وﻫﻲ ﻓﻜﺮة ﻃﺮﺣﻬﺎ ﺑﻮل‬
‫ﻛﺮوﺗﺰن اﻟﺒﺎﺣﺚ ﰲ ﱡ‬
‫ﱡ‬
‫وﺗﻨﺺ ﻋﲆ أﻧﻨﺎ ﻧﻌﻴﺶ ﰲ‬
‫ﺗﻐري املﻨﺎخ وﻳﻮﺟني ﺳﺘﻮرﻣﺮ ﻋﺎﻟِﻢ اﻷﺣﻴﺎء‪،‬‬
‫ﺣﻘﺒﺔ ﺟﻴﻮﻟﻮﺟﻴﺔ زادت ﻓﻴﻬﺎ ﻗﻮة اﻟﺒﴩ ﻋﲆ اﻷرض وﻋﲆ ﻧﻈﻤﻬﺎ اﻟﺒﻴﺌﻴﺔ‪ ،‬ﻣﻤﺎ ﺟﻌﻞ اﻟﺒﴩ‬
‫ﻗﻮ ًة ﺟﻴﻮﻟﻮﺟﻴﺔ‪ .‬ﻓ ﱢﻜﺮ ﰲ اﻟﻨﻤﻮ اﻷ ُ ﱢﳼ ﻷﻋﺪاد اﻟﺒﴩ واملﺎﺷﻴﺔ‪ ،‬وﰲ اﻟﺘﻮﺳﻊ اﻟﻌﻤﺮاﻧﻲ املﺘﺰاﻳﺪ‪،‬‬
‫واﺳﺘﻨﺰاف اﻟﻮﻗﻮد اﻷﺣﻔﻮري‪ ،‬واﻻﺳﺘﺨﺪام اﻟﻬﺎﺋﻞ ﻟﻠﻤﻴﺎه اﻟﻌﺬﺑﺔ‪ ،‬واﻧﻘﺮاض اﻷﻧﻮاع‪ ،‬وإﻃﻼق‬
‫املﻮاد اﻟﺴﺎﻣﺔ‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪ .‬ﻳﻌﺘﻘﺪ اﻟﺒﻌﺾ أن ﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي ﻗﺪ ﺑﺪأت ﻣﻊ اﻟﺜﻮرة‬
‫اﻟﺰراﻋﻴﺔ؛ ﺑﻴﻨﻤﺎ ﻳﺮى آﺧﺮون أﻧﻬﺎ اﻧﻄﻠﻘﺖ ﺑﺎﻧﻄﻼق اﻟﺜﻮرة اﻟﺼﻨﺎﻋﻴﺔ )‪(Crutzen 2006‬‬
‫أو ﺑﻌﺪ اﻟﺤﺮب اﻟﻌﺎملﻴﺔ اﻟﺜﺎﻧﻴﺔ‪ .‬ﻋﲆ أي ﺣﺎل‪ ،‬ﻟﻘﺪ ﻧﺸﺄت ﻗﺼﺔ ﺟﺪﻳﺪة وﺗﺎرﻳﺦ ﺟﺪﻳﺪ‪،‬‬
‫ورﺑﻤﺎ ﺣﺘﻰ ﴎدﻳﺔ ﺟﺪﻳﺪة‪ .‬وﻏﺎﻟﺒًﺎ ﻣﺎ ﻳُﺴﺘﺨﺪَم ﻫﺬا املﻔﻬﻮم ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﴐ ﻹﺛﺎرة اﻟﻘﻠﻖ‬
‫ﱡ‬
‫ﱡ‬
‫اﻟﺘﺨﺼﺼﺎت )ﺑﻤﺎ ﰲ ذﻟﻚ اﻟﻌﻠﻮم‬
‫وﺗﻐري املﻨﺎخ‪ ،‬وﻟﺤﺸﺪ ﻣﺨﺘﻠﻒ‬
‫ﺑﺸﺄن اﻻﺣﺘﺒﺎس اﻟﺤﺮاري‬
‫اﻹﻧﺴﺎﻧﻴﺔ( ﻟﻠﺘﻔﻜري ﰲ ﻣُﺴﺘﻘﺒﻞ اﻟﻜﻮﻛﺐ‪.‬‬
‫ﻻ ﻳﺘﺒﻨﱠﻰ اﻟﺠﻤﻴﻊ ﻫﺬا املﺼﻄﻠﺢ؛ ﻓﻬﻮ ﻣﺼﻄﻠﺢ ﻣُﺜري ﻟﻠﺠﺪل ﺣﺘﻰ ﺑني اﻟﺠﻴﻮﻟﻮﺟﻴني‪،‬‬
‫وﻗﺪ ﺷﻜﻚ اﻟﺒﻌﺾ ﰲ ﺗﺮﻛﻴﺰه ﻋﲆ أﻫﻤﻴﺔ اﻟﺒﴩ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﺟﺎدﻟﺖ ﻫﺎراواي‬
‫)‪ (٢٠١٥‬ﻣﻦ ﻣﻨﻈﻮر ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ ﺑﺄن اﻷﻧﻮاع اﻷﺧﺮى واﻟﻌﻮاﻣﻞ »اﻟﻼﺣﻴﻮﻳﺔ« ﺗﻠﻌﺐ‬
‫ً‬
‫أﻳﻀﺎ دو ًرا ﰲ اﻟﺒﻴﺌﺔ املﺘﺤﻮﻟﺔ‪ .‬وﻟﻜﻦ ﺣﺘﻰ ﻣﻦ دون ﻣﻔﻬﻮم ﻣُﺜري ﻟﻠﺠﺪل ﻣﺜﻞ ﺣﻘﺒﺔ اﻟﺘﺄﺛري‬
‫اﻟﺒﴩي‪ ،‬ﻓﺈن ﱡ‬
‫وﻳﺠﺐ ﻋﲆ اﻟﺴﻴﺎﺳﺔ‬
‫ﺗﻐري املﻨﺎخ واملﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ )اﻷﺧﺮى( ﺳﺘﻈ ﱡﻞ ﺑﺎﻗﻴﺔ‪ِ ،‬‬
‫اﻟﺘﻌﺎﻣُﻞ ﻣﻌﻬﺎ‪ ،‬واﻷﻓﻀﻞ أن ﻳﻜﻮن ذﻟﻚ ﰲ أﻗﺮب ٍ‬
‫وﻗﺖ ﻣﻤﻜﻦ‪ .‬ﻓﻤﺎذا ﻳﻌﻨﻲ ﻫﺬا ﺑﺎﻟﻨﺴﺒﺔ إﱃ‬
‫ﺳﻴﺎﺳﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ؟‬
‫ﻳﻌﺘﻘﺪ اﻟﻌﺪﻳﺪ ﻣﻦ اﻟﺒﺎﺣﺜني أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ واﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ ﻳُﻤﻜﻦ أن‬
‫ﺗُﺴﺎﻋﺪﻧﺎ ً‬
‫أﻳﻀﺎ ﰲ ﻋﻼج اﻟﻌﺪﻳﺪ ﻣﻦ ﻣﺸﻜﻼت اﻟﻌﺎﻟﻢ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ .‬وﻋﲆ ﻏﺮار‬
‫ﺑﺸﻜﻞ ﻋﺎم‪ ،‬ﻳﻤﻜﻦ أن ﻳُﺴﻬﻢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ‬
‫املﻌﻠﻮﻣﺎت اﻟﺮﻗﻤﻴﺔ وﺗﻘﻨﻴﺎت اﻻﺗﺼﺎﻻت‬
‫ٍ‬
‫اﻟﺘﻨﻤﻴﺔ املﺴﺘﺪاﻣﺔ وﰲ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ اﻟﻌﺪﻳﺪ ﻣﻦ املﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ‪ .‬وﻣﻦ ا ُملﺮﺟﱠ ﺢ أن ﻳُﺼﺒﺢ‬
‫ً‬
‫اﺗﺠﺎﻫﺎ ﻧﺎﺟﺤً ﺎ ﰲ اﻟﺒﺤﺚ واﻟﺘﻄﻮﻳﺮ‪ .‬وﻣﻊ ذﻟﻚ‪ ،‬ﻳﻤﻜﻦ أن ﻳﺠﻌﻞ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ا ُملﺴﺘﺪام‬
‫ﱡ‬
‫ﱡ‬
‫ﻳﺨﺼﻨﺎ ﻧﺤﻦ ﺟﻤﻴﻌً ﺎ‪.‬‬
‫ﻳﺨﺺ اﻟﺒﻴﺌﺔ؛ وﺑﺎﻟﺘﺎﱄ ﻓﻴﻤﺎ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻷﻣﻮر أﺳﻮأ ﻓﻴﻤﺎ‬
‫‪127‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫وﻟﻨﺘﺬ ﱠﻛﺮ ﻣﺠﺪدًا زﻳﺎدة اﺳﺘﻬﻼك اﻟﻄﺎﻗﺔ واﻟﻨﻔﺎﻳﺎت‪ .‬وﻣﻦ ﻣﻨﻈﻮر ﻣﺸﻜﻠﺔ ﺣﻘﺒﺔ اﻟﺘﺄﺛري‬
‫اﻟﺒﴩي‪ ،‬ﻓﺈن املﺨﺎﻃﺮة ﺗﻜﻤُﻦ ﰲ أن اﻟﺒﴩ ﻳﻤﻜﻦ أن ﻳﺴﺘﺨﺪﻣﻮا اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻹﺣﻜﺎم‬
‫ﻗﺒﻀﺘﻬﻢ ﻋﲆ اﻷرض‪ ،‬ﻣﻤﺎ ﺳﻴﺰﻳﺪ ﻣﻦ ﺣﺪة املﺸﻜﻠﺔ ً‬
‫ﺑﺪﻻ ﻣﻦ ﺣ ﱢﻠﻬﺎ‪.‬‬
‫ﺑﺸﻜﻞ ﺧﺎص إذا ﻛﻨﺎ ﻧﻨﻈﺮ إﱃ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻴﺲ‬
‫ﻫﺬا ﻳﻌﺘﱪ أﻣ ًﺮا إﺷﻜﺎﻟﻴٍّﺎ‬
‫ٍ‬
‫ﻓﻘﻂ ﺑﻮﺻﻔﻪ ٍّ‬
‫ﺣﻼ وﻟﻜﻦ ﺑﻮﺻﻔﻪ اﻟﺤﻞ اﻟﺮﺋﻴﴘ‪ .‬وﻟﻨﻔﻜﺮ ﰲ ﺳﻴﻨﺎرﻳﻮ اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ ﻟﺬﻛﺎء‬
‫اﺻﻄﻨﺎﻋﻲ ﻳﻌﺮف أﻓﻀﻞ ﻣﻨﺎ ﻧﺤﻦ اﻟﺒﴩ ﻣﺎ ﻫﻮ ﺟﻴﺪ ﻟﻨﺎ‪ :‬ذﻛﺎء اﺻﻄﻨﺎﻋﻲ »ﺣﻤﻴﺪ« ﻳﺨﺪم‬
‫اﻟﺒﴩﻳﺔ ﻣﻦ ﺧﻼل ﺟﻌﻞ اﻟﺒﴩ ﻳﺘﴫﱠﻓﻮن ﻟﺼﺎﻟﺤﻬﻢ وﻟﺼﺎﻟﺢ اﻟﻜﻮﻛﺐ؛ ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪،‬‬
‫اﻵﻟﺔ اﻹﻟﻪ اﻟﺘﻲ ﺗُﻌﺎدل ﺗﻘﻨﻴٍّﺎ املﻠﻚ اﻟﻔﻴﻠﺴﻮف املﺬﻛﻮر ﰲ ﻓﻠﺴﻔﺔ أﻓﻼﻃﻮن‪ .‬ﻳﺤﻞ اﻟﺬﻛﺎء‬
‫اﻻﺻﻄﻨﺎﻋﻲ اﻹﻟﻪ ﻣﺤﻞ اﻹﻧﺴﺎن اﻹﻟﻪ )‪ ،(Harrari 2015‬وﻳﺪﻳﺮ ﻧﻈﺎم دﻋﻢ اﻟﺤﻴﺎة اﻟﺨﺎص‬
‫ﺑﻨﺎ وﻳﺪﻳﺮﻧﺎ‪ .‬ﻓﻠﺤﻞ ﻣﺸﻜﻼت ﺗﻮزﻳﻊ املﻮارد‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻳﻤﻜﻦ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أن‬
‫ﻳﻌﻤﻞ ﺑﻮﺻﻔﻪ »وﺣﺪة ﺧﺪﻣﺔ«‪ ،‬ﻳُﺪﻳﺮ إﻣﻜﺎﻧﻴﺔ وﺻﻮل اﻟﺒﴩ إﱃ املﻮارد‪ .‬وﺳﺘﻜﻮن ﻗﺮاراﺗﻪ‬
‫ﺣﻠﻮل ﺗﻜﻨﻮﻟﻮﺟﻴﺔ‬
‫ﻣُﺴﺘﻨﺪة إﱃ ﺗﺤﻠﻴﻠﻪ ﻷﻧﻤﺎط اﻟﺒﻴﺎﻧﺎت‪ .‬وﻳﻤﻜﻦ دﻣﺞ ﻫﺬا اﻟﺴﻴﻨﺎرﻳﻮ ﻣﻊ‬
‫ٍ‬
‫ﻣﺒﺘﻜﺮة ﻣﺜﻞ اﻟﻬﻨﺪﺳﺔ اﻟﺠﻴﻮﻟﻮﺟﻴﺔ‪ .‬اﻟﺒﴩ ﻟﻴﺴﻮا اﻟﻮﺣﻴﺪﻳﻦ اﻟﺬﻳﻦ ﻳﺤﺘﺎﺟﻮن إﱃ اﻹدارة؛‬
‫ﻓﺎﻟﻜﻮن ﻛﻠﻪ ﰲ ﺣﺎﺟﺔ إﱃ إﻋﺎدة ﻫﻨﺪﺳﺘﻪ‪ .‬وﻣﻦ ﺛَﻢ‪ ،‬ﻳُﻤﻜﻨﻨﺎ اﺳﺘﺨﺪام اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻟ »إﺻﻼح«‬
‫ﻣﺸﻜﻼﺗﻨﺎ وﻣﺸﻜﻼت اﻟﻜﻮﻛﺐ‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻓﺈن ﻫﺬه اﻟﺴﻴﻨﺎرﻳﻮﻫﺎت ﻟﻦ ﺗﻜﻮن ﻓﻘﻂ ﻣﺴﺘﺒﺪة وﺗﺘﻌﺪﱠى ﻋﲆ اﺳﺘﻘﻼﻟﻴﺔ‬
‫اﻟﺒﴩ‪ ،‬ﺑﻞ ﺳﺘﺴﺎﻫﻢ ً‬
‫ﺑﺸﻜﻞ أﺳﺎﳼ ﰲ ﻣﺸﻜﻠﺔ ﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي ﻧﻔﺴﻬﺎ‪ :‬ﻓﺎﻟﻮﻛﺎﻟﺔ‬
‫أﻳﻀﺎ‬
‫ٍ‬
‫اﻟﺒﴩﻳﺔ ا ُملﻔﺮﻃﺔ‪ ،‬ﻫﺬه املﺮة ﻳﺘﻢ ﺗﻔﻮﻳﻀﻬﺎ ﻣﻦ ﻗِ ﺒﻞ اﻟﺒﴩ إﱃ اﻵﻻت‪ ،‬ﺳﺘُﺤﻮل اﻟﻜﻮﻛﺐ ﺑﺄﻛﻤﻠﻪ‬
‫ﻮرد وآﻟﺔ ﻟﻠﺒﴩ‪ .‬ﻳﺘﻢ »ﺣﻞ« ﻣﺸﻜﻠﺔ ﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي ﻣﻦ ﺧﻼل اﻟﻮﺻﻮل‬
‫إﱃ ﻣﺠﺮد ﻣَ ِ‬
‫ﺑﻬﺎ إﱃ اﻟﻨﻘﻴﺾ اﻟﺘﻜﻨﻮﻗﺮاﻃﻲ‪ ،‬ﻣﻤﺎ ﻳﺆدي إﱃ ﻋﺎ َﻟ ٍﻢ ﻣﻦ اﻵﻻت ﻳُﻌﺎﻣَ ﻞ ﻓﻴﻪ اﻟﺒﴩ ً‬
‫أوﻻ ﻛﺄﻃﻔﺎل‬
‫ﻳﺠﺐ رﻋﺎﻳﺘﻬﻢ ورﺑﻤﺎ ﰲ ٍ‬
‫وﻗﺖ ﻻﺣﻖ ﻳﺘﻢ ﺗﺠﺎﻫﻠﻬﻢ ﺗﻤﺎﻣً ﺎ‪ .‬وﰲ ﻫﺬا اﻟﻨﻮع ﻣﻦ اﻟﺘﺄﺛري اﻟﺒﴩي‬
‫ا ُملﺘﻌﻠﻖ ﺑﺎﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ واﻟﺴﻴﻨﺎرﻳﻮ املﺄﻟﻮف ﺟﺪٍّا اﻟﺬي ﻳﺘﻢ ﻓﻴﻪ إﺣﻼل اﻵﻻت ﻣﺤ ﱠﻞ اﻟﺒﴩ‪،‬‬
‫ﻧﻌﻮد ﻣ ﱠﺮة أﺧﺮى إﱃ ﺳﻴﻨﺎرﻳﻮﻫﺎت اﻷﺣﻼم واﻟﻜﻮاﺑﻴﺲ‪.‬‬
‫ﺟﻨﻮن اﻟﻔﻀﺎء اﻟﺠﺪﻳﺪ واﻹﻏﺮاء اﻷﻓﻼﻃﻮﻧﻲ‬
‫ﺗﻐري املﻨﺎخ وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‪ ،‬واﻟﺘﻲ ﻫﻲ ً‬
‫ﺛﻤﱠ ﺔ إﺟﺎﺑﺔ أﺧﺮى ﻋﲆ ﱡ‬
‫أﻳﻀﺎ رؤﻳﺔ ﻣُﻮﻟﻌﺔ‬
‫ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ورﺑﻤﺎ ﺗﺮﺗﺒﻂ أﺣﻴﺎﻧًﺎ ﺑﴪدﻳﺎت ﺗﺠﺎوز اﻟﺒﴩﻳﺔ‪ ،‬وﻫﻲ‪ :‬ﻗﺪ ﻧُﺪﻣﺮ ﻫﺬا اﻟﻜﻮﻛﺐ‪،‬‬
‫وﻟﻜﻦ ﻳُﻤﻜﻨﻨﺎ اﻟﻬﺮب ﻣﻦ اﻷرض واﻟﺬﻫﺎب إﱃ اﻟﻔﻀﺎء‪.‬‬
‫‪128‬‬

‫ﺗﺤﺪﱢي ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ :‬ﺣﻮل اﻷوﻟﻮﻳﺎت وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‬

‫ﻛﺎﻧﺖ اﻟﺼﻮرة اﻷﻳﻘﻮﻧﻴﺔ ﻟﻌﺎم ‪ ٢٠١٨‬ﻫﻲ ﺳﻴﺎرة إﻳﻠﻮن ﻣﺎﺳﻚ اﻟﺮﻳﺎﺿﻴﺔ ﻃﺮاز ﺗﺴﻼ‬
‫وﻫﻲ ﺗﻄﻔﻮ ﰲ اﻟﻔﻀﺎء‪ 3 .‬ﻣﺎﺳﻚ ً‬
‫أﻳﻀﺎ ﻟﺪَﻳﻪ ُﺧﻄﻂ ﻻﺳﺘﻌﻤﺎر املﺮﻳﺦ‪ .‬وﻫﻮ ﻟﻴﺲ اﻟﺸﺨﺺ‬
‫ُﺮاوده ﻫﺬا اﻟﺤﻠﻢ‪ :‬ﻓﻬﻨﺎك اﻫﺘﻤﺎم ﻣُﺘﺰاﻳﺪ ﺑﺎﻟﺬﻫﺎب إﱃ اﻟﻔﻀﺎء‪ .‬وﻫﺬا ﻟﻴﺲ‬
‫اﻟﻮﺣﻴﺪ اﻟﺬي ﻳ ِ‬
‫ﻣﺠﺮد ﺣﻠﻢ‪ .‬إذ ﺗُﺴﺘﺜﻤﺮ أﻣﻮال ﻃﺎﺋﻠﺔ ﰲ ﻣﴩوﻋﺎت اﻟﻔﻀﺎء‪ .‬وﻋﲆ ﻋﻜﺲ ﺳﺒﺎق اﻟﻔﻀﺎء‬
‫اﻟﺬي ﺣﺪث ﰲ اﻟﻘﺮن اﻟﻌﴩﻳﻦ‪ ،‬ﻫﺬه املﴩوﻋﺎت ﻳﺘﻢ دﻋﻤُﻬﺎ ﻣﻦ ﻗِ ﺒﻞ اﻟﴩﻛﺎت اﻟﺨﺎﺻﺔ‪.‬‬
‫واملﻠﻴﻮﻧريات ا ُملﻮﻟﻌﻮن ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻟﻴﺴﻮا اﻟﻮﺣﻴﺪِﻳﻦ ا ُملﻬﺘﻤني ﺑﺎﻟﻔﻀﺎء‪ ،‬ﺑﻞ إن اﻟﻔﻨﺎﻧني ً‬
‫أﻳﻀﺎ‬
‫ﺷﻐﻮﻓﻮن ﺑﻪ ﺑﺸﺪة‪ .‬ﺗُﺨﻄﻂ ﴍﻛﺔ ﺳﺒﻴﺲ إﻛﺲ اﻟﺨﺎﺻﺔ ﺑﺈﻳﻠﻮن ﻣﺎﺳﻚ ﻹرﺳﺎل ﻓﻨﺎﻧني إﱃ‬
‫ﻣﺪار اﻟﻘﻤﺮ‪ 4 .‬وﺗُﻌﺪ اﻟﺴﻴﺎﺣﺔ اﻟﻔﻀﺎﺋﻴﺔ ﻓﻜﺮ ًة أﺧﺮى ﺗﺰداد ﺷﻴﻮﻋً ﺎ‪ .‬ﻓﻤَ ﻦ ﻣﻨﱠﺎ ﻻ ﻳﺮﻏﺐ ﰲ‬
‫ُﻐﺮ ﻟﻠﻐﺎﻳﺔ‪.‬‬
‫اﻟﺬﻫﺎب إﱃ اﻟﻔﻀﺎء؟ اﻟﻔﻀﺎء ﻣ ٍ‬
‫ً‬
‫ﻣﺸﻜﻠﺔ ﰲ ﺣ ﱢﺪ ذاﺗﻪ‪ .‬ﺑﻞ إن ﻟﻪ ﻓﻮاﺋﺪ ﻣُﺤﺘﻤﻠﺔ‪ .‬ﻋﲆ ﺳﺒﻴﻞ‬
‫ﻻ ﻳﻤﺜﻞ اﻟﺬﻫﺎب إﱃ اﻟﻔﻀﺎء‬
‫ً‬
‫ٍ‬
‫ﺗﻄﺮﻓﺎ ﰲ‬
‫ﺑﻴﺌﺎت أﻛﺜﺮ‬
‫املﺜﺎل‪ ،‬ﻳﻤﻜﻦ أن ﺗﺴﺎﻋﺪ اﻷﺑﺤﺎث ﰲ ﻛﻴﻔﻴﺔ اﻟﺒﻘﺎء ﻋﲆ ﻗﻴﺪ اﻟﺤﻴﺎة ﰲ‬
‫اﻟﺘﻌﺎﻣُﻞ ﻣﻊ املﺸﻜﻼت ﻋﲆ اﻷرض‪ ،‬وﰲ اﺧﺘﺒﺎر اﻟﺘﻘﻨﻴﺎت ا ُملﺴﺘﺪاﻣﺔ‪ ،‬واﺗﺨﺎذ ﻣﻨﻈﻮر ﻛﻮﻛﺒﻲ‪.‬‬
‫ً‬
‫ﺿﻊ ﰲ اﻋﺘﺒﺎرك ً‬
‫ﻧﺎﺟﻤﺔ ﻋﻦ أن‬
‫أﻳﻀﺎ أن ﻣﺸﻜﻠﺔ ﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي ﻳُﻤﻜﻦ أن ﺗﻜﻮن‬
‫ﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﻔﻀﺎء ﻣﻨﺬ ﺳﻨﻮات ﻃﻮﻳﻠﺔ أﺗﺎﺣﺖ ﻟﻨﺎ رؤﻳﺔ اﻷرض ﻣﻦ ﺑُﻌﺪ‪ .‬وﺑﺎﻟﻨﻈﺮ إﱃ ﺻﻮرة‬
‫ﺳﻴﺎرة ﻣﺎﺳﻚ ﻣﺮ ًة أﺧﺮى‪ :‬ﻳﻌﺘﻘﺪ ﺑﻌﺾ اﻟﻨﺎس أن اﻟﺴﻴﺎرة اﻟﻜﻬﺮﺑﺎﺋﻴﺔ ﺣ ﱞﻞ ﻣﻦ ﺣﻠﻮل‬
‫املﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ‪ ،‬دون اﻟﺘﺸﻜﻴﻚ ﰲ اﻓﱰاض أن اﻟﺴﻴﺎرات ﻫﻲ أﻓﻀﻞ وﺳﻴﻠﺔ ﻟﻠﻨﻘﻞ ودون‬
‫اﻟﺘﻔﻜري ﰲ ﻛﻴﻔﻴﺔ إﻧﺘﺎج اﻟﻜﻬﺮﺑﺎء‪ .‬ﻋﲆ أي ﺣﺎل‪ ،‬ﻫﻨﺎك أﻓﻜﺎر ﻣﺜرية ﻟﻼﻫﺘﻤﺎم‪.‬‬
‫ً‬
‫إﺷﻜﺎﻟﻴﺔ إذا ﻛﺎﻧﺖ ﻧﺘﻴﺠﺘﻬﺎ ﻫﻲ إﻫﻤﺎل املﺸﻜﻼت اﻷرﺿﻴﺔ‪،‬‬
‫وﻟﻜﻦ أﺣﻼم اﻟﻔﻀﺎء ﺗُﻌﺪ‬
‫ﻋﺮﺿﺎ ﻣﻦ أﻋﺮاض اﻟﺤﺎﻟﺔ اﻟﺘﻲ ﱠ‬
‫ً‬
‫ﺷﺨﺼﺘﻬﺎ ﺣﻨﺔ أرﻧﺖ )‪ (١٩٥٨‬ﺑﺎﻟﻔﻌﻞ ﻋﻨﺪﻣﺎ‬
‫وإذا ﻛﺎﻧﺖ‬
‫ﻛﺘﺒﺖ ﻋﻦ اﻟﺒﴩ‪ :‬اﻟﻜﺜري ﻣﻦ اﻟﺘﺠﺮﻳﺪ واﻻﻏﱰاب‪ .‬أﺷﺎرت ﺣﻨﺔ إﱃ أن اﻟﻌِ ﻠﻢ ﻳﺪﻋﻢ رﻏﺒﺔ دﻓﻴﻨﺔ‬
‫ً‬
‫وأﻳﻀﺎ‬
‫ﰲ ﻣﻐﺎدرة اﻷرض‪ :‬ﺣﺮﻓﻴٍّﺎ‪ ،‬ﻣﻦ ﺧﻼل ﺗﻜﻨﻮﻟﻮﺟﻴﺎ اﻟﻔﻀﺎء )ﰲ ﻋﴫﻫﺎ‪ ،‬ﺳﺒﻮﺗﻨﻴﻚ(‬
‫ﻣﻦ ﺧﻼل ُ‬
‫ﻃﺮق رﻳﺎﺿﻴﺔ ﺗُﺠﺮدﻧﺎ وﺗَﻌﺰﻟﻨﺎ ﻣﻤﺎ ِ‬
‫أﺻﻔﻪ ﺑﺤﻴﺎﺗﻨﺎ اﻷرﺿﻴﺔ اﻟﻔﻮﺿﻮﻳﺔ ا ُمل ﱢ‬
‫ﺘﺠﺴﺪة‬
‫واﻟﺴﻴﺎﺳﻴﺔ‪ .‬وﻣﻦ ﻫﺬا املﻨﻈﻮر‪ ،‬ﻳﻤﻜﻦ ﺗﻔﺴري أﺣﻼم ﻣﺆﻳﺪي ﺗﺠﺎوز اﻟﺒﴩﻳﺔ ﺑﺎﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ‬
‫وﺑﻤُﻐﺎدرة اﻷرض ﻋﲆ أﻧﻬﺎ ﺗﺪاﻋِ ﻴﺎت ﻟﻨﻮع إﺷﻜﺎﱄ ﻣﻦ اﻻﻏﱰاب واﻟﻬﺮوب‪ .‬إﻧﻬﺎ اﻟﻔﻜﺮ‬
‫اﻷﻓﻼﻃﻮﻧﻲ وﻓﻜﺮ ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ ﰲ أوﺿﺢ ﺻﻮره؛ إن اﻟﻔﻜﺮة ﻫﻲ اﻟﺘﻐ ﱡﻠﺐ ﻟﻴﺲ ﻓﻘﻂ‬
‫ﻋﲆ ﻗﻴﻮد اﻟﺠﺴﺪ اﻟﺒﴩي‪ ،‬وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﻋﲆ ﻗﻴﻮد ذﻟﻚ »اﻟﻨﻈﺎم اﻟﺪاﻋﻢ ﻟﻠﺤﻴﺎة«‪ :‬أي اﻷرض‬
‫ﻧﻔﺴﻬﺎ‪ .‬ﻓﺎﻟﺠﺴﺪ ﻟﻴﺲ ﻫﻮ اﻟﺴﺠﻦ اﻟﻮﺣﻴﺪ‪ ،‬ﺑﻞ اﻷرض ﻧﻔﺴﻬﺎ‪ ،‬وﻣﻦ ﺛَﻢ ﻋﻠﻴﻨﺎ أن ﻧﻬ ُﺮب‬
‫ﻣﻨﻬﺎ‪.‬‬
‫‪129‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺑﺎﻟﺘﺎﱄ‪ ،‬ﻓﺈﺣﺪى ﻣﺨﺎﻃﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻫﻲ أﻧﻪ ﻳُﻤ ﱢﻜﻦ ﻫﺬا اﻟﻨﻮع ﻣﻦ اﻟﺘﻔﻜري‬
‫وﻳُﺼﺒﺢ آﻟﺔ ﻟﻼﻏﱰاب‪ :‬أداة ملﻐﺎدرة اﻷرض وإﻧﻜﺎر ﺣﺎﻟﺘﻨﺎ اﻟﻮﺟﻮدﻳﺔ اﻻﻋﺘﻤﺎدﻳﺔ اﻟﻀﻌﻴﻔﺔ‬
‫واﻟﺠﺴﺪﻳﺔ واﻷرﺿﻴﺔ‪ .‬ﺑﻌﺒﺎر ٍة أﺧﺮى‪ :‬ﺻﺎروخ‪ .‬ﻣﺮة أﺧﺮى‪ ،‬ﻻ ﺗُﻤﺜﻞ اﻟﺼﻮارﻳﺦ ﻣﺸﻜﻠﺔ ﰲ‬
‫ﺣ ﱢﺪ ذاﺗﻬﺎ‪ .‬إﻧﻤﺎ املﺸﻜﻠﺔ ﻫﻲ ﻣﺰج ﺗﻘﻨﻴﺎت ﻣُﻌﻴﻨﺔ ﻣﻊ ﴎدﻳﺎت ﻣُﻌﻴﻨﺔ‪ .‬ﻓﻌﲆ اﻟﺮﻏﻢ ﻣﻦ أن‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻤﻜﻦ أن ﻳﻜﻮن ﻗﻮة إﻳﺠﺎﺑﻴﺔ ﺑﺎﻟﻨﺴﺒﺔ إﱃ ﺣﻴﺎﺗﻨﺎ اﻟﺸﺨﺼﻴﺔ‪ ،‬واملﺠﺘﻤﻊ‪،‬‬
‫واﻟﺒﴩﻳﺔ‪ ،‬ﻓﺈن ﻣﺰﻳﺠً ﺎ ﻣﻦ ﺗﻌﺰﻳﺰ اﻻﺗﺠﺎﻫﺎت اﻟﺘﺠﺮﻳﺪﻳﺔ واﻻﻏﱰاﺑﻴﺔ ﰲ اﻟﻌﻠﻮم واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫ﻣﺴﺘﻘﺒﻞ ﺗﻜﻨﻮﻟﻮﺟﻲ ﻣﺆ ٍذ‬
‫ﻣﻊ ﺧﻴﺎﻻت ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ و»ﺗﺠﺎوز اﻷرض« ﻗﺪ ﻳﺆدي إﱃ‬
‫ٍ‬
‫ﻟﻠﺒﴩ وﻟﻠﻜﺎﺋﻨﺎت اﻟﺤﻴﺔ اﻷﺧﺮى ﻋﲆ اﻷرض‪ .‬إذا ﻫﺮﺑﻨﺎ ﻣﻦ ﻣﺸﻜﻼﺗﻨﺎ ً‬
‫ﺑﺪﻻ ﻣﻦ اﻟﺘﻌﺎﻣُﻞ ﻣﻌﻬﺎ‬
‫— ﻛﻤﺎ ﰲ ﻣﺸﻜﻠﺔ ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل — ﻓﻘﺪ ﻧﻔﻮز ﺑﺎملﺮﻳﺦ )ﺣﺘﻰ اﻵن( وﻟﻜﻨﻨﺎ‬
‫ﺳﻮف ﻧﺨﴪ اﻷرض‪.‬‬
‫وﻛﺎﻟﻌﺎدة‪ ،‬ﻫﻨﺎك ﺟﺎﻧﺐ ﺳﻴﺎﳼ َ‬
‫ً‬
‫ﻓﺮﺻﺎ‬
‫آﺧﺮ ﻟﻬﺬا املﻮﺿﻮع‪ :‬إذ ﻳﻤﺘﻠﻚ ﺑﻌﺾ اﻟﻨﺎس‬
‫ً‬
‫ً‬
‫ﻣﻘﺎرﻧﺔ ﺑﺎﻵﺧﺮﻳﻦ‪ .‬املﺸﻜﻠﺔ ﻟﻴﺴﺖ ﻓﻘﻂ ﰲ أن ﺗﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫وﻣﺎﻻ وﻗﺪر ًة أﻛﱪ ﻋﲆ اﻟﻬﺮوب‬
‫اﻟﻔﻀﺎء واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﻬﻤﺎ ﺗﻜﻠﻔﺔ ﺣﻘﻴﻘﻴﺔ ﺑﺎﻟﻨﺴﺒﺔ إﱃ اﻷرض وأن ﻛ ﱠﻞ املﺎل ا ُملﺴﺘﺜﻤَ ﺮ‬
‫ﰲ ﻣﴩوﻋﺎت اﻟﻔﻀﺎء ﻟﻢ ﻳ َ‬
‫ُﻨﻔﻖ ﻋﲆ ﻣﺸﻜﻼت اﻷرض اﻟﺤﻘﻴﻘﻴﺔ ﻣﺜﻞ اﻟﺤﺮوب واﻟﻔﻘﺮ؛ ﺑﻞ‬
‫املﺸﻜﻠﺔ ﻫﻲ أن اﻷﺛﺮﻳﺎء ﺳﻴﻜﻮﻧﻮن ﻗﺎدِ رﻳﻦ ﻋﲆ اﻟﻬﺮوب ﻣﻦ اﻷرض اﻟﺘﻲ ﻳُﺪﻣﱢ ﺮوﻧﻬﺎ‪ ،‬ﰲ‬
‫ﻛﻮﻛﺐ ﻳﺴﺘﺤﻴﻞ اﻟﻌﻴﺶ ﻓﻴﻪ ﺑﺼﻮرة ﻣﺘﺰاﻳﺪة )اﻧﻈﺮ‪ ،‬ﻋﲆ‬
‫ﺣني ﻳﺠﺐ ﻋﲆ ﺑﻘﻴﺘﻨﺎ اﻟﺒﻘﺎء ﻋﲆ‬
‫ٍ‬
‫ﺳﺒﻴﻞ املﺜﺎل‪ ،‬زﻳﻤﺮﻣﺎن ‪ .(٢٠١٥‬وﻣﺜﻞ اﻟﺼﻮارﻳﺦ واﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ اﻷﺧﺮى‪ ،‬ﻳﻤﻜﻦ أن ﻳُﺼﺒﺢ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أداة ﻟ »ﺑﻘﺎء اﻷﻛﺜﺮ ﺛﺮاءً«‪ ،‬ﻛﻤﺎ أوﺿﺢ أﺣﺪ املﻌ ﱢﻠﻘني )‪.(Rushkoff 2018‬‬
‫ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﴐ‪ ،‬ﻳﺤﺪث ذﻟﻚ ﺑﺎﻟﻔﻌﻞ ﻣﻊ ﺗﻘﻨﻴﺎت أﺧﺮى‪ :‬ﻓﻔﻲ ﻣﺪن ﻣﺜﻞ دﻟﻬﻲ وﺑﻜني‪،‬‬
‫ﻳُﻌﺎﻧﻲ ﻣﻌﻈﻢ اﻟﻨﺎس ﻣﻦ ﺗﻠﻮﱡث اﻟﻬﻮاء‪ ،‬ﺑﻴﻨﻤﺎ ﻳﻄري اﻷﺛﺮﻳﺎء إﱃ ﻣﻨﺎﻃﻖ أﻗﻞ ﺗﻠﻮﺛًﺎ أو ﻳﺸﱰون‬
‫ﻫﻮاءً ﻧﻘﻴٍّﺎ ﺑﺎﺳﺘﺨﺪام ﺗﻘﻨﻴﺎت ﺗﻨﻘﻴﺔ اﻟﻬﻮاء‪ .‬ﻟﻴﺲ اﻟﺠﻤﻴﻊ ﱠ‬
‫ﻳﺘﻨﻔﺴﻮن اﻟﻬﻮاء ﻧﻔﺴﻪ‪ .‬واﻵن‪ ،‬ﻫﻞ‬
‫ﺳﻴُﺴﺎﻫﻢ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﰲ ﺗﻮﺳﻴﻊ ﻫﺬه اﻟﻔﺠﻮات ﺑني اﻷﺛﺮﻳﺎء واﻟﻔﻘﺮاء‪ ،‬ﻣﻤﺎ ﻳﺆدي إﱃ‬
‫َﴫﻓﻨﺎ اﻟﺬﻛﺎء‬
‫ﺣﻴﺎة أﻛﺜﺮ ﻛﺮﺑًﺎ وﻏري ﺻﺤﻴﺔ ﻟﻠﺒﻌﺾ وﺣﻴﺎة أﻓﻀﻞ ﻟﻠﺒﻌﺾ اﻵﺧﺮ؟ ﻫﻞ ﺳﻴ ِ‬
‫اﻻﺻﻄﻨﺎﻋﻲ ﻋﻦ املﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ؟ ﻳﺒﺪو أن ﻓﻜﺮة أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻨﺒﻐﻲ أن ﻳﺴﻌﻰ‬
‫إﱃ ﺗﺤﺴني اﻟﺤﻴﺎة ﻋﲆ اﻷرض‪ ،‬ﻟﻠﺠﻤﻴﻊ وﻟﻴﺲ ﻟﻔﺌﺔ ﻣﻌﻴﻨﺔ‪ ،‬ﻣﻊ اﻟﻮﺿﻊ ﰲ اﻻﻋﺘﺒﺎر أن ﺣﻴﺎﺗﻨﺎ‬
‫ﺗﻌﺘﻤﺪ ﻋﲆ ﻛﻮﻛﺐ اﻷرض‪ ،‬ﺗﻌﺪ ﻣﺘﻄﻠﺒًﺎ أﺧﻼﻗﻴٍّﺎ‪ .‬وﻗﺪ ﺗﻌﻴﻖ ﺑﻌﺾ ﴎدﻳﺎت اﻟﻔﻀﺎء ﺗﺤﻘﻴﻖ‬
‫ﻫﺬا اﻟﻬﺪف ً‬
‫ﺑﺪﻻ ﻣﻦ أن ﺗﺴﺎﻋﺪﻧﺎ ﰲ ﺗﺤﻘﻴﻘﻪ‪.‬‬
‫‪130‬‬

‫ﺗﺤﺪﱢي ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ :‬ﺣﻮل اﻷوﻟﻮﻳﺎت وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‬

‫ﻋﻮدة إﱃ اﻷرض‪ :‬ﻧﺤﻮ ذﻛﺎء اﺻﻄﻨﺎﻋﻲ ﻣﺴﺘﺪام‬
‫دﻋﻮﻧﻲ أﻋﻮد إﱃ املﺸﻜﻠﺔ اﻟﻌﻤﻠﻴﺔ ﺟﺪٍّا ﻟﻸوﻟﻮﻳﺎت واملﺨﺎﻃﺮ اﻟﺤﺎﻟﻴﺔ واﻟﺤﻘﻴﻘﻴﺔ ا ُملﺘﻌﻠﻘﺔ‬
‫ﱡ‬
‫ﺑﺘﻐري املﻨﺎخ‪ .‬ﻣﺎذا ﻳﺠﺐ أن ﺗﻔﻌﻞ أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﺳﻴﺎﺳﺎﺗﻪ ﰲ ﺿﻮء ﻫﺬه‬
‫اﻟﺘﺤﺪﻳﺎت؟ وﻋﻨﺪﻣﺎ ﺗﻜﻮن ﻫﻨﺎك ﺧﻼﻓﺎت ﺑﺸﺄن ﻗﻴﻤﺔ ﺣﻴﺎة اﻟﻜﺎﺋﻨﺎت ﻏري اﻟﺒﴩﻳﺔ‪ ،‬ﻓﻜﻴﻒ‬
‫ﻳُﻤﻜﻦ ﺣﻠﻬﺎ؟ ﺳﻴﺘﻔﻖ ﻣﻌﻈﻢ اﻟﻨﺎس ﻋﲆ أن ﺗﺴﻠﻴﻢ اﻟﺴﻴﻄﺮة إﱃ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ أو‬
‫ً‬
‫ﺣﻠﻮﻻ ﺟﻴﺪة‪ .‬ﻟﻜﻦ ﻣﺎ ﻫﻮ اﻟﺤﻞ اﻟﺠﻴﺪ؟ وﻫﻞ ﻳُﻮﺟَ ﺪ ﺣﻞ؟ إذا ﻣﺎ‬
‫اﻟﻬﺮوب ﻣﻦ اﻷرض ﻟﻴﺴﺖ‬
‫ً‬
‫إﺟﺎﺑﺔ ﻧﺎﻓﻌﺔ ﻋﲆ ﻫﺬه اﻷﺳﺌﻠﺔ‪ ،‬ﻓﺴﺘﻘﻮدﻧﺎ ﺑﺎﻟﴬورة إﱃ اﻷﺳﺌﻠﺔ اﻟﻔﻠﺴﻔﻴﺔ املﺘﻌﻠﻘﺔ‬
‫أﺟﺒﻨﺎ‬
‫ً‬
‫ﺑﻜﻴﻔﻴﺔ ﺗﻌﺎﻣُﻠﻨﺎ ﺑﻮﺻﻔﻨﺎ ﺑﴩًا ﻣﻊ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﻣﻊ ﺑﻴﺌﺘﻨﺎ‪ .‬ﻛﻤﺎ ﺗﻘﻮدﻧﺎ أﻳﻀﺎ إﱃ اﻟﻔﺼﻞ‬
‫املﺘﻌﻠﻖ ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ :‬ﻣﺎذا ﻳﻤﻜﻦ أن ﻳﻔﻌﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت ﻣﻦ أﺟﻠﻨﺎ‪،‬‬
‫وﻣﺎذا ﻳُﻤﻜﻨﻨﺎ أن ﱠ‬
‫ﻧﺘﻮﻗﻊ ﻣﻦ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻨﻄﻘﻴٍّﺎ؟‬
‫ﻣﻦ اﻟﻮاﺿﺢ أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﻤﻜﻦ أن ﻳﺴﺎﻋﺪﻧﺎ ﰲ اﻟﺘﺼﺪي ﻟﻠﻤﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ‪.‬‬
‫ﻓﻠﻨُﻔﻜﺮ ً‬
‫ﻣﺜﻼ ﰲ ﱡ‬
‫ﻧﺤﻮ اﺳﺘﺜﻨﺎﺋﻲ أن‬
‫ﺗﻐري املﻨﺎخ‪ .‬ﻳﺒﺪو أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻳﺴﺘﻄﻴﻊ ﻋﲆ ٍ‬
‫ﱠ‬
‫املﻌﻘﺪة‪ .‬إذ ﻳﻤﻜﻦ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺴﺎﻋﺪﺗﻨﺎ‬
‫ﻳﺴﺎﻋﺪﻧﺎ ﰲ ﻣﻮاﺟﻬﺔ ﻣﺜﻞ ﻫﺬه املﺸﻜﻼت‬
‫ﰲ دراﺳﺔ املﺸﻜﻠﺔ‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﻦ ﺧﻼل اﻛﺘﺸﺎف اﻷﻧﻤﺎط اﻟﺘﻲ ﻻ ﻳُﻤﻜﻨﻨﺎ رؤﻳﺘﻬﺎ‬
‫ﰲ اﻟﺒﻴﺎﻧﺎت اﻟﺒﻴﺌﻴﺔ‪ ،‬ﻧﻈ ًﺮا إﱃ ﻛﺜﺮة ﻫﺬه اﻟﺒﻴﺎﻧﺎت وﺗﻌﻘﻴﺪﻫﺎ‪ .‬ﻛﻤﺎ ﻳﻤﻜﻦ أن ﻳﺴﺎﻋﺪﻧﺎ ﰲ‬
‫اﻟﺤﻠﻮل‪ ،‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻣﻦ ﺧﻼل ﻣﺴﺎﻋﺪﺗﻨﺎ ﰲ اﻟﺘﻌﺎﻣﻞ ﻣﻊ ﺗﻌﻘﻴﺪ ﻋﻤﻠﻴﺎت اﻟﺘﻨﺴﻴﻖ وﰲ‬
‫ﺗﻨﻔﻴﺬ ﺗﺪاﺑري ﻣﺜﻞ ﺗﻘﻠﻴﻞ اﻧﺒﻌﺎﺛﺎت املﻮاد اﻟﻀﺎرة‪ ،‬ﻛﻤﺎ اﻗﱰح ﻓﻠﻮرﻳﺪي وآﺧﺮون )‪.(٢٠١٨‬‬
‫وﻋﲆ ﻧﻄﺎق أوﺳﻊ‪ ،‬ﻳﻤﻜﻦ أن ﻳﺴﺎﻋﺪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﻦ ﺧﻼل ﻣﺮاﻗﺒﺔ وﻧﻤﺬﺟﺔ اﻷﻧﻈﻤﺔ‬
‫اﻟﺒﻴﺌﻴﺔ وﺗﻤﻜني ﺣﻠﻮل ﻣﺜﻞ اﻟﺸﺒﻜﺎت اﻟﺬﻛﻴﺔ ﻟﻠﻄﺎﻗﺔ واﻟﺰراﻋﺔ اﻟﺬﻛﻴﺔ‪ ،‬ﻛﻤﺎ اﻗﱰﺣﺖ ﻣُﺪوﻧﺔ‬
‫املﻨﺘﺪى اﻻﻗﺘﺼﺎدي اﻟﻌﺎملﻲ )‪ .(Herweijer 2018‬وﻳﻤﻜﻦ ﻟﻠﺤﻜﻮﻣﺎت وﻟﻠﴩﻛﺎت ً‬
‫أﻳﻀﺎ أن‬
‫ﱠ‬
‫ﺗﺘﻮﱃ اﻷﻣﺮ ﻫﻨﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬اﺳﺘﺨﺪﻣﺖ ﺟﻮﺟﻞ ﺑﺎﻟﻔﻌﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻟﺘﻘﻠﻴﻞ‬
‫اﺳﺘﺨﺪام اﻟﻄﺎﻗﺔ ﰲ ﻣﺮاﻛﺰ اﻟﺒﻴﺎﻧﺎت‪.‬‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻻ ﻳﻌﻨﻲ ﻫﺬا ﺑﺎﻟﴬورة »إﻧﻘﺎذ اﻟﻜﻮﻛﺐ«‪ .‬ﻳﻤﻜﻦ ﻟﻠﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ً‬
‫أﻳﻀﺎ‬
‫أن ﻳُﺴﺒﺐ ﻣﺸﻜﻼت وﻳﺠﻌﻞ اﻷﻣﻮر أﺳﻮأ‪ .‬وﻟﻨﻔﻜﺮ ﻣﺮ ًة أﺧﺮى ﰲ اﻟﺘﺄﺛري اﻟﺒﻴﺌﻲ اﻟﺴﻠﺒﻲ اﻟﺬي‬
‫ِ‬
‫ﻳﻌﺘﻤﺪ‬
‫ﻳﻤﻜﻦ أن ﻳُﺨﻠﻔﻪ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻧﻈ ًﺮا إﱃ اﻟﻄﺎﻗﺔ واﻟ ِﺒﻨﻰ اﻟﺘﺤﺘﻴﺔ واملﻮاد اﻟﺘﻲ‬
‫ﻋﻠﻴﻬﺎ‪ .‬وﻟﻨﻔﻜﺮ ﻟﻴﺲ ﻓﻘﻂ ﰲ اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ وﻟﻜﻦ ً‬
‫أﻳﻀﺎ ﰲ إﻧﺘﺎﺟﻪ‪ :‬ﻗﺪ ﺗﻜﻮن‬
‫اﻟﻜﻬﺮﺑﺎء ﻣُﻨﺘﺠَ ﺔ ﺑﻄﺮق ﻏري ﻣﺴﺘﺪاﻣﺔ‪ ،‬ﻛﻤﺎ أن إﻧﺘﺎج اﻷﺟﻬﺰة املﺪﻋﻮﻣﺔ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫ﻳﺴﺘﻬﻠﻚ اﻟﻄﺎﻗﺔ واملﻮاد اﻟﺨﺎم وﻳﻨﺘﺞ ﻧﻔﺎﻳﺎت‪ .‬أو ﻓﻠﻨﻔﻜﺮ ﰲ »اﻟﺪﻓﻊ اﻟﺬاﺗﻲ« اﻟﺬي اﻗﱰﺣﻪ‬
‫‪131‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫ﺑﻄﺮق ﺑﻴﺌﻴﺔ‬
‫ﻓﻠﻮرﻳﺪي وآﺧﺮون؛ إذ ﻳﻘﱰﺣﻮن أن اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﺪ ﻳُﺴﺎﻋﺪﻧﺎ ﰲ اﻟﺘﴫف‬
‫ٍ‬
‫ﺟﻴﺪة ﻋﻦ ﻃﺮﻳﻖ ﻣﺴﺎﻋﺪﺗﻨﺎ ﰲ اﻻﻟﺘﺰام ﺑﺨﻴﺎرﻧﺎ املﻔﺮوض ذاﺗﻴٍّﺎ‪ .‬وﻟﻜﻦ ﻫﺬا اﻷﻣﺮ ﻳﻨﻄﻮي‬
‫ﻋﲆ ﻣَ ﺨﺎﻃﺮه اﻷﺧﻼﻗﻴﺔ اﻟﺨﺎﺻﺔ‪ :‬ﻓﻠﻴﺲ ﻣﻦ اﻟﻮاﺿﺢ أﻧﻪ ﻳﺤﱰم اﺳﺘﻘﻼل اﻟﺒﴩ وﻛﺮاﻣﺘﻬﻢ‪،‬‬
‫ﻛﻤﺎ ﻳﺪﻋﻲ اﻟ ُﻜﺘﱠﺎب‪ ،‬وﻗﺪ ﻳﺴري ﰲ اﺗﺠﺎه اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺤﻤﻴﺪ اﻟﺬي ﻳﻌﺘﻨﻲ ﺑﺎﻟﺒﴩ ﻟﻜﻨﻪ‬
‫ﻳُﺪﻣﺮ ﺣﺮﻳﺘﻬﻢ وﻳُﺴﺎﻫﻢ ﰲ ﻣﺸﻜﻠﺔ ﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‪ .‬وﻫﻨﺎك ﻋﲆ اﻷﻗﻞ ﺧﻄﻮرة ﻓﺮض‬
‫أﺷﻜﺎل ﺟﺪﻳﺪة ﻣﻦ اﻟﺴﻠﻄﺔ اﻷﺑﻮﻳﺔ واﻻﺳﺘﺒﺪاد‪ .‬ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬ﻗﺪ ﻳﺘﻤﺎﳽ اﺳﺘﺨﺪام اﻟﺬﻛﺎء‬
‫ٍ‬
‫اﻻﺻﻄﻨﺎﻋﻲ ملﻮاﺟﻬﺔ ﱡ‬
‫ﺗﻐري املﻨﺎخ ﻣﻊ اﻟﻨﻈﺮة اﻟﻌﺎملﻴﺔ اﻟﺘﻲ ﺗُﺤﻮﱢل اﻟﻌﺎﻟﻢ إﱃ ﻣﺠﺮد ﻣُﺴﺘﻮدع‬
‫ﺑﻴﺎﻧﺎت وﻣﻊ اﻟﺮؤﻳﺔ اﻟﺘﻲ ﺗﺨﺘﺰل ذﻛﺎء اﻹﻧﺴﺎن إﱃ ﻣﻌﺎﻟﺠﺔ اﻟﺒﻴﺎﻧﺎت؛ ﺑﻞ رﺑﻤﺎ ﻧﻮع أدﻧﻰ‬
‫ﻣﻦ ﻣﻌﺎﻟﺠﺔ اﻟﺒﻴﺎﻧﺎت ﻳﺘﻄ ﱠﻠﺐ اﻟﺘﺤﺴني ﺑﻮاﺳﻄﺔ اﻵﻻت‪ .‬وﻣﻦ ﻏري ا ُملﺮﺟﱠ ﺢ أن ﺗﻌﻴﺪ ﻣﺜﻞ ﻫﺬه‬
‫اﻟﺮؤى ﺗﺸﻜﻴﻞ ﻋﻼﻗﺘﻨﺎ ﺑﺎﻟﺒﻴﺌﺔ ﺑﻄﺮﻳﻘﺔ ﺗُ ﱢ‬
‫ﺨﻔﻒ اﻟﺘﺤﺪﻳﺎت ﻣﺜﻞ ﱡ‬
‫ﺗﻐري املﻨﺎخ واملﺸﻜﻼت املﺸﺎر‬
‫إﻟﻴﻬﺎ ﺑﻤﺼﻄﻠﺢ اﻟﺘﺄﺛري اﻟﺒﴩي‪.‬‬
‫ﻧﻮاﺟﻪ ً‬
‫أﻳﻀﺎ ﺧﻄﺮ اﻟﻨﺰﻋﺔ ﻟﻠﺤﻠﻮل اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ ﺑﻤﻌﻨﻰ أن اﻻﻗﱰاﺣﺎت ﻻﺳﺘﺨﺪام‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ملﻌﺎﻟﺠﺔ املﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ ﻳُﻤﻜﻦ أن ﺗﻔﱰض أن ﻫﻨﺎك ٍّ‬
‫ﺣﻼ ﻧﻬﺎﺋﻴٍّﺎ ﻟﺠﻤﻴﻊ‬
‫املﺸﻜﻼت‪ ،‬وأن اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﺣﺪَﻫﺎ ﻳﻤﻜﻦ أن ﺗُﺠﻴﺐ ﻋﻦ أﺻﻌﺐ أﺳﺌﻠﺘﻨﺎ‪ ،‬وأﻧﻨﺎ ﻳﻤﻜﻦ أن‬
‫ﻧﺤﻞ املﺸﻜﻼت ﺑﺎﻟﻜﺎﻣﻞ ﻋﻦ ﻃﺮﻳﻖ اﺳﺘﺨﺪام اﻟﺬﻛﺎء اﻟﺒﴩي أو اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وﻟﻜﻦ املﺸﻜﻼت‬
‫اﻟﺒﻴﺌﻴﺔ ﻻ ﻳﻤﻜﻦ ﺣ ﱡﻠﻬﺎ ﻋﻦ ﻃﺮﻳﻖ اﻟﺬﻛﺎء اﻟﺘﻜﻨﻮﻟﻮﺟﻲ واﻟﻌﻠﻤﻲ؛ ﻓﻬﻲ ﻣﺮﺗﺒﻄﺔ ً‬
‫أﻳﻀﺎ ﺑﺎملﺸﻜﻼت‬
‫اﻟﺴﻴﺎﺳﻴﺔ واﻻﺟﺘﻤﺎﻋﻴﺔ اﻟﺘﻲ ﻻ ﻳﻤﻜﻦ اﻟﺘﺼﺪي ﻟﻬﺎ ﺑﺎﻟﻜﺎﻣﻞ ﻋﻦ ﻃﺮﻳﻖ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﺣﺪَﻫﺎ‪.‬‬
‫ٍ‬
‫ﻣﺸﻜﻼت ﺑﴩﻳﺔ‪ .‬واﻟﺮﻳﺎﺿﻴﺎت وذُرﻳﺘﻬﺎ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‬
‫ﻛﻤﺎ أن املﺸﻜﻼت اﻟﺒﻴﺌﻴﺔ داﺋﻤً ﺎ ﻣﺎ ﺗﻜﻮن‬
‫ﻫﻲ أدوات ﻣُﻔﻴﺪة ﺟﺪٍّا‪ ،‬وﻟﻜﻨﻬﺎ ﻣﺤﺪودة ﻓﻴﻤﺎ ﻳﺘﻌﻠﻖ ﺑﻔﻬﻢ املﺸﻜﻼت اﻟﺒﴩﻳﺔ واﻟﺘﻌﺎﻣُﻞ‬
‫ﻣﻌﻬﺎ‪ .‬ﻋﲆ ﺳﺒﻴﻞ املﺜﺎل‪ ،‬ﻗﺪ ﺗﺘﻌﺎ َرض اﻟﻘِ ﻴَﻢ‪ .‬وﻟﻦ ﻳﺴﺘﻄﻴﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﺑﺎﻟﴬورة‬
‫أن ﻳُﺴﺎﻋﺪﻧﺎ ﰲ اﻹﺟﺎﺑﺔ ﻋﻦ اﻟﺴﺆال ﺣﻮل اﻷوﻟﻮﻳﺎت‪ ،‬وﻫﻮ ﺳﺆال أﺧﻼﻗﻲ وﺳﻴﺎﳼ ﻣُﻬﻢ ﻳﺠﺐ‬
‫أن ﻧﱰك ﻟﻠﺒﴩ اﻹﺟﺎﺑﺔ ﻋﻨﻪ‪ .‬وﺗُﻌ ﱢﻠﻤﻨﺎ اﻟﻌﻠﻮم اﻹﻧﺴﺎﻧﻴﺔ واﻻﺟﺘﻤﺎﻋﻴﺔ أن ﻧﻜﻮن ﺣﺬِرﻳﻦ ﺟﺪٍّا‬
‫ﺑﺸﺄن اﻟﺤﻠﻮل »اﻟﻨﻬﺎﺋﻴﺔ«‪.‬‬
‫ُ‬
‫ﻋﻼو ًة ﻋﲆ ذﻟﻚ‪ ،‬اﻟﺒﴩ ﻟﻴﺴﻮا اﻟﻮﺣﻴﺪِﻳﻦ اﻟﺬﻳﻦ ﺗﻮاﺟﻬﻬﻢ ﻣﺸﻜﻼت؛ ﻓﺎﻟﻜﺎﺋﻨﺎت ﻏري‬
‫اﻟﺒﴩﻳﺔ ً‬
‫أﻳﻀﺎ ﺗﻮاﺟﻬﻬﺎ ﺻﻌﻮﺑﺎت‪ ،‬واﻟﺘﻲ ﻏﺎﻟﺒًﺎ ﻣﺎ ﺗُﻬﻤَ ﻞ ﰲ املﻨﺎﻗﺸﺎت اﻟﺨﺎﺻﺔ ﺑﻤﺴﺘﻘﺒﻞ‬
‫ﻳﺠﺐ أن ﻧﻬﺮب ﻣﻦ اﻷرض‪ ،‬أو اﻟﺮؤﻳﺔ اﻟﻌﺎملﻴﺔ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ .‬وأﺧريًا‪ ،‬اﻟﺮأي اﻟﻘﺎﺋﻞ ﺑﺄﻧﻨﺎ ِ‬
‫اﻟﺘﻲ ﺗﻘﻮل إن ﻛﻞ ﳾءٍ ﻋﺒﺎرة ﻋﻦ ﺑﻴﺎﻧﺎت ﻧﺴﺘﻄﻴﻊ ﻧﺤﻦ اﻟﺒﴩ اﻟﺘﻼﻋُ ﺐ ﺑﻬﺎ ﺑﻤﺴﺎﻋﺪة اﻵﻻت‪،‬‬
‫ً‬
‫ﻧﻄﺎﻗﺎ‬
‫أﺷﻜﺎل أوﺳﻊ‬
‫ﻳﻤﻜﻦ أن ﻳﺆدﱢﻳﺎ ﰲ اﻟﻨﻬﺎﻳﺔ إﱃ ﺗﻮﺳﻴﻊ اﻟﻔﺠﻮة ﺑني اﻷﻏﻨﻴﺎء واﻟﻔﻘﺮاء وإﱃ‬
‫ٍ‬
‫‪132‬‬

‫ﺗﺤﺪﱢي ﱡ‬
‫ﺗﻐري املﻨﺎخ‪ :‬ﺣﻮل اﻷوﻟﻮﻳﺎت وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‬

‫ﻣﻦ اﻻﺳﺘﻐﻼل واﻻﻧﺘﻬﺎﻛﺎت ﻟﻠﻜﺮاﻣﺔ اﻹﻧﺴﺎﻧﻴﺔ‪ ،‬ﺑﺎﻹﺿﺎﻓﺔ إﱃ ﺗﻬﺪﻳﺪ ﺣﻴﺎة اﻷﺟﻴﺎل اﻟﻘﺎدﻣﺔ‬
‫ﻋﻦ ﻃﺮﻳﻖ املﺨﺎﻃﺮة ﺑﺘﺪﻣري ﻇﺮوف اﻟﺤﻴﺎة ﻋﲆ ﻛﻮﻛﺒﻨﺎ‪ .‬إﻧﻨﺎ ﻧﺤﺘﺎج إﱃ اﻟﺘﻔﻜري اﻟﻌﻤﻴﻖ ﰲ‬
‫ﻛﻴﻔﻴﺔ ﺑﻨﺎء ﻣﺠﺘﻤﻌﺎت وﺑﻴﺌﺎت ﻣُﺴﺘﺪاﻣﺔ؛ إﻧﻨﺎ ﻧﺤﺘﺎج إﱃ اﻟﺘﻔﻜري اﻟﺒﴩي‪.‬‬
‫اﻟﺬﻛﺎء واﻟﺤﻜﻤﺔ‬
‫وﻣﻊ ذﻟﻚ‪ ،‬ﻓﻄﺮﻳﻘﺔ ﺗﻔﻜري اﻟﺒﴩ ﻟﻬﺎ ﺟﻮاﻧﺐ ﻣُﺘﻌﺪدة ً‬
‫أﻳﻀﺎ‪ .‬واﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻣﺮﺗﺒﻂ‬
‫ﺑﻨﻮع واﺣﺪ ﻣﻦ أﻧﻮاع اﻟﺘﻔﻜري اﻟﺒﴩي واﻟﺬﻛﺎء اﻟﺒﴩي‪ :‬اﻟﻨﻮع املﻌﺮﰲ اﻷﻛﺜﺮ ﺗﺠﺮﻳﺪًا‪ .‬ﻫﺬا‬
‫ٍ‬
‫اﻟﻨﻮع ﻣﻦ اﻟﺘﻔﻜري ﻗﺪ أﺛﺒﺖ ﻧﺠﺎﺣً ﺎ ﻛﺒريًا‪ ،‬وﻟﻜﻨﻪ ﻟﻪ ﻗﻴﻮده وﻫﻮ ﻟﻴﺲ اﻟﻨﻮع اﻟﻮﺣﻴﺪ ﻣﻦ‬
‫اﻟﺘﻔﻜري اﻟﺬي ﻳُﻤﻜﻦ أو ﻳﺠﺐ ﻋﻠﻴﻨﺎ ﻣُﻤﺎرﺳﺘﻪ‪ .‬واﻹﺟﺎﺑﺔ ﻋﻦ اﻷﺳﺌﻠﺔ اﻷﺧﻼﻗﻴﺔ واﻟﺴﻴﺎﺳﻴﺔ‬
‫ﺑﺸﻜﻞ أﻓﻀﻞ ﻣﻊ‬
‫ا ُملﺘﻌﻠﻘﺔ ﺑﻜﻴﻔﻴﺔ اﻟﻌﻴﺶ‪ ،‬وﻛﻴﻔﻴﺔ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ ﺑﻴﺌﺘﻨﺎ‪ ،‬وﻛﻴﻔﻴﺔ اﻟﺘﻌﺎﻣُﻞ‬
‫ٍ‬
‫اﻟﻜﺎﺋﻨﺎت اﻟﺤﻴﺔ ﻏري اﻟﺒﴩﻳﺔ ﺗﺘﻄ ﱠﻠﺐ ﻣﺎ ﻫﻮ أﻛﺜﺮ ﻣﻦ اﻟﺬﻛﺎء اﻟﺒﴩي اﻟﺘﺠﺮﻳﺪي )ﻋﲆ ﺳﺒﻴﻞ‬
‫املﺜﺎل‪ ،‬اﻟﺤُ ﺠﺞ‪ ،‬واﻟﻨﻈﺮﻳﺎت‪ ،‬واﻟﻨﻤﺎذج( أو اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻷﻧﻤﺎط ﺑﻮاﺳﻄﺔ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪.‬‬
‫أﺷﺨﺎص أذﻛﻴﺎء وآﻻت ذﻛﻴﺔ‪ ،‬وﻟﻜﻨﻨﺎ ً‬
‫ٍ‬
‫ﺑﺤﺎﺟﺔ إﱃ اﻟﺤﺪس واﻟﺨﱪة اﻟﺘﻲ ﻻ‬
‫أﻳﻀﺎ‬
‫ﻧﺤﺘﺎج إﱃ‬
‫ٍ‬
‫ُ‬
‫ً‬
‫ﱢ‬
‫اﺳﺘﺠﺎﺑﺔ إﱃ‬
‫اﻟﺘﺤﲇ ﺑﺎﻟﺤﻜﻤﺔ اﻟﻌﻤﻠﻴﺔ واﻟﻔﻀﻴﻠﺔ‬
‫وﺻﻔﻬﺎ ﺑﻮﺿﻮح ﻛﺎﻣﻞ‪ ،‬وﻧﺤﺘﺎج إﱃ‬
‫ﻳﻤﻜﻦ‬
‫املﺸﻜﻼت واملﻮاﻗﻒ املﺎدﻳﺔ وﻣﻦ أﺟﻞ ﺗﺤﺪﻳﺪ أوﻟﻮﻳﺎﺗﻨﺎ‪ .‬ﻗﺪ ﺗﺴﺘﻨري ﻫﺬه اﻟﺤﻜﻤﺔ ﺑﺎﻟﻌﻤﻠﻴﺎت‬
‫املﻌﺮﻓﻴﺔ اﻟﺘﺠﺮﻳﺪﻳﺔ وﺑﺘﺤﻠﻴﻞ اﻟﺒﻴﺎﻧﺎت‪ ،‬وﻟﻜﻨﻬﺎ ﺗﺴﺘﻨﺪ ً‬
‫أﻳﻀﺎ إﱃ اﻟﺘﺠﺎرب ا ُمل ﱢ‬
‫ﺘﺠﺴﺪة اﻟﺨﺎﺻﺔ‬
‫ﺑﺎﻟﻌﻼﻗﺎت واملﻮاﻗﻒ اﻟﺘﻲ ﻧﻤ ﱡﺮ ﺑﻬﺎ ﰲ اﻟﻌﺎﻟﻢ‪ ،‬وإﱃ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ أﺷﺨﺎص آﺧﺮﻳﻦ‪ ،‬وﻣﻊ املﺎدﻳﺔ‪،‬‬
‫وﻣﻊ ﺑﻴﺌﺘﻨﺎ اﻟﻄﺒﻴﻌﻴﺔ‪ .‬وﻣﻦ ا ُملﺤﺘﻤﻞ أن ﻳﻌﺘﻤﺪ ﻧﺠﺎﺣﻨﺎ ﰲ اﻟﺘﺼﺪي ﻟﻠﻤﺸﻜﻼت اﻟﻜﺒرية اﻟﺘﻲ‬
‫ﺗُﻮاﺟﻬﻨﺎ ﰲ ﻋﴫﻧﺎ ﻋﲆ ﻣﺰﻳﺞ ﻣﻦ اﻟﺬﻛﺎء اﻟﺘﺠﺮﻳﺪي — اﻟﺒﴩي واﻻﺻﻄﻨﺎﻋﻲ — واﻟﺤﻜﻤﺔ‬
‫اﻟﻌﻤﻠﻴﺔ املﻠﻤﻮﺳﺔ اﻟﺘﻲ ﺗﻢ ﺗﻄﻮﻳﺮﻫﺎ ﻋﲆ أﺳﺎس اﻟﺘﺠﺎرب وا ُملﻤﺎرﺳﺎت اﻟﺒﴩﻳﺔ املﻠﻤﻮﺳﺔ‬
‫واﻟﺨﺎﺻﺔ ﺑﺎملﻮاﻗﻒ‪ ،‬ﺑﻤﺎ ﰲ ذﻟﻚ ﺗﺠﺎرﺑﻨﺎ ﻣﻊ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬وأﻳٍّﺎ ﻛﺎن اﻻﺗﺠﺎه اﻟﺬي ﺳﻴﺴري‬
‫ُﻮاﺟﻬﻮن ﺗﺤﺪﱢي ﺗﻄﻮﻳﺮ ﻫﺬا‬
‫ﻓﻴﻪ ﺗﻄﻮﻳﺮ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ ،‬ﻓﺈن اﻟﺒﴩ وﺣﺪَﻫﻢ ﻫﻢ ﻣَ ﻦ ﻳ ِ‬
‫اﻟﻨﻮع اﻷﺧري ﻣﻦ املﻌﺮﻓﺔ واﻟﺘﻌﻠﻢ‪ .‬وﻋﲆ اﻟﺒﴩ أن ﻳﺘﺼﺪﱠوا ﻟﻪ‪ .‬ﻓﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ ﻗﺎدر ﻋﲆ‬
‫اﻟﺘﻌ ﱡﺮف ﻋﲆ اﻷﻧﻤﺎط‪ ،‬وﻟﻜﻦ اﻟﺤﻜﻤﺔ ﻻ ﻳﻤﻜﻦ ﺗﻔﻮﻳﻀﻬﺎ إﱃ اﻵﻻت‪.‬‬

‫‪133‬‬

‫ﻣﴪد اﳌﺼﻄﻠﺤﺎت‬

‫اﻻﺑﺘﻜﺎر املﺴﺌﻮل‪ :‬ﻧﻬﺞ ﻳﻤﻴﻞ إﱃ ﺟﻌﻞ اﻻﺑﺘﻜﺎر أﻛﺜﺮ أﺧﻼﻗﻴﺔ وﻣﺴﺌﻮﻟﻴﺔ ﻋﲆ اﻟﺼﻌﻴﺪ‬
‫املﺠﺘﻤﻌﻲ‪ ،‬وﻳﻨﻄﻮي ﻋﺎد ًة ﻋﲆ ﺗﻀﻤني اﻷﺧﻼق ﰲ اﻟﺘﺼﻤﻴﻢ وﻣﺮاﻋﺎة آراء أﺻﺤﺎب اﻟﺸﺄن‬
‫وﻣﺼﺎﻟﺤﻬﻢ‪.‬‬
‫اﻷﺧﻼﻗﻴﺎت اﻹﻳﺠﺎﺑﻴﺔ‪ :‬اﻷﺧﻼﻗﻴﺎت املﺮﺗﺒﻄﺔ ﺑﺎﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻳﻨﺒﻐﻲ أن ﻧﻌﻴﺶ ﺑﻬﺎ )ﻣﻌً ﺎ(‪،‬‬
‫وﺗﺴﺘﻨﺪ إﱃ رؤﻳﺔ ﻟﻠﺤﻴﺎة اﻟﺠﻴﺪة واملﺠﺘﻤﻊ اﻟﺠﻴﺪ‪ .‬وﺗﺘﻨﺎﻗﺾ ﻣﻊ اﻷﺧﻼﻗﻴﺎت اﻟﺴﻠﺒﻴﺔ‪ ،‬اﻟﺘﻲ‬
‫ﺗﻀﻊ ﻗﻴﻮدًا وﺗﺤﺪد ﻣﺎ ﻳﻨﺒﻐﻲ أﻻ ﻧﻔﻌﻠﻪ‪.‬‬
‫اﻷﺧﻼﻗﻴﺎت ا ُملﻀﻤﱠ ﻨﺔ ﰲ اﻟﺘﺼﻤﻴﻢ‪ :‬ﻧﻬﺞ ﻷﺧﻼﻗﻴﺎت اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ وﻋﻨﴫ أﺳﺎﳼ ﰲ‬
‫»اﻻﺑﺘﻜﺎر املﺴﺌﻮل« اﻟﺬي ﻳﻬﺪف إﱃ دﻣﺞ اﻷﺧﻼﻗﻴﺎت ﰲ ﻣﺮﺣﻠﺔ ﺗﺼﻤﻴﻢ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬
‫وﺗﻄﻮﻳﺮﻫﺎ‪ .‬وﰲ ﺑﻌﺾ اﻷﺣﻴﺎن‪ ،‬ﻧُﺴﻤﻴﻬﺎ »ﺗﻀﻤني اﻟﻘﻴﻢ ﰲ اﻟﺘﺼﻤﻴﻢ«‪ .‬وﻣﻦ املﺼﻄﻠﺤﺎت‬
‫ﱠ‬
‫اﻟﺤﺴﺎس ﻟﻠﻘِ ﻴَﻢ« و»اﻟﺘﺼﻤﻴﻢ ا ُملﺘﻤﺎﳾ ﻣﻊ اﻷﺧﻼق«‪.‬‬
‫املﺸﺎﺑﻬﺔ ﻟﻬﺬا املﺼﻄﻠﺢ »اﻟﺘﺼﻤﻴﻢ‬
‫ﻳﺠﺐ أن ﻳُﻌﺰزوا أﻧﻔﺴﻬﻢ ﻣﻦ ﺧﻼل اﻟﺘﻘﻨﻴﺎت‬
‫ﺗﺠﺎوز اﻹﻧﺴﺎﻧﻴﺔ‪ :‬اﻻﻋﺘﻘﺎد ﺑﺄن اﻟﺒﴩ ِ‬
‫ا ُملﺘﻘﺪﻣﺔ‪ ،‬وﺑﻬﺬه اﻟﻄﺮﻳﻘﺔ ﻳﺘﺠﺎوزون ﺣﺎﻟﺘﻬﻢ اﻹﻧﺴﺎﻧﻴﺔ؛ ﺑﻤﻌﻨﻰ أن اﻹﻧﺴﺎﻧﻴﺔ ﻳﺠﺐ أن‬
‫ﺗﻨﺘﻘِ ﻞ إﱃ ﻣﺮﺣﻠﺔ ﺟﺪﻳﺪة‪ .‬وﻫﺬه ً‬
‫أﻳﻀﺎ ﺣﺮﻛﺔ دوﻟﻴﺔ‪.‬‬
‫اﻟﺘﺤﻴﺰ‪ :‬اﻟﺘﻤﻴﻴﺰ ﺿﺪ أو ﻟﺼﺎﻟﺢ أﻓﺮاد ﺑﺄﻋﻴُﻨﻬﻢ أو ﻣﺠﻤﻮﻋﺎت ﺑﻌﻴﻨﻬﺎ‪ .‬ﰲ ﺳﻴﺎق اﻷﺧﻼﻗﻴﺎت‬
‫واﻟﺴﻴﺎﺳﺔ‪ ،‬ﻳُﺜﺎر اﻟﺴﺆال ﺣﻮل ﻣﺎ إذا ﻛﺎن ﺗَﺤﻴﱡﺰ ﻣﻌني ﻇﺎ ًملﺎ أو ﻏري ﻋﺎدل‪.‬‬
‫ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ‪ :‬آﻟﺔ أو ﺑﺮﻧﺎﻣﺞ ﻳُﻤﻜﻨﻪ أن ﻳﺘﻌﻠﻢ ﺗﻠﻘﺎﺋﻴٍّﺎ‪ :‬ﻟﻴﺲ ﺑﺎﻟﻄﺮﻳﻘﺔ اﻟﺘﻲ ﻳﺘﻌ ﱠﻠﻢ ﺑﻬﺎ اﻟﺒﴩ‪،‬‬
‫وﻟﻜﻦ ﺑﻨﺎءً ﻋﲆ ﻋﻤﻠﻴﺔ ﺣﺴﺎﺑﻴﺔ وإﺣﺼﺎﺋﻴﺔ‪ .‬ﻳﻤﻜﻦ ﻟﺨﻮارزﻣﻴﺎت اﻟﺘﻌ ﱡﻠﻢ‪ ،‬ﻣﻦ ﺧﻼل ﺗﻐﺬﻳﺘﻬﺎ‬
‫ﺑﺎﻟﺒﻴﺎﻧﺎت‪ ،‬ﺗﺤﺪﻳﺪ اﻷﻧﻤﺎط أو اﻟﻘﻮاﻋﺪ ﰲ اﻟﺒﻴﺎﻧﺎت وإﺟﺮاء ﺗﻮﻗﻌﺎت ﻟﻠﺒﻴﺎﻧﺎت املﺴﺘﻘﺒﻠﻴﺔ‪.‬‬

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬

‫اﻟﺘﻌ ﱡﻠﻢ اﻟﻌﻤﻴﻖ‪ :‬ﺷﻜﻞ ﻣﻦ أﺷﻜﺎل »ﺗﻌﻠﻢ اﻵﻟﺔ« ﻳﺴﺘﺨﺪم اﻟﺸﺒﻜﺎت اﻟﻌﺼﺒﻴﺔ املﻜﻮﻧﺔ ﻣﻦ ﻋﺪة‬
‫ﻃﺒﻘﺎت ﻣﻦ »اﻟﺨﻼﻳﺎ اﻟﻌﺼﺒﻴﺔ«‪ :‬وﺣﺪات ﻣُﻌﺎﻟﺠﺔ ﺑﺴﻴﻄﺔ ﻣﱰاﺑﻄﺔ ﻓﻴﻤﺎ ﺑﻴﻨﻬﺎ وﺗﺘﻔﺎﻋﻞ‪.‬‬
‫اﻟﺘﻔ ﱡﺮد اﻟﺘﻜﻨﻮﻟﻮﺟﻲ‪ :‬اﻟﻔﻜﺮة اﻟﺘﻲ ﺗﻘﻮل ﺑﺄﻧﻪ ﺳﺘﺤني ﻟﺤﻈﺔ ﰲ ﺗﺎرﻳﺦ اﻹﻧﺴﺎن ﻋﻨﺪﻣﺎ‬
‫ﻳﺠﻠﺐ اﻧﻔﺠﺎر ﰲ اﻟﺬﻛﺎء اﻵﱄ ﺗﻐﻴريًا ﺟﺬرﻳٍّﺎ ﰲ ﺣﻀﺎرﺗﻨﺎ ﻳﺠﻌﻠﻨﺎ ﻻ ﻧﻔﻬﻢ ﺑﻌﺪﻫﺎ ﻣﺎ ﻳﺤﺪث‪.‬‬
‫ﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي )اﻷﻧﺜﺮوﺑﻮﺳني(‪ :‬اﻟﺤﻘﺒﺔ اﻟﺠﻴﻮﻟﻮﺟﻴﺔ اﻟﺤﺎﻟﻴﺔ املﺰﻋﻮﻣﺔ اﻟﺘﻲ زادت‬
‫ﻓﻴﻬﺎ ﻗﻮة اﻟﺒﴩ وﺗﺄﺛريﻫﻢ ﻋﲆ اﻷرض وﻧﻈﻤﻬﺎ اﻟﺒﻴﺌﻴﺔ‪ ،‬ﻣﻤﺎ ﺟﻌﻞ اﻟﺒﴩ ﻗﻮة ﺟﻴﻮﻟﻮﺟﻴﺔ‪.‬‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‪ :‬اﻟﺬﻛﺎء اﻟﺬي ﺗُﻈﻬﺮه أو ﺗُﺤﺎﻛﻴﻪ اﻟﻮﺳﺎﺋﻞ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺔ‪ .‬ﻏﺎﻟﺒًﺎ ﻣﺎ‬
‫ﻳُﻔﱰض أن ﻣﻌﻨﻰ »اﻟﺬﻛﺎء« ﰲ ﻫﺬا اﻟﺘﻌﺮﻳﻒ ﻳﺴﺘﻨﺪ إﱃ ﻣﻘﺎﻳﻴﺲ اﻟﺬﻛﺎء اﻟﺒﴩي‪ ،‬وﻳ َ‬
‫ُﻘﺼﺪ‬
‫ﺑﻪ اﻟﻘﺪرات واﻟﺴﻠﻮﻛﻴﺎت اﻟﺬﻛﻴﺔ اﻟﺘﻲ ﻳُﻈﻬﺮﻫﺎ اﻟﺒﴩ‪ .‬وﻳﻤﻜﻦ ً‬
‫أﻳﻀﺎ أن ﻳُﺸري املﺼﻄﻠﺢ‬
‫إﱃ اﻟﻌﻠﻢ أو إﱃ اﻟﺘﻘﻨﻴﺎت‪ ،‬ﻣﺜﻞ ﺧﻮارزﻣﻴﺎت اﻟﺘﻌﻠﻢ‪.‬‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺠﺪﻳﺮ ﺑﺎﻟﺜﻘﺔ‪ :‬اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬي ﻳﻤﻜﻦ ﻟﻺﻧﺴﺎن اﻟﻮﺛﻮق ﻓﻴﻪ‪.‬‬
‫ﻳﻤﻜﻦ أن ﺗُﺸري ﴍوط ﻫﺬه اﻟﺜﻘﺔ إﱃ ﻣﺒﺎدئ أﺧﻼﻗﻴﺔ )أﺧﺮى( ﻣﺜﻞ اﻟﻜﺮاﻣﺔ اﻹﻧﺴﺎﻧﻴﺔ‬
‫واﺣﱰام ﺣﻘﻮق اﻹﻧﺴﺎن‪ ،‬وﻣﺎ إﱃ ذﻟﻚ‪ ،‬و‪/‬أو إﱃ اﻟﻌﻮاﻣﻞ اﻻﺟﺘﻤﺎﻋﻴﺔ واﻟﺘﻘﻨﻴﺔ اﻟﺘﻲ ﺗﺆﺛﺮ‬
‫ﻓﻴﻤﺎ إذا ﻛﺎن اﻟﻨﺎس ﻳﺮﻏﺒﻮن ﰲ اﺳﺘﺨﺪام اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‪ .‬اﺳﺘﺨﺪام ﻣﺼﻄﻠﺢ »اﻟﺜﻘﺔ« ﻓﻴﻤﺎ‬
‫ﻳﺘﻌﻠﻖ ﺑﺎﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ ﻣُﺜري ﻟﻠﺠﺪل‪.‬‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺮﻣﺰي‪ :‬اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬي ﻳﻌﺘﻤﺪ ﻋﲆ اﻟﺘﻤﺜﻴﻼت اﻟﺮﻣﺰﻳﺔ‬
‫ﻟﻠﻤﻬﺎم املﻌﺮﻓﻴﺔ اﻟﻌﻠﻴﺎ‪ ،‬ﻣﺜﻞ اﻟﺘﻔﻜري املﺠﺮد واﺗﺨﺎذ اﻟﻘﺮارات‪ .‬وﻳﻤﻜﻦ أن ﻳﺴﺘﺨﺪم ﺷﺠﺮة‬
‫اﺗﺨﺎذ اﻟﻘﺮار وﻳﺄﺧﺬ ﺷﻜﻞ ﻧﻈﺎم ﺧﺒري ﻳﺘﻄﻠﺐ ﻣﺪﺧﻼت ﻣﻦ ﺧﱪاء املﺠﺎل‪.‬‬
‫ﻧﻄﺎق واﺳﻊ‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻌﺎم‪ :‬اﻟﺬﻛﺎء ا ُملﺸﺎﺑﻪ ﻟﺬﻛﺎء اﻟﺒﴩ‪ ،‬وﻳﻤﻜﻦ ﺗﻄﺒﻴﻘﻪ ﻋﲆ‬
‫ٍ‬
‫ٍ‬
‫ﻣﺸﻜﻠﺔ أو ﻣُﻬﻤﺔ‬
‫ﺑﺎملﻘﺎرﻧﺔ ﻣﻊ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ املﺤﺪود‪ ،‬اﻟﺬي ﻳﻤﻜﻦ ﺗﻄﺒﻴﻘﻪ ﻋﲆ‬
‫ﻣُﻌﻴﻨﺔ ﻓﻘﻂ‪ .‬وﻳُﻄﻠﻖ ﻋﻠﻴﻪ ً‬
‫أﻳﻀﺎ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ »اﻟﻘﻮي« ﰲ ﻣﻘﺎﺑﻞ اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
‫»اﻟﻀﻌﻴﻒ«‪.‬‬
‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﻘﺎﺑﻞ ﻟﻠﺘﻔﺴري‪ :‬اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬي ﻳﻤﻜﻦ أن ﻳﴩح ﻟﻠﺒﴩ‬
‫ﺗﴫﻓﺎﺗﻪ أو ﻗﺮاراﺗﻪ أو ﺗﻮﺻﻴﺎﺗﻪ‪ ،‬أو ﻳﻤﻜﻦ أن ﻳﻮﻓﺮ ﻣﻌﻠﻮﻣﺎت ﻛﺎﻓﻴﺔ ﺣﻮل ﻛﻴﻔﻴﺔ‬
‫اﻟﻮﺻﻮل إﱃ ﻧﺘﻴﺠﺘﻪ‪.‬‬

‫‪136‬‬

‫ﻣﴪد املﺼﻄﻠﺤﺎت‬

‫اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ املﺴﺘﺪام‪ :‬اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ اﻟﺬي ﻳُﻤَ ﱢﻜﻦ وﻳﺴﺎﻫﻢ ﰲ ﻃﺮﻳﻘﺔ ﻋﻴﺶ‬
‫ً‬
‫)وأﻳﻀﺎ‬
‫ﻣﺴﺘﺪاﻣﺔ ﻟﻠﺒﴩﻳﺔ وﻻ ﻳﺪﻣﺮ اﻟﻨﻈﻢ اﻟﺒﻴﺌﻴﺔ ﻋﲆ اﻷرض اﻟﺘﻲ ﻳﻌﺘﻤﺪ ﻋﻠﻴﻬﺎ اﻟﺒﴩ‬
‫اﻟﻌﺪﻳﺪ ﻣﻦ ﻏري اﻟﺒﴩ(‪.‬‬
‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ‪ :‬اﻟﻔﻜﺮة اﻟﺘﻲ ﺗﻘﻮل ﺑﺄن اﻵﻻت ﺳﻮف ﺗﺘﻔﻮﱠق ﻋﲆ ذﻛﺎء اﻹﻧﺴﺎن‪ .‬وﻳﺮﺗﺒﻂ‬
‫اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ أﺣﻴﺎﻧًﺎ ﺑﻔﻜﺮة »اﻧﻔﺠﺎر اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ« اﻟﺬي ﻳُﺴﺒﱢﺒﻪ ﺗﺼﻤﻴﻢ اﻵﻻت‬
‫اﻟﺬﻛﻴﺔ ﻵﻻت أﻛﺜﺮ ذﻛﺎءً‪.‬‬
‫ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت‪ :‬ﻋﻠﻢ ﻣﺘﻌﺪد اﻟﺘﺨﺼﺼﺎت ﻳﺴﺘﺨﺪم اﻹﺣﺼﺎءات واﻟﺨﻮارزﻣﻴﺎت وﻏريﻫﺎ ﻣﻦ‬
‫ٍ‬
‫أﻧﻤﺎط ﻣﻔﻴﺪة وذات ﻣﻌﻨًﻰ ﻣﻦ ﻣﺠﻤﻮﻋﺎت اﻟﺒﻴﺎﻧﺎت؛ املﻌﺮوﻓﺔ أﺣﻴﺎﻧًﺎ‬
‫اﻷﺳﺎﻟﻴﺐ ﻻﺳﺘﺨﺮاج‬
‫ﺑﺎﺳﻢ »اﻟﺒﻴﺎﻧﺎت اﻟﻀﺨﻤﺔ«‪ .‬ﰲ اﻟﻮﻗﺖ اﻟﺤﺎﱄ‪ ،‬ﻳُﺴﺘﺨﺪَم ﺗﻌ ﱡﻠﻢ اﻵﻟﺔ ﰲ ﻫﺬا املﻀﻤﺎر‪ .‬وﺑﺠﺎﻧﺐ‬
‫ﺗﺤﻠﻴﻞ اﻟﺒﻴﺎﻧﺎت‪ ،‬ﻳﻬﺘﻢ ﻋﻠﻢ اﻟﺒﻴﺎﻧﺎت ً‬
‫أﻳﻀﺎ ﺑﺎﺳﺘﺨﺮاج اﻟﺒﻴﺎﻧﺎت وإﻋﺪادﻫﺎ وﺗﻔﺴريﻫﺎ‪.‬‬
‫اﻟﻘﺎﺑﻠﻴﺔ ﻟﻠﺘﻔﺴري‪ :‬اﻟﻘﺪرة ﻋﲆ اﻟﺘﻔﺴري أو ﻗﺎﺑﻠﻴﺔ اﻟﺘﻔﺴري‪ .‬ﰲ ﺳﻴﺎق اﻷﺧﻼﻗﻴﺎت‪ ،‬ﻓﺈﻧﻪ‬
‫ﻳُﺸري إﱃ اﻟﻘﺪرة ﻋﲆ اﻟﴩح ﻟﻶﺧﺮﻳﻦ ملﺎذا َ‬
‫ﻗﻤﺖ ﺑﴚء ﻣُﻌني أو ملﺎذا اﺗﺨﺬت ﻗﺮا ًرا ﺑﻌﻴﻨِﻪ؛‬
‫ً‬
‫ﻣﺴﺌﻮﻻ‪.‬‬
‫وﻫﺬا ﺟﺰء ﻣﻤﺎ ﻳَﻌﻨﻴﻪ أن ﺗﻜﻮن‬
‫ً‬
‫وﺧﺼﻮﺻﺎ املﻜﺎﻧﺔ‬
‫ﻣﺎ ﺑﻌﺪ اﻹﻧﺴﺎﻧﻴﺔ‪ :‬ﻣﺠﻤﻮﻋﺔ ﻣﻦ ا ُملﻌﺘﻘﺪات اﻟﺘﻲ ﺗُﺸﻜﻚ ﰲ اﻹﻧﺴﺎﻧﻴﺔ‪،‬‬
‫املﺤﻮرﻳﺔ ﻟﻺﻧﺴﺎن‪ ،‬وﺗﻮﺳﻊ داﺋﺮة اﻻﻫﺘﻤﺎم اﻷﺧﻼﻗﻲ ﻟﺘﺸﻤﻞ ﻏري اﻟﺒﴩ‪.‬‬
‫ٍ‬
‫ﻛﻤﺮادف ملﻌﻨﻰ أن ﻳﺘﺤﲆ املﺮء ﺑﺎﻷﺧﻼق‪ ،‬وﻣﻦ‬
‫املﺴﺌﻮﻟﻴﺔ اﻷﺧﻼﻗﻴﺔ‪ :‬ﻳﻤﻜﻦ اﺳﺘﺨﺪاﻣﻬﺎ‬
‫ﺛَﻢ ﻓﺈﻧﻬﺎ ﺗﺸري إﱃ ﺗﺤﻘﻴﻖ ﻧﺘﺎﺋﺞ ﺟﻴﺪة أﺧﻼﻗﻴٍّﺎ‪ ،‬واﻻﻟﺘﺰام ﺑﺎملﺒﺎدئ اﻷﺧﻼﻗﻴﺔ‪ ،‬واﻟﺘﻤﺘﻊ‬
‫ﺑﺎﻟﻔﻀﻴﻠﺔ‪ ،‬واﺳﺘﺤﻘﺎق اﻟﺜﻨﺎء‪ ،‬وﻣﺎ إﱃ ذﻟﻚ؛ ﺣﺴﺐ اﻟﻨﻈﺮﻳﺔ املﻌﻴﺎرﻳﺔ ا ُملﻔﱰﺿﺔ‪ .‬ﻳﻤﻜﻦ‬
‫ﻟﻠﻤﺮء ً‬
‫أﻳﻀﺎ أن ﻳﺘﺴﺎءل ﻋﻦ اﻟﴩوط اﻟﺘﻲ ﺑﻤﻮﺟﺒﻬﺎ ﻳﻤﻜﻦ إﺳﻨﺎد املﺴﺌﻮﻟﻴﺔ إﻟﻴﻪ‪ .‬ﺗُﻌﺪ‬
‫ﴍوط إﺳﻨﺎد املﺴﺌﻮﻟﻴﺔ اﻷﺧﻼﻗﻴﺔ ﻫﻲ اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ واملﻌﺮﻓﺔ‪ .‬وﺗﺆﻛﺪ ﻧُﻬُ ﺞ اﻟﻌﻼﻗﺎت‬
‫ً‬
‫ﻣﺴﺌﻮﻻ أﻣﺎم اﻵﺧﺮﻳﻦ‪.‬‬
‫أن املﺮء ﻳﻜﻮن داﺋﻤً ﺎ‬
‫املﻜﺎﻧﺔ اﻷﺧﻼﻗﻴﺔ‪ :‬املﻨﺰﻟﺔ اﻷﺧﻼﻗﻴﺔ اﻟﺘﻲ ﻳﺘﻤﺘﱠﻊ ﺑﻬﺎ ﻛﻴﺎن ﻣﺎ؛ أي ﻛﻴﻒ ﻳﻨﺒﻐﻲ اﻟﺘﻌﺎﻣُﻞ ﻣﻊ‬
‫ﻫﺬا اﻟﻜﻴﺎن‪.‬‬
‫اﻟﻮﻛﺎﻟﺔ اﻷﺧﻼﻗﻴﺔ‪ُ :‬‬
‫اﻟﻘﺪرة ﻋﲆ اﻟﻔﻌﻞ واﻟﺘﻔﻜري واﻟﺤُ ﻜﻢ واﺗﺨﺎذ اﻟﻘﺮار اﻷﺧﻼﻗﻲ‪ً ،‬‬
‫ﺑﺪﻻ ﻣﻦ‬
‫ﻣﺠﺮد وﺟﻮد ﻋﻮاﻗﺐ أﺧﻼﻗﻴﺔ‪.‬‬

‫‪137‬‬

‫ﻣﻼﺣﻈﺎت‬

‫ أﻳﺘﻬﺎ املﺮآة ﻋﲆ اﻟﺤﺎﺋﻂ‬:‫اﻟﻔﺼﻞ اﻷول‬
(1) See https://www.youtube.com/watch?v=D5VN56jQMWM.
(2) See the case of Paul Zilly as told by Fry (2018, 71-72). More details in Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/
machine-bias-risk-assessments-in-criminal-sentencing.
(3) For example, in 2016 a local police zone in Belgium started using
predictive policing software to predict burglaries and vehicle theft (Algorithm Watch 2019, 44).
(4) BuzzFeedVideo, “You Won’t Believe What Obama Says in this
Video!” https://www.youtube.com/watch?v=cQ54GDm1eL0&fbclid=IwA
R1oD0AlopEZa00XHo3WNcey_qNnNqTsvHN_aZsNb0d2t9cmsDbm9oCf
X8A.

‫ اﻟﺬﻛﺎء اﻟﻔﺎﺋﻖ واﻟﻮﺣﻮش وﻧﻬﺎﻳﺔ اﻟﻌﺎﻟﻢ ﺑﺎﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬:‫اﻟﻔﺼﻞ اﻟﺜﺎﻧﻲ‬
(1) Some talk of taming or domesticating AI, although the analogy
with wild animals is problematic, if only because in contrast to the “wild”

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
AI some imagine, animals are limited by their natural faculties and can be
trained and developed only up to some point (Turner 2019).
(2) It is often suggested that Mary Shelley must have been influenced
by her parents, who discussed politics, philosophy, and literature, but also
science, and by her partner Percy Bysshe Shelley, who was an amateur scientist especially interested in electricity.

‫ ﻛﻞ ﻣﺎ ﻟﻪ ﻋﻼﻗﺔ ﺑﺎﻟﺒﴩ‬:‫اﻟﻔﺼﻞ اﻟﺜﺎﻟﺚ‬
(1) Dreyfus was influenced by Edmund Husserl, Martin Heidegger, and
Maurice Merleau-Ponty.

ٍّ ‫ أﻫﻲ‬:‫اﻟﻔﺼﻞ اﻟﺮاﺑﻊ‬
‫ﺣﻘﺎ ﻣﺠﺮد آﻻت؟‬
(1) A real-world case of this was the robot dog Spot who was kicked by
its developers to test it, something that met with surprisingly empathetic
responses: https://www.youtube.com/watch?v=aR5Z6AoMh6U.

‫ اﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ‬:‫اﻟﻔﺼﻞ اﻟﺨﺎﻣﺲ‬
(1) See https://www.humanbrainproject.eu/en/.
(2) See, for example, the European Commission’s AI High Level Expert
Group’s (2018) definition of AI.

َ ‫ ﻻ‬:‫اﻟﻔﺼﻞ اﻟﺴﺎدس‬
‫ﺗﻨﺲ )ﻋﻠﻢ( اﻟﺒﻴﺎﻧﺎت‬
(1) See http://tylervigen.com/spurious-correlations.
(2) Concrete examples such as Facebook, Walmart, American Express,
Hello Barbie, and BMW are drawn from Marr (2018).

140

‫ﻣﻼﺣﻈﺎت‬

ُ
‫ﻻﻣﺴﺌﻮﻟﻴﺔ اﻵﻻت واﻟﻘﺮارات ﻏري ا ُملﱪرة‬
:‫اﻟﻔﺼﻞ اﻟﺜﺎﻣﻦ‬
(1) One could ask, however, if decisions made by AIs really count as
decisions, and if so, if there is a difference in the kind of decisions we delegate or should delegate to AIs. In this sense, the problem regarding responsibility of or for AI raises the very question of what a decision is. The problem also connects with issues about delegation: we delegate decisions to
machines. But what does this delegation entail in terms of responsibility?
(2) Indeed, this case is more complicated since one could argue that
the delegate is then still responsible for that particular task—at least to
some extent—and it may not be clear how the responsibility is distributed
in such cases.
(3) Note that this was and is not always the case; as Turner (2019)
reminds us, there are cases of animals being punished.

‫ اﻟﺘﺤﻴﺰ وﻣﻌﻨﻰ اﻟﺤﻴﺎة‬:‫اﻟﻔﺼﻞ اﻟﺘﺎﺳﻊ‬
(1) Thanks to Bill Price for the thought experiment.

‫ اﻟﺴﻴﺎﺳﺎت املﻘﱰﺣﺔ‬:‫اﻟﻔﺼﻞ اﻟﻌﺎﴍ‬
(1) See: https://www.acrai.at/en/.
(2) The resolution can be found here: http://www.europarl.europa.eu/
doceo/document/TA-8-2017-0051_EN.html?redirect#title1.
(3)

See:

https://www.scu.edu/ethics-in-technology-practice/

conceptual-frameworks/.
(4) See: https://www.partnershiponai.org/.
(5) See: https://www.blog.google/technology/ai/ai-principles/.
(6) See: https://www.microsoft.com/en-us/ai/our-approach-to-ai.
(7) See: https://www.accenture.com/t20160629T012639Z_w_/us-en/
_acnmedia/PDF-24/Accenture-Universal-Principles-Data-Ethics.pdf.

141

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
(8)

See: https://www.businessinsider.de/apple-ceo-tim-cook-on

-privacy-the-free-market-is-not-working-regulations-2018-11?r=
US&IR=T.
(9) See: https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?
bill_id=201720180SB1001.
(10) See: https://www.stopkillerrobots.org/.
(11) See: https://futureoflife.org/ai-principles/.
(12) Consider people such as Batya Friedman and Helen Nissenbaum
in the United States, and later Jeroen van den Hoven and others in the
Netherlands, who have been championing the ethical design of technology
for some time.
(13) See: https://www.tuev-sued.de/company/press/press-archive/
tuv-sud-and-dfki-to-develop-tuv-for-artificial-intelligence.

‫ اﻟﺘﺤﺪﻳﺎت اﻟﺘﻲ ﺗُﻮاﺟﻪ ﺻﺎﻧﻌﻲ اﻟﺴﻴﺎﺳﺎت‬:‫اﻟﻔﺼﻞ اﻟﺤﺎدي ﻋﴩ‬
(1) See: https://ec.europa.eu/digital-single-market/en/european-aialliance.

‫ ﺗﺤﺪﱢي ﱡ‬:‫اﻟﻔﺼﻞ اﻟﺜﺎﻧﻲ ﻋﴩ‬
‫ ﺣﻮل اﻷوﻟﻮﻳﺎت وﺣﻘﺒﺔ اﻟﺘﺄﺛري اﻟﺒﴩي‬:‫ﺗﻐري املﻨﺎخ‬
(1) See: https://hai.stanford.edu/ and https://hcai.mit.edu.
(2) See: https://sustainabledevelopment.un.org/post2015/transform
ingourworld.
(3) See: https://www.theguardian.com/science/2018/feb/07/spaceoddity-elon-musk-spacex-car-mars-falcon-heavy.
(4) See: https://cosmosmagazine.com/space/why-we-need-to-send
-artists-into-space.

142

‫ﻗﺮاءات إﺿﺎﻓﻴﺔ‬

Alpaydin, Ethem, 2016, Machine Learning, Cambridge, MA: MIT Press.
Arendt, Hannah, 1958, The Human Condition, Chicago: Chicago University
Press.
Aristotle, 2002, Nichomachean Ethics, Translated by Christopher Rowe,
with commentary by Sarah Broadie, Oxford: Oxford University Press.
Boddington, Paula, 2017, Towards a Code of Ethics for Artificial Intelligence,
Cham: Springer.
Boden, Margaret A., 2016, AI: Its Nature and Future, Oxford: Oxford University Press.
Bostrom, Nick. 2014, Superintelligence, Oxford: Oxford University Press.
Brynjolfsson, Erik, and Andrew McAfee, 2014, The Second Machine Age,
New York: W. W. Norton.
Coeckelbergh, Mark, 2012, Growing Moral Relations: Critique of Moral
Status Ascription, New York: Palgrave Macmillan.
Crutzen, Paul J., 2006, “The ‘Anthropocene,’” In Earth System Science in
the Anthropocene, edited by Eckart Ehlers and Thomas Krafft, 13–18.
Cham: Springer.

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
Dignum, Virginia, Matteo Baldoni, Cristina Baroglio, Maruiyio Caon, Raja
Chatila, Louise Dennis, Gonzalo Génova, et al. 2018, “Ethics by Design: Necessity or Curse?” Association for the Advancement of Artificial Intelligence. http://www.aies-conference.com/2018/contents/
papers/main/AIES_2018_paper_68.pdf.
Dreyfus, Hubert L., 1972, What Computers Can’t Do, New York: Harper &
Row.
Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice
Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo
Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy
Vayena, 2018, “AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.” Minds
and Machines 28, no. 4: 689–707.
Frankish, Keith, and William M. Ramsey, eds. 2014. The Cambridge
Handbook of Artificial Intelligence. Cambridge: Cambridge University
Press.
European Commission AI HLEG (High-Level Expert Group on Artificial Intelligence). 2019. “Ethics Guidelines for Trustworthy AI.” April 8, 2019.
Brussels: European Commission. https://ec.europa.eu/futurium/en/
ai-alliance-consultation/guidelines#Top.
Fry, Hannah. 2018. Hello World: Being Human in the Age of Algorithms.
New York and London: W. W. Norton.
Fuchs, Christian. 2014. Digital Labour and Karl Marx. New York: Routledge.
Gunkel, David. 2012. The Machine Question. Cambridge, MA: MIT Press.
Harari, Yuval Noah. 2015. Homo Deus: A Brief History of Tomorrow. London: Hervill Secker.
Haraway, Donna. 1991. “A Cyborg Manifesto: Science, Technology, and
Socialist-Feminism in the Late Twentieth Century.” In Simians,

144

‫ﻗﺮاءات إﺿﺎﻓﻴﺔ‬
Cyborgs and Women: The Reinvention of Nature, 149–181. New
York: Routledge.
IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. 2017. “Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems,” Version 2. IEEE, 2017. http://standards.Ieee.org/develop/indconn/ec/
autonomous_systems.html.
Kelleher, John D. and Brendan Tierney. 2018. Data Science. Cambridge, MA:
MIT Press.
Nemitz, Paul Friedrich, 2018. “Constitutional Democracy and Technology in the Age of Artificial Intelligence.” Philosophical Transactions of
the Royal Society A 376, no. 2133. https://doi.org/10.1098/rsta.2018
.0089.
Noble, David F. 1997. The Religion of Technology. New York: Penguin Books.
Reijers, Wessel, David Wright, Philip Brey, Karsten Weber, Rowena Rodrigues, Declan O’Sullivan, and Bert Gordijn. 2018. “Methods for Practising Ethics in Research and Innovation: A Literature Review, Critical
Analysis and Recommendation.” Science and Engineering Ethics 24, no.
5: 1437–1481.
Shelley, Mary. 2017. Frankenstein. Annotated edition. Edited by David H.
Guston, Ed Finn, and Jason Scott Robert. Cambridge, MA: MIT Press.
Turkle, Sherry. 2011. Alone Together: Why We Expect More from Technology
and Less from Each Other. New York: Basic Books.
Wallach, Wendell, and Colin Allen. 2009. Moral Machines: Teaching Robots
Right from Wrong. Oxford: Oxford University Press.

145

‫اﳌﺮاﺟﻊ‬

Accessnow. 2018. “Mapping Regulatory Proposals for Artificial Intelligence in Europe.” https://www.accessnow.org/cms/assets/uploads/
2018/11/mapping_regulatory_proposals_for_AI_in_EU.pdf.
ACRAI (Austria Council on Robotics and Artificial Intelligence). 2018. “Die
Zukunft Österreichs mit Robotik und Künstlicher Intelligenz positive gestalten: White paper des Österreichischen Rats für Robotik und
Künstliche Intelligenz.”
“Algorithm and Blues.” 2016. Nature 537:449.
AlgorithmWatch. 2019. “Automating Society: Taking Stock of Automated
Decision Making in the EU.” A report by AlgorithmWatch in cooperation with Bertelsmann Stiftung. January 2019. Berlin: AW AlgorithmWatch GmbH. http://www.algorithmwatch.org/automatingsociety.
Alpaydin, Ethem. 2016. Machine Learning. Cambridge, MA: MIT Press.
Anderson, Michael and Susan Anderson. 2011. “General Introduction.” In
Machine Ethics, edited by Michael Anderson and Susan Anderson, 1–4.
Cambridge: Cambridge University Press.
Arendt, Hannah. 1958. The Human Condition. Chicago: Chicago University
Press.

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
Arkoudas, Konstantine, and Selmer Bringsjord. 2014. “Philosophical Foundations.” In The Cambridge Handbook of Artificial Intelligence, edited
by Keith Frankish and William M. Ramsey. Cambridge: Cambridge University Press.
Armstrong, Stuart. 2014. Smarter Than Us: The Rise of Machine Intelligence.
Berkeley: Machine Intelligence Research Institute.
Awad, Edmond, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Henrich, Azim Shariff, Jean-François Bonnefon, and Iyad Rahwan. 2018.
“The Moral Machine Experiment.” Nature 563:59–64.
Bacon, Francis. 1964. “The Refutation of Philosophies.” In The Philosophy
of Francis Bacon, edited by Benjamin Farrington, 103–132. Chicago:
University of Chicago Press.
Boddington, Paula. 2016. “The Distinctiveness of AI Ethics, and Implications for Ethical Codes.” Paper presented at the workshop
Ethics for Artificial Intelligence, July 9, 2016, IJCAI-16, New York.
https://www.cs.ox.ac.uk/efai/2016/11/02/the-distinctiveness-ofai-ethics-and-implications-for-ethical-codes/.
Boddington, Paula. 2017. Towards a Code of Ethics for Artificial Intelligence.
Cham: Springer.
Boden, Margaret A. 2016. AI: Its Nature and Future. Oxford: Oxford University Press.
Borowiec, Steven. 2016. “AlphaGo Seals 4–1 Victory Over Go Grandmaster
Lee Sedol.” Guardian, March 15. https://www.theguardian.com/
technology/2016/mar/15/googles-alphago-seals-4-1-victoryover-grandmaster-lee-sedol.
Bostrom, Nick. 2014. Superintelligence. Oxford: Oxford University Press.
Brynjolfsson, Erik, and Andrew McAfee. 2014. The Second Machine Age.
New York: W. W. Norton.

148

‫املﺮاﺟﻊ‬
Bryson, Joanna. 2010. “Robots Should Be Slaves.” In Close Engagements
with Artificial Companions: Key Social, Psychological, Ethical and
Design Issues, edited by Yorick Wilks, 63–74. Amsterdam: John
Benjamins.
Bryson, Joanna. 2018. “AI & Global Governance: No One Should Trust AI.”
United Nations University Centre for Policy Research. AI & Global
Governance, November 13, 2018. https://cpr.unu.edu/ai-globalgovernance-no-one-should-trust-ai.html.
Bryson, Joanna, Mihailis E. Diamantis, and Thomas D. Grant. 2017. “Of, For,
and By the People: The Legal Lacuna of Synthetic Persons.” Artificial
Intelligence & Law 25, no. 3: 273–291.
Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. 2017. “Semantics
Derived Automatically from Language Corpora Contain Human-like
Biases.” Science 356:183–186.
Castelvecchi, Davide. 2016. “Can We Open the Black Box of AI?” Nature
538, no. 7623: 21–23.
CDT (Centre for Democracy & Technology) 2018. “Digital Decisions.”
https://cdt.org/issue/privacy-data/digital-decisions/.
Coeckelbergh, Mark. 2010. “Moral Appearances: Emotions, Robots, and
Human Morality.” Ethics and Information Technology 12, no. 3: 235–
241.
Coeckelbergh, Mark. 2011. “You, Robot: On the Linguistic Construction of
Artificial Others.” AI & Society 26, no. 1: 61–69.
Coeckelbergh, Mark. 2012. Growing Moral Relations: Critique of Moral
Status Ascription. New York: Palgrave Macmillan.
Coeckelbergh, Mark. 2013. Human Being @ Risk: Enhancement, Technology,
and the Evaluation of Vulnerability Transformations. Cham: Springer.
Coeckelbergh, Mark. 2017. New Romantic Cyborgs. Cambridge, MA: MIT
Press.

149

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
Crawford, Kate, and Ryan Calo. 2016. “There Is a Blind Spot in AI Research.”
Nature 538:311–313.
Crutzen, Paul J. 2006. “The ‘Anthropocene.’” In Earth System Science in
the Anthropocene edited by Eckart Ehlers and Thomas Krafft, 13–18.
Cham: Springer.
Darling, Kate, Palash Nandy, and Cynthia Breazeal. 2015. “Empathic Concern and the Effect of Stories in Human-Robot Interaction.” In 2015
24th IEEE International Symposium on Robot and Human Interactive
Communication (RO-MAN), 770–775. New York: IEEE.
Dennett, Daniel C. 1997. “Consciousness in Human and Robot Minds. In
Cognition, Computation, and Consciousness, edited by Masao Ito, Yasushi Miyashita, and Edmund T. Rolls, 17–29. New York: Oxford University Press.
Digital Europe. 2018. “Recommendations on AI Policy: Towards a Sustainable and Innovation-friendly Approach.” Digitaleurope.org, November
7, 2018.
Dignum, Virginia, Matteo Baldoni, Cristina Baroglio, Maruiyio Caon, Raja
Chatila, Louise Dennis, Gonzalo Génova, et al. 2018. “Ethics by Design: Necessity or Curse?” Association for the Advancement of Artificial Intelligence. http://www.aies-conference.com/2018/contents/
papers/main/AIES_2018_paper_68.pdf.
Dowd, Maureen. 2017. “Elon Musk’s Billion-Dollar Crusade to Stop the
A.I. Apocalypse.” Vanity Fair, March 26, 2017. https://www.vanityfair
.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stopai-space-x.
Dreyfus, Hubert L. 1972. What Computers Can’t Do. New York:
HarperCollins.

150

‫املﺮاﺟﻊ‬
Druga, Stefania and Randi Williams. 2017. “Kids, AI Devices, and Intelligent Toys.” MIT Media Lab, June 6, 2017. https://www.media.mit.edu/
posts/kids-ai-devices/f.
European Commission. 2018. “Ethics and Data Protection.” http://
ec.europa.eu/research/participants/data/ref/h2020/grants_manual/
hi/ethics/h2020_hi_ethics-data-protection_en.pdf.
European Commission Directorate-General of Employment, Social Affairs
and Inclusion. 2018. “Employment and Social Developments in Europe
2018.” Luxembourg: Publications Office of the European Union. http://
ec.europa.eu/social/main.jsp?catId=738&langId=en&pubId=8110.
European Commission AI HLEG (High-Level Expert Group on Artificial Intelligence). 2018. “Draft Ethics Guidelines for Trustworthy AI: Working
Document for Stakeholders.” Working document, December 18, 2018.
Brussels: European Commission. https://ec.europa.eu/digital-singlemarket/en/news/draft-ethics-guidelines-trustworthy-ai.
European Commission AI HLEG (High-Level Expert Group on Artificial Intelligence). 2019. “Ethics Guidelines for Trustworthy AI.” April 8, 2019.
Brussels: European Commission. https://ec.europa.eu/futurium/en/
ai-alliance-consultation/guidelines#Top.
EGE (European Group on Ethics in Science and New Technologies). 2018.
“Statement on Artificial Intelligence, Robotics and ‘Autonomous’ Systems.” Brussels: European Commission.
European Parliament and the Council of the European Union. 2016. “General Data Protection Regulation (GDPR).” https://eur-lex.europa.eu/
legal-content/EN/TXT/?uri=celex%3A32016R0679.
Executive Office of the President, National Science and Technology Council
Committee on Technology. 2016. “Preparing for the Future of Artificial
Intelligence.” Washington, DC: Office of Science and Technology Policy
(OSTP).

151

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice
Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo
Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy
Vayena. 2018. “AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.” Minds
and Machines 28, no. 4: 689–707.
Floridi, Luciano, and J. W. Sanders. 2004. “On the Morality of Artificial
Agents.” Minds and Machines 14, no. 3: 349–379.
Ford, Martin. 2015. Rise of the Robots: Technology and the Threat of a
Jobless Future. New York: Basic Books.
Frankish, Keith, and William M. Ramsey. 2014. “Introduction.” In The
Cambridge Handbook of Artificial Intelligence, edited by Keith Frankish and William M. Ramsey, 1–14. Cambridge: Cambridge University
Press.
Frey, Carl Benedikt, and Michael A. Osborne. 2013. “The Future of Employment: How Susceptible Are Jobs to Computerisation?” Working paper,
Oxford Martin Programme on Technology and Employment, University
of Oxford.
Fry, Hannah. 2018. Hello World: Being Human in the Age of Algorithms.
New York: W. W. Norton.
Fuchs, Christian. 2014. Digital Labour and Karl Marx. New York: Routledge.
Goebel, Randy, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep
Akata, Simone Stumpf, Peter Kieseberg, and Andreas Holzinger. 2018.
“Explainable AI: The New 42?” Paper presented at the CD-MAKE 2018,
Hamburg, Germany, August 2018.
Gunkel, David. 2012. The Machine Question. Cambridge, MA: MIT Press.
Gunkel, David. 2018. “The Other Question: Can and Should Robots Have
Rights?” Ethics and Information Technology 20:87–99.

152

‫املﺮاﺟﻊ‬
Harari, Yuval Noah. 2015. Homo Deus: A Brief History of Tomorrow. London: Hervill Secker.
Haraway, Donna. 1991. “A Cyborg Manifesto: Science, Technology, and
Socialist-Feminism in the Late Twentieth Century.” In Simians,
Cyborgs and Women: The Reinvention of Nature, 149–181. New
York: Routledge.
Haraway, Donna. 2015. “Anthropocene, Capitalocene, Plantationocene,
Chthulucene: Making Kin.” Environmental Humanities 6:159–165.
Herweijer, Celine. 2018. “8 Ways AI Can Help Save the Planet.” World
Economic Forum, January 24, 2018. https://www.weforum.org/
agenda/2018/01/8-ways-ai-can-help-save-the-planet/.
House of Commons. 2018. “Algorithms in Decision-Making.” Fourth Report of Session 2017-19, HC351. May 23, 2018.
ICDPPC (International Conference of Data Protection and Privacy Commissioners). 2018. “Declaration on Ethics and Data Protection in Artificial Intelligence.” https://icdppc.org/wp-content/uploads/2018/10/
20180922_ICDPPC-40th_AI-Declaration_ADOPTED.pdf.
IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. 2017. “Ethically Aligned Design: A Vision for Prioritizing Human Well-Being with Autonomous and Intelligent Systems,” Version
2. IEEE. http://standards.Ieee.org/develop/indconn/ec/autonomous_
systems.html.
Ihde, Don. 1990. Technology and the Lifeworld: From Garden to Earth.
Bloomington: Indiana University Press.
Jansen, Philip, Stearns Broadhead, Rowena Rodrigues, David Wright, Philp
Brey, Alice Fox, and Ning Wang. 2018. “State-of-the-Art Review.”
Draft of the D4.1 deliverable submitted to the European Commission
on April 13, 2018. A report for The SIENNA Project, an EU H2020 research and innovation program under grant agreement no. 741716.

153

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
Johnson, Deborah G. 2006. “Computer Systems: Moral Entities but not
Moral Agents.” Ethics and Information Technology 8, no. 4: 195–204.
Kant, Immanuel. 1997. Lectures on Ethics. Edited by Peter Heath and J. B.
Schneewind. Translated by Peter Heath. Cambridge: Cambridge University Press.
Kelleher, John D., and Brendan Tierney. 2018. Data Science. Cambridge,
MA: MIT Press.
Kharpal, Arjun. 2017. “Stephen Hawking Says A.I. Could Be ‘Worst Event
in the History of Our Civilization.’” CNBC. November 6, 2017.
https://www.cnbc.com/2017/11/06/stephen-hawking-ai-couldbe-worst-event-in-civilization.html.
Kubrick, Stanley, dir. 1968. 2001: A Space Odyssey. Beverly Hills, CA:
Metro-Goldwyn-Mayer.
Kurzweil, Ray. 2005. The Singularity Is Near. New York: Viking.
Leta Jones, Meg. 2018. “Silencing Bad Bots: Global, Legal and Political Questions for Mean Machine Communication.” Communication Law and
Policy 23, no. 2: 159–195.
Lin, Patrick, Keith Abney, and George Bekey. 2011. “Robot Ethics: Mapping
the Issues for a Mechanized World.” Artificial Intelligence 175:942–
949.
MacIntyre, Lee C. 2018. Post-Truth. Cambridge, MA: MIT Press.
Marcuse, Herbert. 1991. One-Dimensional Man. Boston: Beacon Press.
Marr, Bernard. 2018. “27 Incredible Examples of AI and Machine Learning in Practice.” Forbes, April 30. https://www.forbes.com/sites/
bernardmarr/2018/04/30/27-incredible-examples-of-ai-and-ma
chine-learning-in-practice/#6b37edf27502.
McAfee, Andrew, and Erik Brynjolfsson. 2017. Machine, Platform, Crowd:
Harnessing Our Digital Future. New York: W. W. Norton.

154

‫املﺮاﺟﻊ‬
Miller, Tim. 2018. “Explanation in Artificial Intelligence: Insights from the
Social Sciences.” arXiv, August 15. https://arxiv.org/pdf/1706.07269
.pdf.
Mouffe, Chantal. 2013. Agonistics: Thinking the World Politically. London:
Verso.
Nemitz, Paul Friedrich, 2018. “Constitutional Democracy and Technology in the Age of Artificial Intelligence.” Philosophical Transactions of
the Royal Society A 376, no. 2133. https://doi.org/10.1098/rsta.2018
.0089.
Noble, David F. 1997. The Religion of Technology. New York: Penguin Books.
Reijers, Wessel, David Wright, Philip Brey, Karsten Weber, Rowena Rodrigues, Declan O’ Sullivan, and Bert Gordijn. 2018. “Methods for Practising Ethics in Research and Innovation: A Literature Review, Critical
Analysis and Recommendation.” Science and Engineering Ethics 24, no.
5: 1437–1481.
Royal Society, the. 2018. “Portrayals and Perceptions of AI and Why They
Matter.” December 11, 2018. https://royalsociety.org/topics-policy/
projects/ai-narratives/.
Rushkoff, Douglas. 2018. “Survival of the Richest.” Medium, July 5.
https://medium.com/s/futurehuman/survival-of-the-richest9ef6cddd0cc1.
Samek, Wojciech, Thomas Wiegand, and Klaus-Robert Müller. 2017. “Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models.” https://arxiv.org/pdf/1708.08296
.pdf.
Schwab, Katharine. 2018. “The Exploitation, Injustice, and Waste
Powering Our AI.” Fast Company. September 18, 2018. https://
www.fastcompany.com/90237802/the-exploitation-injustice-andwaste-powering-our-ai.

155

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
Seseri, Rudina. 2018. “The Problem with ‘Explainable AI.’” Tech Crunch.
June 14, 2018. https://techcrunch.com/2018/06/14/the-problemwith-explainable-ai/?guccounter=1.
Searle, John. R. 1980. “Minds, Brains, and Programs.” Behavioral and Brain
Sciences 3, no. 3: 417–457.
Shanahan, Murray. 2015. The Technological Singularity. Cambridge, MA:
The MIT Press.
Siau, Keng, and Weiyu Wang. 2018. “Building Trust in Artificial Intelligence,
Machine Learning, and Robotics.” Cutter Business Technology Journal
32, no. 2: 46–53.
State Council of China. 2017. “New Generation Artificial Intelligence Development Plan.” Translated by Flora Sapio, Weiming Chen, and Adrian
Lo.

https://flia.org/notice-state-council-issuing-new-generation-

artificial-intelligence-development-plan/.
Stoica, Ion. 2017. “A Berkeley View of Systems Challenges for AI.” Technical Report No. UCB/EECS-2017-159. http://www2.eecs.berkeley.edu/
Pubs/TechRpts/2017/EECS-2017.
Sullins, John. 2006. “When Is a Robot a Moral Agent?” International Review
of Information Ethics 6: 23–30.
Surur. 2017. “Microsoft Aims to Lie to Their AI to Reduce Sexist Bias.”
August 25, 2017. https://mspoweruser.com/microsoft-aims-lie-aireduce-sexist-bias/.
Suzuki, Yutaka, Lisa Galli, Ayaka Ikeda, Shoji Itakura, and Michiteru Kitazaki. 2015. “Measuring Empathy for Human and Robot Hand Pain
Using Electroencephalography.” Scientific Reports 5, article number
15924. https://www.nature.com/articles/srep15924.
Tegmark, Max. 2017. Life 3.0: Being Human in the Age of Artificial
Intelligence. Allen Lane/Penguin Books.

156

‫املﺮاﺟﻊ‬
Turkle, Sherry. 2011. Alone Together: Why We Expect More from Technology
and Less from Each Other. New York: Basic Books.
Turner, Jacob. 2019. Robot Rules: Regulating Artificial Intelligence. Cham:
Palgrave Macmillan.
Université de Montréal. 2017. “Montréal Declaration Responsible AI.”
https://www.montrealdeclaration-responsibleai.com/the-declara
tion.
Vallor, Shannon. 2016. Technology and the Virtues. New York: Oxford University Press.
Vigen, Tyler. 2015. Spurious Correlations. New York: Hachette Books.
Villani, Cédric. 2018. For a Meaningful Artificial Intelligence: Towards a
French and European Strategy. Composition of a parliamentary mission from September 8, 2017, to March 8, 2018, and assigned by the
Prime Minister of France, Èdouard Philippe.
Von Schomberg, René, ed. 2011. “Towards Responsible Research and Innovation in the Information and Communication Technologies and Security Technologies Fields.” A report from the European Commission
Services. Luxembourg: Publications Office of the European Union.
Vu, Mai-Anh T., Tülay Adalı, Demba Ba, György Buzsáki, David Carlson,
Katherine Heller, et al. 2018. “A Shared Vision for Machine Learning in
Neuroscience.” Journal of Neuroscience 38, no. 7: 1601–607.
Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi. 2017. “Why a Right
to Explanation of Automated Decision-Making Does Not Exist in the
General Data Protection Regulation.” International Data Privacy Law,
2017. http://dx.doi.org/10.2139/ssrn.2903469.
Wallach, Wendell and Colin Allen. 2009. Moral Machines: Teaching Robots
Right from Wrong. Oxford: Oxford University Press.
Weld, Daniel S. and Gagan Bansal. 2018. “The Challenge of Crafting Intelligible Intelligence.” https://arxiv.org/pdf/1803.04263.pdf.

157

‫أﺧﻼﻗﻴﺎت اﻟﺬﻛﺎء اﻻﺻﻄﻨﺎﻋﻲ‬
Winfield, Alan F.T. and Marina Jirotka. 2017. “The Case for an Ethical Black
Box.” In Towards Autonomous Robotic Systems, edited by Yang Gao,
Saber Fallah, Yaochu Jin, and Constantina Lekakou (proceedings of
TAROS 2017, Guildford, UK, July 2017), 262–273. Cham: Springer.
Winikoff, Michael. 2018. “Towards Trusting Autonomous Systems.”
In Engineering Multi-Agent Systems, edited by Amal El Fallah
Seghrouchni, Alessandro Ricci, and Son Trao, 3–20. Cham: Springer.
Yampolskiy, Roman V. 2013. “Artificial Intelligence Safety Engineering:
Why Machine Ethics Is a Wrong Approach.” In Philosophy and Theory
of Artificial Intelligence edited by Vincent C. Müller, 289–296. Cham:
Springer.
Yeung, Karen. 2018. “A Study of the Implications of Advanced Digital
Technologies (Including AI Systems) for the Concept of Responsibility within a Human Rights Framework.” A study commissioned for the
Council of Europe Committee of experts on human rights dimensions
of automated data processing and different forms of artificial intelligence. MSI-AUT (2018)05.
Zimmerman, Jess. 2015. “What If the Mega-Rich Just Want Rocket Ships
to Escape the Earth They Destroy?” Guardian, September 16, 2015.
https://www.theguardian.com/commentisfree/2015/sep/16/megarich-rocket-ships-escape-earth.
Zou, James, and Londa Schiebinger. 2018. “Design AI So That It’s Fair.”
Nature 559:324–326.

158

