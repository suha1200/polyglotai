{"pack_id": "english_pack", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "page": 3, "chunk_id": "english_pack_3_0_7cdc49a6", "content": "Ethics in Technology By: Ed Weber 1. Preface About this text, the author, and Open Education Resource (OER) Copyright About this text The purpose of this text is to serve as an Open Education Resource (OER) designed initially to be used in a secondary or post-secondary education setting. It is intended to be a guide to facilitate focused discussions about contemporary issues of the ethical considerations related to technology evolution, development, deployment, and consumption, as well as issues (both known and unknown) of potential misuse and abuse of technology. This is not a traditional Ethics textbook in that it is not intended to provide a survey of all of the history of Ethics through the ages nor is it intended to dive deeply into any particular ethical movements or frameworks as may be the focus of other ethics studies. Rather, it is intended to focus predominantly on the concepts of applying critical and ethical thinking to issues and subsequent decisions related to our interactions with technology in the 21st century. If you came here looking for definitive answers – I’m so sorry to disappoint you. This text does not purport to include any categorical absolutes or any sort of an ethical road map for the reader to adopt. Rather, the intent is to be a starting point for the thinking person to consider one’s own perspectives and understandings of: • what the concept of ‘ethics’ means to you as you begin your exploration... • what influences (family, school, religion, region, ethnicity, gender identity, socioeconomic status, etc.) have come together to make you the ‘who’ you are at this moment… • what are the skills necessary to be able to objectively review a concept or situation so as to understand what its ethical issues may be… • what it actually means to have a choice between more than one option and how to be able to make an informed decision for yourself that includes ethical considerations… • how to recognize that while technology continues to advance, other systems like legal systems, economic systems, political systems, social systems, and individual and group"}
{"pack_id": "english_pack", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "page": 3, "chunk_id": "english_pack_3_1_300ad63a", "content": "its ethical issues may be… • what it actually means to have a choice between more than one option and how to be able to make an informed decision for yourself that includes ethical considerations… • how to recognize that while technology continues to advance, other systems like legal systems, economic systems, political systems, social systems, and individual and group moral and ethical systems often lag significantly behind the rate of change of technology. This text is best used in conjunction with case studies and examples (both historic and contemporary) to serve as springboards for in-depth discussion surrounding the various topics. As an OER, it is expected that some of the case studies and examples that are initially presented throughout the text will become ‘stale’ over time. Therefore, it is encouraged that adopters of this text consider enhancing this base text with additional, contemporary case studies or other source materials which exemplify the topics. Likewise, it is encouraged that students and other consumers of this text also bring suggested new case studies and examples for possible inclusion. New examples of both ethical Page 3 of 125 1. Preface"}
{"pack_id": "english_pack", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "page": 4, "chunk_id": "english_pack_4_0_744f05bf", "content": "Ethics in Technology By: Ed Weber and unethical behaviors, decisions, and applications of technology continue to happen every single day. It is a hope of the author that those who work with this text will begin to adopt a habit of intentionally and closely examining the ethical considerations of their respective interactions with technology. Throughout this text, we will discover that having a shared understanding of the intended definition of critical terms (if not a shared agreement of those meanings) is critical in order to have meaningful discourse about these ethereal concepts. As a result, many terms will be presented throughout this text which will require this shared understanding of the author’s intended meaning. As a result, terms that may be subject to multiple definitions or interpretations are highlighted in bold in this text. The definition of these terms may appear when they are first used, or may be defined at the end of the chapter in which they are used. Throughout the use of this text, it is important that the reader and others involved in discussions about these topics have a minimum of a shared definition of what each critical term means and the context in which the terms are being used. About the Author Hello good reader! I am Ed Weber, an Associate Professor of Computer Science at St. Charles Community College (SCC) in Cottleville, MO. I also am the President and owner of Weber Enterprises, LLC of Wildwood, MO, which was founded in 1995. I have spent my entire career in Information Systems working with both very large companies, and with mid-size and small clients. I have been teaching Information Systems and Computer Science since 1996 originally as an adjunct instructor and then as a full-time Assistant Professor with Millikin University for 9 ½ years before joining SCC. I have seen countless technologies come and go Figure 2: Ed Weber, throughout my career and I have witnessed both the ethical and unethical Assoc. Prof. of Computer implementations of technology as well. Science SCC When I wrote my first textbook, Spreadsheet Fundamentals, in 2018, it was for"}
{"pack_id": "english_pack", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "page": 4, "chunk_id": "english_pack_4_1_f090fd68", "content": "Assistant Professor with Millikin University for 9 ½ years before joining SCC. I have seen countless technologies come and go Figure 2: Ed Weber, throughout my career and I have witnessed both the ethical and unethical Assoc. Prof. of Computer implementations of technology as well. Science SCC When I wrote my first textbook, Spreadsheet Fundamentals, in 2018, it was for the primary purpose of reducing the cost of a required textbook for my class. This was a direct and intentional action when I realized that some of my students were struggling because they didn’t have the financial support to buy the textbook we were previously using. This was my first real exposure to the realities of the ethical issues surrounding the digital divide (see Chapter 6). At that time, I was unaware of the Open Education Resources (OER) concepts and that there was a way for me to publish the textbook to be 100% free of charge. Rather, I was approached by a traditional textbook publisher who helped me in my first publishing endeavor. So, while my first publication allowed me to reduce the cost of the text from nearly $200 to under $40, I have subsequently learned how the OER model affords me an opportunity to make new Page 4 of 125 1. Preface"}
{"pack_id": "english_pack", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "page": 5, "chunk_id": "english_pack_5_0_5be690e5", "content": "Ethics in Technology By: Ed Weber classroom materials for students that will be forever 100% free of charge. That is my primary purpose in developing this text. Throughout my career, I have often found myself in situations where I had to make decisions and take actions which would have significant ethical implications for myself, my family, my colleagues, my staff, my employees, my clients, my organization, and even my entire community. The more often that these types of situations occurred, the more I realized that you don’t want to begin thinking about the ethical issues of your situation when you are already knee-deep in the middle of it! Rather, it seems to be healthier for me to be more proactive when it comes to thinking about ethics and how I incorporate my understandings into my day-to-day life and decisions. Therefore, this text will attempt to lay out just a sampling of some of the major technology-related topics that are happening at this time which have exceptional ethical considerations. It is my hope that by thinking about (and discussing) these topics before you find yourself in the middle of making any significant decisions, you will be able to find yourself much more prepared for the decisions that life will be throwing at you. Open Education Resource (OER) Copyright Open Educational Resources (OER) are teaching, learning, and research materials that are freely available for anyone to use, adapt, and share. These resources can include textbooks, course materials, videos, tests, and software, and are either in the public domain or released under an open license that permits no-cost access, reuse, modification, and redistribution by others. The purpose of OER is to reduce barriers to education by providing high-quality materials that can be tailored to meet local needs and contexts. The material in this textbook is copyrighted and licensed under the Creative Commons Attribution Non-Commercial Share-Alike (CC BY-NC-SA) license. This means you are free to use, share, and adapt the content for non-commercial purposes, as long as you provide appropriate credit to the original creator and distribute any derivative works under the same license."}
{"pack_id": "english_pack", "section_title": "About this text, the author, and Open Education Resource (OER) Copyright", "page": 5, "chunk_id": "english_pack_5_1_f4cce5d7", "content": "to meet local needs and contexts. The material in this textbook is copyrighted and licensed under the Creative Commons Attribution Non-Commercial Share-Alike (CC BY-NC-SA) license. This means you are free to use, share, and adapt the content for non-commercial purposes, as long as you provide appropriate credit to the original creator and distribute any derivative works under the same license. Commercial use of this material is not permitted without explicit permission from the copyright holder. The images found in this text have the following attributions: • The internet memes and Maslow's hierarchy image do not receive any attribution as they are considered in the public domain. • The screenshot of the website is attributed with the citation for the entire content taken from that site • All of the rest of the images were created by me and referenced when using the Pixlr AI tool. Page 5 of 125 1. Preface"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 6, "chunk_id": "english_pack_6_0_b5d18a36", "content": "Ethics in Technology By: Ed Weber 2. Introduction, Ethical Frameworks and Personal Lenses Foundations; Frameworks and Personal Lenses; Key Concepts Foundations From an early age, most people experience the concept of Ethics long before they ever learn that there is a term to describe the concept that they are experiencing. Many individuals might say that the term includes some concept of right vs. wrong, or good vs. evil, and how these concepts affect a person’s behaviors. And maybe, they consider Ethics to be the full collection of a person’s attitude and behaviors and their rightness vs. wrongness. But as we can see, often trying to define one term leads us down a rabbit-hole where we discover that we need to define even more terms! In fact, a large part of the first few chapters of this text will focus on the necessity of defining terms and why it is necessary to have a shared understanding of the critical terms which we use to have discussions about ethics. For example, what do you think is the meaning of the term: Ethics? I’m sure that most folx would be able to come up with their own definition of what they think Ethics means. Take a few moments to see if you can come up with your own definition of what you think the term Ethics means for you… go ahead… we’ll wait. Now that you have your own definition of Ethics in your mind, let’s now consider a follow-up question: How did you come to have this understanding of what you think Ethics means? Did you previously learn a formal definition of the term from a textbook or a dictionary? Did you learn it from some previous class or somewhere else in school? Were you told what the term means from your parents or grand-parents or other family members? Did you learn the term from your religious leaders? Did you read it in a sacred book? Did you learn it from television or the movies? Did you learn it on the Internet? Did you Google it? From social media? From an influencer?"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 6, "chunk_id": "english_pack_6_1_7a9c9de9", "content": "school? Were you told what the term means from your parents or grand-parents or other family members? Did you learn the term from your religious leaders? Did you read it in a sacred book? Did you learn it from television or the movies? Did you learn it on the Internet? Did you Google it? From social media? From an influencer? Did you as a generative AI tool? Really think about it. Where did you get your understanding of this term? And next, while you contemplate all of the possible places where you might have learned the term, the next follow-up question is this: Whomever provided you with this definition… were they right? Where did they learn it from? Is it possible (… just possible …) that they got it wrong? Who, ultimately, gets to decide – for you – what the acceptable definition of this term Ethics may be? Or, for that matter, who gets to decide – for you – what the acceptable definition of any term is going to be? As you can quickly see, this text will be full of a lot more questions than answers! But it is precisely these questions that will help guide you through thoughtful and insightful discussions about how the ethical aspects of technology are becoming more and more significant every day. Page 6 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 7, "chunk_id": "english_pack_7_0_0b54fad4", "content": "Ethics in Technology By: Ed Weber Before continuing, it may be helpful to list some of the significant philosophical movements and just a few of the many ethicists whose work may be considered most relevant to the concept of applying ethical thinking to issues surrounding technology. Applicable Ethical Movements • Deontological Ethics (18th Century) Deontological ethics emphasizes duty, rules, and the inherent rightness or wrongness of actions regardless of consequences. This framework provides essential grounding for technology ethics by establishing inviolable principles such as respect for human dignity, autonomy, and rights that must be preserved regardless of technological benefits. In applied technology ethics, deontological thinking helps establish non-negotiable boundaries around issues like privacy, consent, and human agency that cannot be overridden by utilitarian calculations of greater good. • Rationalism (Enlightenment Era, 17th-18th Century) Rationalism emphasizes reason, logic, and systematic thinking as the primary sources of knowledge and ethical guidance. This movement is foundational to applied technology ethics because it provides methodological approaches for analyzing complex technological systems and their ethical implications through structured reasoning. Rationalist approaches help technologists and ethicists develop systematic frameworks for evaluating emerging technologies rather than relying solely on intuition or tradition. • Utilitarianism (18th-19th Century) Utilitarianism judges actions based on their consequences and seeks to maximize overall well- being or happiness for the greatest number of people. This consequentialist approach is highly relevant to technology ethics because it provides frameworks for weighing the benefits and harms of technological innovations across large populations. Applied technology ethics frequently employs utilitarian analysis when evaluating trade-offs between technological progress and potential societal risks, such as balancing AI efficiency gains against job displacement or privacy concerns. • Humanism (Renaissance/Modern Era, 15th-20th Century) Humanism places human dignity, agency, and flourishing at the center of ethical consideration. This perspective is crucial for technology ethics as it ensures that technological development serves human needs and preserves human agency rather than subordinating humans to technological systems. Applied technology ethics draws on humanist principles to advocate for human-centered design, meaningful human oversight of automated systems, and the preservation of human choice and autonomy in technological environments."}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 7, "chunk_id": "english_pack_7_1_13153154", "content": "center of ethical consideration. This perspective is crucial for technology ethics as it ensures that technological development serves human needs and preserves human agency rather than subordinating humans to technological systems. Applied technology ethics draws on humanist principles to advocate for human-centered design, meaningful human oversight of automated systems, and the preservation of human choice and autonomy in technological environments. Page 7 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 8, "chunk_id": "english_pack_8_0_ff20083b", "content": "Ethics in Technology By: Ed Weber • Feminism (19th-20th Century) Feminist ethics emphasizes care, relationships, context, and the examination of power structures, particularly how they affect marginalized groups. This movement brings essential perspectives to technology ethics by highlighting how technological systems can perpetuate or challenge existing inequalities and by advocating for inclusive design processes. Applied technology ethics incorporates feminist insights to address issues like algorithmic bias, the digital divide, and ensuring diverse voices are included in technological development and governance. • Phenomenology (20th Century) Phenomenology focuses on lived experience, consciousness, and how individuals encounter and make meaning of their world. This movement contributes to applied technology ethics by emphasizing the importance of understanding how people actually experience and interact with technology in their daily lives. Phenomenological approaches help bridge the gap between abstract ethical principles and the concrete realities of how technology affects human experience, informing more nuanced and contextually sensitive ethical frameworks. There are entire courses that could be taken on each one of these broad philosophical movements. But just knowing their names and descriptions can give you a starting point should you should choose to explore them in greater detail. Now, let’s take a quick look at just a few of the historical and contemporary ethicists who have studied, expanded upon, and have otherwise contributed greatly to the pursuit of understanding of applied ethics and summarize some of their major propositions and how they might approach ethical issues related to technology today. Ethicists and Their Approach to Technology Ethics • Immanuel Kant (Deontological Ethics) ◦ Ethical Center: Kant's ethical philosophy is grounded in the \"Categorical Imperative,\" asserting that moral duties are universal and rational, requiring actions that could be applied without contradiction to all individuals and treating humanity as an end in itself, never merely as a means. Morality stems from reason and duty, independent of consequences. ◦ Approach to Technology Today: Kant would scrutinize technological advancements through the lens of human dignity and autonomy, insisting that AI, data collection, and automation must never instrumentalize individuals. He would advocate for strict, universal ethical rules in technology design and"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 8, "chunk_id": "english_pack_8_1_72cf03b9", "content": "as an end in itself, never merely as a means. Morality stems from reason and duty, independent of consequences. ◦ Approach to Technology Today: Kant would scrutinize technological advancements through the lens of human dignity and autonomy, insisting that AI, data collection, and automation must never instrumentalize individuals. He would advocate for strict, universal ethical rules in technology design and usage, such as mandatory privacy by design and Page 8 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 9, "chunk_id": "english_pack_9_0_9f7ae389", "content": "Ethics in Technology By: Ed Weber algorithmic transparency, ensuring that technological systems uphold inherent human rights and rational agency. • René Descartes (Rationalism) ◦ Ethical Center: Descartes' ethical center is built on methodical doubt, systematic reasoning, and the pursuit of clear and distinct knowledge through logical analysis rather than relying on tradition or emotion. His approach emphasizes breaking down complex problems into manageable parts and building knowledge from foundational principles through careful reasoning. ◦ Approach to Technology Today: Addressing contemporary technology ethics, Descartes would advocate for systematic, step-by-step analysis of technological systems, demanding clear logical justification for each design choice and rejecting technological implementations based merely on convenience, profit, or popular opinion without rigorous ethical reasoning. • John Stuart Mill (Utilitarianism) ◦ Ethical Center: Mill's ethical center focuses on maximizing overall happiness and well- being while protecting individual liberty, emphasizing that the greatest good for the greatest number must be balanced against the fundamental importance of personal freedom and self- determination. His harm principle argues that society can only restrict individual liberty to prevent harm to others, creating a framework that values both collective welfare and individual autonomy. ◦ Approach to Technology Today: When applied to technology, Mill’s ideas guide ethical evaluations of innovations like autonomous vehicles or predictive algorithms, helping to weigh benefits (e.g., safety, efficiency) against societal costs (e.g., job loss, privacy erosion). • Martha Nussbaum (Humanism) ◦ Ethical Center: Nussbaum's ethical center emphasizes human capabilities, dignity, and flourishing, arguing that societies should be structured to enable all individuals to develop their full human potential across multiple dimensions of well-being. Her capabilities approach focuses on what people are able to do and be, rather than just material resources, emphasizing the importance of agency, practical reason, and meaningful relationships. ◦ Approach to Technology Today: Approaching contemporary technology ethics, Nussbaum would evaluate digital systems based on whether they enhance or diminish human capabilities – supporting technologies that expand access to education, meaningful work, and social connection while opposing those that create dependency, reduce critical thinking, or undermine human agency and authentic relationships. Page 9 of 125 2. Introduction, Ethical Frameworks"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 9, "chunk_id": "english_pack_9_1_0ffdf49d", "content": "◦ Approach to Technology Today: Approaching contemporary technology ethics, Nussbaum would evaluate digital systems based on whether they enhance or diminish human capabilities – supporting technologies that expand access to education, meaningful work, and social connection while opposing those that create dependency, reduce critical thinking, or undermine human agency and authentic relationships. Page 9 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 10, "chunk_id": "english_pack_10_0_a134bc32", "content": "Ethics in Technology By: Ed Weber • Carol Gilligan (Feminist Ethics) ◦ Ethical Center: Gilligan's ethical center emphasizes an ethics of care that values relationships, context, and responsibility, challenging traditional moral frameworks that prioritize abstract rights and justice over concrete care and connection. Her work highlights how ethical reasoning often involves understanding particular situations and maintaining relationships rather than applying universal principles, and she emphasizes the importance of listening to marginalized voices, especially women's moral perspectives. ◦ Approach to Technology Today: In addressing technology ethics today, Gilligan would focus on how digital systems affect relationships and care networks, advocating for inclusive design processes that center the experiences of marginalized users and questioning whether technologies strengthen or weaken our capacity for empathy, care, and authentic human connection. • Don Ihde (Phenomenology) ◦ Ethical Center: Ihde's ethical center focuses on human-technology relations and how humans and technologies mutually shape each other's existence, arguing that we cannot understand human experience without examining our relationships with technological artifacts. His postphenomenological approach emphasizes that technologies are neither neutral tools nor autonomous forces, but rather extend and transform human capabilities while simultaneously shaping how we perceive and act in the world. ◦ Approach to Technology Today: Approaching contemporary technology ethics, Ihde would analyze how specific technologies – from smartphones to AI systems – alter our ways of being-in-the-world, advocating for careful attention to how digital interfaces change our perceptual habits, social relationships, and bodily engagement with our environment, while emphasizing that ethical evaluation must consider the concrete, lived experience of human- technology interactions rather than abstract technological assessments. As you review the descriptions above and the works of these (or other) historical and contemporary ethicists, you will quickly discover that there are not singular, universally accepted definitions for the terms that are used to discuss ethics. As a matter of course, as scholars work to build their own understandings within their own disciplines, they often find themselves redefining previously accepted terms with new or altered meanings and nuances. In fact, you will discover completely new terms being coined to describe specific concepts or novel interpretations of"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 10, "chunk_id": "english_pack_10_1_9713dfff", "content": "definitions for the terms that are used to discuss ethics. As a matter of course, as scholars work to build their own understandings within their own disciplines, they often find themselves redefining previously accepted terms with new or altered meanings and nuances. In fact, you will discover completely new terms being coined to describe specific concepts or novel interpretations of life observations. Page 10 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 11, "chunk_id": "english_pack_11_0_7ace849a", "content": "Ethics in Technology By: Ed Weber Frameworks and Personal Lenses In this section we need to explore a bit more detailed information about myself as the author, and how this information is necessary to establish the framework in which the rest of the material for this text will be presented. It is highly suggested that everyone who utilizes this text (instructors and students alike) prepare their own supplemental Full Disclosure section (found below) to help you to identify and understand your own personal frameworks and personal lenses. This will greatly facilitate future topic discussions. Full Disclosure: Your textbook author is a 61-year-old, white, cisgender, heterosexual married man. He was born and raised Catholic and went to Catholic elementary and high schools. He grew up in the Midwest of the USA in a lower-middle-class household within a homogeneous neighborhood (re: ethnicity, religion, socioeconomic status). He has been married to his wife, Kim for 40 years and they have no children by choice (… except for dogs… there will always be dogs!) Your author currently consider himself to be in the middle class on a socioeconomic scale. While your author does have some minor health issues and wears glasses to correct his vision, he would consider himself non-disabled. He is no longer a Catholic and identifies instead as an agnostic. Your textbook author initially earned a Certificate of Proficiency in Data Processing in the late 1980s. This credential allowed him to begin a decades-long career in Information Technology. Much later on, he earned a Bachelor’s Degree in Psychology with a Minor in Computer Science after having already been actively working in the field for over 20 years. In 1995, he started his own computer consulting firm which is still active today. In the mid-2000s, he earned a Master’s Degree in Computer Science so as to facilitate his transition into full-time teaching at colleges and universities. He considers himself to be a life-long learner and enjoys working with others on challenging activities. Now, considering these details about your textbook author, I am going to shift modes into a conversational mode just to talk"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 11, "chunk_id": "english_pack_11_1_ecd01528", "content": "the mid-2000s, he earned a Master’s Degree in Computer Science so as to facilitate his transition into full-time teaching at colleges and universities. He considers himself to be a life-long learner and enjoys working with others on challenging activities. Now, considering these details about your textbook author, I am going to shift modes into a conversational mode just to talk to you about why I shared all of those details: I have shared these detail – not because the I consider myself to be significant or special in any way – but rather it is because these facts (and more) are the things that make up the ‘who’ that I am today as I compose this text. And, as a matter of practice, whenever I attempt to think about the ethics of a given situation or technology, I try to actively take a step backwards and try to view the situation through all of the lenses that make me who I am. It can be very enlightening when I discover, “… Oh! That is where my attitude about this particular thing is coming from!” Sometimes, this type of introspection leads me to be able to say, “Wait a minute! It isn’t ME that thinks this particular way about this particular thing… Rather, I was taught/told/indoctrinated/instructed to think this particular way about this particular thing! And now that I really think about it – for myself – I realize that I am actually free to choose how I actually think about this thing – all on my own!” So, I offer up my own details here just so that you will have a good understanding of the framework that I am using to present and discuss the various concepts we cover throughout the rest of this text. It Page 11 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 12, "chunk_id": "english_pack_12_0_dc3ad10d", "content": "Ethics in Technology By: Ed Weber is by intentionally inspecting and acknowledging our own lenses, that we might be able to uncover and appropriately restrain our own preconceived biases. As you engage with the concepts and case studies in this textbook, it is important to recognize the unique set of experiences, values, and perspectives that shape your own ethical viewpoints. In this text, we will call these perspectives your personal lenses. Honest self-reflection can help you become more aware of your own assumptions and biases as you look through your personal lenses, allowing for more thoughtful and inclusive ethical reasoning. To help us consider these personal lenses, consider this internet meme: Figure 3: Truth perceived from different perspectives The person standing near the orange light source and looking at the shadow only might say that the shape making this shadow is a square or a cube. And from their perspective at that moment in time, this may seem to be true. But the person standing near the blue light source and looking at the shadow only might say that the shape making this shadow is a circle or a sphere. And, once again, from their perspective at that moment in time, this too may seem to be true. Page 12 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 13, "chunk_id": "english_pack_13_0_6fcc15bd", "content": "Ethics in Technology By: Ed Weber However, by taking a step back to view even more pertinent information, a third observer can see that the shape appears to be – as seen from this third perspective – a cylinder suspended in such a way that the two light sources cast these two unique shadows. Understanding that there may, in fact, be a difference between ‘Truth’ as defined with a capital ‘T’ when viewed from an objective perspective, as compared to what appears to be ‘true’ from our own unique perspective at any moment in time, may help us think more critically about the importance of understanding our own personal lenses. Consider asking yourself the following questions: • What aspects of my background (such as age, race, ethnicity, gender identity, sexual orientation, religion, socioeconomic status, education, or geographic location) have most influenced the way I see the world? • How did my family, community, or culture shape my attitudes toward technology, authority, and ethics? • What are my core values, and where did they come from? Have any of my values changed over time? If so, why? • Are there beliefs or viewpoints I hold mainly because they were taught to me, rather than ones I have critically examined for myself? • How do my personal experiences with privilege or marginalization affect the way I interpret ethical dilemmas? • Have I ever changed my mind about a major ethical issue? What prompted that change? • In what ways do my current roles (student, employee, family member, etc.) influence my perspective on ethical questions? • Are there perspectives or experiences I am less familiar with? How might I seek out and learn from voices different from my own? • When I encounter a viewpoint that challenges my own, how do I typically respond? Am I open to reconsidering my position? • What steps can I take to recognize and address my own biases as I study Ethics in Technology? By thoughtfully considering these questions, you can better understand the framework through which you interpret ethical issues and strive for greater objectivity and"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 13, "chunk_id": "english_pack_13_1_226bc9f3", "content": "a viewpoint that challenges my own, how do I typically respond? Am I open to reconsidering my position? • What steps can I take to recognize and address my own biases as I study Ethics in Technology? By thoughtfully considering these questions, you can better understand the framework through which you interpret ethical issues and strive for greater objectivity and empathy in your analysis. Let’s also look at this concept of reviewing a situation both passively (ignoring our own personal lenses and perspectives) and then subsequently reviewing the same situation intentionally Page 13 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 14, "chunk_id": "english_pack_14_0_56da74fb", "content": "Ethics in Technology By: Ed Weber (acknowledging and evaluating our own personal lenses and perspectives.) Consider this initial internet meme: Figure 4: 6 vs. 9 as evaluated from one's own perspective This meme, similar to the one shown in the Figure 2 earlier in this chapter, illustrates how the same ‘Truth’ (with a capital ‘T’) can actually represent two significantly different ‘truths’ when one only focuses on their own personal lenses and their own current perspective. In fact, the caption within this meme attempts to establish the importance of trying to help the viewer consider what the other perspectives may look like. But is this easy to do? Consider, for example, all of the personal lenses that this textbook author shared in the previous section. Will I ever be able to realistically look at (or more rightly so – even imagine) any situation through the lenses of a non-white, economically struggling, lesbian, Baptist, still wanting to finish their GED someday…? My personal lenses are so significantly different than their lenses – and yet we may be looking at the exact same situation. But is this realistic? Is it realistically even possible for us to completely recognize, understand, acknowledge and then fully contain all of our own personal lenses and our own current, unique perspective? Are we certain that there may not be any residual biases still influencing our interpretation of what we perceive to be both ‘true’ as well as the ‘Truth’? Do I have what it takes to be able to step away from my own personal lenses (without forgetting them or failing to acknowledge them) so that I can try to see through what I imagine another person’s personal lenses might look like? Do I also recognize that my own imaginations of someone else’s Page 14 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 15, "chunk_id": "english_pack_15_0_08dd481e", "content": "Ethics in Technology By: Ed Weber personal lenses may be pure fantasy? How can I go about really trying to understand what another’s personal lenses really look like for them? For me, this kind of effort begins and ends with communications. Finding ways to actually ask another about their own personal lenses and perspectives seems to be a profound beginning. Then, actively listening as they share their own experiences – without judgment – and trying to repeat back your own understanding of what they actually say seems like an effective follow-up. Only after we have a shared understanding of each other’s personal lenses can we really begin to have effective ethical discussions about various technical topics. Now, for contrast, consider this follow-up internet meme that was based on the original theme: Figure 5: 6 vs. 9 reconsidered from an objective view This meme attempts to suggest that ‘Truth’, with a capital ‘T’ exists (a concept that has been heavily debated by many ethicists throughout history!) and can be justly recognized if one simply chooses to step away from their own perspective and view the situation objectively. So, while we Page 15 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 16, "chunk_id": "english_pack_16_0_eea7b3c2", "content": "Ethics in Technology By: Ed Weber might agree with the concept of trying to step outside of our own personal lenses to see a situation from a different perspective, this meme author seems more intent to show that the value from the act itself isn’t about fostering empathy for each person’s perspective (and personal lenses). Rather, this meme author seems to imply that the value is more-so that one of these people can use this technique to prove the other one wrong! For this unknown meme author, it’s not about empathy but rather it is about winning. Key Foundational Concepts As you continue deeper into this text, you will discover there are a number of key concepts that are used repeatedly to help facilitate greater discussions and understandings of the various topics. This list will also serve as a preview of some of the upcoming chapter concepts. • Definition of terms – Whenever not previously defined, new critical terms will be defined at each introduction so as to present what I, as the author, am using as the definition of each given term. This does not mean I am claiming supreme authority regarding the term! Rather, it is the starting point from which you can determine if you agree with the definition or not and what, if any, compromises might need to be made so as to have a meaningful discussion. It is imperative that individuals who intend to have meaningful interactions at least have a shared understanding of the terms being used – if not full agreement regarding the terms! • Technology can be found almost everywhere – If you are reading this text, it is most likely that you are already familiar with very many forms of technology. For the vast majority of individuals, the year 2025 is synonymous with technology being integrated into almost every aspect of our existence. And as such, technology is often taken for granted without full consideration of any potential ethical considerations of that tech. For example, how often do we think about how a particular piece of tech was developed or created"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 16, "chunk_id": "english_pack_16_1_100ca374", "content": "For the vast majority of individuals, the year 2025 is synonymous with technology being integrated into almost every aspect of our existence. And as such, technology is often taken for granted without full consideration of any potential ethical considerations of that tech. For example, how often do we think about how a particular piece of tech was developed or created or distributed? Do we ever wonder just who has access to that tech and who does not have access? Are there any unintended uses of the tech that we should be thinking about? • The rate of change of technology is NOT linear – Throughout most of human history, technological change was slow and incremental, with major breakthroughs – like the control of fire, the invention of the wheel, and the development of agriculture – occurring over thousands of years. This gradual pace continued through the Bronze and Iron Ages and even into early modern times. However, beginning with the Industrial Revolution, the rate of technological advancement accelerated dramatically, as mechanization, mass production, and new forms of energy rapidly transformed societies in mere centuries. Now within less than the last 100 years, the emergence of computers, the internet, and digital technologies, have brought an unprecedented surge in innovation and human adoption rates. The current era is unique for its exponentially rapid, transformative impact on nearly every aspect of life. Page 16 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 17, "chunk_id": "english_pack_17_0_5a3065f4", "content": "Ethics in Technology By: Ed Weber • There is no universally accepted Ethical system – The absence of a universally accepted ethical system means that actions are often interpreted through a variety of moral frameworks, leading to nuanced discrepancies or even diametrically opposed viewpoints. For example, the act of taking another person’s life can be labeled as “murder,” “justifiable homicide,” or “self- defense,” depending on the ethical system, cultural context, or legal tradition applied. What one society or theory may condemn as inherently immoral, another may view as permissible or even obligatory under certain circumstances, illustrating how ethical judgments can range from subtle distinctions to fundamentally conflicting positions. This diversity reflects the influence of different normative theories – such as utilitarianism, deontology, virtue ethics, and cultural relativism – each offering its own criteria for evaluating right and wrong. This illuminates the complexity and subjectivity inherent in ethical reasoning. • Legal and Societal systems attempt to implement a well defined and accepted Ethical system – Legal and societal systems tend to follow a well-defined and accepted ethical system because such alignment provides a consistent moral foundation for laws and policies, ensuring that actions and decisions are guided by shared principles of fairness, justice, and respect for all individuals. This ethical grounding fosters public trust, transparency, and accountability, helping to protect the rights and interests of all members of society while reducing arbitrary or biased decision-making. Ultimately, integrating ethics into legal and societal frameworks promotes social cohesion, supports long-term sustainability, and enhances the legitimacy and effectiveness of institutions by aligning them with the values and expectations of the community. • When considering the Ethics of Technology, the issue is almost never the Technology, but rather the issue is almost always the Ethics – The ethical evaluation of technology often reveals that the core issue lies not with the technology itself, but with how humans choose to use it, as the same contemporary tool can enable outcomes at nearly opposite ends of the moral spectrum. For example, artificial intelligence can be harnessed to improve healthcare diagnostics, enhance disaster response, and promote environmental sustainability,"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 17, "chunk_id": "english_pack_17_1_ed429975", "content": "– The ethical evaluation of technology often reveals that the core issue lies not with the technology itself, but with how humans choose to use it, as the same contemporary tool can enable outcomes at nearly opposite ends of the moral spectrum. For example, artificial intelligence can be harnessed to improve healthcare diagnostics, enhance disaster response, and promote environmental sustainability, yet the very same AI systems can also be deployed for mass surveillance, autonomous weaponry, or discriminatory decision-making. This dual-use dilemma illustrates that technologies are inherently neutral, but their ethical character is defined by human intentions, societal values, and regulatory choices, resulting in applications that can be seen as highly beneficial or deeply problematic depending on context and use. Thus, it is the ethical framework guiding deployment and oversight – not the technology itself – that determines whether its impact is viewed as just, responsible, or harmful. • With technology, very often science fiction can be viewed as science fact that just hasn’t happened yet – Since the age of radio and television, science fiction authors, movies, and Page 17 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 18, "chunk_id": "english_pack_18_0_cb91c39c", "content": "Ethics in Technology By: Ed Weber shows have repeatedly imagined technologies that later became reality, often serving as inspiration or conceptual blueprints for real-world innovation. Classic examples include video calling, featured in \"The Jetsons\" and \"Metropolis,\" now realized through platforms like Zoom; wireless earbuds, reminiscent of Ray Bradbury’s \"Fahrenheit 451,\" now ubiquitous as devices like AirPods; and handheld communicators from \"Star Trek,\" which anticipated today’s smartphones. Science fiction has also envisioned self-driving cars, as described by Isaac Asimov, and immersive virtual reality, as seen in \"The Matrix,\" both of which are now active areas of technological development. Even broader concepts, such as global information networks and intelligent digital assistants, were explored in early fiction long before the internet or AI became commonplace. This demonstrates how speculative storytelling has consistently anticipated – and sometimes directly influenced – the trajectory of technological advancement. Textbook Definitions – Introduction, Ethical Frameworks and Personal Lenses • right vs. wrong – The distinction between actions or choices considered morally acceptable and those considered morally unacceptable. • good vs. evil – The contrast between that which is morally virtuous, beneficial, or constructive and that which is morally wrong, harmful, or destructive. • rightness vs. wrongness – The quality of being in accordance with moral or ethical principles versus being in violation of them. • defining terms – The process of clearly explaining the meaning of words or concepts to ensure clarity and understanding. • shared understanding – A mutual agreement or common interpretation of ideas, terms, or values among individuals or groups. • Ethics – The branch of philosophy concerned with moral principles that govern behavior and decision-making. • Deontological Ethics – An ethical theory that judges the morality of actions based on adherence to rules or duties, regardless of their consequences. • Rationalism – A philosophical view that emphasizes reason and logical analysis as the primary sources of knowledge and ethical judgment. • Utilitarianism – An ethical framework that evaluates actions based on their outcomes, aiming to maximize overall happiness or well-being. Page 18 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Foundations; Frameworks and Personal Lenses; Key Concepts", "page": 19, "chunk_id": "english_pack_19_0_c350a455", "content": "Ethics in Technology By: Ed Weber • Humanism – A worldview that centers human dignity, agency, and the promotion of individual and collective flourishing as ethical priorities. • Feminism – An ethical and social movement that advocates for gender equality and emphasizes the importance of care, context, and power dynamics in moral decision-making. • Phenomenology – A philosophical approach that focuses on individuals’ lived experiences and the ways in which they perceive and interpret the world around them. • personal lenses – The unique perspectives shaped by an individual’s experiences, values, and cultural background through which they interpret the world. • rate of technological advancement – The speed at which new technologies are developed and adopted within society. Page 19 of 125 2. Introduction, Ethical Frameworks and Personal Lenses"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 20, "chunk_id": "english_pack_20_0_9a982f26", "content": "Ethics in Technology By: Ed Weber 3. Defining Ethics and Related Terminology Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases Why are we here? Wow! If THAT isn’t a loaded question! But that is exactly what we would like to tackle at this point. Why should we think about this concept of Ethics and how it can be defined and applied to technology? To get our own thoughts germinating, I asked several generative AI platforms (Perplexity, ChatGPT, Claude, and Gemini) to provide a hypothetical response to this question as written from the perspective of several selected ethicists. Here is what was generated: • Immanuel Kant (Deontological Ethics) \"We must think about ethics in technology because human beings possess inherent dignity as rational agents, and technology must never treat us merely as means to an end. Every technological system embeds moral choices about how humans should be treated, and without ethical reflection, we risk creating tools that violate our fundamental duty to respect human autonomy and dignity. The categorical imperative demands that we only develop technologies whose principles we could will to be universal laws—would we want to live in a world where all technology operates according to the ethical assumptions built into our current systems?\" • René Descartes (Rationalism) \"We must apply systematic ethical reasoning to technology because, without methodical analysis, we build upon uncertain foundations that may lead us astray. Just as I doubted everything to reach clear and distinct knowledge, we must doubt the moral assumptions embedded in our technologies and rebuild them on solid rational principles. Technology amplifies human action, so any errors in our ethical reasoning will be magnified across society —we cannot afford to proceed on mere tradition, convenience, or popular opinion when the stakes are so high.\" • John Stuart Mill (Utilitarianism) \"Ethics in technology matters because technology's power to generate both tremendous benefits and serious harms across vast populations demands careful calculation of consequences. We must evaluate whether our technological choices truly maximize overall human happiness and well-being, while vigilantly protecting individual liberty from"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 20, "chunk_id": "english_pack_20_1_1fee3caa", "content": "mere tradition, convenience, or popular opinion when the stakes are so high.\" • John Stuart Mill (Utilitarianism) \"Ethics in technology matters because technology's power to generate both tremendous benefits and serious harms across vast populations demands careful calculation of consequences. We must evaluate whether our technological choices truly maximize overall human happiness and well-being, while vigilantly protecting individual liberty from technological systems that might restrict human self-determination. The harm principle requires us to examine whether our technologies prevent individuals from pursuing their own good in their own way, even when such technologies might benefit the majority.\" Page 20 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 21, "chunk_id": "english_pack_21_0_94f50f68", "content": "Ethics in Technology By: Ed Weber • Martha Nussbaum (Humanism) \"We need ethics in technology because technology should serve human flourishing by expanding rather than constraining human capabilities. Every technological design choice affects whether people can live fully human lives—whether they can engage in practical reason, form meaningful relationships, experience joy, exercise bodily autonomy, and participate in political life. Without ethical reflection, we risk creating technologies that diminish rather than enhance what makes us human, particularly for those who are already marginalized or vulnerable in society.\" • Carol Gilligan (Feminist Ethics) \"Ethics in technology is essential because technological decisions affect relationships, care networks, and the voices that get heard in our society. We must ask not just whether technologies work efficiently, but whether they strengthen or weaken our capacity for care, empathy, and connection with others. Ethical reflection helps us recognize whose perspectives have been excluded from technological design and ensures that we consider how our tools affect the most vulnerable members of our communities, particularly when those voices are often silenced in traditional ethical frameworks.\" • Don Ihde (Phenomenology) \"We must think ethically about technology because humans and technologies co-constitute each other's existence—there is no neutral technology, and every technological relation shapes how we perceive, act, and experience the world. Ethics helps us examine how specific technologies alter our ways of being-in-the-world, sometimes in ways we don't immediately recognize. Without phenomenological reflection on our actual lived experience with technologies, we may inadvertently adopt tools that fundamentally change what it means to be human without conscious deliberation about whether those changes align with our values.\" Did these hypothetical responses from historic and contemporary ethicists actually help you discover or clarify your own reason for being here? For some individuals, simply reviewing what others have previously thought or discovered about a topic can be truly enlightening and inspiring! Academics and Ethicists through the ages have invested much of their lives and studies to review, categorize, differentiate, and subsequently expand and expound on what others have to say about Ethics. And sometimes, they find that one or more of these other people"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 21, "chunk_id": "english_pack_21_1_c1928e67", "content": "reviewing what others have previously thought or discovered about a topic can be truly enlightening and inspiring! Academics and Ethicists through the ages have invested much of their lives and studies to review, categorize, differentiate, and subsequently expand and expound on what others have to say about Ethics. And sometimes, they find that one or more of these other people had previously found ways to express, precisely or nearly so, just what it was that they, themselves, think about the topic. And yet for other individuals, the flood of disparate and often contradictory positions on the supposed same topic seems to simply add levels of confusion and introduce unintentional biases. Page 21 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 22, "chunk_id": "english_pack_22_0_23aa67a2", "content": "Ethics in Technology By: Ed Weber These individuals feel that the research actually gets in the way of their own ability to figure anything out for themselves. So, whichever kind of person you may be, I’d like to re-ask the questions: Why are we here? Why should we think about this concept of Ethics and how it can be defined and applied to technology? Very early in my career, I learned an important lesson that has informed my entire professional and teaching philosophy: When working with adults, if all individuals involved can first understand and agree on ‘the WHY’ of whatever it is that they are doing, then adults tend to be more receptive to fully engage in the related tasks. So, for me, I try to consistently first ask and answer the question, “Why are we doing this again?” before we even get started. For me, the WHY is that, as a technology professional for my entire career, I have seen too often – first hand – what the unethical implementation of technology looks like and what I have perceived to be the harmful ramifications of these unethical situations. And as such, I choose to intentionally explore the ethics surrounding technology so that I can be more prepared to work with students, colleagues and clients as we collectively discover just what the ethical issues might be. This has helped me identify and recognize some of my own biases and has helped me learn how to be more objective in my ethical analysis of new technological advancements. Also, as previously mentioned, the proactive intentional examination of the Ethics of Technology may reward individuals by helping them to feel more prepared when they find themselves required to make decisions or take actions with significant ethical implications. Textbook Definitions – Defining Ethics and Related Terminology In order to create a baseline of terminology that will be used throughout the rest of the text, this section now provides a collection of terms along with definitions to be used as some of the basic ‘textbook definitions’ for continuing conversations. These definitions are not to"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 22, "chunk_id": "english_pack_22_1_74e4c37e", "content": "actions with significant ethical implications. Textbook Definitions – Defining Ethics and Related Terminology In order to create a baseline of terminology that will be used throughout the rest of the text, this section now provides a collection of terms along with definitions to be used as some of the basic ‘textbook definitions’ for continuing conversations. These definitions are not to be considered absolute! In fact, it will be important for you to ask yourself if you agree with the definition as presented or, if not, what aspects do you think may need clarification. Also, in future chapters, the additional terms will be listed at the end of the chapter vs. here, in the middle of the chapter. • Ethics – The systematized principles and standards of right and wrong behavior, typically established and endorsed by a community or society. • Morals – Personal beliefs and internalized values about what is right and wrong, guiding individual behavior. • Virtues – Positive character traits or qualities, such as honesty or courage, that are considered morally good and enable individuals to act in alignment with ethical principles. Page 22 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 23, "chunk_id": "english_pack_23_0_889b43f8", "content": "Ethics in Technology By: Ed Weber • Vices – Negative character traits or habits, such as dishonesty or greed, that are considered morally bad and detract from ethical behavior. • Beliefs – Convictions or accepted ideas that shape an individual's morals, virtues, and worldview. • Right – Actions or behaviors that are considered morally or ethically acceptable or good. • Wrong – Actions or behaviors that are considered morally or ethically unacceptable or bad. • Education – The process of acquiring knowledge, skills, values, and attitudes, often shaping ethical understanding and personal development. • Traditions – Long-standing customs or practices passed down within a culture or community, influencing values and behavior. • Life Experiences – Personal events and interactions that shape one’s perspectives, beliefs, and ethical outlook. • Culture – The shared values, norms, practices, and artifacts of a group that influence behavior and ethical perspectives. • Religion – Organized systems of beliefs and practices related to the sacred or divine, often providing ethical guidance. • Gender – Socially constructed roles, behaviors, and identities associated with being male, female, or non-binary, influencing experiences and perspectives. • Age – The length of time a person has lived, often affecting their perspectives and ethical viewpoints. • Personal Lenses – The unique set of experiences, values, and perspectives that shape your own current ethical viewpoints. • Critical Thinking – The disciplined process of actively analyzing, evaluating, and synthesizing information to form reasoned judgments and make well-informed decisions. • Semantics – The branch of linguistics that studies the meaning of words, phrases, and sentences, examining how meaning arises from language structure, word choice, and context to convey and interpret information. • Individual – A single person, distinct from a group, with unique experiences and perspectives. • Group – A collection of individuals who interact and share common characteristics, goals, or interests. • Group Member – An individual who belongs to and participates in a group. Page 23 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 24, "chunk_id": "english_pack_24_0_8952625e", "content": "Ethics in Technology By: Ed Weber • Integrity – The quality of being honest and having strong moral principles, consistently acting in accordance with ethical standards. • Law – A system of rules established by a governing authority to regulate behavior within a society. • Legal – Actions or behaviors that are permitted or recognized by law. • Illegal – Actions or behaviors that are forbidden by law. • Moral – Conforming to principles or standards of right conduct; virtuous. • Amoral – Lacking a sense of morality or indifference to right and wrong. • Ethical – In accordance with accepted principles of right and wrong, especially within a professional or societal context. • Unethical – Contrary to accepted standards of right and wrong. • Misconduct – Improper, unethical, or illegal behavior. • Lying – Knowingly making a false statement with the intent to deceive. • Cheating – Acting dishonestly or unfairly to gain an advantage. • Stealing – Taking something that does not belong to you without permission or legal right. • Abusive Behavior – Actions that cause harm, mistreatment, or suffering to others. • Discrimination – Unjust or prejudicial treatment of individuals or groups based on characteristics such as race, gender, age, or religion. • Hazardous – Posing a risk of harm or danger. • Conflict of Interest – A situation in which a person’s personal interests could improperly influence their professional decisions or actions. • Falsifying Information – Deliberately altering, inventing, or misrepresenting information with the intent to deceive. • Honesty – The quality of being truthful, transparent, and free from deceit. • Fairness – Treating people equally and justly, without favoritism or discrimination. • Responsibility – The obligation to act correctly and be accountable for one’s actions. • Duty – A moral or legal obligation to perform or refrain from certain actions. Page 24 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 25, "chunk_id": "english_pack_25_0_4a1079e2", "content": "Ethics in Technology By: Ed Weber • Obligation – A requirement to act in a particular way, often arising from law, contract, or moral principle. • Stakeholder – Any individual or group affected by or having an interest in the actions and decisions of an organization. • Shareholder – An individual or entity that owns shares in a corporation and has a financial interest in its performance. • Consumers – Individuals or groups who purchase and use goods or services. • Customers – People or organizations that buy goods or services from a business. • Employers – Individuals or organizations that hire and pay people to work for them. • Suppliers – Entities that provide goods or services to other organizations. • Community – A group of people living in the same area or sharing common interests, values, or goals. • Environment – The natural world, including air, water, land, and ecosystems, affected by human activity. • Corporation – A legal entity that is separate from its owners, with its own rights and responsibilities. • Corporate Social Responsibility – A business model in which companies integrate social and environmental concerns into their operations and interactions with stakeholders. • Sustainability – Meeting present and continuing needs without compromising the ability of future generations to meet their own needs, especially regarding environmental stewardship. • Consistency – Acting in the same way over time, maintaining coherence in values, principles, and behavior. • Goodwill – A positive reputation or relationship built through ethical actions and trustworthiness. • Protection – The act of keeping people, property, or the environment safe from harm. • Favorable – Producing or indicating a positive outcome or approval. • Unfavorable – Producing or indicating a negative outcome or disapproval. • Diversity – The presence of a wide range of different characteristics, backgrounds, and perspectives within a group or organization. • Respect – Recognition and regard for the rights, feelings, and dignity of others. Page 25 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 26, "chunk_id": "english_pack_26_0_dd583499", "content": "Ethics in Technology By: Ed Weber • Principles – Fundamental truths or rules that guide behavior and decision-making. • Standards – Established benchmarks or criteria used to measure and guide conduct or performance. • Reward – Something given in recognition of service, effort, or achievement. • Punishment – A penalty imposed for wrongdoing or violation of rules. • Profit – The financial gain obtained when revenue exceeds expenses. • Loss – The negative financial result when expenses exceed revenue. • Empowered – Having the authority, confidence, or power to make decisions and take action. • Disenfranchised – Deprived of rights, power, or access, especially to participate in decision- making. • Code of Ethics – A formal set of guidelines and principles designed to help professionals conduct business honestly and with integrity. • Leading by Example – Demonstrating desired behaviors and standards through one’s own actions, serving as a model for others. As new content is introduced, additional terms will be collected and presented at the end of each chapter to facilitate future discussions using those same terms. If you feel strongly that your own personal definition of any particular term varies significantly from these ‘textbook definitions’, you should always discuss these differences with the rest of the individuals in your discussion group to determine how these differences may or may not affect the ongoing conversations. It is important to not simply dismiss these differences as minor differences in semantics when, in fact, these differences may represent the root cause of major differences in perspectives. As you can see, in order to fully examine a concept like Ethics in Technology, a large part of the preparatory work involved is making sure that we each have a shared, common baseline from which to begin our work together. This includes coming to a shared agreement on a large number of terms that may effect the conversations. Here are just a few examples of terms that will need to be defined in upcoming chapters as part of their exploration: intention, manipulation, transparency, true vs. false, excess, greed, bias, preferential treatment, conflict of interest, bribery"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 26, "chunk_id": "english_pack_26_1_688dca6d", "content": "to begin our work together. This includes coming to a shared agreement on a large number of terms that may effect the conversations. Here are just a few examples of terms that will need to be defined in upcoming chapters as part of their exploration: intention, manipulation, transparency, true vs. false, excess, greed, bias, preferential treatment, conflict of interest, bribery vs. gift, fraud, whistle-blowing, negligence, reasonable person, adult, child, assault, threat, harm, freedom of speech, hate speech, censorship, obscenities, rights, privileges, national security, responsibility, liability, expectation of privacy, safety, liberty, risk, customer lock-in, Page 26 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 27, "chunk_id": "english_pack_27_0_84d0f200", "content": "Ethics in Technology By: Ed Weber profit, loss, capitalism, socialism, communism, feudalism, representation, health and welfare, marketing, stalking, exploitation, employee rights vs. employer rights, … so many more… Not to mention all of the technology-related terms we will explore in future chapters such as: automation, robotics, artificial intelligence (AI), big data, natural language processing (NLP), machine learning, telecommunications, vulnerabilities, spam, phishing, virtual private networks (VPNs), biometrics, passwords, personally-identifiable information (PII), firewalls, prevention, surveillance, remote monitoring, doxing, sexting, copyright, trademark, intellectual property, trade secret, cybersquatting, industrial espionage, quality assurance, telemedicine, 3D printing, 3D bio- printing, planned obsolescence, chatbots, digital assistants, product liability, breaches, electronic medical records (EMR), cyberstalking, cyberbullying, sexual predators, revenge porn, … and so many more of these as well! Ethical vs. Legal Using the definitions of Ethical, Unethical, Legal, and Illegal, as defined above, consider this matrix: Figure 6: Ethical vs. Legal Matrix Several concepts that tend to be regularly (but incorrectly) confounded during discussions, are the concepts of something being either Ethical or Unethical, as well as that same thing being either Legal or Illegal. Ethics and Legalities are two different concepts. As you can see by the matrix above, a single item or situation can exist in any single one of the white cells. In other words, something can be both Ethical as well as Legal – which many individuals think is the ideal situation. But our shared reality has shown us that the ideal situation isn’t always what we experience. Rather, you may discover that another thing may be completely Legal, while simultaneously being completely Unethical. It many societies, one can simply examine the current or historic laws or codes of conduct and they will inevitably be able to discover numerous instances of Page 27 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 28, "chunk_id": "english_pack_28_0_34cf9ed0", "content": "Ethics in Technology By: Ed Weber laws that, at the time, were considered Legal, however they can simultaneously be considered completely Unethical if not abhorrent. Likewise, there can be many situations where something is completely Ethical, but may be simultaneously Illegal! And finally, something could be completely Unethical and also completely Illegal. We all have probably seen these kinds of situations as these often end up as the Breaking New stories because of their often significant shock factor! Take a few minutes now and see if you can come up with several instances of situations for each of those four possible matrix cells. Then, ask yourself how you would be able to present, to someone else, your reasoning for placing each individual situation into its own respective cell. Understanding the ‘WHY’ that you have used to come to your conclusions is a major aspect of applied critical thinking skills. Critical Thinking and Limiting Biases Thinking now about the terms that have been presented in this chapter, what happens when one or more of our personal lenses illuminates a conflict when accepting the definition of a particular term? What happens when an education lens defines a concept one way and that definition is in conflict with a definition previously constructed through a religious lens? What about when a definition changes when viewed through a gender lens vs. using a cultural lens? How about when your family lens defines something one way, but your work community defines that same thing a different way? Do you know how to first identify whenever there is a conflict in definitions? Do you then know how to isolate the differences so that points of agreement and points of disagreement can be fully fleshed out? The ability to review your own perspectives from an objective point of view is a very large part of applying the concept called Critical Thinking. This activity can help you to become well-prepared to have in-depth, meaningful discussions about the underlying ethics of any given topic. This helps you be able to see things more clearly through your own personal lenses as"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 28, "chunk_id": "english_pack_28_1_b4e7f779", "content": "to review your own perspectives from an objective point of view is a very large part of applying the concept called Critical Thinking. This activity can help you to become well-prepared to have in-depth, meaningful discussions about the underlying ethics of any given topic. This helps you be able to see things more clearly through your own personal lenses as well as helps you be able to more closely understand how others see the same concept through their own personal lenses. In the previous section, you were asked to think of several situations that might be appropriately placed into each of the four white cells in the Ethical vs. Legal Matrix above. Re-consider those issues or situations once again, but this time, review each instance or situation while intentionally looking through your multiple personal lenses. Can you identify which lens or lenses most significantly influence your decisions as you categorized where each situation should end up? Do the lenses that you use weigh more heavily with one lens vs. the others? What if someone else’s lenses would put the same situation into a different cell? Are you prepared to have an objective conversation with that person to delve down to discover the root differences? Page 28 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Why are we here?; Textbook Definitions – the basics; Ethical vs. Legal; Critical Thinking and Limiting Biases", "page": 29, "chunk_id": "english_pack_29_0_c52c1dc6", "content": "Ethics in Technology By: Ed Weber The more you can practice these skills of self-reflection, the greater your capacity will become to truly understand yourself as well as your ability to empathize with those who may be viewing situations through other personal lenses or through lenses with different weights than your own. This effort pays for itself through greater shared understandings and, hopefully, with greater positive ethical decisions. Page 29 of 125 3. Defining Ethics and Related Terminology"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 30, "chunk_id": "english_pack_30_0_44e30429", "content": "Ethics in Technology By: Ed Weber 4. Ethics for Tech Developers and Tech Consumers Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making Tech Consumer Responsibilities Considering the typical audience for this textbook, nearly 100% of you would most likely consider yourselves to be consumers of tech. Take a few moments to think about all of the tech that you regularly use as a part of your day-to-day experiences. From checking your smartphone for messages and social media updates, to using laptops or tablets for school or work assignments, streaming music and videos, playing video games, and even relying on smartwatches or fitness trackers to monitor your health – technology is woven into nearly every aspect of your daily routine. Whether you’re ordering food through an app, catching up on news, attending virtual classes, or collaborating on group projects using cloud-based tools, it’s clear that tech plays a central role in how you learn, work, connect, and entertain yourselves. What would be some of the ethical considerations for consumers of tech? As a starting point for this discussion, I posed this exact question to a generative-AI tool (Perplexity) to see the response. Here is what Perplexity had to say: “Some key ethical considerations for consumers of tech include protecting personal privacy and data, being aware of how their information is collected and used, ensuring informed consent when using digital services, recognizing and avoiding technologies or companies that engage in discriminatory or unfair practices, respecting intellectual property, and considering the broader social and environmental impacts of their tech use. Consumers should also be mindful of the potential for technology to amplify bias or harm vulnerable groups, and strive to make choices that support transparency, fairness, and accountability in the tech industry.” Interesting indeed! Let’s look at each of these concepts and turn them each into questions for you to review through your own personal lenses: • protecting personal privacy and data – Is your personal privacy and data actually protected? Do you know where (all) this data exists and who (all) has access to it? Does it really need to be kept"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 30, "chunk_id": "english_pack_30_1_77050dc8", "content": "indeed! Let’s look at each of these concepts and turn them each into questions for you to review through your own personal lenses: • protecting personal privacy and data – Is your personal privacy and data actually protected? Do you know where (all) this data exists and who (all) has access to it? Does it really need to be kept private and protected? • being aware of how their information is collected and used – Do you have any control (or say) over what is collected or how it might be used (or shared, or sold)? Does it matter? Page 30 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 31, "chunk_id": "english_pack_31_0_87e950ad", "content": "Ethics in Technology By: Ed Weber • informed consent when using digital services – Have you willingly and knowingly given away your information? How about your rights? Can you change your mind? If you do change your mind, is it already too late? Do you have any recourse? • recognizing and avoiding technologies or companies that engage in discriminatory or unfair practices – Do you ever investigate the provider(s) of the tech you consume and does that information affect your decision to work with (and consequently support and enable) those companies? Do you have any responsibility to not support and enable unethical companies? • respecting intellectual property – Do things like pirating games, software, or music have ethical implications? Does it matter if we’re talking about an artist’s intellectual property or a company’s intellectual property? What about your colleagues’ work? If they didn’t ‘legally’ protect it, is it fair-game? • considering the broader social and environmental impacts – Does it matter that buying the newest version of a phone will mean that the ‘old’ phone may become e-waste with long-term global environmental harm? Does it matter when trash-talking someone in a gaming app if we are completely unaware of their own propensity toward self-harm? Do you have any responsibility and/or culpability in the outcomes that derive from your own actions with tech? • technology amplifying bias or harming vulnerable groups – Am I using platforms or tools that have been shown to perpetuate or amplify biases, such as facial recognition systems with higher error rates for people with darker skin tones or recommendation algorithms that reinforce stereotypes or exclude certain groups? How might my engagement with algorithm-driven content – such as clicking, sharing, or purchasing – contribute to feedback loops that reinforce existing biases or marginalize underrepresented communities? As technology becomes ever more integrated into daily life, intentionally reflecting on how we interact with it is essential for fostering a more ethical and responsible digital world. By asking ourselves thoughtful questions about privacy, consent, fairness, and the broader impacts of our choices, we can move beyond passive consumption and become"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 31, "chunk_id": "english_pack_31_1_9fb0baf7", "content": "that reinforce existing biases or marginalize underrepresented communities? As technology becomes ever more integrated into daily life, intentionally reflecting on how we interact with it is essential for fostering a more ethical and responsible digital world. By asking ourselves thoughtful questions about privacy, consent, fairness, and the broader impacts of our choices, we can move beyond passive consumption and become conscious participants in shaping the tech landscape. This means not only protecting our own interests but also considering the well-being of others and the environment, holding companies accountable, and striving for transparency and inclusivity in all our digital interactions. Through ongoing awareness and deliberate action, each of us can contribute to a culture where technology empowers rather than exploits, and where ethical considerations guide both innovation and everyday use. Professional Codes Page 31 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 32, "chunk_id": "english_pack_32_0_54f62bcf", "content": "Ethics in Technology By: Ed Weber Now, for some of you, technology isn’t going to be something you simply consume. Rather, some of you may be heading down a path toward becoming a technology practitioner. As a technology practitioner, all of the previously discussed ethical responsibilities of a tech consumer exist – first and foremost – but there are also additional ethical factors that will be found in addition to those basic considerations. Some common tech-related job titles include the following: • Software Engineer • AR/VR Developer • Data Scientist • Systems Administrator • Network Architect • Cloud Architect • Cybersecurity Analyst • Business Systems Analyst • IT Support Specialist • UI/UX Designer • DevOps Manager • AI Computer Scientist Think of all of the people that you will have an ethical relationship with as a part of your technology-based profession. This diagram represents some of the main relationships you will experience in your IT careers. In your tech-related path, you will find yourself working with very many different individuals who may have many different expectations as to how you will apply ethics in your day-to-day interactions. Think about all of the different people shown here and remember that each person has their own set of personal lenses which they are using (whether intentionally or subconsciously) during each interaction with you. Often, the details of these relationships may be spelled out (at least partially) via various relationship agreements. These agreements can take many forms (i.e. contracts, non-disclosure agreements, license agreements, professional codes of conduct, etc.) with many of these forms having both ethical and legal Figure 7: IT Professionals' Ethical Relationships ramifications. Page 32 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 33, "chunk_id": "english_pack_33_0_bdc51ad3", "content": "Ethics in Technology By: Ed Weber But at other times, the details of the relationships are not spelled out at all! And, as a result, conflicts can certainly arise when it becomes evident that there are competing interests being considered and viewed through conflicting personal lenses. A great starting point for considering the additional ethical responsibilities of a tech practitioner is to review the Association for Computing Machinery (ACM) Code of Ethics and Professional Conduct. This document attempts to codify the ethical responsibilities of tech professionals. But with even a cursory review of this professional code, one can easily discover how conflicts can arise when different constituents prioritize their own agendas related to tech development. Let’s consider the following case study: Case Study: Apple Settles ‘Batterygate’ Class Action Suit with an additional $113 Million1 In November of 2020, Apple agreed to pay $113 million to settle consumer fraud lawsuits. These lawsuits were brought by more than 30 states over alleging that Apple was intentionally and without notice slowing down and shutting off iPhones resulting in the devices having sluggish performance or completely shutting down. At first, Apple denied that it purposefully impeded the devices’ performance in any way. Later, Apple admitted that it did, in fact, alter the devices’ performance but it was for the purpose to “…preserve battery life amid widespread reports of iPhones unexpectedly turning off.” \"Many consumers decided that the only way to get improved performance was to purchase a newer-model iPhone from Apple,\" Arizona Attorney General Mark Brnovich wrote in the complaint. \"Apple, of course, fully understood such effects on sales.\" The slowdown effected phones that were released between 2014 and 2016, but it wasn’t until December of 2017 that Apple eventually admitted to the slowdowns. Then, they issued an apology of sorts by saying, \"We have never – and would never – do anything to intentionally shorten the life of any Apple product, or degrade the user experience to drive customer upgrades.\" But they settled anyway! Initially, they agreed to a settlement of $500 Million to pay affected consumers $25 per phone. The $113 Million"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 33, "chunk_id": "english_pack_33_1_9ef11efd", "content": "Then, they issued an apology of sorts by saying, \"We have never – and would never – do anything to intentionally shorten the life of any Apple product, or degrade the user experience to drive customer upgrades.\" But they settled anyway! Initially, they agreed to a settlement of $500 Million to pay affected consumers $25 per phone. The $113 Million was in addition to the initial $500 Million. To think about this case in more detail, let’s put ourselves in different shoes and examine this situation from different perspectives with differing lenses. 1 “Apple Agrees To Pay $113 Million To Settle 'Batterygate' Case Over iPhone Slowdowns”, NPR, updated Nov. 18, 2020, https://www.npr.org/2020/11/18/936268845/apple-agrees-to-pay-113-million-to-settle-batterygate-case-over- iphone-slowdowns Page 33 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 34, "chunk_id": "english_pack_34_0_68b79e69", "content": "Ethics in Technology By: Ed Weber Person 1: Apple Executive – We’ve sold and shipped millions of units of phones from 2014 through 2016 – and they are excellent products! Maybe too good. But that was then. How will we continue to get new sales if our older products are still working just fine? Why should our customers throw away perfectly good and working devices if they are still working for them? If we don’t continue to have the same (or better) sales growth, my position (and my bonuses) will be at risk. And if the new features of our new devices aren’t so appealing that the consumers won’t willingly abandon their working tech, then we need to make the older devices less-than-desirable if we will have any chance of getting those new replacement sales. But we also can’t disclose what we’re doing because we can’t risk our competitors seeing our strategies. Let’s just find a way to make the old phones ‘go away’. (planned obsolescence, trade secrets, intellectual property) Person 2: Apple Customer – I just spent hundreds of dollars on a device less than 5 years ago and it did everything I needed it to do. And for several years now, it has been a great device! But now, for no apparent reason, my regular apps have become unmanageably sluggish and, sometimes, without warning, my phone will just shut itself off. Apple said that it didn’t do anything, but I certainly didn’t change anything! The device was working perfectly, and then all of a sudden, it is no longer functioning the way it should. Isn’t it reasonable to have an expectation that something that is working will continue to work as designed without interruption or performance degradation? Also, when I ask Apple about what’s going on, shouldn’t they have a responsibility to tell me the truth right up front? And if this device no longer works for me, then it won’t work for anyone. Where will this device end up? In a landfill I suppose… oh, the waste! (corporate responsibility, transparency, environmental responsibility) Person 3: YOU – as"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 34, "chunk_id": "english_pack_34_1_65927885", "content": "when I ask Apple about what’s going on, shouldn’t they have a responsibility to tell me the truth right up front? And if this device no longer works for me, then it won’t work for anyone. Where will this device end up? In a landfill I suppose… oh, the waste! (corporate responsibility, transparency, environmental responsibility) Person 3: YOU – as any one of a number of different technology professionals working at Apple. Developer – The boss just asked me to start working on a program that would run constantly – undetectable and behind-the-scenes – that would basically do nothing but would consume a lot of clock cycles of the processor. The purpose of this program is simply to intentionally drain the battery as quickly as possible. I’m no dummy. I know exactly why I am being asked to do this. If I didn’t work here at Apple, and I wrote something like this, I would be considered a cyberterrorist. Am I really OK with being a part of ‘planned obsolescence’? Is this why I went to school to learn how to be an app developer? Is this the kind of app I thought I’d be developing? Is this really what I signed up for? Marketing – The boss just asked me to develop a campaign targeting existing phone users telling them how much they are missing out because their older devices just can’t keep up with the new apps. I’m supposed to focus on new apps that only run on the newest devices. We’ve already seen that the ‘new apps’ aren’t that big of a hit because we aren’t getting a lot of ‘new’ customers at this time anyway. It doesn’t seem to be true that the new Page 34 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 35, "chunk_id": "english_pack_35_0_a2ba5d59", "content": "Ethics in Technology By: Ed Weber apps are all that they’re supposed to be. When does ‘exaggeration’ become ‘lying’? Is this really what I signed up for? Customer Service – The boss just told us that we are experiencing a huge increase in disgruntled customers because their batteries keep draining very quickly and some phones are just shutting off. The boss told me, “Your response is that the customer is just using technology that is too old and they should upgrade to a newer phone right away” and then I should try to transfer them to Sales. I asked the boss, “What changed? Why have the devices suddenly started failing?” The boss said, ‘That’s above our pay-grade.’ and left me on my own. Something doesn’t seem right here. If I were the customer, I would expect more of a concrete answer. And I certainly wouldn’t appreciate a ‘hard sales pitch’ if the reason I called in was to get my current device restored to the way it was working fine just a bit ago. And why can’t my own company be transparent about what is going on? Is this really what I signed up for? Sales Person – The boss just told me that we will begin using two different sales pitches for our potential customers. First, we have to find out if they are a ‘new’ customer (without one of our previous models.) If so, then we are supposed to first pitch our great Apple brand (be one of the cool kids), and then next we should pitch the differences between our brand and the other brands, and then, finally, we should close with the pitch that the new phones are positioned to handle any new technology advancements that may come in the future. But if the prospect is an ‘old’ customer, then the pitch needs to be just ‘sad commiseration’. They will be grousing about how they thought their old phone should have lasted a lot longer and how they always took great care with it… I am supposed to just sadly nod my head and say, “yeah"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 35, "chunk_id": "english_pack_35_1_95abda82", "content": "may come in the future. But if the prospect is an ‘old’ customer, then the pitch needs to be just ‘sad commiseration’. They will be grousing about how they thought their old phone should have lasted a lot longer and how they always took great care with it… I am supposed to just sadly nod my head and say, “yeah – the tech just keeps changing… it’s getting hard to keep up… but what are you gonna’ do? You need the latest tech to be able to do all of the things you’ve gotten used to doing.” But then the boss says I should also use the pitch that the new phones are positioned to handle any new technology advancements that may come in the future. Are you kidding me? How am I supposed to do that with a straight face? Isn’t the reality that this investment is designed to fail in less than 3 – 5 years? Is this really what I signed up for? When looking at this exact same situation through the different perspectives of different involved people, we can see that there are quite a number of ethical considerations that should not be too easily dismissed. One of the most common conflicts when it comes to the ethics associated with business and technology involves the distinctions and differentiations between two different groups of people: stockholders vs. stakeholders. Page 35 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 36, "chunk_id": "english_pack_36_0_7c83ced6", "content": "Ethics in Technology By: Ed Weber Stockholders (or shareholders) are individuals, companies, or institutions that own shares in a corporation, giving them partial ownership and certain rights such as voting on major company decisions and receiving dividends when profits are distributed. Their primary interest lies in the financial performance of the company, as their returns depend on stock value and dividend payouts, and they can typically buy or sell their shares at will. In contrast, stakeholders encompass a much broader group, including not only shareholders but also employees, customers, suppliers, and members of the local community, all of whom have an interest in the company's performance and impact, even if they do not own any shares. While stockholders are mainly concerned with financial returns, stakeholders may prioritize long-term stability, ethical practices, job security, product quality, and the company’s social and environmental responsibilities. This means that stakeholders’ interests may not always align with those of stockholders. It has been this author’s experience that the vast majority of corporate boards and executive leadership prioritize stockholder interests over stakeholder interests. So, where does this leave us when we consider how a Professional Code of Conduct can help us in making our ethical decisions? First, this can be something that one can intentionally look for when interviewing for a position with a potential employer. individuals can find out if the organization has adopted the ACM Code of Ethics and Professional Conduct. Or perhaps they have their own code of conduct that they have developed. individuals can also discover the policies and procedures that the organization uses for dealing with conflicts that arise from competing priorities. Some organizations utilize internal and/or external mediation boards to help provide unbiased, objective reviews and conflict resolutions. Additionally, individuals can adopt their own code of ethics and professional conduct to be a guide for their own, personal decisions. But in order to do this, one should also proactively consider what they may do when they are asked to do something that violates their adopted code of ethics. A look at one company’s Code of Conduct The company Enterprise Mobility"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 36, "chunk_id": "english_pack_36_1_edf4781f", "content": "can adopt their own code of ethics and professional conduct to be a guide for their own, personal decisions. But in order to do this, one should also proactively consider what they may do when they are asked to do something that violates their adopted code of ethics. A look at one company’s Code of Conduct The company Enterprise Mobility (formerly Enterprise Holdings) is a 68-year-old company that includes several brands – such as Enterprise Rent-A-Car, National Car Rental and Alamo – and provides services such as fleet management, car-sharing, van-pooling, truck rental, luxury rental, retail car sales and vehicle subscription. Enterprise currently has a global fleet of more than 2.3 million vehicles with rental locations in more than 90 countries and territories, including more than 40 countries across Europe. In a strong example of corporate responsibility, transparency, and commitment to a professional code of conduct, Enterprise publishes their Code of Conduct here: Page 36 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 37, "chunk_id": "english_pack_37_0_484daa75", "content": "Ethics in Technology By: Ed Weber https://www.enterprisemobility.com/content/dam/enterpriseholdings/marketing/about-us/compliance- and-ethics/enterprise-holdings-employee-code-of-conduct.pdf This comprehensive document defines and discloses precisely what is expected of its employees as well as how its customers and vendors can expect their relationships to exist. The introductory letter from the President and Chief Executive Office (CEO) of Enterprise, Chrissy Taylor, clearly illustrates the ‘WHY’ that Enterprise has adopted and publicly declared their commitment to this Code of Conduct. Figure 8: Enterprise's Code of Conduct Introduction Letter by Chrissy Taylor, President and CEO Contained within their code of conduct, Enterprise shows that there is a clearly defined path that individuals can use (but, more importantly, also have a responsibility to use) to report and resolve issues of ethical concern. This path includes one’s immediate supervisor, a next-level supervisor, the Human Resources department, or the Compliance and Ethics Committee. They even have an ethics hotline which includes a link for online reporting as well as a toll-free number which are both available and monitored 24 hours a day, 7 days a week, and 365 days a year! Take a few minutes to review this one company’s published Code of Conduct. Just a few of the topics that you will find covered here include concepts that allow us to also introduce additional terms to be further explored: • Obligations of Leadership – leaders are required to ‘lead by example’ as a requirement of their position. • Copyright, trade secret, patent, intellectual property – creations of the mind that the law defines and protects. Page 37 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 38, "chunk_id": "english_pack_38_0_a3b4c3dd", "content": "Ethics in Technology By: Ed Weber • Conflicts of Interest – occurs when one’s personal interests interfere with their ability to make unbiased decisions on behalf of the organization. • Gift – Something of value given willingly to another person without any expectation of return or influence. • Bribe – Something of value offered or given with the intent to influence the recipient’s actions or decisions for the giver’s benefit. • Insider Trading – buying or selling of a company's securities by individuals who possess material, nonpublic information about that company, often in violation of a duty to keep that information confidential. • Anti-Corruption Laws – these vary by region/country and can be complex, but these remain part of the legal requirement as well as an ethical requirement. • Harassment – any unwelcome behavior toward another person relating to a person’s legally protected characteristics that have the purpose or effect of creating an intimidating, hostile, or offensive work environment. Such conduct may be physical, sexual, or psychological. Review the Enterprise Code of Conduct and compare and contrast what you find there with the ACM Code of Ethics and Professional Conduct. Where do you discover similarities? Are there differences? Do you feel that any differences are significant? Which, if any (and in which document), would you suggest should be changed and why? How do these codes of conduct currently match up to your own way of thinking about your own personal ethics? Everyday Decision-Making So far in this chapter you have: • considered your own ethical responsibilities as a consumer of tech • reviewed the generic but widely adopted ACM Code of Ethics and Professional Conduct • reviewed one company’s comprehensive and transparent Code of Ethics It is now time to consider what these might mean for you in your everyday decision-making process. For example, there appear to be some rather valid reason for individuals to consider their own ethical consumption of all things tech – for both their own health, safety and well-being – but also for how their actions might impact others. There appear to be some strong starting"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 38, "chunk_id": "english_pack_38_1_cc54d2ac", "content": "consider what these might mean for you in your everyday decision-making process. For example, there appear to be some rather valid reason for individuals to consider their own ethical consumption of all things tech – for both their own health, safety and well-being – but also for how their actions might impact others. There appear to be some strong starting points and examples of Page 38 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Tech Consumer Responsibilities; Professional Codes; and Everyday Decision-Making", "page": 39, "chunk_id": "english_pack_39_0_adca798d", "content": "Ethics in Technology By: Ed Weber Ethical Codes of Conduct for both individuals and organizations to use to help define, clarify, and formalize their own approaches to adopting ethics in their own day-to-day actions. So now, here come the real questions for this chapter: What does your own, personal Ethical Code of Conduct look like? What are some of the areas that you feel are pretty well defined? What are some areas that you feel may be in conflict – depending on which personal lenses you choose to use? What are some areas of your own personal code of ethics that may be in conflict with your school, or your work, or your family, or your church, or your community, or your culture? Can you imagine writing up your own, personal code of ethics? What all would be included? What do you feel is still too undefined or situational, so much so that it means that you may not have a consistent ethical response in certain situations? What are the risks of inconsistently or sporadically adhering to a personal code of ethics? Is it OK to change your ethical position on a particular subject? If so, what circumstances would allow for this? As promised in the early chapters of this text… we aren’t providing any concrete, absolute answers to these questions. But rather, we are hoping that by listing a collection of questions, these may help individuals to discover and implement some intentionality into what it means to be an ethical person making informed, ethical decisions in their day-to-day activities. Page 39 of 125 4. Ethics for Tech Developers and Tech Consumers"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 40, "chunk_id": "english_pack_40_0_91725103", "content": "Ethics in Technology By: Ed Weber 5. Cybersecurity, Hacking, and Digital Identity Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation In the previous chapter, we explored some of your own personal ethical responsibilities as they relate to you as a consumer of all things ‘tech’. We also, explored some of your ethical responsibilities as they relate to your role in an organization that utilizes ‘tech’. Now, for this chapter and each of our subsequent chapters, we will look at various individual aspects of technology and try to focus on the ethical considerations within some narrower contexts. As we will quickly see, however, all of these chapters cannot be completely isolated from each other. Rather, it will be common to discover that there are aspects of each of these subsequent chapters that significantly overlap along with other chapters. Cybersecurity To begin this discussion, once again, let’s use a generative-AI tool (Perplexity) to help us provide the ‘textbook definition’ of cybersecurity. Here is the prompt that was used: “In a single sentence, please define the term 'Cybersecurity'.” Here are the results we received: Cybersecurity – The art and practice of protecting networks, devices, programs, and data from unauthorized access, cyberattacks, or criminal use, ensuring the confidentiality, integrity, and availability of information. Figure 9: Graphic representing It seems that there may be some significant cybersecurity omissions from this definition: We already have a feel for ‘Why’ cybersecurity is important… But what about the other questions? ‘Who’ is responsible for effective cybersecurity? ‘When’ do we need to think about cybersecurity? ‘Where’ do we need to be cognizant of cybersecurity issues? And ‘How’ do we actually address cybersecurity concerns? So, here is a follow-up prompt for our friend Perplexity: Page 40 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 41, "chunk_id": "english_pack_41_0_96591f9f", "content": "Ethics in Technology By: Ed Weber “This definition answers the question of 'what' is Cybersecurity. Assuming that we already know the 'why' associated with a need for cybersecurity, in a single sentence each, what would be the answers to the questions 'when', 'where', 'how', and most importantly, 'who' that should be associated with the concept of effective cybersecurity?” Here are the additional results: When: Effective cybersecurity must be practiced continuously, adapting to evolving threats and technological changes, and is especially critical whenever digital systems, networks, or sensitive data are in use. Where: Cybersecurity is required wherever digital assets exist, including on-premises networks, cloud environments, remote work setups, mobile devices, and any location where information is stored, processed, or transmitted. How: Effective cybersecurity is achieved through a combination of risk assessments, robust security policies, technical controls like encryption and multi-factor authentication, continuous monitoring, regular training, and incident response planning. Who: Everyone with access to digital systems – including individuals, employees, IT professionals, organizational leaders, and specialized cybersecurity teams – shares responsibility for maintaining effective cybersecurity, with organizations often relying on dedicated experts, external consultants, and collaborative initiatives like cybersecurity clinics to strengthen their defenses. The generative AI seems to get this one nearly 100% right! (When we look into the AI systems themselves in greater detail in Chapter 11, we will see this is not always the case!) Everyone is responsible – in part at least – for effective cybersecurity. The following sub-topics of this chapter will explore just a few of the concepts related to cybersecurity where a deeper dive into the corresponding ethics can illustrate some interesting and maybe even unexpected challenges! Vulnerabilities Vulnerabilities in digital systems represent weaknesses or flaws that can be exploited by malicious actors to gain unauthorized access, disrupt operations, or compromise sensitive information. These vulnerabilities may be targeted through various forms of cybercrime, such as phishing, ransomware, or distributed denial-of-service (DDoS) attacks, often leveraging exploits that take advantage of unpatched software or misconfigured systems. Page 41 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 42, "chunk_id": "english_pack_42_0_849df034", "content": "Ethics in Technology By: Ed Weber Ethically speaking, addressing vulnerabilities requires timely identification and remediation – typically through \"fixes\" or patches – to prevent harm to individuals and organizations. Without proper prevention and remediation, threats like viruses, Trojan horses, botnets, logic bombs, or blended threats that can propagate across interconnected networks. The presence of sophisticated threats, such as rootkits or phishing campaigns (including spear phishing, smishing, and vishing, etc.), underscores the ongoing responsibility of cybersecurity professionals and tech users alike to remain vigilant, promote best practices, and uphold principles of fairness, accountability, and non-maleficence in protecting digital assets and identities. Hacking Hacking, in its broadest sense, refers to the act of gaining unauthorized access to computer systems or networks, but the motivations and ethical implications of hacking can vary widely depending on the individuals involved. Black-hat hackers, for example, engage in hacking for malicious purposes such as stealing data, causing disruption, or committing cybercrime, often motivated by personal gain or the intent to inflict harm. In contrast, white-hat hackers use their technical skills to identify and report vulnerabilities, helping organizations strengthen their security by acting as ethical defenders – sometimes as part of formal roles like penetration testers or through coordinated vulnerability disclosure programs like hack-a-thons. The landscape of hacking also includes figures such as crackers, who break into systems to bypass protections or copy software illegally, and hacktivists, who use hacking as a form of protest or to promote social or political causes. The actions of hacktivists can raise complex ethical questions, as their activities may be intended to expose injustice or raise awareness, yet still involve unauthorized access and potential harm to innocent parties. As technology evolves, so too do the methods and motivations of hackers, making it essential for society to continually reassess the ethical boundaries of hacking, the responsibilities of those with advanced technical knowledge, and the appropriate legal and organizational responses to both harmful and beneficial forms of hacking. Digital Identity Digital identity, as it relates to an individual, is the collection of digitally or electronically captured attributes, behaviors, credentials, and data points that uniquely"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 42, "chunk_id": "english_pack_42_1_cfb625a5", "content": "essential for society to continually reassess the ethical boundaries of hacking, the responsibilities of those with advanced technical knowledge, and the appropriate legal and organizational responses to both harmful and beneficial forms of hacking. Digital Identity Digital identity, as it relates to an individual, is the collection of digitally or electronically captured attributes, behaviors, credentials, and data points that uniquely verify and represent a person online. This identity is not static; it is dynamically shaped by both the information individuals actively provide – such as usernames, email addresses, social media profiles, and biometric data – as well as the data passively collected through their online activities, including browsing habits, search histories, and transaction records. Key aspects that make up a person's digital identity include personally identifiable information (PII) like social security numbers, dates of birth, and biometric traits; login Page 42 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 43, "chunk_id": "english_pack_43_0_e2383340", "content": "Ethics in Technology By: Ed Weber credentials; device identifiers; behavioral patterns; and contextual signals such as location and device usage. Bad actors often seek to access and exploit digital identities through methods like phishing, credential theft, or malware, using the compromised information for identity theft, financial fraud, or unauthorized access to sensitive accounts. Once a digital identity is breached, attackers can leverage it to impersonate individuals, commit cybercrime, or even build more convincing attacks against others by harvesting further data from compromised accounts. Ethically, individuals have a responsibility to be intentional and mindful about the information they share and the digital footprint they create, as their digital identity not only reflects on their personal reputation but also affects their privacy and security. Practicing thoughtful self-representation and safeguarding personal data are essential not just for personal protection but also for fostering a trustworthy and respectful digital environment. Online Reputation This leads us to our final concept for this chapter – one’s online reputation. Online reputation refers to the collective perception and judgment that others form about an individual based on their digital presence, including the content they create, share, and are associated with across various online platforms. Unlike digital identity, which is the sum of all information that identifies a person online, online reputation is shaped not only by one's own actions but also by what others post, comment, or tag about them, and is visible to third parties through search engines, forums, blogs, and especially social media. Social media usage plays a significant role in building or damaging online reputation, as posts, comments, likes, and shares contribute to the overall digital footprint, and even a single viral incident can have lasting effects – positive or negative – on how a person is viewed by peers, employers, and the broader public. Also, in today’s society, there is often an ongoing blending of work and personal life which appears to be mostly unavoidable. At work, you may use the corporate computer to prepare for some personal meetings such as with your doctor, etc. Additionally, some organizations have bring-your-own-device (BYOD) requirements and/or policies"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 43, "chunk_id": "english_pack_43_1_e76ae98a", "content": "person is viewed by peers, employers, and the broader public. Also, in today’s society, there is often an ongoing blending of work and personal life which appears to be mostly unavoidable. At work, you may use the corporate computer to prepare for some personal meetings such as with your doctor, etc. Additionally, some organizations have bring-your-own-device (BYOD) requirements and/or policies which stipulate if and how you will use your own devices in the work setting. These kinds of intersections between what is ‘personal’ vs. what is ‘public’ introduces unique risks to one’s online reputation. When individuals commingle work and home information on shared devices, they increase the chances of accidental data leaks, inappropriate content exposure, or breaches that could affect both professional and personal reputations. For example, a security lapse on a personal device used for work could expose sensitive corporate information or inadvertently link personal social media activity with professional contacts, complicating the separation between private and public personas. Virtual Private Networks (VPNs) are often used to enhance privacy by encrypting internet traffic and masking a user's IP address. However, using a VPN – especially one provided by a third party – Page 43 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 44, "chunk_id": "english_pack_44_0_bc6da41f", "content": "Ethics in Technology By: Ed Weber does not guarantee true anonymity or untraceability. While a VPN can obscure activity from local networks or Internet Service Providers (ISPs), the VPN provider itself can potentially (and usually does!) log user activity. They do this because if the tech fails for any reason, it is only through reviewing the logs that the provider can discover and remedy the failure! As a result of this known logging, both law enforcement as well as sophisticated attackers may still trace actions back to the individual if the VPN is compromised or if endpoints are not secure. Direct-to-endpoint VPNs (such as those connecting directly to a corporate network) offer more control but still do not provide absolute anonymity, highlighting the need for individuals to remain vigilant and intentional about their online actions and the security tools they use. Ultimately, maintaining a positive online reputation requires individuals to be mindful of their digital footprint and the potential consequences of their online behavior. Ethical self-management involves regularly reviewing privacy settings, thinking critically before posting or sharing information, and understanding that online actions can have far-reaching effects on credibility, trustworthiness, and future opportunities. Textbook Definitions – Cybersecurity • cybercrime – Illegal activities conducted using computers or networks, including theft, fraud, or disruption of services. • phishing – A deceptive technique where attackers impersonate legitimate entities to trick individuals into revealing sensitive information, such as passwords or financial details. • ransomware – Malicious software that encrypts a victim's data and demands payment for its release. • distributed denial-of-service (DDoS) – An attack in which multiple compromised systems flood a target with traffic, overwhelming it and rendering services unavailable to legitimate users. • viruses – Malicious programs that attach themselves to legitimate files or programs and replicate, spreading to other systems and causing harm. • Trojan horse – Malicious software disguised as legitimate applications, which, when executed, enable unauthorized access or cause damage. • botnets – Networks of compromised computers, controlled remotely by attackers, used to perform coordinated malicious activities such as DDoS attacks or spam distribution. • logic bombs – Malicious"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 44, "chunk_id": "english_pack_44_1_10c6366a", "content": "files or programs and replicate, spreading to other systems and causing harm. • Trojan horse – Malicious software disguised as legitimate applications, which, when executed, enable unauthorized access or cause damage. • botnets – Networks of compromised computers, controlled remotely by attackers, used to perform coordinated malicious activities such as DDoS attacks or spam distribution. • logic bombs – Malicious code embedded in software that triggers a harmful action when specific conditions are met. Page 44 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 45, "chunk_id": "english_pack_45_0_3e8f4099", "content": "Ethics in Technology By: Ed Weber • blended threats – Attacks that combine multiple types of malware or attack methods to exploit different vulnerabilities simultaneously. • rootkits – Malicious tools designed to hide the existence of certain processes or programs, allowing continued privileged access to a system. • spear phishing – Targeted phishing attacks aimed at specific individuals or organizations, often using personalized information to increase credibility. • smishing – Phishing attacks delivered via SMS text messages, aiming to trick recipients into divulging sensitive information. • vishing – Voice-based phishing attacks conducted over the phone to deceive individuals into providing confidential information. • black-hat hackers – Individuals who exploit vulnerabilities in systems for malicious purposes, personal gain, or to cause harm. • white-hat hackers – Ethical hackers who identify and help fix security vulnerabilities to improve system security, often with permission. • penetration testers – Security professionals who simulate cyberattacks on systems or networks to identify and address vulnerabilities before malicious actors can exploit them. • hack-a-thons – Collaborative events where programmers and security experts work intensively to solve problems, develop software, or test security in a short period. • crackers – Individuals who break into computer systems or software, often to bypass protections or copy software illegally. • hacktivists – Hackers who use their skills to promote social or political causes, often through unauthorized digital actions. • credentials – Usernames, passwords, or other authentication information used to verify identity and gain access to systems. • biometrics – Unique physical or behavioral characteristics, such as fingerprints or facial recognition, used for automated identity verification. • personally identifiable information (PII) – Data that can be used to uniquely identify an individual, such as name, address, social security number, or date of birth. • credential theft – The act of stealing authentication information, such as usernames and passwords, to gain unauthorized access to systems or data. Page 45 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "section_title": "Cybersecurity; Vulnerabilities; Hacking; Digital Identity; Online Reputation", "page": 46, "chunk_id": "english_pack_46_0_9e7352bb", "content": "Ethics in Technology By: Ed Weber • malware – Malicious software designed to disrupt, damage, or gain unauthorized access to computer systems or networks. • digital footprint – The trail of data and activity a person leaves behind when using digital services, including social media posts, browsing history, and online transactions. • bring-your-own-device (BYOD) – A policy or practice where employees use their personal devices for work purposes, often increasing security and privacy risks. • Virtual Private Networks (VPNs) – Services that encrypt internet traffic and route it through a secure server, providing privacy and security for online activities. • Internet Service Providers (ISPs) – Companies that provide individuals and organizations with access to the internet. Page 46 of 125 5. Cybersecurity, Hacking, and Digital Identity"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 47, "chunk_id": "english_pack_47_0_99218861", "content": "Ethics in Technology By: Ed Weber 6. Technology, Justice, and Social Equity Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide; Tech in Education The integration of technology in education has transformed how students learn, educators teach, and institutions deliver knowledge. From digital textbooks and online learning platforms to adaptive learning software and virtual classrooms, technology can help bridge gaps in access to quality education, particularly for students in remote or underserved areas. However, the ethical considerations are complex: unequal access to devices and reliable internet can reinforce existing educational disparities, and the use of student data for algorithmic personalization raises questions about privacy, informed consent, and potential bias. Educators and policymakers must grapple with the responsibility to ensure that technology enhances learning equitably, rather than exacerbating divides. This involves not only providing hardware and connectivity but also supporting digital literacy, offering accessible content for students with disabilities, and critically evaluating the impact of educational technologies on student well-being and autonomy. Ultimately, the ethical deployment of technology in education requires ongoing reflection on who benefits, who may be left behind, and how to foster a more just and inclusive learning environment. Healthcare Access and Tech Technology has revolutionized healthcare delivery through telemedicine, electronic medical records (EMR), wearable health monitors, and AI-driven diagnostics. These advances can increase access to care for rural populations, streamline patient management, and enable earlier detection of disease. Yet, ethical challenges persist: not all patients have equal access to the internet or smart devices, and the digital skills needed to navigate modern healthcare tools are unevenly distributed. There is also the risk that algorithmic decision-making in healthcare may reflect or amplify existing biases, leading to disparities in diagnosis or treatment. Protecting patient privacy and ensuring informed consent are paramount as more sensitive health data is collected and shared across digital platforms. Ethically, healthcare providers and technologists must work to ensure that technological innovation does not widen the gap between those who can and cannot access high-quality care, but instead promotes justice by making healthcare more inclusive,"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 47, "chunk_id": "english_pack_47_1_75151e12", "content": "in diagnosis or treatment. Protecting patient privacy and ensuring informed consent are paramount as more sensitive health data is collected and shared across digital platforms. Ethically, healthcare providers and technologists must work to ensure that technological innovation does not widen the gap between those who can and cannot access high-quality care, but instead promotes justice by making healthcare more inclusive, affordable, and responsive to the needs of all communities. Page 47 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 48, "chunk_id": "english_pack_48_0_039816f9", "content": "Ethics in Technology By: Ed Weber Tech for Accessibility and Inclusion One of the most promising aspects of technology is its potential to empower individuals with disabilities and promote broader social inclusion. Assistive technologies – such as screen readers, voice recognition software, and adaptive hardware – can enable people with visual, auditory, motor, or cognitive impairments to participate more fully in education, employment, and civic life. The ethical imperative is to design technology that is accessible by default, not as an afterthought, and to involve people with disabilities in the design and evaluation process. At the same time, barriers remain: not all digital content is accessible, and some emerging technologies (like AI-powered interfaces) may introduce new obstacles if not thoughtfully implemented. Promoting digital inclusion means addressing affordability, usability, and cultural relevance, while also challenging stereotypes and assumptions about disability. Ethically, the goal is to create a digital world where everyone can participate with dignity and autonomy, regardless of ability. Maslow’s Hierarchy of Needs and Tech Maslow’s Hierarchy of Needs provides a useful framework for considering the ethical implications of technology’s role in fulfilling human needs. Figure 10: Maslow's Hierarchy of Needs Page 48 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 49, "chunk_id": "english_pack_49_0_24a553c1", "content": "Ethics in Technology By: Ed Weber At the base level, technology can support physiological needs – such as food, water, and shelter – through innovations in agriculture, clean water delivery and clean air maintenance, and smart housing. Safety needs are addressed through security systems, health monitoring and health care, and emergency communication tools. As we move up the hierarchy, technology supports belonging and esteem through social media, online communities, and platforms for self-expression. It has often been said that we currently live in a time where literally all of the basic needs at the lowest levels of Maslow’s hierarchy could be assured for all people of the world! We already have the technology for production, distribution and re-distribution, monitoring for need, and all associated communication needs, to completely eradicate food insecurity (hunger, thirst and starvation), clothing needs and housing needs (shelter and safety), assure clean air, land and water (pollution removal), health care (diagnostics and treatment) and provide the peace-of-mind and mental wellness that comes from having all of these other needs addressed. The technology isn’t what is getting in the way of achieving all of this! Rather, it is the insistence of holding on to the status-quo of man-made economic systems which favor one economic group to the detriment of all others. This is the only thing that is preventing all of these achievements. It isn’t the tech that is lagging. Rather, it is the socioeconomic constructs that are the root cause preventing the ethical implementation of the tech. Even worse, additional ethical tensions arise when technology is used in ways that actually exacerbate rather than work to address Maslow’s hierarchy needs: for example, when farmers are paid to not produce food to keep prices at an artificial level – all while people go hungry. Or when drug companies are allowed to charge obscene prices for life-saving treatments resulting in millionaire-class executives, while simultaneously standing idly by while poor people die without the treatments. Or when surveillance systems undermines privacy (a component of safety), or when social media algorithms foster isolation or harm self-esteem. The challenge for technologists and"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 49, "chunk_id": "english_pack_49_1_91f6f35c", "content": "– all while people go hungry. Or when drug companies are allowed to charge obscene prices for life-saving treatments resulting in millionaire-class executives, while simultaneously standing idly by while poor people die without the treatments. Or when surveillance systems undermines privacy (a component of safety), or when social media algorithms foster isolation or harm self-esteem. The challenge for technologists and society is to ensure that digital tools are designed and deployed in ways that genuinely enhance human flourishing at every level of Maslow’s hierarchy, being mindful of unintended consequences and the needs of the most vulnerable. Digital Divide The digital divide refers to the gap between those who have ready access to computers, the internet, and possess digital literacy, and those who do not. This digital divide is most readily recognized along lines of income, geography, age, and/or ability. This divide often limits opportunities for education, employment, healthcare, and civic participation, reinforcing cycles of disadvantage. Ethically, bridging the digital divide is not just a matter of providing hardware or connectivity, but also of addressing affordability, digital skills, and culturally relevant content. Page 49 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 50, "chunk_id": "english_pack_50_0_e46cb2c8", "content": "Ethics in Technology By: Ed Weber Consider this hypothetical case: A 24-hour convenience store in New York has been robbed multiple times in the last several years. Fortunately, no employees were ever physically harmed although most have experienced some amount of psychological trauma and many of them have subsequently quit. In fact, it is very hard to find any employees to work because of the continuing threat of being robbed at knife or gun-point. So, the business owner decides that an ideal solution is to simply stop accepting cash! Instead, the owner places signs that says, “No CASH accepted and No CASH on premises.” and “Credit or Debit Cards only accepted.” The owner tells the local news team that they decided to go this route because, ‘… nobody ever robbed a store and asked them to hand over all of their credit card receipts! The cash is the problem.’ The owner said, the technology will allow them to have a safe store once again, and this will allow the store to stay in business. But there is only one problem with this plan… in 2020, New York passed a law that said it was illegal for a business to “not accept cash”. The law stated that its aim was to protect the rights of the ‘unbanked’ and ‘underbanked’ population. The state used, as part of its assertion, the picture of a $1 bill noting the inscription found on each bill of US currency: Figure 11: US $1 bill highlighting 'legal tender' phrase The state argues that it is illegal for anyone to not accept cash. They say that the currency says “all debts, public and private”… not just the debts that anyone wants to choose. Page 50 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 51, "chunk_id": "english_pack_51_0_1a017ccc", "content": "Ethics in Technology By: Ed Weber Note, that there actually have been multiple legal cases about this very topic – and not just in New York! But, for the moment, let’s not focus on the legal aspects of the case, but rather, let’s think about the ethical aspects of the case. If we look at the situation through the bodega owner’s perspective and lenses, we can potentially see the following: • The owner is trying to make a living and provide for their family while providing goods for their community at reasonable prices. (Maslow’s hierarchy level 1) • The owner needs workers to supplement their own work and keep the store open 24 hours as there are many customers who work all 3 shifts in the neighborhood. (Maslow’s hierarchy level 1) • The workers need to feel safe and the risk of being robbed for cash prevents this. (Maslow’s hierarchy level 2) • A tech solutions (cashless-payments) exists that address the need previously mentioned. • The owner wants everyone to have a cashless payment option and says it is the banks responsibility to give a card to anyone who has cash and let the bank take on the exclusive risk of being robbed for their cash. But now, let’s look at it from the bank’s perspective and lenses: • The bank will only allow for a new account to be opened if there is a minimum original deposit, and a minimum maintained balance, and/or a repeating direct deposit. • The bank claims that this is necessary because the cost of maintaining an account requires these balances. • Meanwhile, the bank has reported that once again this year, the bank has earned record profits and its executives are making multi-million-dollar bonuses. • The bank points to the statement emblazoned on the currency itself and declares – the cash is already here so it’s not our problem. Finally, let’s look at the situation from one particular customer’s perspective and lenses: • I was laid off from my previous job and I am currently freelancing odd jobs just to get by. • The"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 51, "chunk_id": "english_pack_51_1_4a31c3a0", "content": "• The bank points to the statement emblazoned on the currency itself and declares – the cash is already here so it’s not our problem. Finally, let’s look at the situation from one particular customer’s perspective and lenses: • I was laid off from my previous job and I am currently freelancing odd jobs just to get by. • The jobs I can get all pay me in cash. • I had to close my bank account because I couldn’t maintain a minimum balance and I no longer have direct deposit. Since I had to file for bankruptcy, I can’t get a credit card anymore. Page 51 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 52, "chunk_id": "english_pack_52_0_c4d66ea9", "content": "Ethics in Technology By: Ed Weber • I am just barely getting by – literally – and pretty much all that I have to my name is right here in my pocket. • I just need to get some food before I drive over to the park to crash for a few hours in my car. As we look at this kind of situation through the various lenses of the different individuals involved, it can become pretty obvious pretty quickly that the issues that are causing concern are not the tech! But rather, the issues surround the facts that the socioeconomic systems, and the legal systems, have not kept pace with the changes that have been brought about by tech advancements. And, rather than focusing on the ethical considerations of the situation, our current society tends to put greater focus on the legal considerations that have too often tended to foster adherence to the status-quo. And it is this perpetuation of the status-quo that continues to exacerbate the divides (socioeconomic and technological) which become an ever-widening and unsustainable downward spiral. Efforts to close the digital divide must be intentional and sustained, involving collaboration among governments, private sector, educators, and community organizations. There is also an ethical obligation to consider the environmental and social impacts of technology deployment, ensuring that solutions are sustainable and respect the needs and voices of marginalized communities. In a world increasingly shaped by digital technology, promoting justice and social equity means ensuring that everyone has the opportunity to participate fully and fairly in the digital society. Textbook Definitions – Tech, Justice and Social Equity • Algorithmic personalization – The use of computer algorithms to tailor digital content, services, or experiences to individual users based on their data and behaviors, raising ethical questions about fairness, autonomy, and the reinforcement of social inequalities. • Privacy – The right of individuals to control access to their personal information and data, particularly regarding how it is collected, used, and shared by technology platforms. • Informed consent – The process by which individuals are provided with clear, understandable information about how"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 52, "chunk_id": "english_pack_52_1_ab3cb3b0", "content": "data and behaviors, raising ethical questions about fairness, autonomy, and the reinforcement of social inequalities. • Privacy – The right of individuals to control access to their personal information and data, particularly regarding how it is collected, used, and shared by technology platforms. • Informed consent – The process by which individuals are provided with clear, understandable information about how their data will be used by technology systems, enabling them to make voluntary and knowledgeable decisions about their participation. • Digital Divide – Refers to the gap between those who have ready access to computers, the internet, and possess digital literacy, and those who do not. The digital divide is often obviously recognized along socioeconomic lines. Page 52 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 53, "chunk_id": "english_pack_53_0_6d7b2c85", "content": "Ethics in Technology By: Ed Weber • Bias – Systematic and unfair discrimination that can be embedded in technological systems, such as algorithms, which may perpetuate or amplify existing social inequalities and injustices. • Digital literacy – The ability to critically understand, evaluate, and effectively use digital technologies, which is essential for individuals to navigate, question, and challenge the impacts of technology on justice and social equity. • Autonomy – The capacity for individuals to make self-directed, informed choices in digital environments, which can be threatened by technologies that manipulate or constrain decision- making without transparency or consent. • Telemedicine – The remote delivery of healthcare services and clinical information using telecommunications technology, which expands access to care but raises ethical concerns about patient privacy, confidentiality, and the quality of the patient-provider relationship. • Electronic medical records (EMR) – Digital versions of patients’ medical histories maintained by healthcare providers, designed to improve care coordination and efficiency while presenting challenges related to data security, privacy, and equitable access. • Wearable health monitors – Technology-enabled devices worn on or in the body that continuously collect and transmit health data, offering opportunities for proactive health management but also raising issues of data privacy, consent, and potential disparities in access. • AI-driven diagnostics – The use of AI systems to analyze medical data and assist in diagnosing health conditions, which can enhance diagnostic accuracy and efficiency but may simultaneously introduce algorithmic bias, and lack of transparency, and accountability. • Patient privacy – The ethical and legal obligation to protect individuals’ health information from unauthorized access or disclosure, a critical concern heightened by the use of digital health technologies such as telemedicine, wearable health monitors and EMRs. • Screen readers – Software applications that convert digital text into synthesized speech or braille, enabling people with visual impairments to access and interact with digital content and promoting greater accessibility and inclusion. • Voice recognition – Technology that interprets and processes spoken language, allowing users – especially those with mobility or dexterity challenges – to control devices and input information hands-free, thereby enhancing digital accessibility. • Adaptive"}
{"pack_id": "english_pack", "section_title": "Tech in Education; Healthcare Access and Tech; Tech for Accessibility and Inclusion; Maslow’s Hierarchy of Needs and Tech; Digital Divide;", "page": 53, "chunk_id": "english_pack_53_1_e5a19ca9", "content": "digital text into synthesized speech or braille, enabling people with visual impairments to access and interact with digital content and promoting greater accessibility and inclusion. • Voice recognition – Technology that interprets and processes spoken language, allowing users – especially those with mobility or dexterity challenges – to control devices and input information hands-free, thereby enhancing digital accessibility. • Adaptive hardware – Specialized physical devices designed to accommodate the needs of individuals with disabilities, such as modified keyboards or alternative input devices, which help remove barriers and foster inclusive participation in technology use. Page 53 of 125 6. Technology, Justice, and Social Equity"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 54, "chunk_id": "english_pack_54_0_42dbd494", "content": "Ethics in Technology By: Ed Weber 7. Technology in Personal and Social Life Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction Technology has become deeply woven into the fabric of daily life, shaping how individuals interact, form relationships, and manage personal well-being. As digital tools and platforms increasingly mediate everything from communication to leisure, they bring both opportunities for connection and challenges around autonomy, privacy, and mental health. The pervasive nature of technology means that personal choices – such as how much time to spend online or which platforms to use – can have far-reaching effects on social dynamics, emotional resilience, and even one’s sense of self. Navigating this landscape requires a thoughtful approach to the Figure 12: Graphic ethical questions that arise when technology intersects with representation of a digital personal and social spheres. Issues such as the management of footprint digital footprints, the boundaries between public and private life, and the impact of constant connectivity on relationships demand careful consideration. As society continues to adapt to evolving digital norms, individuals must balance the benefits of technological convenience with the responsibility to protect their own well-being and that of their communities. Digital Relationships Digital relationships are connections that are primarily formed, maintained, or deepened through digital communication channels such as social media, messaging apps, online forums, or video calls. Unlike traditional relationships that rely on physical proximity and face-to-face interaction (i.e. in-real- life (IRL), digital relationships transcend geographical boundaries and can develop between individuals who may never meet in person. These connections can be romantic, platonic, or professional, and are often characterized by the use of text, images, video, and other digital media to convey emotion, share experiences, and build trust. While some view digital relationships as fleeting or “throw-away” connections – easily formed and just as easily discarded – others experience profound intimacy and authenticity in their online interactions. The digital environment can lower barriers to self-disclosure, allowing individuals to share personal thoughts and feelings more openly than they might in person. This can lead some to value Page 54 of 125 7. Technology in"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 54, "chunk_id": "english_pack_54_1_42d63ec4", "content": "as fleeting or “throw-away” connections – easily formed and just as easily discarded – others experience profound intimacy and authenticity in their online interactions. The digital environment can lower barriers to self-disclosure, allowing individuals to share personal thoughts and feelings more openly than they might in person. This can lead some to value Page 54 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 55, "chunk_id": "english_pack_55_0_6e239283", "content": "Ethics in Technology By: Ed Weber digital connections as deeply, or even more so, than their in-person relationships. However, the lack of physical cues and the potential for curated online personas can blur the lines between genuine connection and illusion, sometimes causing individuals to lose perspective on the nature and depth of their digital ties. Online Dating Online dating refers to the practice of seeking romantic or sexual partners via internet platforms, typically through dedicated websites or mobile apps that facilitate the creation of personal profiles and digital communication. These platforms allow users to present curated aspects of their identity – such as interests, values, and appearance – and to browse or be matched with others based on compatibility algorithms or personal preferences. The convenience, broad reach, and relative anonymity of online dating have made it a mainstream method for meeting new people, offering opportunities to connect beyond traditional social circles and geographic boundaries. For online dating sites to be considered valuable, they must foster a safe, respectful environment, provide accurate and meaningful matches, and protect user privacy. Features such as robust identity verification, transparent algorithms, and clear communication tools contribute to a platform's credibility and user trust. However, online dating also presents significant risks and ethical challenges. Algorithmic bias can skew matches, reinforcing stereotypes or excluding certain groups, while aggressive data collection and the sale of personal information raise concerns about user privacy and consent. Some platforms may personalize content or matches to such an extent that they inadvertently censor or limit users’ choices, reducing the diversity of potential connections. The prevalence of deceptive practices – including catfishing (posing as someone else), fraudulent schemes, and the creation of fake profiles – can lead to emotional harm or financial loss. These issues highlight the need for ethical oversight, transparency in data use, and robust safeguards to ensure that online dating remains a positive and equitable experience for all users. Personal Data Tracking In an era of 'big-data', there is no such thing as 'TMI' (too-much-information). Every digital interaction, no matter how trivial it may seem, contributes to a vast and"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 55, "chunk_id": "english_pack_55_1_d63762ff", "content": "These issues highlight the need for ethical oversight, transparency in data use, and robust safeguards to ensure that online dating remains a positive and equitable experience for all users. Personal Data Tracking In an era of 'big-data', there is no such thing as 'TMI' (too-much-information). Every digital interaction, no matter how trivial it may seem, contributes to a vast and ever-expanding profile of personal data. Companies routinely track direct data points such as names, email addresses, phone numbers, and purchase histories, but the scope goes far beyond this. Indirectly, organizations can derive sensitive information from patterns in browsing behavior, location data from GPS, device fingerprints, app usage, social media activity, and even metadata embedded in videos, photos and messages. By aggregating and analyzing these diverse data streams, companies can infer a person’s habits, preferences, health status, social circles, and even political leanings. Page 55 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 56, "chunk_id": "english_pack_56_0_765d5ee9", "content": "Ethics in Technology By: Ed Weber Consider a hypothetical user who only provides their name, email address, date of birth, and country when signing up for a service on their smartphone. Even with just these four data points, the device and associated apps can collect a wealth of additional information. The smartphone’s operating system and apps may automatically log the user’s IP address, device type, language settings, and geolocation. By linking the email address to other online accounts, data brokers can cross-reference social media profiles, public records, and past purchase histories. The date of birth enables age-based profiling, while the country helps narrow down cultural, legal, and economic backgrounds. Meanwhile, passive data collection – such as app usage patterns, movement tracked via GPS, and browsing history – can reveal daily routines, frequented locations, and social interactions. Machine learning algorithms can then synthesize these disparate data points to construct an eerily accurate and invasive profile: predicting the user’s income bracket, relationship status, health risks, interests, and even likely future behaviors. This comprehensive profiling, often invisible to the user, highlights the profound privacy risks and ethical dilemmas posed by ubiquitous personal data tracking in the digital age. Digital Minimalism Digital minimalism is often seen as a response to the overwhelming realities of big-data collection, where every online action contributes to a growing digital footprint that is neither fully transparent nor easily controlled. For some, embracing digital minimalism can resemble “sticking one’s head in the sand” – a knee-jerk reaction to the anxiety and fatigue caused by constant notifications, information overload, and the persistent sense of surveillance. In this light, digital minimalism may appear as an attempt to escape rather than confront the pervasive reach of technology. This is especially apparent when “opting out” entirely is rarely practical (… or even possible…) in a world where digital connectivity underpins nearly every aspect of work, social life, and civic engagement. Despite these limitations, the appeal of digital minimalism lies in its promise to restore balance, focus, and well-being by encouraging a more intentional and mindful relationship with technology. Rather than rejecting digital tools outright,"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 56, "chunk_id": "english_pack_56_1_857c68c2", "content": "out” entirely is rarely practical (… or even possible…) in a world where digital connectivity underpins nearly every aspect of work, social life, and civic engagement. Despite these limitations, the appeal of digital minimalism lies in its promise to restore balance, focus, and well-being by encouraging a more intentional and mindful relationship with technology. Rather than rejecting digital tools outright, digital minimalism advocates for curating one’s digital environment to prioritize high-value activities that align with personal values and goals. Practitioners report benefits such as improved mental clarity, stronger relationships, and enhanced productivity, as they reduce digital clutter and reclaim their attention from low-value distractions. However, the practical challenge remains: even the most disciplined digital minimalist cannot fully escape the data-driven infrastructure that shapes modern existence. Thus, digital minimalism is less about total withdrawal and more about making conscious choices to engage with technology in ways that support, rather than undermine, a meaningful and healthy life. Page 56 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 57, "chunk_id": "english_pack_57_0_98651a7a", "content": "Ethics in Technology By: Ed Weber Technology Addiction Technology addiction is a behavioral disorder characterized by compulsive and excessive engagement with digital devices and online activities, often to the detriment of personal, social, and professional well-being. Like other addictions, those affected may not recognize – or may actively deny – that their technology use has become problematic. Denial is a common defense mechanism, with individuals believing they are in control or rationalizing their behavior as normal, even as they neglect responsibilities, relationships, or self-care. This lack of self-awareness is compounded by the ubiquity of technology in daily life, making it difficult to distinguish between healthy use and dependency. Many app developers and tech companies intentionally design their products to maximize user engagement, leveraging psychological principles such as variable rewards and social validation to create habit-forming experiences. Features like endless scrolling, push notifications, and algorithmic content feeds are engineered to keep users returning, increasing screen time and, ultimately, advertising revenue. There have been high-profile legal actions against major tech firms alleging that their platforms are intentionally addictive, particularly to young users. For example, lawsuits have been filed against social media companies for allegedly exploiting vulnerabilities in children and teens to encourage compulsive use, with claims that these practices contribute to mental health crises. In response, some governments and advocacy groups have called for increased regulation, transparency in algorithm design, and the implementation of features like screen time limits and digital well-being tools. It is their hope and intent that these additional approaches will help users regain control over their technology use. Despite these efforts, addressing technology addiction remains a significant challenge in an increasingly digital world. Textbook Definitions – Technology in Personal and Social Life • digital footprint – The unique trail of data created by an individual’s online activities, both intentionally and unintentionally, including websites visited, emails sent, and information submitted online. • Digital relationships – Connections formed and maintained primarily through digital communication channels such as social media, messaging apps, or online forums. • in-real-life (IRL) – Interactions or relationships that occur in the physical, offline world rather than"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 57, "chunk_id": "english_pack_57_1_fe9c8840", "content": "trail of data created by an individual’s online activities, both intentionally and unintentionally, including websites visited, emails sent, and information submitted online. • Digital relationships – Connections formed and maintained primarily through digital communication channels such as social media, messaging apps, or online forums. • in-real-life (IRL) – Interactions or relationships that occur in the physical, offline world rather than through digital platforms. • “throw-away” connections – Brief, low-commitment digital interactions that are easily formed and just as easily discarded, often lacking depth or long-term significance. Page 57 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 58, "chunk_id": "english_pack_58_0_1804aebc", "content": "Ethics in Technology By: Ed Weber • curated online personas – Carefully crafted digital identities where individuals selectively present aspects of themselves to shape how they are perceived online. • Online dating – The practice of seeking romantic or sexual partners through internet platforms that facilitate profile creation, matching, and digital communication. • relative anonymity – The condition in which users can interact or share information online without fully revealing their true identities, often lowering barriers to self-disclosure. • transparent algorithms – Algorithms whose functioning, criteria, and decision-making processes are openly disclosed and understandable to users. • Algorithmic bias – Systematic and unfair discrimination embedded in automated decision- making processes, often reflecting or amplifying existing social prejudices. • censor – To suppress, limit, or remove content or information from digital platforms, often based on specific rules, policies, or external pressures. • catfishing – The act of creating a fake digital identity to deceive others, typically for personal, financial, or emotional gain. • fake profiles – Online accounts that use false or misleading information to impersonate someone else or create a fictitious persona. • ethical oversight – The process of monitoring and guiding technology development and deployment to ensure alignment with ethical standards and societal values. • transparency in data use – The practice of clearly informing users about how their personal data is collected, processed, shared, and stored. • big-data – Extremely large and complex datasets that are analyzed computationally to reveal patterns, trends, and associations, especially relating to human behavior. • TMI (too-much-information) – The idea that excessive sharing or collection of personal data can occur, though in the context of big-data, there is often no perceived limit to what is gathered or analyzed. • information overload – A state in which the volume of information received exceeds an individual’s capacity to process or make decisions effectively. • patterns in browsing behavior – Trends and habits revealed by analyzing the websites and content an individual visits or interacts with online. • location data – Information about the geographical position of a device or user, often collected via GPS, Wi-Fi,"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 58, "chunk_id": "english_pack_58_1_8cfbc29f", "content": "state in which the volume of information received exceeds an individual’s capacity to process or make decisions effectively. • patterns in browsing behavior – Trends and habits revealed by analyzing the websites and content an individual visits or interacts with online. • location data – Information about the geographical position of a device or user, often collected via GPS, Wi-Fi, or IP address. Page 58 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 59, "chunk_id": "english_pack_59_0_990049ea", "content": "Ethics in Technology By: Ed Weber • device fingerprints – Unique combinations of device attributes (such as browser type, operating system, screen size) used to identify and track users across digital platforms. • metadata – Data that provides information about other data, such as time stamps, location, author, or device details associated with digital files or communications. • data brokers – Companies or entities that collect, aggregate, and sell personal data from various sources, often without direct user consent. • profiling – The process of analyzing and combining personal data to create detailed user profiles that predict behaviors, preferences, or characteristics. • Digital minimalism – A lifestyle approach that emphasizes intentional and selective use of digital technologies to reduce distractions and focus on meaningful activities. • surveillance – The continuous monitoring or observation of individuals’ activities, often through digital means, for purposes such as security, marketing, or data collection. • opting out – The act of choosing not to participate in certain digital services or data collection practices, often to protect privacy or reduce digital exposure. • Technology addiction – A behavioral disorder involving compulsive and excessive use of digital devices or online platforms, leading to negative impacts on daily life. • variable rewards – Unpredictable and intermittent incentives that reinforce repeated engagement with digital platforms, making them more habit-forming. • social validation – The psychological reinforcement gained from receiving approval, likes, or positive feedback from others in digital environments. • endless scrolling – A design feature that allows users to continuously view new content without explicit breaks, encouraging prolonged engagement. • push notifications – Alerts sent by apps or websites to users’ devices to prompt immediate attention or action. • algorithmic content feeds – Streams of information or media curated and delivered to users based on automated analysis of their preferences and behaviors. • screen time – The amount of time an individual spends using digital devices or engaging with screens. • advertising revenue – Income generated by digital platforms through displaying ads to users, often driven by high engagement and data collection. Page 59 of 125 7. Technology"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 59, "chunk_id": "english_pack_59_1_bf7aaa85", "content": "curated and delivered to users based on automated analysis of their preferences and behaviors. • screen time – The amount of time an individual spends using digital devices or engaging with screens. • advertising revenue – Income generated by digital platforms through displaying ads to users, often driven by high engagement and data collection. Page 59 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "section_title": "Digital Relationships; Online Dating; Personal Data Tracking; Digital Minimalism; Technology Addiction", "page": 60, "chunk_id": "english_pack_60_0_512d91e6", "content": "Ethics in Technology By: Ed Weber • compulsive use – Repetitive and uncontrollable engagement with digital technology, often despite negative consequences. • transparency in algorithm design – The practice of making the logic, criteria, and functioning of algorithms open and understandable to users and stakeholders. • screen time limits – Tools or policies that restrict the duration an individual can spend on digital devices or specific applications to promote healthier usage patterns. Page 60 of 125 7. Technology in Personal and Social Life"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 61, "chunk_id": "english_pack_61_0_60b2a08f", "content": "Ethics in Technology By: Ed Weber 8. Privacy, Surveillance, and Data Ethics Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions As our lives become increasingly intertwined with digital technologies, the boundaries between public and private spheres have grown more complex. The preceding chapters have explored how technology shapes our identities, relationships, and access to opportunities, while also highlighting the responsibilities of both developers and consumers in navigating ethical challenges. From the responsibilities of tech users and professionals, to the impact of technology on justice, equity, and personal well-being, we have seen that ethical decision-making is rarely straightforward – often requiring us to balance competing values and anticipate unintended consequences. Building on these foundations, this chapter delves into the critical issues of privacy, surveillance, and data ethics. In a world driven by big data, cloud computing, and ubiquitous connectivity, questions about who owns our information, how it is collected, and for what purposes it is used have become central to the ethical landscape. We will examine the evolving definitions of privacy in the digital age, the rise of urban surveillance and smart cities, and the ethical dilemmas posed by large- scale data collection and consent. By considering the implications of data ownership and the responsibilities of both individuals and organizations, this chapter aims to equip readers with the tools to critically assess the ethical dimensions of privacy and surveillance in contemporary society. Big Data and Privacy The rise of big data has fundamentally transformed the landscape of personal privacy. Every day, individuals generate vast amounts of digital information through online interactions, purchases, social media activity, and even passive data collection via mobile devices and smart home technology. This information is not only collected by the platforms and services individuals use directly, but is also routinely shared with third-party data aggregators, sold to marketers, and analyzed by a wide array of organizations seeking to infer deeper insights about users’ behaviors and preferences. The sheer scale and interconnectedness of data collection means that a single piece of personal"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 61, "chunk_id": "english_pack_61_1_d38e94b4", "content": "technology. This information is not only collected by the platforms and services individuals use directly, but is also routinely shared with third-party data aggregators, sold to marketers, and analyzed by a wide array of organizations seeking to infer deeper insights about users’ behaviors and preferences. The sheer scale and interconnectedness of data collection means that a single piece of personal information – such as an email address or geolocation – can be replicated, cross-referenced, and stored in dozens, if not hundreds, of separate databases worldwide. A crude (but conservative) estimate suggests that for any active digital user, there may be hundreds to thousands of copies of their personal data distributed across various entities. Each online service, retailer, social media platform, and app may create its own record; data brokers and aggregators further multiply these records as they buy, sell, and combine data sets; and backup systems, cloud Page 61 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 62, "chunk_id": "english_pack_62_0_8623305f", "content": "Ethics in Technology By: Ed Weber storage, and analytics platforms add further redundancy. Moreover, big data analytics can infer additional attributes and connections, effectively creating new “copies” of information by extrapolating from existing data points. This exponential proliferation makes it nearly impossible for individuals to fully track or control the spread of their digital footprints. Consider this scenario: A big cell phone provider sells anonymized (de-identified) data points to a data aggregator and analysis group which cover a period of one-week for a particular region. The data was requested because a store in this region wants to learn more about the individuals who showed up for their big sale event. The aggregator starts by focusing on all of the cell phones that went to that particular store during the store’s event. But, they don’t stop there. They then review where the data points go after they left the store to see the other destinations for these devices. They discover that quite a number of phones left the store and went to food establishments. This might suggest that the store should have some ‘snacks’ available during their next sale event. They also discover that a number of the devices went to a competitor’s store… this may be interesting to both the original store as well as to the competitor store. But then, they follow the phones to their ‘final destinations’ for that day to see where they ended up that night. Then, they repeat this process for each day (not just the sale day) to see what else they can learn about the ‘anonymous’ data points. Just by analyzing where the phones end up for the ‘end-of- the-day’ these data points may likely represent the ‘homes’ of the ‘anonymous data points’. And tracking where the phones go each day, they are likely to discover other patterns as well. So much for anonymized data! Privacy, in its most common definition, refers to the ability of individuals to control the collection, usage, and distribution of their personal information. It encompasses the right to decide what information is shared, with whom, and for what"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 62, "chunk_id": "english_pack_62_1_846aae72", "content": "And tracking where the phones go each day, they are likely to discover other patterns as well. So much for anonymized data! Privacy, in its most common definition, refers to the ability of individuals to control the collection, usage, and distribution of their personal information. It encompasses the right to decide what information is shared, with whom, and for what purposes. In the digital age, however, this expectation is increasingly challenged. The default practices of data collection, the complexity of data flows, and the lack of transparency in how information is shared or sold mean that true control over personal data is often illusory. While privacy remains a foundational value and a legal right in many jurisdictions, the reality is that maintaining a reasonable expectation of privacy online requires significant effort, technical literacy, and often, a willingness to opt out of many modern conveniences. Thus, while the principle of privacy is still widely recognized, its practical realization in the age of big data is fraught with challenges and, for many, may no longer be a fully reasonable expectation without substantial systemic change. Page 62 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 63, "chunk_id": "english_pack_63_0_d228ed8d", "content": "Ethics in Technology By: Ed Weber Public vs. Private In the previous section, we considered the expectation of privacy in the age of big-data. So it seems now we should differentiate between the legal definitions vs. the ethical definitions of the terms ‘Public’ vs. ‘Private’. The distinction between ‘public’ and ‘private’ is foundational both in legal and ethical discussions, yet the definitions and boundaries can shift depending on context. Legally, ‘public’ typically refers to spaces, actions, or information that are accessible or visible to the general population and where individuals have a reduced expectation of privacy. ‘Private,’ on the other hand, denotes areas, behaviors, or data that are restricted to individuals or select groups, where a higher expectation of privacy is recognized and protected by law. Ethically, the distinction often hinges on the reasonable expectations of those involved. Consider the scenario of looking through an open window into someone’s home: while the window may be open and the view technically accessible from a public sidewalk, most people would agree that peering inside or, more invasively, taking a photo or video crosses an ethical line. The act transitions from a passive observation in a public space to an active intrusion into someone’s private life, highlighting how context and intent matter. Similarly, recording audio or video of people inside a grocery store – where there is a general expectation of being in a semi-public space – differs ethically (and sometimes legally) from recording those same people outside on a public sidewalk. The boundaries blur further in places like restaurants, public transportation, or even online forums, where the mix of public accessibility and private interaction complicates the ethical calculus. Additional examples illustrate these nuances. In a workplace, conversations in a private office are generally considered private, while those in a break room may not be. In digital contexts, posting on a public social media page is typically considered public, but sending a direct message is private – though the technical ability to copy, share, or leak messages challenges this expectation. Even in public spaces, certain activities, such as using a restroom or changing"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 63, "chunk_id": "english_pack_63_1_cfd0b258", "content": "generally considered private, while those in a break room may not be. In digital contexts, posting on a public social media page is typically considered public, but sending a direct message is private – though the technical ability to copy, share, or leak messages challenges this expectation. Even in public spaces, certain activities, such as using a restroom or changing clothes in a locker room, retain strong legal and ethical protections of privacy despite their location. Ultimately, the legal definitions of public versus private are shaped by statutes and case law, often focusing on the impact of actions on society versus individuals. Ethically, the distinction is more fluid, relying on context, societal norms, and the reasonable expectations of those involved. As technology continues to blur these boundaries – through ubiquitous cameras, data collection, and online sharing – it becomes increasingly important to critically examine not just what is legally permissible, but what is ethically respectful of individuals’ privacy and autonomy. Page 63 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 64, "chunk_id": "english_pack_64_0_fb95a713", "content": "Ethics in Technology By: Ed Weber Urban Surveillance and Smart Cities Urban surveillance and the development of smart cities have introduced a range of technologies that promise to enhance public safety, improve efficiency, and optimize city services. Traffic cameras, for instance, are widely deployed to monitor intersections, enforce traffic laws, and provide real-time data to manage congestion. These systems can reduce accidents and improve emergency response times by allowing authorities to quickly identify and address incidents. Similarly, vehicle tracking – enabled through license plate readers and various connected sensors – can help locate stolen vehicles, optimize public transportation routes, and even support environmental goals by monitoring emissions and traffic patterns. However, these same technologies raise significant concerns about privacy and the potential for misuse. Traffic cameras and vehicle tracking systems can be repurposed for mass surveillance, enabling authorities or third parties to monitor individuals’ movements without their knowledge or consent. This persistent observation can erode the sense of urban anonymity and create a chilling effect on personal freedom, as people may change their behaviors if they feel constantly watched. The aggregation of vehicle movement data, when combined with other data sources, can reveal sensitive patterns about individuals’ routines and associations. Facial recognition technology represents another powerful but controversial tool in the smart city arsenal. On the positive side, it can assist in locating missing persons, identifying suspects in criminal investigations, and enhancing security at large public events. Yet, the deployment of facial recognition in public spaces has sparked intense debate over accuracy, bias, and the risk of wrongful identification. Moreover, the widespread use of facial recognition can enable pervasive government or corporate monitoring, undermining civil liberties and disproportionately impacting marginalized communities. Other notable examples include smart utility meters and environmental sensors. Smart meters can help residents and city officials monitor and reduce energy and water consumption, contributing to sustainability goals and lowering costs. Environmental sensors, such as those monitoring air quality or flood risks, can provide early warnings and improve public health outcomes. Yet, both technologies collect detailed data about residents’ habits and activities, raising questions about who has access"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 64, "chunk_id": "english_pack_64_1_cb1a0aab", "content": "sensors. Smart meters can help residents and city officials monitor and reduce energy and water consumption, contributing to sustainability goals and lowering costs. Environmental sensors, such as those monitoring air quality or flood risks, can provide early warnings and improve public health outcomes. Yet, both technologies collect detailed data about residents’ habits and activities, raising questions about who has access to this information and how it might be used beyond its intended purpose. Ultimately, while urban surveillance and smart city technologies offer clear benefits – improved safety, efficiency, and sustainability – they also introduce complex ethical challenges. The risk of cyberattacks, unauthorized data sharing, and the erosion of privacy demands robust governance, transparent policies, and meaningful public engagement to ensure that technological progress does not come at the expense of individual rights and community trust. Page 64 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 65, "chunk_id": "english_pack_65_0_535176a9", "content": "Ethics in Technology By: Ed Weber Data Collection and Consent The distinction between explicit and implied consent is central to understanding how data is collected and used in the digital environment. Explicit consent requires a clear, affirmative action from the user – such as checking a box, signing a form, or clicking an “I Agree” button – indicating unambiguous agreement to the collection and processing of their data. This type of consent is often accompanied by detailed language in End-User License Agreements (EULAs) or Terms of Service (ToS), specifying what data will be collected, how it will be used, and who it may be shared with. For example, a ToS might state, “We collect your name, email, and usage data to provide and improve our services,” and the company requires the user to actively accept these terms before proceeding. Implied consent, by contrast, is inferred from a user’s actions or the context in which those actions occur. If a user continues to browse a website after being notified of a cookie policy, or submits a contact form expecting a response, their behavior is interpreted as agreement to certain data practices – even if they have not explicitly acknowledged them. Implied consent is often used for routine or less sensitive data collection, but it is inherently less transparent and can lead to ambiguity or disputes over what the user actually agreed to. A critical nuance in these agreements is the use of open-ended language regarding data use. For instance, a clause might state, “We may use your data for purposes such as backups or translation to another language.” The phrase “such as” does not restrict the company to only those listed uses; rather, it leaves the door open for additional, unspecified uses of the data. The stated intent behind this language (if it is ever actually stated) is said by the company to provide flexibility for operational needs. However, it can also be used to mask broader data exploitation. For example, data collected for “service improvement” could be repurposed for targeted advertising, profiling, or even sold to third parties – uses"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 65, "chunk_id": "english_pack_65_1_2848b206", "content": "uses of the data. The stated intent behind this language (if it is ever actually stated) is said by the company to provide flexibility for operational needs. However, it can also be used to mask broader data exploitation. For example, data collected for “service improvement” could be repurposed for targeted advertising, profiling, or even sold to third parties – uses not explicitly disclosed in the original agreement – but made legal through the agreement as written. The concept of intent becomes central here. While a company may claim its intent is benign – such as improving user experience or ensuring data security – the same permissions can be leveraged for more intrusive or profit-driven activities, like behavioral advertising, location tracking, or sharing data with law enforcement or other organizations without further user notification. Other examples include using voice recordings from smart speakers to train AI beyond the stated purpose, or aggregating fitness tracker data for insurance risk assessment, even when the original consent was for personal health monitoring. Legally, the sufficiency of consent – whether explicit or implied – depends on the jurisdiction and the sensitivity of the data involved. Regulations like the General Data Protection Regulation (GDPR) developed and implemented in the European Union, require explicit, informed consent for most personal data processing, especially for sensitive categories, and place the burden on organizations to demonstrate that valid consent was obtained. Page 65 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 66, "chunk_id": "english_pack_66_0_df7db5b5", "content": "Ethics in Technology By: Ed Weber Ethically, the bar is even higher: true consent should be informed, freely given, and revocable, with users fully understanding both the scope and intent of data collection. In practice, however, the complexity of agreements and the opacity of data flows make it difficult to prove that users have genuinely understood or agreed to all possible uses of their data. Likewise, demonstrating the true intent of a company’s data practices is challenging, as broad or ambiguous language can be exploited for purposes far beyond those originally disclosed. As a result, both proving consent and intent remains a fraught process, highlighting the ongoing need for clearer communication, stronger regulation, and more transparent data practices. Cloud Computing Cloud computing has become deeply integrated into the daily routines of non-corporate users, offering convenience and flexibility across a range of applications. Common examples include file storage and sharing services like Google Drive, Apple iCloud, and Dropbox, which allow users to save documents, photos, and videos remotely and access them from any device. Email services such as Gmail and Yahoo Mail rely on the cloud to store messages and attachments, making communication seamless and accessible from anywhere. Social media platforms like Facebook and Instagram use cloud infrastructure to let users upload and share photos, videos, and other content. Streaming services, including Netflix and Spotify, leverage the cloud to deliver on-demand entertainment to millions, while cloud-based productivity suites like Google Docs and Microsoft 365 enable real-time collaboration and document editing without the need for local software installations. The primary appeal of these cloud-based applications lies in their promise of accessibility across devices, ease of use, and the ability to synchronize data across multiple devices. Users are drawn to the convenience of automatic backups, the ability to share files instantly, and the reduction in the need for physical storage or device-specific software. Cloud computing also supports mobile banking, online education, and even health and fitness tracking, making it a central pillar of modern digital life. However, as discussed in the previous section on data collection and consent, the agreements users accept when"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 66, "chunk_id": "english_pack_66_1_bf7d9c38", "content": "automatic backups, the ability to share files instantly, and the reduction in the need for physical storage or device-specific software. Cloud computing also supports mobile banking, online education, and even health and fitness tracking, making it a central pillar of modern digital life. However, as discussed in the previous section on data collection and consent, the agreements users accept when adopting cloud services often grant providers broad rights over their personal information. While the stated intent may be to facilitate backups or enhance user experience, the legal language typically allows providers to use, analyze, and even share user data for purposes far beyond those original use cases. This creates a significant imbalance: the value extracted from user data – through targeted advertising, analytics, or third- Figure 13: There is no cloud... Page 66 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 67, "chunk_id": "english_pack_67_0_1efd9b5a", "content": "Ethics in Technology By: Ed Weber party partnerships – can exceed the utility provided to the user in the form of basic storage or convenience. Consider this figure describing ‘the cloud’. If we understand that there is no cloud, but rather, it is just someone else’s computer, we begin to understand that we are just using their computers for storage, and that they are perpetually peeking at literally everything you put up there! Transparency remains a major issue. Once data is uploaded to the cloud, users have little visibility into where it is stored, how it is processed, or with whom it is shared. The lack of clear, accessible information about data practices means that users cannot easily verify how their information is being used or if it is being sold or repurposed for profit. This opacity is compounded by the trend of phasing out traditional, locally-installed productivity software in favor of cloud-based, subscription-only models. Companies are increasingly steering users toward exclusive cloud solutions to ensure recurring revenue, gain greater control over software updates, and – crucially – maintain ongoing access to user data. As a result, users are often left with little choice but to accept these terms if they wish to continue using familiar tools, further eroding their control over personal information and privacy in the digital age. Data Ownership and Open-Source Solutions Data ownership refers to the legal rights, control, and authority an individual or entity has over specific sets of data, including how that data is accessed, used, modified, shared, or deleted. It is about both possession and responsibility, granting the owner the power to determine the fate of the data and to enforce those rights legally and ethically. Data ownership is foundational for accountability, privacy, and security in a world where personal and organizational data are invaluable assets. Questions to Consider About Data Ownership: • Does a person own their own name, or is it merely a public identifier? • Who owns an individual’s email address: the person, the email provider, or both? • If you purchase a phone, do you own all the data"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 67, "chunk_id": "english_pack_67_1_db3bb1e4", "content": "and security in a world where personal and organizational data are invaluable assets. Questions to Consider About Data Ownership: • Does a person own their own name, or is it merely a public identifier? • Who owns an individual’s email address: the person, the email provider, or both? • If you purchase a phone, do you own all the data stored on it, or does the manufacturer or service provider retain some rights? • Is your fingerprint your property, or does an entity that collects and stores its digital representation (e.g., for authentication) share ownership? • Who owns your DNA sequence: you, your healthcare provider, or the company that analyzes it? • If a company collects your location data via a mobile app, do you retain ownership, or does the company claim rights through its terms of service? Page 67 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 68, "chunk_id": "english_pack_68_0_78d2bfc4", "content": "Ethics in Technology By: Ed Weber • Who owns the photos and messages you upload to social media platforms – you, the platform, or both? • If you generate creative works (art, writing, code) using a cloud-based app, do you own the content, or does the app provider have rights to it? • When you use voice assistants, do you own the recordings, or does the service provider? • Who owns aggregated or anonymized data derived from your personal information? • If your data is sold to third parties, do you still have any ownership or control over it? • Who owns the metadata (such as timestamps, device info, or usage statistics) generated by your interactions with digital services? • If a government agency collects your data for public health or security, do you retain any ownership or rights over that data? These questions illustrate the complexity and spectrum of data formats – ranging from personally identifiable information (PII) like names and fingerprints, to digital content, behavioral metadata, and even biological data. Legally, ownership can depend on jurisdiction, contractual agreements, and the nature of the data, while ethically, many argue individuals should retain primary rights and control over their personal information. Some types of data, such as biometric identifiers (fingerprints, facial scans, DNA) and commonly accessed data (emails, social media posts), are inherently difficult to isolate and protect due to the way they are collected, stored, and shared across platforms and organizations. Once digitized and uploaded, these data types often become subject to broad terms of service that can dilute individual ownership and control. By contrast, data that a user creates – such as documents, code, or media files – can, actually be more readily controlled and protected! Open-source software and personal computing resources provide the mechanisms by which users can take a modicum of control over digital information that they create. Open-source solutions empower users to retain ownership by allowing them to store, manage, and modify their data locally or on self-hosted platforms, free from restrictive proprietary agreements. This approach not only enhances privacy and security but also aligns"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 68, "chunk_id": "english_pack_68_1_d5601d9a", "content": "and personal computing resources provide the mechanisms by which users can take a modicum of control over digital information that they create. Open-source solutions empower users to retain ownership by allowing them to store, manage, and modify their data locally or on self-hosted platforms, free from restrictive proprietary agreements. This approach not only enhances privacy and security but also aligns with the ethical principle that individuals should have meaningful control over their own digital creations and personal information. There are many open-source solutions for the vast majority of the computing activities that typical users experience. Here is a brief list (as of this publication) of just some of the open-source titles and their typical uses: Page 68 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 69, "chunk_id": "english_pack_69_0_c6297f7d", "content": "Ethics in Technology By: Ed Weber Here are several of the most popular open-source software titles across a wide range of productivity and creative tasks, suitable for non-corporate users: Operating Systems • Ubuntu • Linux Mint • Debian • Fedora • Manjaro • OpenBSD • FreeBSD • Puppy Linux Personal Information Managers & Email • Thunderbird • Evolution • KOrganizer/KMail Office Applications • LibreOffice • OnlyOffice • Calligra Suite Artistic and Image Editing • GIMP (Photo Editing) • Inkscape (vector graphics) • Krita (digital painting) Video Editing and Production • Shotcut • Blender (also for 3D modeling • OBS Studio (Open and animation) Broadcaster Software) Audio Editing and Production • Audacity • LMMS (Linux Multi Media • Ardour Studio) Page 69 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 70, "chunk_id": "english_pack_70_0_823baa5a", "content": "Ethics in Technology By: Ed Weber Other Productivity and Creative Tools • VLC Media Player • Nextcloud (personal cloud • Joplin (note-taking and (media playback) storage and collaboration) to-do lists) • Scribus (desktop • Darktable (photo workflow • Calibre (e-book publishing) and raw development) management) • Rocket.Chat (team • Jupyter Notebook (interactive communication) computing and data science) These tools provide robust alternatives to proprietary solutions and empower users to retain greater control over their data and creative output. Textbook Definitions – Privacy, Surveillance, and Data Ethics • privacy – The right and ability of individuals to control the collection, use, and sharing of their personal information, ensuring freedom from unwarranted intrusion into their lives. • surveillance – The monitoring or observation of individuals or groups, often by authorities or organizations, to collect information or ensure security, which can threaten privacy if unwarranted. • data ethics – The moral principles and guidelines that govern the collection, analysis, and use of data, emphasizing privacy, transparency, accountability, and fairness. • big data – Extremely large and complex datasets generated from various sources, analyzed to reveal patterns, trends, and associations, especially relating to human behavior. • cloud computing – The delivery of computing services – including storage, processing, and software – over the internet, allowing users to access and manage data and applications remotely. • ubiquitous connectivity – The state of being continuously connected to digital networks and services from virtually anywhere, enabling constant data exchange. • urban surveillance – The use of technology such as cameras, sensors, and tracking systems in cities to monitor public spaces and activities for safety, efficiency, or control. • data collection – The process of gathering information from various sources, either actively or passively, for analysis, storage, or decision-making. Page 70 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 71, "chunk_id": "english_pack_71_0_dd58eaea", "content": "Ethics in Technology By: Ed Weber • consent – Permission granted by individuals for the collection and use of their data, which should be informed, freely given, and revocable. • data ownership – The legal rights and control an individual or entity has over specific data, including how it is accessed, used, shared, or deleted. • passive data collection – Gathering information from users without their direct input or awareness, often through background processes or device sensors. • smart home technology – Devices and systems within a home that use internet connectivity to automate and control functions such as lighting, security, and climate. • data aggregators and brokers – Entities that collect, combine, and sell data from multiple sources, often creating detailed profiles of individuals. • cloud storage – A service that allows users to save data on remote servers accessed via the internet, rather than on local devices. • lack of transparency – The absence of clear, accessible information about how data is collected, used, or shared, making it difficult for individuals to understand or control their data. • expectation of privacy – The belief or assumption that one’s personal information or activities will not be observed or disclosed without consent. • Public – Legally and ethically, spaces, actions, or information accessible to the general population, where individuals have a reduced expectation of privacy. • Private – Spaces, actions, or information restricted to individuals or select groups, where a higher expectation of privacy is recognized and protected. • reasonable expectations – What an average person would consider appropriate regarding privacy or data use in a given context. • context – The circumstances or setting in which data is collected, used, or observed, which influence privacy expectations and ethical considerations. • intent – The purpose or motivation behind collecting, using, or sharing data, which affects the ethical evaluation of those actions. • societal norms – The shared expectations and rules within a community that shape perceptions of privacy, consent, and acceptable data practices. • Traffic cameras – Cameras installed in public areas to monitor vehicle flow, enforce traffic laws, and"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 71, "chunk_id": "english_pack_71_1_9a1b5fe4", "content": "• intent – The purpose or motivation behind collecting, using, or sharing data, which affects the ethical evaluation of those actions. • societal norms – The shared expectations and rules within a community that shape perceptions of privacy, consent, and acceptable data practices. • Traffic cameras – Cameras installed in public areas to monitor vehicle flow, enforce traffic laws, and enhance public safety. Page 71 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 72, "chunk_id": "english_pack_72_0_d9f2ab77", "content": "Ethics in Technology By: Ed Weber • license plate readers – Automated systems that capture and process images of vehicle license plates for law enforcement or traffic management. • connected sensors – Devices embedded in infrastructure or vehicles to collect and transmit data on movement, environment, or system status. • mass surveillance – The large-scale monitoring of populations, often by governments, using technology to collect and analyze vast amounts of data. • Facial recognition – Technology that identifies or verifies individuals by analyzing facial features from images or video. • accuracy – The degree to which a system or process correctly identifies, measures, or represents information, crucial for fair outcomes in surveillance and data use. • bias – Systematic errors or prejudices in data collection, analysis, or technology that can lead to unfair or discriminatory outcomes. • wrongful identification – Incorrectly matching or labeling an individual by surveillance or recognition systems, leading to potential harm. • civil liberties – Fundamental rights and freedoms, such as privacy and free expression, that are protected from excessive government or organizational intrusion. • marginalized communities – Groups that experience discrimination or disadvantage, often disproportionately affected by surveillance and data misuse. • cyberattacks – Malicious attempts to access, disrupt, or damage digital systems or data. • Explicit consent – Clear, affirmative agreement to data collection or processing, usually given through direct actions like checking a box or clicking “I Agree”. • End-User License Agreements (EULAs) – Legal contracts between software providers and users outlining the terms for using the software, including data rights. • Terms of Service (ToS) – Agreements specifying the rules, responsibilities, and data practices associated with using a digital service. • Implied consent – Permission inferred from a person’s actions or the context, rather than a direct statement or agreement. • cookie policy – A statement on a website detailing how cookies are used to collect and process user data. • General Data Protection Regulation (GDPR) – A comprehensive European Union law that governs data protection and privacy, emphasizing informed, explicit consent and user rights. Page 72 of 125 8. Privacy, Surveillance,"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 72, "chunk_id": "english_pack_72_1_ed38329e", "content": "context, rather than a direct statement or agreement. • cookie policy – A statement on a website detailing how cookies are used to collect and process user data. • General Data Protection Regulation (GDPR) – A comprehensive European Union law that governs data protection and privacy, emphasizing informed, explicit consent and user rights. Page 72 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Big Data and Privacy; Public vs. Private; Urban Surveillance and Smart Cities; Data Collection and Consent; Cloud Computing; Data Ownership and Open-Source Solutions", "page": 73, "chunk_id": "english_pack_73_0_545717ad", "content": "Ethics in Technology By: Ed Weber • informed – Having adequate information to understand the implications and risks before agreeing to data collection or use. • freely given – Consent provided voluntarily, without coercion or undue pressure. • revocable – The ability to withdraw consent at any time, stopping further data collection or use. • Cloud Computing – The practice of using remote servers on the internet to store, manage, and process data, rather than relying on local hardware. • accessibility across devices – The capability to use data and applications seamlessly from multiple devices via cloud services. • synchronize data across multiple devices – Keeping files, settings, and information consistent and updated on all user devices through cloud-based solutions. • automatic backups – The process of regularly copying data to a remote server to prevent loss and ensure recovery. • Data ownership – The legal and ethical right to control, access, and manage one’s own data, including decisions about its use and sharing. • personally identifiable information (PII) – Any data that can be used to identify a specific individual, such as names, addresses, or Social Security numbers. • biological data – Information derived from an individual’s biological characteristics, including DNA, fingerprints, and other biometrics. • biometric identifiers – Unique physical or behavioral traits, such as fingerprints, facial scans, or iris patterns, used for identification. • fingerprints – Distinctive patterns on the tips of fingers, often used as a biometric identifier. • facial scans – Digital representations of facial features used for identification or authentication. • DNA – The genetic material that carries an individual’s hereditary information, unique to each person. • Open-source software – Software with publicly available source code that can be freely used, modified, and distributed by anyone. • personal computing resources – Devices and infrastructure owned and controlled by individuals, enabling them to manage and store their own data locally. Page 73 of 125 8. Privacy, Surveillance, and Data Ethics"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 74, "chunk_id": "english_pack_74_0_84f08f03", "content": "Ethics in Technology By: Ed Weber 9. Digital Communication, Social Media, Misinformation and Democracy Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy The digital revolution has profoundly transformed the ways in which people connect, share ideas, and participate in civic life. This chapter explores how the tools and platforms that facilitate these interactions also raise complex ethical questions that touch on every aspect of our personal and collective existence. The rapid spread of information – and the ease with which it can be shaped or distorted – has forced societies to confront new challenges regarding trust, credibility, and the responsibilities of both individuals and institutions. These dynamics are deeply interwoven with our earlier discussions on privacy, data ethics, and the broader societal impacts of technology. This all serves to highlight the need for nuanced approaches to digital citizenship. As digital spaces become central to public discourse, the boundaries between private expression and public consequence have blurred. The ethical dilemmas introduced here are not isolated; they are amplified by the same technological advancements that enable unprecedented connectivity and innovation. Issues explored in previous chapters – such as the responsibilities of tech developers and consumers, the vulnerabilities of digital identities, and the implications of surveillance – are now seen through the lens of how information is shared, consumed, and manipulated. This chapter examines the ways in which digital communication shapes social norms, influences decision-making, and can both empower as well as undermine democratic processes. It is here that the ethical frameworks introduced at the outset of this text are put to the test, as readers are invited to consider how technology mediates our relationships with each other and with the wider world. Social Media Ethics Social media has redefined how individuals engage with information and with one another, creating a dynamic environment where both users and platforms play crucial ethical roles. As consumers, people are constantly exposed to a vast array of content – news, opinions, entertainment, and more – often algorithmically curated and designed to maximize engagement rather than assure accuracy."}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 74, "chunk_id": "english_pack_74_1_f6a083f6", "content": "Social Media Ethics Social media has redefined how individuals engage with information and with one another, creating a dynamic environment where both users and platforms play crucial ethical roles. As consumers, people are constantly exposed to a vast array of content – news, opinions, entertainment, and more – often algorithmically curated and designed to maximize engagement rather than assure accuracy. This places a unique responsibility on users to critically evaluate the information they encounter. Ethical participation means more than simply sharing or reacting; it involves considering the potential impact of one's posts and interactions. Users must weigh the value of free expression against the potential harm caused by spreading misinformation, engaging in harmful rhetoric, or participating in online harassment. The rise of digital anonymity can sometimes embolden individuals to act in ways they would not in face-to-face interactions, underscoring the importance of empathy, respect, and accountability in online spaces. Page 74 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 75, "chunk_id": "english_pack_75_0_0dccd8f0", "content": "Ethics in Technology By: Ed Weber Platforms, on the other hand, bear a distinct set of ethical responsibilities. While users must exercise personal judgment, social media companies are tasked with balancing the principles of free speech with the need to prevent harm and maintain a safe, inclusive environment – all while keeping their financial bottom line in mind. This balancing act often manifests in debates over censorship – where does moderation cross the line into undue suppression of ideas? Platforms must also grapple with the challenge of distinguishing between legitimate satire and deliberately misleading content. The expectation of fact-checking is a contentious issue: while some argue that platforms should take a more active role in verifying information, others warn of the dangers of overreach and the potential for bias in content moderation. Ultimately, both participants and platforms share an ethical obligation to foster an online ecosystem that encourages constructive dialogue, protects against harm, and upholds the integrity of public discourse – a challenge that grows ever more complex as the digital landscape continues to evolve. Cyberbullying and Harassment Cyberbullying and harassment are two closely related forms of harmful behavior that occur through digital channels. Cyberbullying is defined as the use of technology – such as social media, messaging apps, or online games – to harass, threaten, embarrass, or target another person. It often involves repeated actions intended to harm, and can include sending mean or aggressive messages, spreading rumors, posting embarrassing photos or videos, or deliberately excluding someone from online groups. Harassment is a broader term that encompasses any unwanted behavior intended to annoy, threaten, or intimidate another person, and in a digital context, this can range from persistent unwanted messages to explicit threats or hate speech. Both cyberbullying and harassment can have severe emotional and psychological consequences, especially since digital content can be widely and permanently distributed. Examples of these behaviors are numerous and can include cyberstalking, where an individual monitors or follows someone’s online activity obsessively, often with threatening intent; doxxing, which involves maliciously sharing someone’s personal information online without consent; and the distribution of inappropriate material, such"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 75, "chunk_id": "english_pack_75_1_b6f978bc", "content": "harassment can have severe emotional and psychological consequences, especially since digital content can be widely and permanently distributed. Examples of these behaviors are numerous and can include cyberstalking, where an individual monitors or follows someone’s online activity obsessively, often with threatening intent; doxxing, which involves maliciously sharing someone’s personal information online without consent; and the distribution of inappropriate material, such as revenge porn, which is the sharing of explicit images or videos without consent to humiliate or blackmail the victim. Other mechanisms include impersonation (creating fake profiles to harm someone’s reputation), trolling (posting inflammatory or offensive comments to provoke a reaction), and flaming (sending hostile and insulting messages). These actions not only violate privacy but can also escalate into situations where victims feel unsafe in both digital and physical spaces. From a young age, many children and adolescents may be exposed to digital environments where the culture of “trash-talking” – playful or aggressive banter often aimed at opponents in online games – is prevalent. While initially intended as harmless competition, such behavior can quickly escalate if not Page 75 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 76, "chunk_id": "english_pack_76_0_8a8abe12", "content": "Ethics in Technology By: Ed Weber moderated, leading to more serious forms of cyberbullying or harassment. The anonymity and distance provided by digital platforms can embolden individuals to cross ethical boundaries. As a result, what begins as teasing can easily spiral into targeted campaigns of abuse. Over time, repeated exposure to or participation in such behavior can desensitize young people to the harm caused by their words and actions, making it crucial for both individuals and platform providers to foster respectful and accountable online communities. Deepfakes, Misinformation and Manipulation Deepfakes, misinformation, and manipulation represent some of the most complex ethical challenges in today’s digital landscape. Deepfakes – realistic, AI-generated images, videos, or audio – can blur the line between truth and fiction, with both creative and destructive potential. On the positive side, deepfake technology has been used to enhance public awareness campaigns, such as the “Malaria Must Die” initiative, where David Beckham appeared to speak in nine different languages, helping to reach a global audience. In media, Reuters has employed AI-generated presenters for personalized news summaries, making content more accessible and engaging. Other beneficial uses include voice cloning for individuals with speech impairments, de-aging actors for films, and creating immersive educational or historical experiences. However, deepfakes have also led to significant legal and ethical controversies. Lawsuits have arisen over non-consensual use of individuals’ likenesses – most notably in cases involving revenge porn, where deepfakes have been used to create explicit content without consent, leading to litigation and demands for stricter regulation. High-profile cases also include financial scams, where deepfake voices or videos impersonated executives to authorize fraudulent transactions, resulting in millions in losses and subsequent lawsuits. Celebrities and public figures have similarly pursued legal action against unauthorized deepfake impersonations that damaged their reputations or misled the public. Misinformation and manipulation, meanwhile, are often amplified by automated tools such as bots, which can flood social media platforms with false or misleading content. Bots are designed to mimic human behavior, allowing them to interact with users, post comments, and even “like” or share content en masse. This orchestrated activity can artificially boost"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 76, "chunk_id": "english_pack_76_1_0003d077", "content": "damaged their reputations or misled the public. Misinformation and manipulation, meanwhile, are often amplified by automated tools such as bots, which can flood social media platforms with false or misleading content. Bots are designed to mimic human behavior, allowing them to interact with users, post comments, and even “like” or share content en masse. This orchestrated activity can artificially boost the visibility of certain narratives, pushing curated lists of users toward trending misinformation. The intent is often to manipulate public opinion, influence elections, or sow discord by making fringe ideas appear more widely accepted than they actually are. The combination of deepfakes and bot-driven misinformation creates a potent tool for manipulation, challenging both individuals and platforms to discern fact from fiction in an increasingly synthetic information environment. Page 76 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 77, "chunk_id": "english_pack_77_0_8bc0cb75", "content": "Ethics in Technology By: Ed Weber Free Speech vs. Hate Speech The legal definitions of “free speech” and “hate speech” have evolved through a complex interplay of constitutional principles, court decisions, and ongoing debates about ethics and public order. In the United States, the First Amendment protects freedom of speech as a foundational right, barring the government from restricting expression based on viewpoint, even when that expression is offensive or hateful. The intent behind this legal framework was to uphold robust public discourse and protect minority voices, recognizing that ethical considerations – such as the need to prevent harm and promote dignity – must be balanced against the imperative of open debate. Over time, courts have clarified that speech can only be restricted if it directly incites imminent lawless action or constitutes a true threat. Despite these legal boundaries, ethical debates persist over what constitutes acceptable speech. Hate speech, while not legally defined in the U.S., is generally understood as expression intended to vilify, humiliate, or incite hatred against a group or class of people based on characteristics such as race, religion, gender, or sexual identity. The challenge arises because the same words or phrases can be interpreted differently depending on the observer’s perspective, cultural background, or personal experience. Can you think of some phrases that have been used by one group as a rallying cry of ‘free speech’ while others attempt to vilify anyone who uses the exact same phrase with accusations of ‘hate speech’? When communities or governments attempt to define and regulate these terms, the result is often confusion, ambiguity, or outright contradiction. The subjective nature of what constitutes hate speech or offensive speech means that any attempt to codify these concepts risks either overreach – suppressing legitimate debate – or underreach – failing to protect vulnerable groups from harm. This tension is heightened in diverse societies, where different groups may have conflicting values and interpretations of what is ethical or acceptable. As a result, legal definitions rarely align perfectly with the full spectrum of ethical considerations, and the process of defining these terms remains a contentious"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 77, "chunk_id": "english_pack_77_1_7b5d0a53", "content": "legitimate debate – or underreach – failing to protect vulnerable groups from harm. This tension is heightened in diverse societies, where different groups may have conflicting values and interpretations of what is ethical or acceptable. As a result, legal definitions rarely align perfectly with the full spectrum of ethical considerations, and the process of defining these terms remains a contentious and evolving challenge for both lawmakers and society at large. Influencer Culture Influencer Culture refers to the social phenomenon in which individuals – both online and off – build communities around themselves and exert significant commercial and non-commercial influence over their followers. This culture is not new: throughout history, prominent figures such as royalty, philosophers, political leaders, and celebrities have shaped public opinion, set trends, and influenced consumer behavior. In the digital age, however, the barriers to becoming an influencer have dropped dramatically, and the speed and reach of influence have expanded exponentially. Before the rise of social media, influencers included figures like Eleanor Roosevelt, who used her newspaper column and radio appearances to shape public opinion and advocate for social causes. In the Page 77 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 78, "chunk_id": "english_pack_78_0_8d7cfd67", "content": "Ethics in Technology By: Ed Weber 20th century, celebrities such as The Beatles, Marilyn Monroe, and Audrey Hepburn became trendsetters whose choices in fashion, music, and lifestyle were widely emulated. Today, influencers are typically individuals who have built large followings on platforms like Instagram, YouTube, and TikTok. These influencers often arrive without credentials or any specific expertise. Rather, they excel at social media engagement and, perhaps, have a likable or convincing personality. As influencer culture has grown, so too have debates about the responsibilities of influencers themselves. Some have faced backlash and legal repercussions for promoting harmful products, spreading misinformation, or engaging in unethical behavior. In response, there have been calls – and sometimes legal actions – to hold influencers accountable for the consequences of their actions, particularly when those actions mislead or harm their audiences. This includes demands for greater transparency in sponsored content, as well as accountability for endorsing products or ideas that may have negative real-world effects. The rise of influencers goes beyond mere entertainment. For many followers, influencers fill voids left by traditional institutions, offering advice, companionship, or a sense of belonging that may be missing from their everyday lives. Influencers often create parasocial relationships – one-sided bonds where followers feel a personal connection to the influencer – which can be a source of comfort, inspiration, or even identity formation. This dynamic can make influencers powerful agents of change but also places significant responsibility on their shoulders. Despite the potential for lasting impact, many influencers experience the ephemeral nature of fame. The phrase “15 minutes of fame” is especially apt, as viral success can be fleeting, and the public’s attention is fickle. Some influencers exhaust their popularity through overexposure, scandal, or controversial behavior, leading to a rapid loss of followers and influence. Others “crash and burn” more dramatically, facing public backlash or legal issues that end their careers as quickly as they began. This cycle highlights both the opportunities and the risks inherent in influencer culture, underscoring the need for ethical awareness and resilience in the digital age. Media Literacy Media Literacy is the ability to"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 78, "chunk_id": "english_pack_78_1_baca040e", "content": "rapid loss of followers and influence. Others “crash and burn” more dramatically, facing public backlash or legal issues that end their careers as quickly as they began. This cycle highlights both the opportunities and the risks inherent in influencer culture, underscoring the need for ethical awareness and resilience in the digital age. Media Literacy Media Literacy is the ability to access, analyze, evaluate, create, and act using all forms of communication. It goes beyond simply understanding information; it involves critical thinking about the messages we encounter, their sources, and their impact. Media literacy empowers individuals to navigate the complex media landscape, discerning credible information from misinformation or manipulation. A cornerstone of media literacy is the use of multiple sources to verify facts. By comparing information from various reputable outlets, consumers can identify patterns, inconsistencies, or biases. Evaluating the credibility of sources is also essential. This includes considering the reputation Page 78 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 79, "chunk_id": "english_pack_79_0_8db55ef7", "content": "Ethics in Technology By: Ed Weber of the publisher, the author’s expertise, and the presence of citations or references to original research. Traditional methods also involve checking for objectivity, transparency about funding or affiliations, and whether the information is current and relevant. Determining whether information is factual or opinion-based requires careful analysis. Facts are statements that can be objectively verified with evidence, while opinions reflect personal beliefs or interpretations. Facts are often presented with quantifiable data without qualification with an intent to inform. Whereas opinions are often subjectively presented with adjectives and adverbs intended to persuade, or in some other way elicit an emotional response. A simple way to consider whether some content is more fact-based or opinion-based is to simply count the parts of speech. If the piece has notably more numerals, nouns, and verbs (objective) than it has adjectives and adverbs (subjective) then the piece may be more fact-based than opinion-based. But if the piece has more subjective language than objective language, you already know that the piece is more opinion than fact. Content creators bear the responsibility of producing accurate, transparent, and ethical media if they are, in fact, acting in an ethical framework. This means clearly distinguishing between facts and opinions, disclosing conflicts of interest, and correcting errors promptly. Creators should also be mindful of the potential impact of their messages on audiences, striving to avoid harm and promote informed understanding. Content consumers, on the other hand, must approach media with a critical mindset. This includes questioning the motives behind messages, recognizing bias, and seeking out diverse perspectives. Consumers should also engage in reflection about how media influences their thoughts and behaviors, and take action – such as sharing reliable information or educating others – to contribute positively to public discourse. By embracing these practices, both creators and consumers can foster a media environment that supports truth, accountability, and informed civic participation. Textbook Definitions – Digital Communication, Social Media, Misinformation and Democracy • Social Media Ethics – The moral principles and guidelines that govern responsible, respectful, and ethical behavior on social media platforms. • Maximize engagement"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 79, "chunk_id": "english_pack_79_1_a8ede3ee", "content": "contribute positively to public discourse. By embracing these practices, both creators and consumers can foster a media environment that supports truth, accountability, and informed civic participation. Textbook Definitions – Digital Communication, Social Media, Misinformation and Democracy • Social Media Ethics – The moral principles and guidelines that govern responsible, respectful, and ethical behavior on social media platforms. • Maximize engagement – Strategies designed to increase user interaction, such as likes, shares, and comments, on digital content. • Accuracy – The degree to which information is free from errors, distortions, or misrepresentations. • Ethical participation – Engaging online in a manner that is respectful, honest, and mindful of the impact on others. Page 79 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 80, "chunk_id": "english_pack_80_0_8ea87e57", "content": "Ethics in Technology By: Ed Weber • Misinformation – False or inaccurate information that is spread, regardless of intent to deceive. • Harassment – Unwanted behavior intended to annoy, threaten, or intimidate another person, especially repeatedly. • Accountability – The obligation to take responsibility for one’s actions and accept the consequences. • Censorship – The suppression or prohibition of speech, writing, or other forms of expression considered objectionable or harmful. • Moderation – The process of monitoring and managing online content to ensure it complies with rules or standards. • Suppression – The deliberate act of preventing information or expression from being shared or seen. • Satire – The use of humor, irony, or exaggeration to criticize or mock people, ideas, or institutions. • Misleading content – Information that is designed or likely to deceive or misinform the audience. • Fact-checking – The process of verifying the accuracy of claims made in content or statements. • Bias in content moderation – Prejudiced or unfair treatment in the review and management of online content. • Cyberbullying – The use of digital technology to harass, threaten, embarrass, or target another person. • Cyberstalking – The repeated use of digital technology to monitor, follow, or harass someone. • Doxxing – The malicious act of publicly revealing private or identifying information about an individual without their consent. • Inappropriate material – Content that is offensive, explicit, or otherwise unsuitable for its intended audience. • Revenge porn – The distribution of explicit images or videos without consent, often to humiliate or blackmail. • Impersonation – Pretending to be someone else online, often for malicious or deceptive purposes. • Trolling – Posting inflammatory, offensive, or disruptive comments or messages to provoke a reaction. Page 80 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 81, "chunk_id": "english_pack_81_0_10698fef", "content": "Ethics in Technology By: Ed Weber • Flaming – Sending hostile and insulting messages, often in online discussions or forums. • Deepfakes – Realistic, AI-generated images, videos, or audio that can make it appear someone said or did something they did not. • Impersonated executives – Individuals falsely represented as company leaders, often in scams or fraudulent schemes. • Bots – Automated software programs designed to perform tasks online, such as posting messages or mimicking human behavior. • Trending – The state of being widely discussed or shared on social media at a given time. • Free speech – The right to express opinions and ideas without fear of government retaliation or censorship. • Hate speech – Expression intended to vilify, humiliate, or incite hatred against a group or class of people. • Open debate – The free exchange of ideas and perspectives in public discourse. • Overreach – Excessive or unjustified restriction of rights, such as speech, beyond what is necessary or appropriate. • Underreach – Failing to provide sufficient protection or regulation, resulting in harm or injustice. • Influencer Culture – The social phenomenon in which individuals build communities and exert significant influence over their followers’ opinions and behaviors. • Credible information – Information that is trustworthy, reliable, and supported by evidence. • Reputable outlets – Media sources known for accuracy, fairness, and reliability in reporting. • Credibility of sources – The degree to which a source is considered trustworthy and authoritative. • Objectivity – The practice of presenting information in a neutral and unbiased manner. • Transparency – Openness and clarity about intentions, actions, and sources of information. • Fact-based – Information that is grounded in verifiable evidence and data. • Opinion-based – Information that reflects personal beliefs, interpretations, or judgments. • Questioning motives – The act of critically examining the reasons behind someone’s actions or statements. Page 81 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Social Media Ethics; Cyberbullying and Harassment; Deepfakes; Misinformation; Manipulation; Free Speech vs. Hate Speech; Influencer Culture; Media Literacy", "page": 82, "chunk_id": "english_pack_82_0_365b948f", "content": "Ethics in Technology By: Ed Weber • Recognizing bias – Identifying personal or systemic prejudices that may affect the presentation or interpretation of information. Page 82 of 125 9. Digital Communication, Social Media, Misinformation and Democracy"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 83, "chunk_id": "english_pack_83_0_55922160", "content": "Ethics in Technology By: Ed Weber 10. Intellectual Property, Digital Art, and Emerging Economies Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI The digital era has fundamentally transformed how we create, share, and profit from intellectual works. At the heart of this transformation are the concepts of intellectual property – copyrights, patents, and trade secrets – which were originally established to balance the interests of creators, inventors, and the broader public. These legal protections were ethically motivated: they aimed to reward creativity and innovation, ensuring that inventors and artists could benefit from their labor while ultimately enriching society as a whole. By granting temporary monopolies, societies hoped to incentivize the production of new knowledge, art, and technology, while eventually returning these works to the public domain for communal benefit. However, as technology has evolved, so too have the ethical challenges surrounding ownership and control of ideas. Today, questions arise about the fairness and validity of these systems – especially when the legal owner of a creative work is not the original creator. For example, when a music label owns the rights to a song rather than the artist who composed and performed it, or when companies hold patents and trade secrets developed by employees, it prompts us to reconsider the original ethical justification for these protections. Are these arrangements still serving the public good, or have they shifted too far in favor of corporate interests? Do current laws adequately recognize the contributions of individual creators, or do they perpetuate power imbalances in the digital economy? As you explore this chapter, consider: • Who truly benefits from intellectual property laws in a digital, globalized world? • Should the rights of creators be prioritized over those of corporations, or vice versa? • How do emerging technologies like blockchain and generative AI challenge or reinforce traditional notions of ownership and authorship? • What ethical responsibilities do companies have to the individuals whose innovations they profit from? These questions invite you to critically examine not only the legal structures that govern intellectual property, but also"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 83, "chunk_id": "english_pack_83_1_4a4f1195", "content": "prioritized over those of corporations, or vice versa? • How do emerging technologies like blockchain and generative AI challenge or reinforce traditional notions of ownership and authorship? • What ethical responsibilities do companies have to the individuals whose innovations they profit from? These questions invite you to critically examine not only the legal structures that govern intellectual property, but also the underlying ethical principles that justify – or challenge – them in today’s rapidly changing technological landscape. Intellectual Property Page 83 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 84, "chunk_id": "english_pack_84_0_5dee90a3", "content": "Ethics in Technology By: Ed Weber Intellectual property (IP) refers to creations of the mind – such as inventions, literary and artistic works, designs, symbols, names, and images used in commerce – that are legally protected from unauthorized use or reproduction. Unlike physical property such as real estate, vehicles, or consumable goods, intellectual property is intangible: it can be shared or copied without depriving the original owner of its use. For example, while only one person can drive a specific car or eat a particular loaf of bread at a time, an unlimited number of people could read the same digital book or listen to a song without exhausting the original asset. The concept of ownership in intellectual property mirrors some aspects of tangible property – such as the ability to sell, license, or bequeath rights to others – but also differs in important ways. IP rights can be transferred, inherited, or assigned, much like physical property, allowing creators or owners to grant permission for use, sell their rights, or pass them on to heirs. However, the time-bound and territorial nature of most IP rights means that, unlike land or a house, these rights eventually expire and the protected works enter the public domain, becoming freely available to all. There are also clear legal and ethical boundaries regarding what can be owned. For instance, ideas themselves, natural phenomena, and mathematical formulas are generally not subject to ownership, though their specific expressions or applications might be. This creates gray areas – such as disputes over genetic information, traditional knowledge, or the line between inspiration and infringement – where the boundaries of ownership are continually negotiated. The evolving landscape of IP law reflects ongoing debates about how best to balance private rights with public benefit in an era where intangible assets are increasingly valuable. Patents and Copyright Patents and copyright are two foundational forms of intellectual property protection, each with a long and evolving history rooted in the desire to encourage creativity and innovation. The earliest known patent-like rights date back to Ancient Greece. In medieval Europe, the concept matured: the Republic"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 84, "chunk_id": "english_pack_84_1_550f20ec", "content": "with public benefit in an era where intangible assets are increasingly valuable. Patents and Copyright Patents and copyright are two foundational forms of intellectual property protection, each with a long and evolving history rooted in the desire to encourage creativity and innovation. The earliest known patent-like rights date back to Ancient Greece. In medieval Europe, the concept matured: the Republic of Venice’s 1474 Patent Statute is considered the first codified patent system, granting inventors exclusive rights to new devices for a limited time to encourage disclosure and public benefit. Similarly, early copyright law emerged in England with the 1710 Statute of Anne, which shifted the focus from publisher monopolies to author rights, aiming to promote learning and the progress of knowledge by granting authors exclusive rights for a limited period. In the United States, these traditions were enshrined in the Constitution, empowering Congress to grant authors and inventors exclusive rights “for limited times” to promote the progress of science and useful arts. The first federal copyright law, enacted in 1790, protected books, maps, and charts for 14 years, renewable for another 14 years if the author was still alive. Today, U.S. patents generally last 20 years from the filing date and cannot be renewed, though certain extensions are possible in specific Page 84 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 85, "chunk_id": "english_pack_85_0_66ef3bea", "content": "Ethics in Technology By: Ed Weber cases (such as pharmaceuticals). Copyright protection for works created after January 1, 1978, typically endures for the life of the author plus 70 years. For works made for hire, anonymous, or pseudonymous works, the term is 95 years from publication or 120 years from creation, whichever is shorter. Copyrights cannot be “renewed” in the traditional sense, but older works under previous laws sometimes allowed for renewal terms. The rapid pace of technological change raises important questions about whether these traditional time frames remain appropriate. In fields like software and digital technology, products and inventions can become obsolete within a few years, long before the expiration of a 20-year patent or multi-decade copyright. Shortening the protection period for rapidly evolving technologies could accelerate their entry into the public domain, fostering greater innovation and competition. For example, a system could be envisioned where software patents expire after 5–10 years, or where digital works have a reduced copyright term. This would allow society to benefit from shared knowledge and creative works more quickly, while still providing inventors and creators with a period of exclusive benefit. Such reforms would need to carefully balance the incentives for innovation with the broader public interest in access and progress. Trade Secrets and National Security Trade secrets are a powerful tool used by corporations to protect valuable information that gives them a competitive edge, such as formulas, algorithms, or business processes. Unlike patents or copyrights, which require public disclosure in exchange for legal protection, trade secrets are maintained through confidentiality and internal security measures. Famous examples include the Coca- Cola recipe and Google’s search algorithm, both of which remain undisclosed to the public and are closely guarded to maintain their economic value. However, this secrecy can sometimes conflict with the public interest, especially when withheld information – such as pharmaceutical data or environmental impact data – could benefit society at large. Corporations may claim trade secret status not only to protect legitimate business interests but also to avoid scrutiny or regulation, raising ethical questions about where to draw the line between proprietary"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 85, "chunk_id": "english_pack_85_1_1122f4c5", "content": "However, this secrecy can sometimes conflict with the public interest, especially when withheld information – such as pharmaceutical data or environmental impact data – could benefit society at large. Corporations may claim trade secret status not only to protect legitimate business interests but also to avoid scrutiny or regulation, raising ethical questions about where to draw the line between proprietary knowledge and the public’s right to know. Similarly, governments often invoke “national security” as a reason to withhold information from the public, sometimes even when disclosure might serve the greater good. While there are legitimate reasons to keep certain details confidential – such as protecting citizens or critical infrastructure – the concept can be misused to obscure wrongdoing, prevent accountability, or stifle public debate. Both trade secrets and national security claims thus present a tension between the need for confidentiality and the ethical imperative for transparency. Striking the right balance is challenging: too much secrecy can erode trust and hinder oversight, while too much transparency can expose sensitive information to Page 85 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 86, "chunk_id": "english_pack_86_0_f54effc5", "content": "Ethics in Technology By: Ed Weber misuse or harm. These dilemmas prompt a range of critical ethical questions: • Who gets to decide what qualifies as a trade secret or a matter of national security? • What standards or rubrics are used to make these determinations, and are they consistent? • How can these decisions be independently audited or reviewed to prevent abuse? • Should there be time limits or periodic reviews for information classified as secret? • Would greater transparency reduce the need for whistleblowers, or are some secrets always inevitable? • How should the public interest be weighed against corporate or governmental interests in secrecy? • What safeguards exist to ensure that claims of secrecy are not used to cover up misconduct or avoid accountability? • Are there circumstances where the ethical imperative to disclose outweighs legal protections for secrecy? • How can stakeholders – including employees, citizens, and regulators – challenge or appeal secrecy claims? • What role should external watchdogs or independent panels play in overseeing decisions about secrecy? These questions highlight the ongoing need for robust debate and oversight to ensure that trade secrets and national security claims serve the public interest rather than merely protecting private or institutional power. Blockchain and Cryptocurrency Blockchain technology has been promoted as a transformative solution for protecting digital assets by providing a decentralized, transparent, and tamper-resistant ledger. Unlike traditional databases, blockchain distributes records across a network of computers, making it extremely difficult to alter or erase past transactions. This architecture is seen as a major step forward in safeguarding traditional digital works – such as audio, video, and images – by ensuring clear, immutable records of ownership and provenance. Page 86 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 87, "chunk_id": "english_pack_87_0_cc6157ae", "content": "Ethics in Technology By: Ed Weber Likewise, the rise of Non-Fungible Tokens (NFTs) exemplifies this: NFTs are unique digital tokens on a blockchain that verify ownership and authenticity of digital items, ranging from digital art and music to virtual real estate and collectibles. For instance, an artist can mint an NFT representing a digital painting, which can then be bought, sold, or traded with its ownership history securely tracked on the blockchain. Similarly, cryptocurrencies like Bitcoin and Ethereum use blockchain to secure financial transactions, while also enabling new forms of digital property and decentralized finance. Despite these advantages, blockchain-based asset protection is not without significant risks. One of the most critical vulnerabilities is the risk of permanent loss if a user loses access to their private wallet keys or passwords. Unlike traditional banking systems, there is no central authority to recover lost credentials, and it is estimated that up to 25% of all Bitcoin in circulation may be permanently inaccessible due to lost keys. This highlights the importance of robust key management and secure custody solutions, especially as digital assets become more valuable and widely adopted. Additionally, while blockchain is currently considered highly secure, the advent of quantum computing poses a potential existential threat. Quantum computers, once they reach sufficient power – a milestone sometimes referred to as \"Q-Day\" – could theoretically break all of the cryptographic algorithms that underpin blockchain security, making it possible to forge transactions or steal assets. While estimates for Q-Day vary, some experts believe it could occur within the next decade, prompting urgent research into quantum-resistant cryptography and other safeguards to ensure the long-term viability of blockchain-based protections. As a result, while blockchain and related technologies offer powerful tools for digital asset protection and new models of ownership, they also introduce new categories of risk and uncertainty that must be carefully managed as the technology and its threats continue to evolve. Digital Art and Generative-AI The digital revolution has dramatically expanded the possibilities for both creating and copying art, while also blurring the boundaries between original works, forgeries, and homages. In traditional terms, a forgery"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 87, "chunk_id": "english_pack_87_1_3710f093", "content": "models of ownership, they also introduce new categories of risk and uncertainty that must be carefully managed as the technology and its threats continue to evolve. Digital Art and Generative-AI The digital revolution has dramatically expanded the possibilities for both creating and copying art, while also blurring the boundaries between original works, forgeries, and homages. In traditional terms, a forgery is an unauthorized imitation of an existing work, intended to deceive by passing off as the original. In the digital realm, the distinction between a forgery and a simple digital copy becomes less clear, as digital files can be reproduced perfectly and infinitely. Meanwhile, an homage refers to a new work that deliberately Figure 14: Picture of Dog using references or emulates the style of a particular artist, ChatGPT generated by Pixlr often as a form of respect or creative exploration rather Page 87 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 88, "chunk_id": "english_pack_88_0_282bee56", "content": "Ethics in Technology By: Ed Weber than deception. The challenge lies in distinguishing between these categories, especially as generative AI tools can now produce images, music, or text that closely mimic the style of well-known creators. Technological advances in forgery detection – using AI, blockchain, and watermarking – have made it possible to analyze digital artworks for inconsistencies, provenance, and originality. Yet, even with sophisticated tools, it can be difficult to determine whether a digital piece is a genuine original, a direct copy, a forgery, or a legitimate homage, particularly when AI-generated works are involved. This ambiguity complicates questions of ownership: if an AI model is trained on thousands of works by a specific artist and then generates a new piece \"in their style,\" who owns the result? Is it the person who provided the prompt, the creators of the AI tool, or the original artists whose work was used to train the model? These complexities raise a host of ethical questions, especially when considering the perspectives of art creators, tool/platform providers, and those whose works are used as models: • Is it ethical for AI tools to be trained on copyrighted works without the original creator’s consent? • Should the person who provides a prompt to a generative-AI model be considered the creator or owner of the resulting artwork? • What rights, if any, should the original artists have when their styles or works are used to train generative models? • If a platform profits from AI-generated art, should it compensate the creators whose works were used as training data? • How can we distinguish between homage and unauthorized imitation in the age of generative AI? • Should digital forgeries be treated differently from physical forgeries in terms of legal and ethical consequences? • Who is responsible if generative-AI art is used to deceive or defraud others? • Can or should ownership of AI-generated art be transferred, inherited, or sold like traditional artworks? • What mechanisms should exist for artists to opt in or out of having their works used as AI training data? • How do we ensure"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 88, "chunk_id": "english_pack_88_1_2d50bdd9", "content": "legal and ethical consequences? • Who is responsible if generative-AI art is used to deceive or defraud others? • Can or should ownership of AI-generated art be transferred, inherited, or sold like traditional artworks? • What mechanisms should exist for artists to opt in or out of having their works used as AI training data? • How do we ensure that innovation and creative freedom are not stifled by overly restrictive ownership rules in digital and AI art? Page 88 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 89, "chunk_id": "english_pack_89_0_72f953f2", "content": "Ethics in Technology By: Ed Weber These questions highlight the evolving landscape of digital art and generative AI, where traditional notions of authorship, authenticity, and ownership are being fundamentally reexamined. Textbook Definitions – Intellectual Property, Digital Art, and Emerging Economies • intellectual property – Creations of the mind, such as inventions, literary and artistic works, designs, symbols, names, and images, that are protected by law and can be owned, transferred, or licensed. • copyrights – Legal rights granted to creators for their original literary, artistic, or musical works, allowing them to control reproduction, distribution, and adaptation of those works. • patents – Exclusive rights granted for new inventions, processes, or designs, giving inventors control over the use and commercialization of their inventions for a limited period. • trade secrets – Confidential business information, such as formulas, practices, or processes, that provide a competitive advantage and are protected by secrecy rather than public registration. • temporary monopolies – Time-limited exclusive rights granted to creators or inventors to control the use of their intellectual property, intended to incentivize innovation before works enter the public domain. • fairness – The ethical principle of treating all parties justly and equitably, especially in the distribution and enforcement of rights and benefits. • power imbalances – Situations where one party holds significantly more influence or control over resources, decisions, or rights than others, often leading to ethical concerns. • Intellectual property (IP) – A category of property that includes intangible creations of human intellect, such as inventions, works of art, and symbols, protected by law. • inventions – Novel devices, methods, or processes resulting from creativity and ingenuity, often eligible for patent protection. • literary works – Original written creations, such as books, poems, and articles, protected by copyright law. • artistic works – Creative visual or performance pieces, including paintings, sculptures, music, and films, covered by copyright protection. • transferred – The act of legally moving ownership or rights from one party to another, such as through sale or assignment. Page 89 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 90, "chunk_id": "english_pack_90_0_cc6ae934", "content": "Ethics in Technology By: Ed Weber • inherited – The process by which ownership or rights are passed down from one person to another, typically upon the original owner’s death. • assigned – The legal transfer of rights or interests in intellectual property from one party to another, often through a formal agreement. • grant permission – To authorize another party to use, reproduce, or otherwise exploit a work or invention under specified conditions. • expire – To come to the end of a legally defined period of protection, after which exclusive rights are no longer enforceable. • public domain – The status of a work or invention whose intellectual property rights have expired or never existed, making it freely available for public use. • public interest – The welfare or well-being of the general public, often considered in legal and ethical decisions about access to information or resources. • Blockchain – A decentralized, distributed digital ledger technology that records transactions securely and transparently across multiple computers. • Non-Fungible Tokens (NFTs) – Unique digital tokens recorded on a blockchain that certify ownership and authenticity of a specific digital asset, such as art, music, or collectibles. • quantum computing – A field of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform calculations far beyond the capabilities of classical computers. • Q-Day – The anticipated future date when quantum computers will be powerful enough to break current cryptographic systems, potentially compromising blockchain security. • cryptography – The practice and study of techniques for securing communication and information through encoding, ensuring confidentiality, integrity, and authenticity. • forgery – The act of creating a false or unauthorized imitation of a work, typically with the intent to deceive others about its authenticity. • digital copy – An exact reproduction of a digital file or work, which can be duplicated without loss of quality or originality. • homage – A new work created in deliberate imitation or tribute to the style or influence of another artist, usually as a sign of respect rather than deception. • ownership – The legal right to"}
{"pack_id": "english_pack", "section_title": "Intellectual Property; Patents and Copyright; Trade Secrets and National Security; Blockchain and Cryptocurrency; Digital Art and Generative-AI", "page": 90, "chunk_id": "english_pack_90_1_7233e4f4", "content": "authenticity. • digital copy – An exact reproduction of a digital file or work, which can be duplicated without loss of quality or originality. • homage – A new work created in deliberate imitation or tribute to the style or influence of another artist, usually as a sign of respect rather than deception. • ownership – The legal right to possess, use, control, and transfer a property or asset, including intellectual property. Page 90 of 125 10. Intellectual Property, Digital Art, and Emerging Economies"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 91, "chunk_id": "english_pack_91_0_b6165272", "content": "Ethics in Technology By: Ed Weber 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing The story of automation is one of both disruption and transformation, shaping the very fabric of society from the earliest days of agriculture to the dawn of the Information Age. In the agricultural era, simple tools and animal-driven machines revolutionized food production, freeing human labor for other pursuits. The Industrial Revolution brought mechanized factories and assembly lines, dramatically increasing productivity but also displacing traditional crafts and altering social structures. The advent of computers in the 20th century marked another, automating complex calculations and data management, and laying the groundwork for the digital revolution. Today, as we enter the era of artificial intelligence (AI), automation, and robotics, the Figure 15: Robot typing at computer. pace of change is accelerating at an unprecedented rate, touching every aspect of our economic, social, and personal lives. Technologies such as advanced AI, autonomous vehicles, chatbots, and robotics are no longer confined to research labs or science fiction – they are rapidly becoming integral to how we work, communicate, and make decisions. AI systems now perform tasks ranging from diagnosing medical conditions to driving cars and moderating online content. Automation is transforming industries, from manufacturing and logistics to finance and customer service, while algorithmic decision-making increasingly shapes everything from hiring practices to law enforcement through predictive policing. This growing ubiquity brings both promise and peril: while these technologies offer the potential for greater efficiency, safety, and convenience, they also raise profound ethical questions about bias, accountability, and the distribution of power and opportunity. As these innovations continue to evolve, we must grapple with the sustainability of our current economic and social systems. Will the continued rise of AI, automation, and robotics lead to widespread job displacement, deepen existing inequalities, or erode human agency? Or can these technologies be harnessed to create a more just, equitable, and sustainable society? The answers to Page 91 of 125 11. Artificial Intelligence (AI), Automation and Robotics,"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 91, "chunk_id": "english_pack_91_1_851aaf26", "content": "with the sustainability of our current economic and social systems. Will the continued rise of AI, automation, and robotics lead to widespread job displacement, deepen existing inequalities, or erode human agency? Or can these technologies be harnessed to create a more just, equitable, and sustainable society? The answers to Page 91 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 92, "chunk_id": "english_pack_92_0_84bc1cf6", "content": "Ethics in Technology By: Ed Weber these questions will depend not only on technical advancements, but also on the ethical frameworks and policies we establish to guide their development and deployment. Levels of AI Artificial Intelligence (AI) exists along a spectrum of complexity and capability, often described in terms of “levels.” Early AI systems, such as expert systems, were designed to mimic the decision- making abilities of human specialists within narrow domains – think medical diagnosis or troubleshooting technical issues. These systems rely on predefined rules and logic, and while they can outperform humans in specific, well-defined tasks, they lack the flexibility and adaptability of broader intelligence. At the other end of the spectrum is Artificial General Intelligence (AGI), a theoretical form of AI that can understand, learn, and apply knowledge across a wide range of tasks at a human- like level. Beyond AGI lies Artificial Superintelligence (ASI), which would surpass human intelligence in virtually every field, including creativity, problem-solving, and social intelligence. Most of what is marketed as “AI” today – such as large language models (LLMs) and natural language processing (NLP) systems – falls far short of AGI or ASI. These models, including popular chatbots and content generators, are trained on vast, curated datasets but do not actively or continuously learn from new data once deployed. Instead, they are periodically “tuned” by their creators, often for specific domains or applications, which can introduce or reinforce biases and inaccuracies present in the training data. The curated nature of these datasets means that AI outputs can reflect the perspectives, limitations, and prejudices of the data and those who select it, leading to algorithmic bias and fairness issues. Despite rapid advances, none of today’s mainstream AI systems possess the autonomy, adaptability, or self-awareness associated with AGI. The path to AGI – and, by extension, ASI – remains uncertain, but many experts believe that once AGI is achieved, an immediate, unavoidable and unstoppable transition to ASI will follow. Given the potential for self-improvement and recursive learning (without curated input, interruption, and without specified domain limitations) this prospect raises profound questions about control"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 92, "chunk_id": "english_pack_92_1_724a404e", "content": "or self-awareness associated with AGI. The path to AGI – and, by extension, ASI – remains uncertain, but many experts believe that once AGI is achieved, an immediate, unavoidable and unstoppable transition to ASI will follow. Given the potential for self-improvement and recursive learning (without curated input, interruption, and without specified domain limitations) this prospect raises profound questions about control and safety. The assumption that AGI or ASI could be reliably “controlled” is widely regarded as hubristic, given the unpredictable nature and potential power of such systems. Compounding these concerns is the lack of universal ethical definitions or standards in the data used to train AI, making it impossible to predict what kind of “ethical center” an advanced AI might develop. As a result, society faces urgent questions about how to guide the development of increasingly capable AI systems in ways that align with shared values and long-term human interests. Page 92 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 93, "chunk_id": "english_pack_93_0_8b0b2ed9", "content": "Ethics in Technology By: Ed Weber AI Moral Agency Current AI systems – including expert systems, large language models, and other advanced tools – are best understood as sophisticated instruments rather than independent moral agents. These systems currently lack consciousness, intentionality, and the capacity for ethical judgment, so moral agency and culpability remain with the humans who design, deploy, and use them. Developers are responsible for building systems that are safe and fair, operators must ensure proper oversight, and users must understand the tool’s limitations and risks. Attributing moral agency to these tools can lead to confusion, misplaced accountability, and the dangerous illusion that ethical responsibility can be delegated to technology. The conversation shifts dramatically when considering the hypothetical emergence of Artificial General Intelligence (AGI) or Artificial Superintelligence (ASI). If an AI system were to achieve human-level understanding, autonomy, and the ability to make independent decisions (which a number of AI researchers and companies are actively pursuing), the question of moral agency becomes more complex and contentious. Would such a system deserve to be treated as a moral agent, or even as a legal entity, responsible for its actions? This debate is reminiscent of the gradual transfer of moral agency from parent to child: children initially lack full moral responsibility, which is, instead, held by their parents or guardians. But as children develop autonomy and understanding, they gradually assume agency for their own actions. Similarly, if AGI or ASI were to demonstrate genuine autonomy and ethical reasoning, there could be a case for shifting some degree of responsibility from the creators or users to the AI itself. However, this transition would be fraught with uncertainty, as we currently lack clear ethical rubrics, legal frameworks, or even a consensus on what would constitute an “ethical center” for such entities. Autonomous Vehicles Autonomous vehicles (AVs) are rapidly transforming transportation, with trucking and freight leading the way in the adoption of high-level autonomy. The Society of Automotive Engineers (SAE) defines six levels of vehicle autonomy, from Level 0 (no automation) to Level 5 (full automation, with no human intervention required at any point)."}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 93, "chunk_id": "english_pack_93_1_d91b79c5", "content": "would constitute an “ethical center” for such entities. Autonomous Vehicles Autonomous vehicles (AVs) are rapidly transforming transportation, with trucking and freight leading the way in the adoption of high-level autonomy. The Society of Automotive Engineers (SAE) defines six levels of vehicle autonomy, from Level 0 (no automation) to Level 5 (full automation, with no human intervention required at any point). Most consumer vehicles today feature Level 2 or Level 3 autonomy, offering driver assistance and partial automation. However, the most groundbreaking developments are occurring at Levels 4 and 5, where vehicles can operate independently in specific conditions or, eventually, in all environments. In the United States, fully autonomous trucking is no longer a distant vision. Aurora Innovation launched driverless trucks on the I-45 corridor between Dallas and Houston in 2025. Other companies Page 93 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 94, "chunk_id": "english_pack_94_0_99720a9e", "content": "Ethics in Technology By: Ed Weber such as Kodiak Robotics, Gatik, and Waabi are also advancing hub-to-hub autonomous trucking, particularly in states like Texas, Arizona, and Florida, where regulations are more permissive. Internationally, China’s Inceptio Technology and Germany’s on-road trials are pushing the envelope in large-scale autonomous truck deployment. These trucks promise to address driver shortages, increase operational efficiency, and reduce costs, with the potential to revolutionize logistics and supply chains globally. One of the most compelling arguments for autonomous vehicles is their potential to dramatically reduce vehicular crashes. Human error is responsible for over 90% of traffic accidents; by removing fatigue, distraction, and impaired driving from the equation, AVs could save thousands of lives annually. However, the transition is not without challenges. Legal and ethical questions loom large: when an autonomous vehicle is involved in a crash, who is responsible – the manufacturer, the software developer, the fleet operator, or the owner? Current legal frameworks are struggling to keep pace, and there is ongoing debate about how to assign liability and ensure accountability as vehicles become more autonomous. These questions will only grow in importance as AV technology becomes more ubiquitous, raising fundamental issues about trust, transparency, and the future of transportation. Chatbots Chatbots have evolved dramatically from their origins as simple, rule-based programs designed for entertainment or to answer basic questions. Early chatbots, like ELIZA in the 1960s, relied on scripted responses and could only handle straightforward, predictable interactions. As technology advanced, chatbots became popular in business settings for providing 24/7 customer service, automating frequently asked questions, and reducing the workload for human agents. The introduction of natural language processing (NLP) and machine learning (ML) allowed chatbots to better understand context and intent, leading to more sophisticated conversational agents that could manage more complex queries. Today, chatbots are widely used not only for customer service but also for telemarketing, sales, and customer engagement, often serving as the first point of contact between companies and their customers. Despite these advancements, significant limitations persist. Most chatbots, even those powered by large language models, are trained on curated datasets and operate"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 94, "chunk_id": "english_pack_94_1_f55d60f7", "content": "agents that could manage more complex queries. Today, chatbots are widely used not only for customer service but also for telemarketing, sales, and customer engagement, often serving as the first point of contact between companies and their customers. Despite these advancements, significant limitations persist. Most chatbots, even those powered by large language models, are trained on curated datasets and operate within restricted domains; they struggle to adapt when conversations deviate from expected patterns, often resulting in user frustration when the system cannot process nuanced or evolving requests. Additionally, modern chatbots increasingly use synthesized voice recordings, complete with intonations and inflections, to simulate emotion and create a more “human-like” interaction. This can enhance user experience but also blurs the line between machine and human, raising important ethical questions: Page 94 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 95, "chunk_id": "english_pack_95_0_301e66c8", "content": "Ethics in Technology By: Ed Weber • Is it ethical to replace human customer service jobs with chatbots, especially when the technology is still imperfect? • Should companies be required to disclose when a customer is interacting with a chatbot rather than a real person? • What are the risks of chatbots providing false, misleading, or “hallucinated” information to users? • How can companies ensure that chatbots do not exploit users by establishing artificial relationships or manipulating emotions? • Who is responsible if a chatbot causes harm, either through misinformation or inappropriate interactions? • Should there be regulations governing the use of voice synthesis to prevent deception or emotional manipulation? • How can biases and inaccuracies in chatbot responses be effectively identified and corrected? • What safeguards should be in place to protect vulnerable populations from exploitation by automated systems? • How can transparency and accountability be maintained as chatbots become more autonomous and integrated into everyday life? These questions highlight the ethical complexities that accompany the rapid integration of chatbots into business and society, underscoring the need for thoughtful oversight and responsible development as the technology continues to advance. Robotics and Robot Ethics Robotics is the interdisciplinary field of engineering and computer science focused on the design, construction, operation, and use of programmable machines – robots – that can replicate, substitute, or assist human actions in various tasks. Some of the earliest robots were ancient automata, such as mechanical birds in ancient Greece and water clocks in China, but the modern concept of the robot emerged in the 20th century with inventions like George Devol’s Unimate, the first industrial robotic arm, which began operating at a General Motors facility in 1959. The field of robotics was further defined by Isaac Asimov’s introduction of the “Three Laws of Robotics,” which have influenced ethical thinking about robots ever since. Asimov’s three laws of robotics were defined as follows: Isaac Asimov's Three Laws of Robotics are: Page 95 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 96, "chunk_id": "english_pack_96_0_1300601d", "content": "Ethics in Technology By: Ed Weber 1. A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Although begun as a work of science fiction, these three laws have become a foundational starting- point for very many philosophical and ethical positions regarding how robotics should be ethically developed and utilized. Most industrial robots today are fully programmed using programmable logic controllers (PLCs) or computer numerical control (CNC) systems, enabling them to perform repetitive tasks such as welding, assembly, and painting within tightly controlled environments. These robots are typically limited to their pre-programmed domains and cannot adapt to new tasks without human intervention or reprogramming. However, advances in robotics have produced machines capable of operating in more diverse and less structured environments, such as autonomous mobile robots, manufacturing and warehouse automation systems, and even robots that can assist in surgery or explore hazardous locations. These more advanced robots use sensors, AI, and machine learning to make decisions and adapt to changing conditions, reducing the need for direct human oversight and expanding the potential applications of robotics. Some ethical questions raised by the increasing use of robotics include: • What are the societal consequences of job displacement caused by robotics without corresponding changes in the existing economic model? • Should robots be used for police or military operations, and what are the risks of delegating lethal force to machines? • Is it ethical to use robots to administer medicines or perform medical procedures, and who is responsible if something goes wrong? • Should robots be permitted to manufacture or design other robots, potentially accelerating automation and reducing human oversight? • How do we ensure safety and accountability when robots operate in public or shared spaces? • What rights, if any, should humans have to intervene in or override robot"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 96, "chunk_id": "english_pack_96_1_a4bfee3a", "content": "or perform medical procedures, and who is responsible if something goes wrong? • Should robots be permitted to manufacture or design other robots, potentially accelerating automation and reducing human oversight? • How do we ensure safety and accountability when robots operate in public or shared spaces? • What rights, if any, should humans have to intervene in or override robot decisions in critical situations? Page 96 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 97, "chunk_id": "english_pack_97_0_b4e0bb30", "content": "Ethics in Technology By: Ed Weber • How can we prevent bias or discrimination in robots programmed for social or service roles? • Should there be universal standards or regulations for the ethical design and deployment of robots? • How do we balance innovation with the need to protect vulnerable populations from unintended harm caused by robotics? These questions highlight the complex ethical landscape that accompanies the rapid advancement and integration of robotics into society. Algorithmic Bias Algorithmic bias arises because AI systems are fundamentally shaped by the data used to train them, the domains they are intended to operate within, and the objectives set by their developers. Most AI is trained on curated datasets that reflect the perspectives, limitations, and sometimes the prejudices of those who collect and label the data. These models are typically fixed within a specific domain, meaning their understanding and decision-making are limited to the patterns present in their training environment. Furthermore, the intended outcomes – what the AI is supposed to optimize or predict – are defined in advance by the tool’s creators, embedding their assumptions and priorities into the system. This results in inherent biases, which can become self-perpetuating as the AI consistently produces outputs that reinforce the patterns and disparities present in its training data. Imagine a hypothetical, national healthcare system that adopts an AI-powered tool to help prioritize patients for specialist referrals. The model is trained on historical data from urban hospitals, where access to care and patient demographics differ significantly from rural areas. Because the data underrepresents rural patients and overrepresents certain ethnic groups, the AI learns to prioritize urban, majority-population patients for referrals. Over time, this bias is amplified: rural and minority patients are systematically deprioritized, leading to poorer health outcomes and widening existing disparities. The system’s recommendations are trusted as “objective” because they come from an advanced AI, making it difficult for affected groups to challenge the results or for administrators to recognize the underlying bias. If machine learning environments begin to “learn on their own” – continuously updating their models based on new data – the risk"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 97, "chunk_id": "english_pack_97_1_2ddf3707", "content": "health outcomes and widening existing disparities. The system’s recommendations are trusted as “objective” because they come from an advanced AI, making it difficult for affected groups to challenge the results or for administrators to recognize the underlying bias. If machine learning environments begin to “learn on their own” – continuously updating their models based on new data – the risk of algorithmic bias may become even more pronounced. Without explicit mechanisms to recognize and correct for bias, an AI could reinforce and amplify prejudices present in both its initial and ongoing data streams. How would such a system recognize that its data is incomplete or skewed? Could it ever truly understand the social and ethical context behind the data it consumes? Would it be able to distinguish between correlation and causation, or between majority patterns and minority needs? If an AI is left to self-train, who is responsible for monitoring and Page 97 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 98, "chunk_id": "english_pack_98_0_4289c65f", "content": "Ethics in Technology By: Ed Weber correcting its outputs, and how can we ensure transparency and accountability in such a dynamic system? These questions highlight the risk that algorithmic bias may be inevitable unless there is continuous human oversight, robust auditing, and deliberate efforts to diversify and scrutinize training data. But how will this be accomplished if the creators of the AI systems are allowed to claim ‘trade secrets’ or ‘national security’ and then withhold this information? As AI systems become more autonomous, the challenge of ensuring fairness and ethical outcomes will only grow more complex – demanding vigilance, innovation, and a commitment to equity at every stage of development and deployment. Automation Automation refers to the use of technology to perform tasks without human intervention, marking a fundamental shift from humans merely using tools to tools independently executing work. The earliest automation can be traced back to inventions like water mills and mechanical clocks, which reduced the need for constant human oversight. The Industrial Revolution accelerated this trend with machines such as the Jacquard loom and assembly line systems, which automated textile production and manufacturing processes. Over time, automation evolved from simple mechanical aids to sophisticated systems capable of performing complex, repetitive, or hazardous tasks with minimal human input. The primary drivers for automation include improving health and safety by removing humans from dangerous environments, surpassing human physical and cognitive limitations, increasing speed and productivity, reducing fatigue and stoppages, and enhancing accuracy and consistency. Automation also allows for 24/7 operation, minimizes waste, and ensures higher quality control, all of which contribute to significant cost savings and increased profits for businesses. While these benefits are often framed in terms of operational efficiency, flexibility, and safety, they are ultimately subordinate to economic motivations: the adoption of automation is primarily justified by its potential to reduce labor costs, increase output, and boost competitiveness in the marketplace. Today, automation extends far beyond manufacturing. In logistics, automated warehouses and self- driving delivery vehicles streamline supply chains. In healthcare, robotic surgery and automated diagnostics improve precision and efficiency. Financial services use algorithmic trading and automated"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 98, "chunk_id": "english_pack_98_1_464e34fe", "content": "economic motivations: the adoption of automation is primarily justified by its potential to reduce labor costs, increase output, and boost competitiveness in the marketplace. Today, automation extends far beyond manufacturing. In logistics, automated warehouses and self- driving delivery vehicles streamline supply chains. In healthcare, robotic surgery and automated diagnostics improve precision and efficiency. Financial services use algorithmic trading and automated fraud detection, while agriculture benefits from autonomous tractors and drones for planting and crop monitoring. Even creative industries are seeing automation in content generation and design. If automation continues unchecked across all sectors, it could potentially replace most traditional forms of human employment, fundamentally challenging the status quo of the current economic model. The question remains: can our existing economic systems – rooted in wage labor and job-based income Page 98 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 99, "chunk_id": "english_pack_99_0_6b58b3f3", "content": "Ethics in Technology By: Ed Weber – sustain the rapid pace of automation adoption? Or, will we need to rethink how value, work, and livelihood are distributed in a world where machines do most of the work? Predictive Policing Before the existence of formal laws, societies were governed by shared ethical norms – unwritten rules about right and wrong that guided individual and collective behavior. Laws and legal systems only emerged after these societal ethics were violated, requiring a codification of values into enforceable rules to maintain order and address breaches. Policing, as a profession and practice, arose to uphold these laws, maintain social order, and protect the community through the prevention, detection, and investigation of crime. The role of policing has always been closely tied to ethics, as officers are entrusted with significant power and discretion, and their actions can profoundly affect life, liberty, and public trust. Policing, however, has not always been a force for good. Throughout history, the institution has been subject to abuse – ranging from corruption and discrimination to excessive use of force and the protection of political interests over public welfare. These abuses highlight the ongoing tension between the ideals of ethical policing – courage, respect, empathy, and public service – and the realities of institutional culture and unchecked discretionary power. The evolution of policing models, from crime control to social peacekeeping, reflects an ongoing struggle to balance authority, accountability, and the ethical imperative to serve the public fairly and justly. Predictive policing is a recent development that uses algorithms and data analysis to forecast where crimes are likely to occur or who might be involved, with the aim of deploying resources more efficiently and preventing crime before it happens. Proponents argue that predictive policing can improve efficiency, reduce crime rates, and help allocate police resources more effectively. However, critics warn that these systems can amplify existing biases, lack transparency, and lead to over-policing of already marginalized communities. The risks of algorithmic bias, lack of oversight, and ethical ambiguity – discussed in previous sections – are especially acute in predictive policing, where flawed data"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 99, "chunk_id": "english_pack_99_1_55819366", "content": "policing can improve efficiency, reduce crime rates, and help allocate police resources more effectively. However, critics warn that these systems can amplify existing biases, lack transparency, and lead to over-policing of already marginalized communities. The risks of algorithmic bias, lack of oversight, and ethical ambiguity – discussed in previous sections – are especially acute in predictive policing, where flawed data or unchecked models can result in large-scale injustices, erode public trust, and perpetuate cycles of discrimination. As predictive policing becomes more prevalent, the amplification of these risks could manifest in widespread surveillance, unfair targeting, and diminished civil liberties. Without rigorous ethical standards, transparency, and accountability, predictive policing could undermine the very societal values and ethical foundations that laws and policing were meant to protect. Page 99 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 100, "chunk_id": "english_pack_100_0_f9678f5f", "content": "Ethics in Technology By: Ed Weber Textbook Definitions – Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics • Automation – The use of technology, machines, or systems to perform tasks with minimal or no human intervention, streamlining processes and increasing efficiency. • Information Age – The current era characterized by the rapid transmission, processing, and accessibility of information through digital technology and computing. • agricultural era – A historical period when societies were primarily based on farming and the cultivation of crops and livestock. • Industrial Revolution – The period of major industrialization during the late 18th and early 19th centuries marked by the shift from hand production to machines and factory systems. • artificial intelligence (AI) – The development of computer systems capable of performing tasks that typically require human intelligence, such as reasoning, learning, and problem- solving. • Robotics – The branch of technology that deals with the design, construction, operation, and application of robots to perform automated tasks. • Autonomous vehicles (AVs) – Vehicles equipped with technology that enables them to navigate and operate without direct human control. • Chatbots – Software applications that simulate human conversation using text or voice, often for customer service or information retrieval. • hiring practices – The methods and criteria organizations use to recruit, select, and employ personnel. • Predictive policing – The use of data analysis and algorithms to forecast potential criminal activity and inform law enforcement strategies. • bias – A systematic inclination or prejudice in favor of or against certain outcomes, groups, or data, often leading to unfair or inaccurate results. • accountability – The obligation to explain, justify, and take responsibility for one's actions or decisions. • distribution of power and opportunity – The way authority, resources, and chances for advancement are allocated among individuals or groups in a society. • sustainability – The capacity to maintain or support processes, systems, or resources over the long term without depleting them. Page 100 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 101, "chunk_id": "english_pack_101_0_73a6723b", "content": "Ethics in Technology By: Ed Weber • job displacement – The loss of employment opportunities due to technological change, automation, or other factors. • human agency – The capacity of individuals to act independently and make their own free choices. • expert systems – Computer programs that emulate the decision-making abilities of human experts in specific domains using predefined rules. • domains – Specific areas of knowledge, activity, or expertise within which a system or individual operates. • predefined rules and logic – Explicitly programmed instructions and decision criteria that govern the behavior of a system or process. • Artificial General Intelligence (AGI) – A theoretical form of AI capable of understanding, learning, and applying knowledge across a wide range of tasks at a human-like level. • Artificial Superintelligence (ASI) – A hypothetical AI that surpasses human intelligence in all respects, including creativity, reasoning, and problem-solving. • large language models (LLMs) – Advanced AI models trained on extensive text data to generate, summarize, and understand human language. • natural language processing (NLP) – The field of AI focused on enabling computers to interpret, process, and generate human language. • curated datasets – Carefully selected and organized collections of data used to train or evaluate AI models. • tuned – Adjusted or refined by developers to improve a model’s performance or adapt it to specific tasks or domains. • reinforce biases – To perpetuate or amplify existing prejudices or patterns present in training data through repeated outputs. • Algorithmic bias – Systematic and repeatable errors in AI outputs that result from biases in the data, design, or implementation of algorithms. • recursive learning – A process where AI systems iteratively update and improve themselves by learning from their own outputs or new data. • ethical center – The core set of moral principles or values that guide decision-making and behavior in an individual or system. Page 101 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 102, "chunk_id": "english_pack_102_0_e5fa5e0b", "content": "Ethics in Technology By: Ed Weber • independent moral agents – Entities capable of making ethical decisions and being held responsible for their actions without external control. • moral agency – The ability to discern right from wrong and to be held accountable for one’s actions. • culpability – The degree to which an individual or entity is responsible for a fault or wrong. • Human error – Mistakes or failures in judgment, perception, or action made by people, often leading to unintended consequences. • simulate emotion – The act of mimicking or reproducing emotional expressions or responses using technology. • automata – Self-operating machines or mechanisms, often designed to follow a predetermined sequence of operations. • Three Laws of Robotics – A set of ethical rules devised by science fiction writer Isaac Asimov to govern the behavior of robots. • programmable logic controllers (PLCs) – Industrial digital computers used to control manufacturing processes or machinery. • computer numerical control (CNC) – The automated control of machining tools and 3D printers by means of a computer. • manufacturing and warehouse automation – The use of automated systems and robots to perform tasks in production and storage facilities with minimal human involvement. • intended outcomes – The specific goals or results that a system or process is designed to achieve. • self-perpetuating – Capable of continuing or reinforcing itself without external input or intervention. • systematically deprioritized – Consistently assigned lower importance or priority in a structured or organized manner. • correlation – A statistical relationship or association between two or more variables. • causation – The action of causing something to happen; a direct cause-and-effect relationship. • majority patterns – Trends or behaviors that are most common within a given dataset or population. • minority needs – The specific requirements or interests of less-represented groups within a population. Page 102 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Levels of AI; AI Moral Agency; Autonomous Vehicles; Chatbots; Robotics and Robot Ethics; Algorithmic Bias; Automation; Predictive Policing", "page": 103, "chunk_id": "english_pack_103_0_f8d6628e", "content": "Ethics in Technology By: Ed Weber • oversight – The act of supervising, monitoring, or regulating processes or organizations to ensure proper conduct. • auditing – The systematic examination and evaluation of processes, systems, or data to ensure accuracy, compliance, and integrity. • algorithmic trading – The use of computer algorithms to automatically execute financial trades at high speed and volume. • autonomous tractors – Self-driving agricultural vehicles capable of performing tasks such as plowing, planting, and harvesting without human intervention. • drones – Unmanned aerial vehicles operated remotely or autonomously for various purposes, including surveillance, delivery, and data collection. • Policing – The activities and responsibilities of maintaining public order, enforcing laws, and preventing and investigating crime. • corruption – Dishonest or unethical conduct by those in power, typically involving bribery or the abuse of authority for personal gain. • discrimination – Unfair or prejudicial treatment of individuals or groups based on characteristics such as race, gender, or age. • excessive use of force – The application of more physical power than is necessary or justified in a given situation, often by law enforcement. • unchecked discretionary power – Authority exercised without sufficient oversight, limits, or accountability, increasing the risk of abuse. • authority – The legitimate power to make decisions, enforce rules, and command obedience. • accountability – The requirement to answer for one’s actions and decisions, especially in positions of power or responsibility. Page 103 of 125 11. Artificial Intelligence (AI), Automation and Robotics, and Algorithmic Ethics"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 104, "chunk_id": "english_pack_104_0_1cf8263a", "content": "Ethics in Technology By: Ed Weber 12. Bioethics and Human Enhancement Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain- Computer Interfaces; Biotechnology; Cloning From the earliest days of recorded history, humans have striven to overcome injury, disease, and the limitations imposed by nature. Ancient civilizations developed rudimentary forms of medicine, using herbal remedies, ritualistic healing, and early surgical techniques to treat wounds and illnesses. Over centuries, the figure of the healer evolved into the professional doctor, as societies formalized the study of anatomy, pharmacology, and hygiene. The establishment of medical institutions and the codification of ethical standards, such as the original Hippocratic Oath as well as the current versions as it has evolved over the years, marked significant milestones in the professionalization of medicine. These advancements, coupled with improvements in sanitation, nutrition, and public health, contributed to dramatic increases in birth rates and steadily rising life expectancies across much of the world. In the modern era, the fusion of medicine and technology has ushered in a new age of diagnostics and treatment. Innovations such as magnetic resonance imaging (MRI), robotic-assisted surgery, gene sequencing, and targeted therapies have become accessible – and even commonplace – in many developed regions. These breakthroughs have enabled earlier detection of disease, more precise interventions, and improved outcomes for patients. Technologies like wearable health monitors, telemedicine platforms, and personalized medicine are reshaping the patient experience, making healthcare more efficient and, in some cases, more equitable. Yet, these advances are not uniformly distributed, and significant disparities in access to care persist both within and between nations. Today, the frontier of technology and human biology is rapidly expanding beyond traditional treatment. Emerging capabilities in genetic engineering allow for the possibility of designing offspring with selected traits, raising profound ethical questions about autonomy, consent, and the very definition of humanity. Human augmentation – whether through biological enhancements, neural interfaces, or hybrid bio-robotic systems – challenges our understanding of ability, identity, and fairness. Cloning and advanced biotechnologies further blur the boundaries between natural and artificial life. These developments amplify longstanding issues of ethics and equity, as access to cutting-edge"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 104, "chunk_id": "english_pack_104_1_b5c7ef79", "content": "profound ethical questions about autonomy, consent, and the very definition of humanity. Human augmentation – whether through biological enhancements, neural interfaces, or hybrid bio-robotic systems – challenges our understanding of ability, identity, and fairness. Cloning and advanced biotechnologies further blur the boundaries between natural and artificial life. These developments amplify longstanding issues of ethics and equity, as access to cutting-edge interventions often remains limited by socioeconomic status, geography, and policy. As we look to the future, society must grapple with how to ensure that the benefits of bioethical innovation are shared broadly, while safeguarding individual rights and addressing the risks of deepening inequality. Page 104 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 105, "chunk_id": "english_pack_105_0_92e01ab5", "content": "Ethics in Technology By: Ed Weber Genetic Engineering and CRISPR For millennia, humans have shaped the natural world through selective breeding and cross- breeding, long before the discovery of DNA or the advent of modern biotechnology. Early agriculturalists learned to cultivate plants and animals with desirable traits – such as higher yields, resistance to disease, or improved taste – by intentionally mating individuals that exhibited these characteristics. Hybridization, the crossing of different species or varieties, produced vigorous new crops like hybrid grains and apples, while grafting and cloning techniques allowed for the propagation of seedless fruits such as bananas and larger, juicier varieties of produce. The transformation of wild teosinte (a Mexican grass) into modern maize (corn) is a striking example of how traditional breeding practices could fundamentally alter a species over generations. Similarly, the development of hybrid corn in the early 20th century revolutionized agriculture by increasing crop productivity. As scientific understanding deepened, especially following the discovery of DNA’s structure, genetic manipulation became more precise. By the mid-20th century, plant breeders were using radiation and chemicals to induce random mutations, further expanding the genetic toolkit available for crop improvement. The real turning point came in the 1970s, when researchers developed techniques to directly modify DNA – splicing genes from one organism into another, regardless of species boundaries. Early successes included the creation of recombinant bacteria and the first genetically modified plants, such as tobacco engineered for antibiotic resistance. In animals, transgenic mice paved the way for more complex genetic research and applications. The mapping of the human genome at the turn of the 21st century marked a watershed moment, providing a comprehensive blueprint of human genetic information. This achievement set the stage for the development of CRISPR, a revolutionary gene-editing technology that allows scientists to precisely \"cut and paste\" sections of DNA within living organisms. Today, CRISPR is being used in a wide range of applications – from developing disease-resistant crops and livestock to exploring potential cures for genetic disorders in humans. Researchers are even investigating the possibility of resurrecting extinct species by editing the genomes of living relatives."}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 105, "chunk_id": "english_pack_105_1_10fd2441", "content": "gene-editing technology that allows scientists to precisely \"cut and paste\" sections of DNA within living organisms. Today, CRISPR is being used in a wide range of applications – from developing disease-resistant crops and livestock to exploring potential cures for genetic disorders in humans. Researchers are even investigating the possibility of resurrecting extinct species by editing the genomes of living relatives. As we have done previously, let’s consider some ethical questions surrounding the concepts of genetic engineering: • Who should decide which genetic traits are considered \"normal,\" \"desirable,\" or \"disorders\" when it comes to genetic engineering in humans, plants, or animals? • Is it ethical to use gene editing technologies like CRISPR for human enhancement (such as increasing intelligence or physical ability), rather than solely for treating diseases? • How can society ensure fair and equitable access to genetic engineering technologies, so that benefits are not limited to the wealthy or privileged? Page 105 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 106, "chunk_id": "english_pack_106_0_c98b7730", "content": "Ethics in Technology By: Ed Weber • What are the potential long-term and unintended consequences of editing the human genome, given that changes could be passed to future generations who cannot consent? • Should it be permissible to patent genetically engineered organisms, genes, or gene-editing techniques, and what are the implications for intellectual property, innovation, and access? • How might widespread use of gene editing affect societal acceptance of people with disabilities or differences, and could it lead to new forms of discrimination or eugenics? • What responsibilities do scientists and companies have to ensure transparency, informed consent, and environmental stewardship when releasing genetically engineered organisms into the environment? • Is it morally acceptable to genetically engineer animals for human benefit, such as for food production or medical research, and what are the welfare considerations for these animals? • Where should the line be drawn between therapeutic uses of genetic engineering and non- therapeutic, elective, or cosmetic applications? • How should regulatory frameworks evolve to address the rapid pace of genetic engineering technology, especially given current ambiguities in law and policy? Legal and ethical frameworks have struggled to keep pace with these rapid advancements. In the United States, it was once legal to patent isolated human genes, a practice that sparked significant controversy over ownership and access to genetic information. However, a 2013 Supreme Court decision ruled that naturally occurring human genes could not be patented, though synthetic DNA (cDNA) remains patentable. The legal landscape for genetic engineering in plants, animals, and humans remains ambiguous, with regulations varying widely by country and often lagging behind technological capabilities. This uncertainty raises pressing questions about equity, access, and the responsible use of genetic technologies as society moves deeper into the era of bioengineering. Human Augmentations Throughout history, humans have sought ways to restore lost function and even enhance their bodies, as evidenced by archaeological discoveries of ancient prosthetics and artificial enhancements. Remains from ancient Egypt reveal prosthetic toes dating back nearly 3,000 years, crafted from wood and leather, suggesting both practical and possibly symbolic purposes. In China, a 2,200-year-old man of modest"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 106, "chunk_id": "english_pack_106_1_4236e7c5", "content": "bioengineering. Human Augmentations Throughout history, humans have sought ways to restore lost function and even enhance their bodies, as evidenced by archaeological discoveries of ancient prosthetics and artificial enhancements. Remains from ancient Egypt reveal prosthetic toes dating back nearly 3,000 years, crafted from wood and leather, suggesting both practical and possibly symbolic purposes. In China, a 2,200-year-old man of modest means was discovered having a prosthetic leg made from poplar wood, ox horn, and horse hoof. This limb was designed to help its owner – who suffered from a fused knee – walk more easily. Similarly, in medieval Europe, prosthetic hands and legs have been unearthed, some simple and Page 106 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 107, "chunk_id": "english_pack_107_0_ca512ae1", "content": "Ethics in Technology By: Ed Weber functional, others more elaborate, reflecting both the medical ingenuity of the time and the social significance attached to bodily integrity and appearance. These early prosthetics were primarily functional, aiming to restore lost mobility or utility. However, some may have also served as markers of status, identity, or resilience, particularly when crafted with care or adorned with valuable materials. Over centuries, the evolution of prosthetic technology has mirrored advances in materials science, medicine, and engineering – from wood and metal devices fastened with leather straps to today’s lightweight carbon fiber limbs and sophisticated bionic prosthetics that can be controlled by neural signals. Modern human augmentation has moved beyond mere replacement of lost function. Today’s prosthetics can not only restore, but also enhance, physical abilities – sometimes surpassing what is considered “normal” human performance. Athletes with advanced running blades, for example, challenge conventional definitions of ability and fairness. Neural implants, exoskeletons, and sensory enhancements are pushing the boundaries of what it means to be human, raising profound questions about identity, equity, and the future of human evolution. Consider these ethical questions surrounding the topic of human augmentation: • Should there be limits on augmentations that enhance abilities beyond the typical human range, such as strength, speed, or cognition? • Who should have access to advanced augmentations – should they be available to all, or only to those who can afford them? • Could widespread augmentation create new forms of inequality or discrimination between “augmented” and “non-augmented” individuals? • How should society regulate the use of neural implants or brain-computer interfaces that could alter thought, memory, or personality? • If a person replaces most or all of their biological body with artificial parts, are they still the same person – philosophically or legally? • Should children be allowed or required to receive certain augmentations to compete or participate in society? • What responsibilities do designers and manufacturers have if an augmentation malfunctions or is hacked? • How might human augmentation affect the value society places on natural abilities or disabilities? Page 107 of 125 12. Bioethics and"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 107, "chunk_id": "english_pack_107_1_25eacb98", "content": "same person – philosophically or legally? • Should children be allowed or required to receive certain augmentations to compete or participate in society? • What responsibilities do designers and manufacturers have if an augmentation malfunctions or is hacked? • How might human augmentation affect the value society places on natural abilities or disabilities? Page 107 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 108, "chunk_id": "english_pack_108_0_41519c85", "content": "Ethics in Technology By: Ed Weber • Should employers or governments be allowed to require or incentivize certain augmentations for work or public service? • What rights and protections should individuals have regarding the data generated by their augmented bodies? This last set of questions echoes the ancient philosophical thought experiment known as the Ship of Theseus: if every board of a ship is replaced over time, is it still the same ship? Applied to human augmentation, if all parts of a person are gradually replaced with artificial components, does their identity persist – or does something fundamentally change? This debate sits at the heart of the ethical, legal, and existential challenges posed by the future of human enhancement. Neuroethics and Brain-Computer Interfaces Neuroethics and brain-computer interfaces (BCIs) represent one of the most rapidly evolving frontiers in both neuroscience and technology. At the core of this field are neurological sensors, which can be broadly categorized as active or passive. Active sensors, such as deep brain stimulators and implanted electrodes, not only record neural activity but can also deliver electrical stimulation to targeted brain regions. Passive sensors, including electroencephalography (EEG) caps and functional MRI (fMRI), non-invasively monitor the brain’s electrical or metabolic activity for diagnostic and research purposes. These technologies have become invaluable in understanding neurological disorders, mapping brain Figure 16: Exaggeration of brain- function, and developing treatments for conditions such as computer-interface epilepsy, Parkinson’s disease, and severe paralysis. Brain-computer interfaces leverage these advances to create direct communication pathways between the brain and external devices. The most promising use cases include restoring movement or communication for individuals with paralysis, enabling control of prosthetic limbs, and providing new ways for people with severe disabilities to interact with the world. BCIs are also being explored for cognitive enhancement, mental health interventions, and even immersive gaming experiences. The ability to decode neural signals and translate them into digital commands holds transformative potential for medicine, rehabilitation, and human-computer interaction. However, the scale and complexity of data collected by neurological sensors and BCIs present significant challenges. Current technology cannot isolate individual thoughts or intentions with Page"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 108, "chunk_id": "english_pack_108_1_9c4bfe1f", "content": "being explored for cognitive enhancement, mental health interventions, and even immersive gaming experiences. The ability to decode neural signals and translate them into digital commands holds transformative potential for medicine, rehabilitation, and human-computer interaction. However, the scale and complexity of data collected by neurological sensors and BCIs present significant challenges. Current technology cannot isolate individual thoughts or intentions with Page 108 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 109, "chunk_id": "english_pack_109_0_731697dc", "content": "Ethics in Technology By: Ed Weber precision; instead, it captures vast streams of brain activity, resulting in the collection of far more data about a person than is necessary for a specific research or clinical goal. This phenomenon mirrors broader concerns previously discussed in the chapter on Privacy, Surveillance, and Data Ethics. In that chapter we discussed how ‘big data’, the aggregation and analysis of massive datasets, can inadvertently expose sensitive personal information, create privacy risks, and lead to unintended uses of data. Just like with big data, the capture, storage, and analysis of neurological data repeats the same ethical concerns which include informed consent, data ownership, potential misuse of neural data, and the risk of surveillance or discrimination based on brain activity patterns. Neuralink, a leading company in the BCI space, has recently achieved a major milestone by successfully implanting its “Telepathy” device in a human subject. This coin-sized implant uses ultra- fine threads equipped with thousands of electrodes to record neural activity at a high resolution. The device has demonstrated the ability to detect neuron spikes and correlate brain signals with intended motor actions, allowing users to control computers or external devices directly through thought. Neuralink’s approach combines advanced neurosurgical robotics for precise implantation with custom electronics that process and transmit neural data. Neuralink is currently engaged in human trials, offering hope for individuals with severe neurological conditions and opening new possibilities for human-computer integration. Here, again, are several ethical questions surrounding neuroethics and BCIs: • To what extent could brain-computer interfaces (BCIs) be used to read or decode private thoughts and memories, and what safeguards should be in place to protect mental privacy? • If a BCI could send signals to the brain that override or contradict a person’s intended actions (such as controlling movement or behavior), who is responsible for the outcome, and how should consent be managed? • What ethical concerns arise if technology advances to the point where data can be written to the brain – potentially altering memories, perceptions, or even personality traits – rather than just reading from it? • How can"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 109, "chunk_id": "english_pack_109_1_8daf17a7", "content": "a person’s intended actions (such as controlling movement or behavior), who is responsible for the outcome, and how should consent be managed? • What ethical concerns arise if technology advances to the point where data can be written to the brain – potentially altering memories, perceptions, or even personality traits – rather than just reading from it? • How can individuals maintain autonomy and freedom of thought in a future where neurotechnology might make it possible for others to access or influence their mental states? • Should there be limits on the collection and analysis of neural data, given that current BCIs capture far more information than is needed for specific tasks, raising big-data privacy and consent issues? • In the event of a malfunction, hack, or unauthorized access to a BCI, what protections and recourse should users have if their thoughts or actions are affected without their consent? Page 109 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 110, "chunk_id": "english_pack_110_0_03570935", "content": "Ethics in Technology By: Ed Weber • How should society address the possibility of BCIs being used for enhancement or manipulation, such as boosting cognitive abilities or influencing decisions, especially if access is unequal or coerced? As BCI technology advances, the ethical landscape will require ongoing scrutiny, balancing the immense potential for benefit with the need to protect individual rights and societal values. Biotechnology Biotechnology is a broad field that encompasses the use of living organisms, cells, and biological systems to develop products and processes that benefit society. Many of the topics previously discussed – such as genetic engineering, CRISPR, brain-computer interfaces, and bioethics – are all integral parts of the biotechnology landscape. However, the reach of biotechnology extends even further, touching on a range of emerging technologies and applications that are reshaping medicine, agriculture, industry, and environmental management. Beyond gene editing and medical diagnostics, biotechnology now includes advanced innovations like nanotechnology for direct cell repair and targeted cancer therapies. Nanotech-enabled particles can be engineered to seek out and destroy cancer cells without harming healthy tissue, offering more precise and less invasive treatments. In addition, biotechnology has enabled the development of bioengineered organisms – microbes or plants designed to clean up pollution through processes like bioremediation and phytoremediation. These organisms can break down toxic substances in soil and water, helping to restore contaminated environments and improve public health. Another rapidly growing area is the production of bio-printed or lab-grown food. Using 3D printing technology and cell culture techniques, scientists can now create meat, organs, and other tissues in the lab, potentially reducing the environmental impact of traditional agriculture and providing new sources of nutrition. This technology is also being explored for medical applications, such as printing skin, bone, or even entire organs for transplantation. While the benefits of biotechnology are substantial, significant risks and uncertainties remain. One major concern is the possibility of unintended release of engineered organisms or nanotech agents into the environment. Once released, these entities may not be easily converted from active to dormant or inert states, raising fears about long-term ecological impacts or the creation of"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 110, "chunk_id": "english_pack_110_1_5026de2b", "content": "entire organs for transplantation. While the benefits of biotechnology are substantial, significant risks and uncertainties remain. One major concern is the possibility of unintended release of engineered organisms or nanotech agents into the environment. Once released, these entities may not be easily converted from active to dormant or inert states, raising fears about long-term ecological impacts or the creation of new, hard-to-control forms of pollution. The microscopic or nanoscale nature of many biotech interventions also makes transparency and oversight difficult, complicating efforts to monitor their behavior and effects. For example, bioengineered microbes used to clean up oil spills or toxic waste could themselves become hazardous if they mutate or interact with other organisms in unexpected ways. While the pros of such applications include cleaner water and soil, the cons may involve the organisms becoming toxic Page 110 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 111, "chunk_id": "english_pack_111_0_aefb5579", "content": "Ethics in Technology By: Ed Weber to humans or disrupting local ecosystems. Similarly, lab-grown foods promise sustainability and food security, but raise questions about safety, labeling, and the social and economic impacts on traditional farming communities. Biotechnology is revolutionizing how we approach health, food, and environmental challenges, but it also demands careful consideration of the risks, especially regarding safety, transparency, and long- term sustainability. As these technologies become more integrated into daily life, ongoing ethical, legal, and societal debates will be essential to ensure they are used responsibly and equitably. Cloning Cloning, as a concept, has long fascinated humanity, appearing in ancient myths, literature, and modern entertainment as the idea of creating identical copies of organisms. The scientific journey toward cloning began in the late 19th century, when researchers like Hans Driesch demonstrated artificial embryo twinning in sea urchins, showing that separated embryonic cells could each develop into whole organisms. In the 20th century, landmark experiments included the cloning of frogs by nuclear transfer in the 1950s and the cloning of mammals from embryonic and adult cells in the 1980s and 1990s. The most famous breakthrough came in 1996 with the birth of Dolly the sheep, the first mammal cloned from an adult somatic cell, announced by Ian Wilmut and his team at the Roslin Institute in Scotland. Dolly’s creation proved that specialized adult cells could be reprogrammed to create an entire organism, igniting both scientific excitement and ethical debate. Other notable milestones include the cloning of cows, cats, and even monkeys, as well as the cloning of animals for agriculture, research, and pet reproduction. Attempts at human cloning have been more controversial and less successful. In 2001, scientists at Advanced Cell Technology in Massachusetts cloned human embryos for the first time, aiming for therapeutic rather than reproductive purposes. In 2013, a team led by Shoukhrat Mitalipov achieved a breakthrough in human cloning by creating embryonic stem cells from cloned human embryos. While some fringe groups and individuals have claimed to have cloned humans, there is no verified scientific evidence of a live human clone. News of human cloning efforts"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 111, "chunk_id": "english_pack_111_1_dc5499a9", "content": "first time, aiming for therapeutic rather than reproductive purposes. In 2013, a team led by Shoukhrat Mitalipov achieved a breakthrough in human cloning by creating embryonic stem cells from cloned human embryos. While some fringe groups and individuals have claimed to have cloned humans, there is no verified scientific evidence of a live human clone. News of human cloning efforts has generally been met with skepticism and concern within the scientific and medical communities, and has sparked strong opposition from religious, ethical, and political groups around the world. Reactions have ranged from moral outrage and calls for bans to cautious support for therapeutic cloning aimed at treating disease. Currently, human reproductive cloning is illegal or heavily restricted in the United States and many other countries. Therapeutic cloning – using cloned embryos to derive stem cells for research or medical treatment – remains a gray area, with regulations varying by state and ongoing debates about its ethical and legal status. The technology continues to raise profound questions about identity, individuality, and the boundaries of human intervention in nature. Page 111 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 112, "chunk_id": "english_pack_112_0_b3c106bc", "content": "Ethics in Technology By: Ed Weber Consider these ethical questions surrounding the concept of cloning: • Is it ethical to create a human clone for reproductive purposes, knowing the potential risks and uncertainties involved? • Should cloning be allowed for therapeutic purposes, such as generating tissues or organs for transplantation? • What rights and status would a human clone have in society – would they be treated as individuals or property? • Could the widespread use of cloning undermine the value of genetic diversity or lead to new forms of discrimination? • How should society regulate or oversee cloning technology to prevent abuse or unintended consequences? • Would the existence of human clones challenge traditional notions of family, parenthood, and identity? • What are the long-term psychological and social impacts on clones and their families? Textbook Definitions – Bioethics and Human Enhancement • medicine – The science and practice of diagnosing, treating, and preventing disease and injury in humans. • herbal remedies – Treatments derived from plants and plant extracts used for their medicinal properties. • professional doctor – A person formally trained and licensed to practice medicine and provide healthcare. • anatomy – The study of the structure of living organisms, especially their internal systems and organs. • pharmacology – The branch of medicine concerned with the study of drugs and their effects on the body. • hygiene – Practices and conditions that promote health and prevent disease, especially through cleanliness. • Hippocratic Oath – An ancient ethical code historically taken by physicians, emphasizing medical ethics and patient care. The current accepted version of this oath (as of 2017) is: Page 112 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 113, "chunk_id": "english_pack_113_0_a5b63553", "content": "Ethics in Technology By: Ed Weber ◦ AS A MEMBER OF THE MEDICAL PROFESSION: ▪ I SOLEMNLY PLEDGE to dedicate my life to the service of humanity; ▪ THE HEALTH AND WELL-BEING OF MY PATIENT will be my first consideration; ▪ I WILL RESPECT the autonomy and dignity of my patient; ▪ I WILL MAINTAIN the utmost respect for human life; ▪ I WILL NOT PERMIT considerations of age, disease or disability, creed, ethnic origin, gender, nationality, political affiliation, race, sexual orientation, social standing or any other factor to intervene between my duty and my patient; ▪ I WILL RESPECT the secrets that are confided in me, even after the patient has died; ▪ I WILL PRACTICE my profession with conscience and dignity and in accordance with good medical practice; ▪ I WILL FOSTER the honor and noble traditions of the medical profession; ▪ I WILL GIVE to my teachers, colleagues, and students the respect and gratitude that is their due; ▪ I WILL SHARE my medical knowledge for the benefit of the patient and the advancement of healthcare; ▪ I WILL ATTEND TO my own health, well-being, and abilities in order to provide care of the highest standard; ▪ I WILL NOT USE my medical knowledge to violate human rights and civil liberties, even under threat; ▪ I MAKE THESE PROMISES solemnly, freely, and upon my honor. • sanitation – Measures and practices that maintain cleanliness and prevent the spread of disease, especially through waste management. • nutrition – The process by which living organisms obtain and use food to support growth, health, and maintenance. • public health – The science and practice of protecting and improving the health of communities through education, policy, and preventive measures. • diagnostics – Techniques and tools used to identify diseases or medical conditions in individuals. • treatment – Medical care or intervention given to manage or cure illness or injury. • magnetic resonance imaging (MRI) – A non-invasive imaging technique that uses magnetic fields and radio waves to create detailed images of internal body structures. • robotic-assisted surgery – Surgical procedures performed"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 113, "chunk_id": "english_pack_113_1_158ed89e", "content": "– Techniques and tools used to identify diseases or medical conditions in individuals. • treatment – Medical care or intervention given to manage or cure illness or injury. • magnetic resonance imaging (MRI) – A non-invasive imaging technique that uses magnetic fields and radio waves to create detailed images of internal body structures. • robotic-assisted surgery – Surgical procedures performed with the aid of robotic systems to enhance precision and control Page 113 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 114, "chunk_id": "english_pack_114_0_d6ae26fb", "content": "Ethics in Technology By: Ed Weber • gene sequencing – The process of determining the exact order of nucleotides in a DNA molecule. • wearable health monitors – Electronic devices worn on the body that track health metrics such as heart rate, activity, or sleep. • telemedicine – The remote diagnosis and treatment of patients using telecommunications technology. • personalized medicine – Medical care tailored to an individual’s genetic, environmental, and lifestyle factors. • genetic engineering – The direct manipulation of an organism’s DNA to alter its characteristics or functions. • autonomy – The right or condition of self-government, especially in making informed decisions about one’s own body and health. • consent – Permission for something to happen or agreement to do something, especially after being informed of the risks and benefits. • Human augmentation – The use of technology to enhance or extend human physical or cognitive abilities. • neural interfaces – Devices or systems that enable direct communication between the brain and external devices. • hybrid bio-robotic systems – Integrated systems combining biological and robotic components to enhance function or performance. • Cloning – The process of producing genetically identical copies of an organism, cell, or DNA sequence. • selective breeding – The intentional mating of organisms with desirable traits to produce offspring with those traits. • cross-breeding – The process of mating individuals from different breeds or species to produce hybrid offspring. • DNA – Deoxyribonucleic acid, the molecule that carries genetic information in living organisms. • Hybridization – The process of combining different varieties or species to produce a hybrid with traits from both parents. Page 114 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 115, "chunk_id": "english_pack_115_0_b2b47889", "content": "Ethics in Technology By: Ed Weber • random mutations – Unplanned changes in DNA that can result in new traits or variations in organisms. • CRISPR – A gene-editing technology that allows precise modifications to DNA sequences in living organisms. • eugenics – The controversial practice or belief in improving the genetic quality of a human population through selective breeding or genetic intervention. • transparency – Openness and clarity about processes, decisions, and data, especially in science and ethics. • informed consent – The process of providing individuals with sufficient information to make knowledgeable decisions about participation in medical or research activities. • environmental stewardship – The responsible management and care of the environment and natural resources. • therapeutic – Intended to heal or treat disease or medical conditions. • elective – Chosen or optional, especially referring to medical procedures that are not medically necessary. • cosmetic – Intended to improve appearance rather than health or function. • prosthetics – Artificial devices that replace missing body parts to restore function or appearance. • carbon fiber – A strong, lightweight material commonly used in advanced prosthetics and other high-performance applications. • bionic prosthetics – Artificial limbs or devices enhanced with electronic or mechanical components to mimic or surpass natural function. • neural signals – Electrical impulses generated by neurons that transmit information within the nervous system. • running blades – Curved, spring-like prosthetic limbs designed to enable or enhance running performance. • Neural implants – Devices surgically placed in the brain or nervous system to restore or enhance function. • exoskeletons – Wearable robotic frameworks that support or augment human movement and strength. Page 115 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 116, "chunk_id": "english_pack_116_0_a3699024", "content": "Ethics in Technology By: Ed Weber • sensory enhancements – Technologies or interventions that improve or extend human sensory perception. • thought – A mental process involving ideas, reasoning, or imagination. • memory – The mental capacity to store, retain, and recall information or experiences. • personality – The combination of characteristics or qualities that form an individual’s distinctive character. • Neuroethics – The study of ethical, legal, and social issues arising from neuroscience and neurotechnology. • brain-computer interfaces (BCIs) – Systems that enable direct communication between the brain and external devices, often for control or interaction. • Active sensors – Devices that both detect and interact with biological signals, often by sending or receiving electrical impulses. • implanted electrodes – Electrodes surgically placed in the body or brain to monitor or stimulate neural activity. • Passive sensors – Devices that detect and record biological signals without actively interacting with the system. • electroencephalography (EEG) – A non-invasive method for recording electrical activity of the brain using electrodes placed on the scalp. • functional MRI (fMRI) – An imaging technique that measures brain activity by detecting changes in blood flow. • cognitive enhancement – The use of technology or interventions to improve mental functions such as memory, attention, or intelligence. • intended motor actions – Movements or actions that a person consciously plans or attempts to perform. • neural data – Information collected from the nervous system, especially brain activity signals. • Biotechnology – The use of living organisms, cells, or biological systems to develop products and technologies for human benefit. • nanotechnology – The manipulation and application of materials at the molecular or atomic scale, often for medical or technological purposes. • bioengineered organisms – Living organisms whose genetic material has been deliberately modified for specific purposes. Page 116 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Genetic Engineering and CRISPR; Human Augmentations; Neuroethics and Brain-Computer Interfaces; Biotechnology; Cloning", "page": 117, "chunk_id": "english_pack_117_0_672feff7", "content": "Ethics in Technology By: Ed Weber • bioremediation – The use of living organisms, such as microbes or plants, to clean up environmental pollutants. • phytoremediation – The use of plants to absorb, remove, or neutralize contaminants from soil or water. • bio-printed – Created using 3D printing techniques with biological materials, often for medical or food applications. • unintended release – The accidental escape or spread of engineered organisms or substances into the environment. • human reproductive cloning – The creation of a human being that is genetically identical to another individual through cloning techniques. • Therapeutic cloning – The creation of cloned embryos for the purpose of generating stem cells for medical research or treatment. Page 117 of 125 12. Bioethics and Human Enhancement"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 118, "chunk_id": "english_pack_118_0_ae0ebca4", "content": "Ethics in Technology By: Ed Weber 13. Technological Disruption and the Paradox of Progress Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability Throughout history, the greatest leaps forward in human civilization have often been catalyzed by technological disruption. From the invention of the printing press, which democratized knowledge and upended centuries-old power structures, to the assembly line that revolutionized manufacturing and made goods accessible to millions, each wave of innovation has brought both profound progress and significant upheaval. These disruptions are rarely met with universal enthusiasm; while some individuals and industries embrace, adopt, and expand upon new technologies, others resist, fearing loss of livelihood, status, or control. The introduction of the personal computer, for example, was welcomed by early adopters and visionaries but met skepticism by those invested in mainframe computing or manual record keeping. Similarly, the rapid rise of the Internet in the 1990s transformed everything from commerce to communication, sparking both excitement and anxiety about its societal implications. Looking back at the past 50 to 75 years, many of the most notable disruptions align with the topics explored in earlier chapters. The digital revolution – driven by personal computers, the Internet, and later, mobile devices – has fundamentally altered how we work, learn, and connect. Advances in genetics and biotechnology, such as the mapping of the human genome and the development of CRISPR, have opened new frontiers in medicine and ethics. The rise of artificial intelligence, automation, and robotics has transformed industries, from manufacturing to healthcare, while also raising concerns about job displacement and algorithmic bias. In recent years, technologies like 3D printing, spatial computing, and wearable and embedded devices have further blurred the boundaries between the physical and digital worlds, creating new opportunities and challenges. A defining feature of modern technological disruption is its exponential, rather than linear, rate of change. Innovations that once took decades to diffuse now reach global scale in a matter of years – or even months. The adoption curve for technologies like smartphones, streaming services, and generative AI has been breathtakingly steep,"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 118, "chunk_id": "english_pack_118_1_5e49655b", "content": "and digital worlds, creating new opportunities and challenges. A defining feature of modern technological disruption is its exponential, rather than linear, rate of change. Innovations that once took decades to diffuse now reach global scale in a matter of years – or even months. The adoption curve for technologies like smartphones, streaming services, and generative AI has been breathtakingly steep, leaving little time for societies to adapt before the next wave arrives. As we stand on the cusp of further disruption, the window between major breakthroughs grows ever shorter, and the potential impacts – both positive and negative – become more profound. Looking to the near horizon, several major technological breakthroughs seem poised to reshape our world. These may include: • Widespread deployment of advanced AI systems capable of autonomous decision-making in critical sectors. Page 118 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 119, "chunk_id": "english_pack_119_0_813bd7da", "content": "Ethics in Technology By: Ed Weber • Quantum computing breakthroughs that render current encryption obsolete and enable new scientific discoveries. • Scalable, affordable bioengineering solutions for disease treatment, food production, and environmental restoration. • Mainstream adoption of brain-computer interfaces, enabling direct neural interaction with digital systems. • The rise of fully immersive spatial computing environments, transforming work, education, and entertainment. • Next-generation energy technologies, such as fusion or advanced battery storage, that could disrupt global energy markets. The paradox of progress is that while technological disruption drives unprecedented advancement, it also brings new challenges – planned and inevitable obsolescence, tech lock-in, erosion of economic participation, and environmental consequences – that demand thoughtful navigation in the decades ahead. Obsolescence – Planned vs. Inevitable The story of technological progress is often told through the lens of obsolescence, where each new leap forward renders a previous standard obsolete: the abacus gave way to the calculator, which was then eclipsed by the computer; buggy-whips disappeared as automobiles replaced horse-drawn carriages, soon followed by windshield wipers as standard equipment; the telegraph was overtaken by telephones, which themselves have been transformed by digital and VoIP communication; vinyl records yielded to cassette tapes, then CDs, and now streaming audio; and film cameras faded as digital photography became the norm. Each of these transitions highlights how new technologies not only replace old ones but also reshape industries, economies, and daily life. Inevitable obsolescence is a natural byproduct of technological disruption, as newer, better, or more efficient solutions emerge and make older versions less useful or even unserviceable. This cycle is accelerating, with product and component life cycles growing shorter as innovation speeds up and consumer expectations rise. The ethical implications of this relentless churn are complex. While much attention is paid to job displacement and the need to support those affected by disruption, a deeper question arises: should our ethical frameworks focus solely on preserving the status quo, or should they also empower us to reimagine the very structure of work, value, and participation in society? As automation and AI threaten to upend traditional employment models, it Page"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 119, "chunk_id": "english_pack_119_1_f09e693c", "content": "attention is paid to job displacement and the need to support those affected by disruption, a deeper question arises: should our ethical frameworks focus solely on preserving the status quo, or should they also empower us to reimagine the very structure of work, value, and participation in society? As automation and AI threaten to upend traditional employment models, it Page 119 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 120, "chunk_id": "english_pack_120_0_79b094bd", "content": "Ethics in Technology By: Ed Weber may be time to challenge the assumption that widespread employment is the only path to economic security and personal fulfillment. In response to the threat of inevitable obsolescence, some businesses have adopted the strategy of planned obsolescence – intentionally designing products with limited lifespans or incremental improvements to encourage repeated purchases and maintain brand relevance. This approach allows companies to attempt to control the pace of change and manage consumer expectations, but it also raises ethical concerns about waste, resource use, and consumer manipulation. Ultimately, the interplay between inevitable and planned obsolescence shapes not only the technology landscape but also the broader social and ethical context in which innovation unfolds. Tech Lock-In and 3D Printing As a facet of planned obsolescence, many companies have increasingly adopted the practice of tech lock-in – designing products and systems that require vendor-specific components, consumables, or software, and sometimes even restricting or disabling functionality to ensure ongoing customer dependence. Classic examples include printer manufacturers requiring proprietary ink cartridges, smartphone ecosystems that only accept certified accessories, and enterprise software platforms that limit interoperability or export options. In the digital realm, cloud-based services and software-as-a- service (SaaS) solutions often lock users into proprietary file formats, APIs, or user experiences, making it difficult and costly to migrate to alternative providers. Major vendors like Apple, Salesforce, and Amazon Web Services are well-known for creating tightly integrated ecosystems that discourage switching by making data migration complex, costly, or incomplete. These strategies are further reinforced by contractual constraints, such as multi-year commitments, tiered pricing, and auto- renewals, which add financial friction to any potential move. This lock-in effect is exacerbated by the concepts of sunk costs and high conversion costs. Organizations and individuals invest significant time, money, and training into a particular platform or ecosystem, making the prospect of switching even more daunting. The more customized and integrated a solution becomes, the harder it is to leave – creating a cycle where users tolerate limitations or incremental upgrades rather than face the disruption and expense of change. As a result, tech lock-in not only"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 120, "chunk_id": "english_pack_120_1_6eb46d4f", "content": "significant time, money, and training into a particular platform or ecosystem, making the prospect of switching even more daunting. The more customized and integrated a solution becomes, the harder it is to leave – creating a cycle where users tolerate limitations or incremental upgrades rather than face the disruption and expense of change. As a result, tech lock-in not only prolongs the viability of existing brands and products but also shapes the pace and direction of technological progress. But this tech lock-in also results in diminished innovation, and often a resignation to accepting inferior products due to lack of reasonable options. Page 120 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 121, "chunk_id": "english_pack_121_0_450b74c2", "content": "Ethics in Technology By: Ed Weber Enter 3D printing, or additive manufacturing, which is the process of creating three-dimensional objects from digital models by layering material – such as plastics, metals, ceramics, or even biological substances – one layer at a time. Since its inception in the 1980s, 3D printing has rapidly evolved from a prototyping tool to a transformative manufacturing technology. Today, it encompasses everything from nano-scale components to large-scale construction, including the printing of custom medical implants, automotive parts, aerospace components, and even entire homes. The technology’s versatility is evident in applications such as on-demand spare parts, personalized prosthetics, bio-printed tissues, and rapid prototyping for innovation across industries. Figure 17: 3D Printer creating prosthetic If the full potential of 3D printing were unleashed, it could disrupt several major status quos: • Traditional manufacturing and supply chains could be decentralized, with goods produced locally or even at home, reducing the need for mass production and global shipping. • Proprietary replacement parts and consumables could be bypassed, undermining tech lock-in strategies and empowering consumers to repair or modify products independently. • The barriers to entry for new inventors and small businesses would be dramatically lowered, fostering innovation and competition. • Entire industries, from construction to healthcare, could be transformed by the ability to produce complex, customized items on demand. • Environmental impacts could be mitigated by reducing waste, transportation emissions, and excess inventory. Ultimately, widespread adoption of 3D printing has the potential to challenge both the economic and ethical foundations of planned obsolescence and tech lock-in, shifting power from centralized producers to distributed creators and consumers. AR/VR & Tech Progress From the earliest days of photography and moving pictures, humanity has sought to capture, replicate, and even enhance reality through technology. The journey from static images to immersive Page 121 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 122, "chunk_id": "english_pack_122_0_f7304c70", "content": "Ethics in Technology By: Ed Weber digital environments has been marked by continual innovation: stereoscopes in the 1800s introduced three-dimensional imagery, while the 20th century brought movies, television, and eventually holography, each step deepening our ability to simulate and augment the world around us. By the late 1960s, the first head-mounted display, “The Sword of Damocles,” laid the groundwork for both virtual reality (VR) and augmented reality (AR), offering users computer-generated graphics that blended with or replaced their sensory experience of the real world. Over the decades, milestones such as flight simulators, interactive “artificial reality” labs, and data gloves paved the way for today’s spatial computing – where AR and VR converge to create interactive, immersive environments that respond to users in real time. Today, AR and VR technologies are transforming a wide range of industries. In entertainment, VR headsets and AR mobile games like Pokémon Go have redefined gaming and storytelling. Aerospace and automotive companies use VR for prototyping and immersive design, while AR assists with maintenance and training. In education, students explore historical sites or conduct virtual science experiments. The medical field employs VR for surgical training and pain management, and AR for overlaying critical information during procedures. Retailers offer virtual try-ons, architects visualize buildings at scale, and therapists use immersive simulations for mental health treatments. Even manufacturing and logistics benefit from AR overlays that guide workers or optimize warehouse operations. Looking ahead, the fusion of AR/VR with machine learning and other emerging technologies promises to disrupt even more status quos. Imagine: • Virtual prototyping of clothing or products, allowing users to “try before they print” with 3D printing. • Entire public spaces transformed through AR, offering personalized information, art, or advertising on demand. • Realistic VR/AR simulations for social skills training, therapy, or remote collaboration. • Educational experiences that adapt in real time to student performance, providing personalized learning paths. • Remote medical consultations using AR overlays to guide both patient and provider. • Urban planning tools that let communities visualize and interact with proposed changes before they happen. • Fully immersive remote workspaces, blurring the line"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 122, "chunk_id": "english_pack_122_1_21f02292", "content": "for social skills training, therapy, or remote collaboration. • Educational experiences that adapt in real time to student performance, providing personalized learning paths. • Remote medical consultations using AR overlays to guide both patient and provider. • Urban planning tools that let communities visualize and interact with proposed changes before they happen. • Fully immersive remote workspaces, blurring the line between physical presence and digital collaboration. Page 122 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 123, "chunk_id": "english_pack_123_0_21ed5f2e", "content": "Ethics in Technology By: Ed Weber As these technologies accelerate, several forward-looking questions arise that tie together themes from previous chapters: • How do we ensure equitable access to immersive technologies, so benefits aren’t limited to the privileged? • In what ways might AR/VR amplify existing biases, privacy concerns, or misinformation challenges? • What ethical responsibilities do creators and users have when virtual experiences become indistinguishable from reality? • How can we balance the immense potential for progress with the risks of addiction, surveillance, or deepening digital divides? • Will the next wave of disruption redefine not just how we interact with technology, but how we understand identity, agency, and community itself? Embracing technological progress means not only harnessing these tools for innovation and growth, but also facing the ethical challenges they bring – ensuring that the future we build is both immersive and inclusive. Erosion of Economic Sustainability Earlier in this chapter, we looked at obsolescence and how it could be considered either inevitable or planned. Now let’s consider our current economic models, specifically their sustainability. The most prevalent model globally is capitalism, which is defined by private ownership of resources and means of production, with goods and services exchanged in markets driven by supply and demand. Another model is socialism, where the state or community owns the means of production and aims to distribute wealth more equally. There are also mixed economies, which blend elements of both systems to varying degrees. Each of these models has evolved to address the needs and challenges of their times, but all are fundamentally shaped by the dynamics of labor, consumption, and resource allocation. The accelerating rate of technological change, coupled with the disruptive nature of tech progress, poses significant challenges to the sustainability of these economic systems. Automation, artificial intelligence, and digital platforms are rapidly transforming industries, often rendering traditional jobs obsolete faster than new roles can be created. This disruption threatens the foundation of economic participation in models like capitalism, which rely on widespread employment and consumer spending to drive growth. To counteract these effects, societies have experimented with artificial"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 123, "chunk_id": "english_pack_123_1_cc45d509", "content": "the sustainability of these economic systems. Automation, artificial intelligence, and digital platforms are rapidly transforming industries, often rendering traditional jobs obsolete faster than new roles can be created. This disruption threatens the foundation of economic participation in models like capitalism, which rely on widespread employment and consumer spending to drive growth. To counteract these effects, societies have experimented with artificial supports such as subsidies, retraining programs, and universal basic income (UBI). However, these measures often fail to address the root causes of disruption, serving as only temporary fixes rather than sustainable solutions. Page 123 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 124, "chunk_id": "english_pack_124_0_8fd1afe5", "content": "Ethics in Technology By: Ed Weber It is important to recognize that Maslow’s hierarchy of needs – a foundational theory of human motivation – does not include “make a lot of money” as a requirement at any level. Instead, Maslow’s pyramid begins with physiological needs (food, water, shelter), followed by safety, belonging, esteem, and ultimately self-actualization. While money can help secure basic needs, research shows that happiness and fulfillment plateau once a certain level of financial security is achieved. If we imagine a world where technological advancements – automation, AI, biotechnology, and beyond – are harnessed intentionally to meet all of Maslow’s needs directly, the necessity for traditional economic systems would need to be fundamentally reevaluated. In such a scenario, access to food, shelter, healthcare, education, and even opportunities for personal growth could be decoupled from employment and income, challenging us to envision new models of economic and social organization that prioritize human well-being over perpetual economic growth. Textbook Definitions – Technological Disruption and the Paradox of Progress • technological disruption – A fundamental change that occurs when a new technology radically alters the way consumers, businesses, or industries operate, often making existing products or processes obsolete. • digital revolution – The transition from analogue devices to digital technology, marking the beginning of the Information Era and profoundly transforming societies and economies worldwide. • 3D printing – A manufacturing process that creates three-dimensional objects by layering materials according to digital models, enabling rapid prototyping and customized production. • spatial computing – The use of digital technology to interact with and manipulate physical space, blending real and virtual environments for immersive experiences. • wearable and embedded devices – Electronic gadgets designed to be worn on the body or integrated into physical objects, often to collect data or enhance functionality. • adoption curve – A graphical representation of how new technologies or products are adopted over time by different segments of a population. • planned obsolescence – The deliberate design of products with a limited useful life so that they will need to be replaced, driving ongoing consumption. • inevitable obsolescence –"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 124, "chunk_id": "english_pack_124_1_3d284f2b", "content": "objects, often to collect data or enhance functionality. • adoption curve – A graphical representation of how new technologies or products are adopted over time by different segments of a population. • planned obsolescence – The deliberate design of products with a limited useful life so that they will need to be replaced, driving ongoing consumption. • inevitable obsolescence – The natural process by which products or technologies become outdated due to advancements and innovation, regardless of intentional design. Page 124 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "section_title": "Obsolescence – Planned vs. Inevitable; Tech Lock-In and 3D Printing; AR/VR & Tech Progress; Erosion of Economic Sustainability", "page": 125, "chunk_id": "english_pack_125_0_85a1da53", "content": "Ethics in Technology By: Ed Weber • tech lock-in – A situation where users are dependent on a specific technology, vendor, or ecosystem, making it difficult or costly to switch to alternatives. • obsolescence – The process by which something becomes outdated or no longer used, typically due to newer alternatives. • software-as-a-service (SaaS) – A software distribution model in which applications are hosted by a provider and accessed by users over the internet, typically via subscription. • sunk costs – Investments of time, money, or resources that cannot be recovered once made, often influencing future decision-making. • conversion costs – The expenses and effort required to switch from one product, service, or system to another. • virtual reality (VR) – A computer-generated simulation of a three-dimensional environment that users can interact with, typically through specialized headsets and controllers. • augmented reality (AR) – Technology that overlays digital information or images onto the real world, enhancing the user’s perception of their environment. • economic models – Frameworks or systems that describe how resources are allocated, goods and services are produced, and wealth is distributed within a society. • capitalism – An economic system characterized by private ownership of the means of production and operation for profit within competitive markets. • socialism – An economic system in which the means of production are owned and controlled collectively or by the state, with an emphasis on equal distribution of wealth. • economic participation – The involvement of individuals or groups in the production, distribution, and consumption of goods and services within an economy. • subsidies – Financial assistance provided by governments to support businesses, industries, or individuals, often to promote economic activity or stabilize prices. • retraining programs – Initiatives designed to teach new skills to workers, especially those displaced by technological or economic changes. • universal basic income (UBI) – A policy proposal in which all citizens receive a regular, unconditional sum of money from the government to cover basic living expenses. Page 125 of 125 13. Technological Disruption and the Paradox of Progress"}
{"pack_id": "english_pack", "section_title": "Cover", "page": 3, "chunk_id": "english_pack_3_0_7aff787a", "content": "InfoTech Governance, Policy, Ethics & Law"}
{"pack_id": "english_pack", "section_title": "Cover", "page": 5, "chunk_id": "english_pack_5_0_c86c7bc1", "content": "INFOTECH GOVERNANCE, POLICY, ETHICS & LAW DAVID TUFFLEY Griffith University Brisbane"}
{"pack_id": "english_pack", "section_title": "Cover", "page": 6, "chunk_id": "english_pack_6_0_cb0d19ee", "content": "InfoTech Governance, Policy, Ethics & Law Copyright © 2025 by David Tuffley is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, except where otherwise noted. This book was published by Griffith University via the Council of Australasian University Librarians Open Educational Resources Collective. The online version is available at https://oercollective.caul.edu.au/infotech-governance-policy-ethics Disclaimer Note that corporate logos and branding are specifically excluded from the Creative Commons Attribution NonCommercial (CC BY-NC) 4.0 Licence of this work, and may not be reproduced under any circumstances without the express written permission of the copyright holders. Copyright InfoTech Governance, Policy, Ethics & Law by David Tuffley is licensed under a Creative Commons Attribution NonCommercial (CC BY-NC) 4.0 Licence by Griffith University. Additionally, the following content is specifically excluded from the Creative Commons Attribution NonCommercial (CC BY-NC) 4.0 Licence of this work, and may not be reproduced under any circumstances without the express written permission of the copyright holders: • Cover (illustration – Earth Network Blockchain) by"}
{"pack_id": "english_pack", "section_title": "Cover", "page": 7, "chunk_id": "english_pack_7_0_58bde9ef", "content": "geralt is licensed and used under the Pixabay Content Licence • Module 4 – Check mark icon from Uicons by Flaticon and used under the Flaticon licence Recommended citation Tuffley, D. (2025). InfoTech Governance, Policy, Ethics & Law. Griffith University. https://oercollective.caul.edu.au/ infotech-governance-policy-ethics Recommended attribution InfoTech Governance, Policy, Ethics & Law by David Tuffley is licensed under a Creative Commons Attribution NonCommercial (CC BY-NC) 4.0 Licence by Griffith University."}
{"pack_id": "english_pack", "section_title": "Contents", "page": 9, "chunk_id": "english_pack_9_0_f2c0e148", "content": "CONTENTS Acknowledgement of Country xi About the Author xii Accessibility Information xiv Acknowledgements xvii Introduction 1 Modules MODULE 1: IT GOVERNANCE FRAMEWORKS 5 MODULE 2: CYBERSECURITY & DATA 42 PROTECTION MODULE 3: DATA BREACH PREPARATION & 80 RESPONSE MODULE 4: CYBERSEC INCIDENT 126 MANAGEMENT MATURITY MODEL MODULE 5: META-COGNITION, ETHICAL 184 DECISION MAKING, ETHICAL THEORIES"}
{"pack_id": "english_pack", "section_title": "Contents", "page": 10, "chunk_id": "english_pack_10_0_451dd957", "content": "MODULE 6: INTELLECTUAL PROPERTY & 246 COPYRIGHT MODULE 7: LEGAL GOVERNANCE, CYBER 293 FORENSICS, CYBER INTELLIGENCE MODULE 8: IMPACT OF IT ON SOCIETY 314 MODULE 9: CYBER LOSS PROCESS & CYBER 344 INSURANCE MODULE 10: E-GOV & DIGITAL 385 TRANSFORMATION Appendix A: List of Acronyms 435 Appendix B: Applying the Ethical Decision 448 Model Appendix C: Common scenarios 464 Appendix D: Software licensing 469 Appendix E: Propaganda techniques 475 Appendix F: Military technology 482 Appendix G: Select Bibliographies 488 Versioning History 508"}
{"pack_id": "english_pack", "section_title": "Acknowledgement of Country", "page": 11, "chunk_id": "english_pack_11_0_05942c6a", "content": "ACKNOWLEDGEMENT OF COUNTRY | XI ACKNOWLEDGEMENT OF COUNTRY I acknowledge the Turrbal people, the Traditional Custodians of the land on which this book was written. I pay my respects to their Elders past and present, and extend that respect to all Aboriginal and Torres Strait Islander peoples. May we walk together in gratitude, respect, and care for this Country and one another."}
{"pack_id": "english_pack", "section_title": "About the Author", "page": 12, "chunk_id": "english_pack_12_0_aa8e4028", "content": "XII | ABOUT THE AUTHOR ABOUT THE AUTHOR David Tuffley is a Senior Lecturer in Applied Ethics and CyberSecurity at Griffith University’s School of ICT in Brisbane/Gold Coast. He is also a Senior Fellow of the Higher Education Academy. He can be contacted at d.tuffley@griffith.edu.au David’s formal qualifications include a PhD (Software Engineering), M Phil (Information Systems), Graduate Certificate in Higher Education (Griffith University), Bachelor of Arts (Psychology, English Literature, Anthropology) (Queensland). David is an internationally recognized thought leader on the social impacts of technology. His diverse expertise spans software engineering, cybersecurity, ethics, futurism, and communication. David’s research and writings on how emerging technologies like AI will transform employment and society have reached over 2.75 million readers globally. He is a regular contributor to mainstream media, a sought-after speaker, and an inspirational educator guiding the next generation of technologists and leaders. David’s professional accomplishments range from publishing 100+ non-fiction books to being a sought-after"}
{"pack_id": "english_pack", "section_title": "About the Author", "page": 13, "chunk_id": "english_pack_13_0_080d58cd", "content": "ABOUT THE AUTHOR | XIII “techsplainer” on national and international radio/TV. David regularly visits Berlin and Silicon Valley to study the mechanisms of global innovation. With decades of experience across academia, research, industry, and government, David is well positioned to engage with organizations worldwide on projects at the intersection of technology, ethics, policy, and society."}
{"pack_id": "english_pack", "section_title": "Accessibility Information", "page": 14, "chunk_id": "english_pack_14_0_21b4d373", "content": "XIV | ACCESSIBILITY INFORMATION ACCESSIBILITY INFORMATION We believe that education must be available to everyone which means supporting the creation of free, open, and accessible educational resources. We are actively committed to increasing the accessibility and usability of the textbooks we produce. Accessibility features of the web version of this resource The web version of this resource has been designed with accessibility in mind by incorporating the following features: • It has been optimized for people who use screen-reader technology. ◦ all content can be navigated using a keyboard ◦ links, headings, and tables are formatted to work with screen readers ◦ images have alt tags • Information is not conveyed by colour alone."}
{"pack_id": "english_pack", "section_title": "Accessibility Information", "page": 15, "chunk_id": "english_pack_15_0_ac5c0413", "content": "ACCESSIBILITY INFORMATION | XV Other file formats available In addition to the web version, this book is available in a number of file formats including PDF, EPUB (for eReaders), and various editable files. Choose from the selection of available file types from the ‘Download this book’ drop-down menu. This option appears below the book cover image on the eBook’s landing page. Third-Party Content In some cases, our open text includes third-party content. In these cases, it is often not possible to ensure accessibility of this content. Known accessibility issues and areas for improvement While we strive to ensure that this resource is as accessible and usable as possible, we might not always get it right. Any issues we identify will be listed below. There are currently no known issues."}
{"pack_id": "english_pack", "section_title": "Accessibility Information", "page": 16, "chunk_id": "english_pack_16_0_cd77c073", "content": "XVI | ACCESSIBILITY INFORMATION List of Known Accessibility Issues Location of Need for Work Timeline issue improvement around Accessibility Improvements While we strive to ensure that this resource is as accessible and usable as possible, we might not always get it right. We are always looking for ways to make our resources more accessible. If you have problems accessing this resource, please contact d.tuffley@griffith.edu.au to let us know so we can fix the issue. Copyright Note: This accessibility disclaimer is adapted from BCampus’s Accessibility Toolkit, licensed under a Creative Commons Attribution 4.0 International license and University of Southern Queensland’s Enhancing Inclusion, Diversity, Equity and Accessibility (IDEA) in Open Educational Resources (OER) licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."}
{"pack_id": "english_pack", "section_title": "Acknowledgements", "page": 17, "chunk_id": "english_pack_17_0_6069919e", "content": "ACKNOWLEDGEMENTS | XVII ACKNOWLEDGEMENTS David acknowledges the support of Griffith University in the preparation of this text."}
{"pack_id": "english_pack", "section_title": "Introduction", "page": 19, "chunk_id": "english_pack_19_0_4e00d767", "content": "INTRODUCTION | 1 INTRODUCTION Welcome to this textbook on IT Governance, Policy, Ethics and Law. It is the culmination of 30 years of practical experience and academic research to create a practical guide for IT professionals. The text is organised into nine thematically linked chapters: Chapter 1 explores the different IT governance frameworks that are used to align IT strategy with business goals, manage IT risks and resources, and ensure IT compliance and accountability. We compare the main models, such as COBIT, ITIL, ISO 27000, and NIST. Chapter 2 focuses on cybersecurity and data protection, two of the most critical challenges for IT today. We learn about the threats and vulnerabilities that affect IT systems and data, and the best practices and standards to prevent and respond to them. We will also discuss the legal and ethical issues related to data privacy and security, such as GDPR, HIPAA, and CCPA. Chapter 3 examines the benefits and challenges of cloud computing and outsourcing, two of the most popular ways to deliver IT services in the modern world. We will look to the different types of cloud services and models, such as SaaS, PaaS, IaaS, public, private, and hybrid clouds. We also analyse"}
{"pack_id": "english_pack", "section_title": "Introduction", "page": 20, "chunk_id": "english_pack_20_0_ce3f24f0", "content": "2 | INTRODUCTION the risks and opportunities of outsourcing IT functions to third parties, such as vendors, contractors, or consultants. Chapter 4 investigates the topic of digital ethics and responsible AI, two of the most rapidly emerging and controversial areas of IT. We explore the ethical principles and dilemmas that arise from the use of digital technologies and artificial intelligence in various domains, such as health care, education, business, or entertainment. We will also learn about the frameworks and guidelines that aim to ensure that AI is fair, transparent, accountable, and human-centric. Chapter 5 covers the topic of intellectual property and copyright, two of the most important legal aspects of IT. We learn about the different types of intellectual property rights that apply to IT products and services, such as patents, trademarks, trade secrets, and copyrights. We will also understand the implications and challenges of protecting and enforcing these rights in the digital age. Chapter 6 discusses e-gov and digital transformation, two of the most significant trends in IT today. We see how governments use IT to improve their efficiency, effectiveness, transparency, and citizen engagement. We will also examine how organizations use IT to transform their processes, products, services, and culture. Chapter 7 looks at the impact of IT on society, both positive and negative. We evaluate how IT affects various aspects of social life, such as communication, education, work, leisure, culture, democracy, inequality, environment, health,"}
{"pack_id": "english_pack", "section_title": "Introduction", "page": 21, "chunk_id": "english_pack_21_0_07d2141c", "content": "INTRODUCTION | 3 and security. We will also reflect on our own roles and responsibilities as IT users and creators. Chapter 8 addresses employee IT usage and policies, two of the most relevant issues for IT managers and workers. We learn about the rights and obligations of employees regarding their use of IT resources and devices in the workplace. We will also review the best practices and policies to regulate employee IT behaviour, such as acceptable use policies, social media policies, and BYOD policies. Chapter 9 examines IoT security and privacy, two urgent problems for IT governance in the era of smart devices and connected things. We will understand the architecture and applications of IoT systems, such as smart homes, smart cities, smart health, and smart agriculture. We will also identify the risks and challenges of securing and protecting the data and devices that make up IoT networks. I trust you will find this this book useful for your personal and professional development. David Tuffley"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 23, "chunk_id": "english_pack_23_0_24762ec9", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 5 MODULE 1: IT GOVERNANCE FRAMEWORKS IT governance frameworks are the rules and guidelines that help organizations manage their IT resources and processes effectively. They help align IT goals with business objectives, ensure compliance with laws and regulations, and protect data from unauthorized access or loss. Some examples of IT governance frameworks are COBIT, ITIL, ISO 27001, and NIST. IT compliance and regulatory standards are the requirements that organizations must follow to meet the expectations of external stakeholders, such as customers, auditors, or government agencies. They help ensure quality, security, privacy, and accountability of IT services and products. Some examples of IT compliance and regulatory standards are GDPR, HIPAA (Health Insurance Portability and Accountability), PCI DSS, and SOX. Data retention and deletion are the policies and practices that decide how long and where organizations store their data, and when and how they dispose of it. They help balance the needs of data availability, performance, cost, and risk. Some"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 24, "chunk_id": "english_pack_24_0_d59e0e85", "content": "6 | MODULE 1: IT GOVERNANCE FRAMEWORKS examples of data retention and deletion factors are legal obligations, business value, storage capacity, and backup frequency. In this module workshop, you will learn how to apply IT governance frameworks to your organization, how to comply with IT standards and regulations, and how to design and implement data retention and deletion policies. You will also learn how to assess the benefits and challenges of IT governance, compliance, and data management in different scenarios. 1.1. IT Governance Frameworks There are several IT governance frameworks available, each with its own strengths, weaknesses, and applicability. Some of the most common frameworks are: • COBIT. This is a comprehensive framework that covers 37 IT processes, each with detailed objectives, practices, inputs, outputs, activities, and metrics. COBIT helps organizations achieve effective IT governance and management by linking IT goals to business goals, ensuring IT resources are optimized, and managing IT risks and performance. • AS8015-2005. This is a simple and concise framework developed in Australia that defines six principles for good IT governance: establish clearly understood"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 25, "chunk_id": "english_pack_25_0_f7c06769", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 7 responsibilities for IT; plan IT to best support the organization; acquire IT validly; ensure that IT performs well; ensure that IT conforms with formal rules; and respect human factors in IT. • ISO/IEC 38500. This is an international standard that provides high-level guidance on the principles, roles, and responsibilities for effective IT governance. ISO/IEC 38500 helps organizations evaluate, direct, and monitor their use of IT to achieve their business objectives and fulfill their legal and ethical obligations. • ITIL. This is a widely adopted framework that focuses on the delivery and management of quality IT services that meet the needs and expectations of customers and stakeholders. ITIL covers the entire service lifecycle from strategy to design, transition, operation, and improvement. ITIL helps organizations improve their service efficiency, effectiveness, reliability, and value. Choosing the right IT governance framework depends on various factors such as the size, complexity, culture, industry, and maturity of the organization. It is also possible to adopt a hybrid or customized approach that combines elements from different frameworks to suit the specific needs and context of the organization. IT governance frameworks are not static or one-size-fits-all solutions. They require regular review and adaptation to keep up with the changing business environment and technology"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 26, "chunk_id": "english_pack_26_0_24d83269", "content": "8 | MODULE 1: IT GOVERNANCE FRAMEWORKS landscape. They also require strong leadership commitment, stakeholder involvement, clear communication, and continuous improvement to ensure successful implementation and outcomes. COBIT Orchestrating Control & Assurance COBIT is a comprehensive framework for the governance and management of enterprise information and technology (I&T (Information Technology)). It helps organizations align their I&T goals with their business objectives, optimize their I&T resources and processes, and ensure effective control and assurance over their I&T activities. COBIT consists of seven enablers: principles, policies and frameworks; processes; organizational structures; culture, ethics and behaviour; information; services, infrastructure and applications; and people, skills and competencies. COBIT & IT Governance Frameworks IT governance frameworks are essential for ensuring that I&T supports the achievement of enterprise goals, delivers value to stakeholders, manages risks and complies with external requirements. COBIT provides a holistic and integrated approach to IT"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 27, "chunk_id": "english_pack_27_0_373813b7", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 9 governance that covers all aspects of I&T from strategy to operations. COBIT also provides a common language and terminology for I&T governance that can be understood by all stakeholders, including business executives, IT managers, auditors and regulators. How COBIT controls and assures One of the key benefits of COBIT is that it enables organizations to establish and maintain a system of internal control and assurance over their I&T activities. COBIT defines control as “the means of managing risk to ensure that enterprise objectives will be achieved” and assurance as “the provision of objective evidence that the design and operation of the system of internal control meets the agreed-upon requirements”. COBIT provides guidance on how to design, implement, monitor, evaluate and improve the system of internal control and assurance using the following processes: MEA01: Managed Performance and Conformance Monitoring. This process collects, validates, and evaluates enterprise and alignment goals and metrics, monitors that processes and practices are performing against agreed performance and conformance goals and metrics, provides systematic and timely reporting, and provides transparency of performance and conformance and drives achievement of goals."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 28, "chunk_id": "english_pack_28_0_af94c32a", "content": "10 | MODULE 1: IT GOVERNANCE FRAMEWORKS MEA02: Managed System of Internal Control. This process continuously monitors and evaluates the control environment, including self-assessments and self- awareness, enables management to identify control deficiencies and inefficiencies and to initiate improvement actions, plans, organizes and maintains standards for internal control assessment and process control effectiveness, obtains transparency for key stakeholders on the adequacy of the system of internal controls. MEA03: Managed Compliance with External Requirements This process evaluates that I&T processes and I&T- supported business processes are compliant with laws, regulations and contractual requirements, obtains assurance that the requirements have been identified and complied with; integrates IT compliance with overall enterprise compliance, ensures that the enterprise is compliant with all applicable external requirements. MEA04: Managed Assurance This process plans, scopes and executes assurance initiatives to comply with internal requirements, laws, regulations and strategic objectives, enables management to deliver adequate and sustainable assurance in the enterprise by performing independent assurance reviews and activities, enables the organization to design and develop efficient and effective assurance initiatives."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 29, "chunk_id": "english_pack_29_0_9141d8ea", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 11 ITIL Elevating Service Management ITIL is a library of best practices used in IT Service Management (ITSM). ITSM is the process of designing, delivering, managing and improving IT services that meet the needs and expectations of customers and stakeholders. ITSM covers a wide range of activities, such as incident management, change management, problem management, service level management, service design, service transition, service operation and continual service improvement. ITIL provides a comprehensive and consistent framework for ITSM that is aligned with business goals and customer value. ITIL helps organizations to: • Improve customer satisfaction by delivering reliable and high-quality IT services. • Enhance IT services delivered using best practice procedures. • Reduce costs and risks by optimizing the use of resources and avoiding service disruptions. • Increase agility and innovation by enabling faster and more effective changes to IT services. • Support digital transformation by integrating ITSM with other frameworks such as DevOps, Agile and SRE. Elevating service management with ITIL To elevate service management with ITIL, you need to adopt a"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 30, "chunk_id": "english_pack_30_0_58bded0b", "content": "12 | MODULE 1: IT GOVERNANCE FRAMEWORKS holistic and value-driven approach that encompasses the entire service lifecycle. You need to understand the needs and expectations of your customers and stakeholders, and design, deliver, manage, and improve IT services that create value for them. You need to establish clear and measurable service levels, and ensure that they are properly assessed, monitored and managed against these targets. Collaborate with other teams and departments across the organization, and leverage the capabilities of people, processes, information and technology. You need to foster a culture of continual improvement that seeks feedback, learns from mistakes, identifies opportunities and implements changes. Here are some practical steps you can take to elevate service management with ITIL: Assess the current state of your ITSM practices and identify gaps and areas for improvement. Define a vision and strategy for your ITSM that aligns with your organizational goals and customer value propositions. Implement the ITIL Service Value System (SVS) that consists of five components: guiding principles, governance, service value chain, practices, and continual improvement. Use the SVS to plan, engage, design, transition, obtain/ build, deliver/support and improve your IT services. Apply the seven guiding principles of ITIL to guide your decisions and actions: focus on value, start where you are, progress iteratively with feedback, collaborate and promote"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 31, "chunk_id": "english_pack_31_0_3bf7512e", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 13 visibility, think and work holistically, keep it simple and practical, optimize and automate. Establish a Service Management Office (SMO) that provides a central point for consistency and governance in organizational best practice. Monitor and measure your service performance using relevant metrics and indicators. Report and communicate your service achievements and challenges to your customers and stakeholders. Review and evaluate your service outcomes and feedback using various methods such as surveys, audits, reviews, benchmarks etc. Identify and prioritize improvement initiatives using techniques such as SWOT analysis, gap analysis, root cause analysis etc. Implement improvement actions using methods such as PDCA cycle (plan-do-check-act), CSI approach (what is the vision? where are we now? where do we want to be? how do we get there? did we get there? how do we keep the momentum going?) etc. ISO/IEC 38500 The Governing Standard Advice on ISO/IEC 38500 The Governing Standard. ISO/ IEC 38500 is an international standard for the corporate governance of information technology (IT), and provides"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 32, "chunk_id": "english_pack_32_0_ae26f457", "content": "14 | MODULE 1: IT GOVERNANCE FRAMEWORKS guidance to those persons advising, informing or assisting directors on the effective and acceptable use of IT within the organization. It is based on six principles and a model for good governance of IT. Principles The six principles of ISO/IEC 38500 are: Responsibility. Assigning roles and responsibilities for the use of IT. Strategy. Aligning the use of IT with the organizational objectives. Acquisition. Procuring IT solutions and services to meet the organizational needs. Performance. Measuring and evaluating the contribution of IT to the organization Conformance. Ensuring compliance with laws, regulations, and policies. Human Behaviour. Considering the human aspects of IT use. Model The model of ISO/IEC 38500 has four main elements: Governing Body. The individual or group of individuals responsible and accountable for the performance and conformance of the organization"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 33, "chunk_id": "english_pack_33_0_d73dfa07", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 15 Evaluation. The process of assessing the current and future use of IT Direction. The process of deciding on the objectives and policies for the use of IT Monitoring. The process of verifying that the use of IT meets the objectives and policies. The governing body should evaluate, direct, and monitor the use of IT in a continuous cycle, considering the six principles and the stakeholders’ interests. Benefits The benefits of applying ISO/IEC 38500 include: • Improving the alignment of IT with the organizational strategy. • Enhancing the delivery of value from IT investments. • Reducing risks related to IT projects and operations. • Increasing transparency and accountability for IT decisions and outcomes. • Fostering a culture of trust and collaboration among IT stakeholders. • Supporting continuous improvement and innovation in IT."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 34, "chunk_id": "english_pack_34_0_ea561eb5", "content": "16 | MODULE 1: IT GOVERNANCE FRAMEWORKS Unifying Business & Technology IT governance is a process that enables the IT staff to better manage risk and operate at its most efficient to the benefit of the organization. It is part of the corporate governance, which is a collection of processes that are designed to keep the entire corporation effective and efficient. IT governance aims to: • Ensure business value is generated by information and technology. • Oversee the performance of IT managers. • Assess risks associated with the IT department and mitigate them as needed. The significance of IT Governance IT governance is important because it helps the organization to align its IT priorities, decisions and investments with its strategic goals and stakeholder requirements. It also helps the organization to comply with legal, contractual and policy obligations that impact IT. Furthermore, it supports the continuous improvement and optimization of IT services and resources. Implementing IT Governance There are different frameworks and standards that can guide"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 35, "chunk_id": "english_pack_35_0_4fdb4dbb", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 17 the implementation of IT governance in an organization. Some of the most common ones are: • COBIT. This is a comprehensive framework that covers 37 IT processes, with each process having a set of objectives, inputs, outputs, activities, roles, and responsibilities. It also provides maturity models, performance indicators and best practices for each process. • AS8015-2005. This is a technical standard developed in Australia that defines six principles for good IT governance: establish clearly understood responsibilities for IT; plan IT to best support the organization; acquire IT validly; ensure that IT performs well, whenever required; ensure IT conforms with formal rules; ensure respect for human factors. • ISO/IEC 38500. This is an international standard that provides a high-level framework for effective governance of IT. It defines six principles for good IT governance: responsibility; strategy; acquisition; performance; conformance; human behaviour. These frameworks and standards can be adapted to suit the specific needs and context of each organization. However, some common steps for implementing IT governance are: • Define the scope and objectives of IT governance."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 36, "chunk_id": "english_pack_36_0_36d6bd32", "content": "18 | MODULE 1: IT GOVERNANCE FRAMEWORKS • Establish the roles and responsibilities of IT governance stakeholders. • Identify the key IT processes and activities that need to be governed. • Define the policies, procedures, guidelines, and standards that govern IT. • Establish the mechanisms and tools for monitoring, reporting, and evaluating IT performance and compliance. • Implement continuous improvement initiatives to enhance IT value and maturity. Guidelines for Decision-Making Guidelines for decision-making in IT governance frameworks, based on research and best practice: • What is IT governance and why is it important? • What are the key principles of IT governance? • What are the common IT governance frameworks and how do they support decision-making? • How to define the roles and responsibilities of decision- makers in IT governance? • How to ensure transparency, accountability, and compliance in IT governance decisions? What is IT governance and why is it"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 37, "chunk_id": "english_pack_37_0_c14ceecc", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 19 important? IT governance is the process of defining the structures and processes that enable the organization to effectively oversee, direct and control its IT resources and processes. It involves evaluating stakeholder requirements, setting direction, prioritizing investments, monitoring performance, and ensuring compliance with legal, contractual and policy requirements that impact IT. IT governance is important because it helps the organization to: • Achieve its strategic goals and objectives by aligning IT with the business needs and expectations. • Optimize the value of IT by delivering benefits to the organization and its stakeholders. • Manage the risks associated with IT by identifying, assessing, and mitigating them. • Enhance the performance of IT by improving the quality, efficiency, and effectiveness of IT services. • Foster a culture of continuous improvement by learning from feedback and best practices. Ensuring compliance in IT governance decisions Another key aspect of effective decision-making in IT governance is to ensure that the decisions are transparent,"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 38, "chunk_id": "english_pack_38_0_644f4cf4", "content": "20 | MODULE 1: IT GOVERNANCE FRAMEWORKS accountable and compliant with the relevant legal, contractual and policy requirements that impact IT. Some of the ways to ensure transparency, accountability and compliance in IT governance decisions are: • Documenting and communicating the IT governance framework, including the principles, rules, processes, roles, responsibilities and authorities that guide decision- making. • Establishing and maintaining a repository of IT governance decisions, including the rationale, criteria, evidence, alternatives and impacts of each decision. • Implementing and monitoring a set of KPIs and metrics that measure the performance and outcomes of IT governance decisions. • Conducting regular audits and reviews of IT governance decisions to verify their validity, effectiveness and efficiency. • Reporting and disclosing IT governance decisions to relevant stakeholders, such as senior management, board of directors, regulators, customers and suppliers • Establishing and enforcing a mechanism for escalating, resolving and learning from issues, disputes and complaints related to IT governance decisions. By following these steps, organizations can enhance the trust,"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 39, "chunk_id": "english_pack_39_0_99c7aa06", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 21 confidence and satisfaction of their stakeholders regarding their IT governance decisions. Risk Management & Mitigation Risk management and mitigation is the process of identifying, analysing, evaluating, and treating the potential threats and vulnerabilities that could affect the performance, security, reliability, and compliance of IT systems and processes. It also involves monitoring and reviewing the risk situation and taking corrective actions as needed. Risk management and mitigation is important because it helps organizations to: • Protect their assets, data, reputation, and stakeholders from harm or loss. • Ensure the continuity and availability of their IT services and operations. • Achieve their strategic objectives and deliver value to their customers. • Comply with legal, regulatory, contractual, and ethical obligations. • Enhance their decision-making and innovation capabilities. • Reduce costs and optimize resources. Implementing Risk Management & Mitigation?"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 40, "chunk_id": "english_pack_40_0_47e73429", "content": "22 | MODULE 1: IT GOVERNANCE FRAMEWORKS To implement a successful risk management and mitigation strategy, organizations should follow these steps: • Establish a risk management framework that defines the scope, objectives, roles, responsibilities, policies, procedures, tools, and metrics for managing and mitigating risks. • Conduct a risk assessment that identifies and prioritizes the sources and impacts of risks for each IT system and process. • Develop a risk treatment plan that specifies the actions, resources, timelines, and owners for reducing or eliminating the risks or their consequences. • Implement the risk treatment plan by executing the actions and allocating the resources as planned. • Monitor and review the risk situation by measuring the performance, effectiveness, and efficiency of the risk treatment actions and reporting the results and progress. • Update the risk management framework, assessment, treatment plan, and actions as needed to reflect changes in the internal or external environment or feedback from stakeholders. Ethical & Legal Compliance An IT governance framework is a set of policies, processes, roles and responsibilities that guide the creation, use and"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 41, "chunk_id": "english_pack_41_0_4593f3e9", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 23 management of information technology (IT) assets and services in an organisation. It helps to ensure that IT supports the organisation’s strategy, objectives and performance, while also managing the risks, costs and benefits of IT. An IT governance framework should be aligned with the organisation’s overall governance framework, which provides a holistic overview of how the organisation creates and manages its enterprise-wide information assets (records, information and data). Ethical and legal compliance is important for several reasons: • It helps to build trust and reputation among stakeholders, which can enhance customer loyalty, employee engagement, partner collaboration and social responsibility. • It helps to avoid or minimise legal liabilities, fines, sanctions or lawsuits that can result from violating laws, regulations or standards that apply to the organisation’s IT activities. • It helps to prevent or mitigate ethical issues or dilemmas that can arise from the use or misuse of IT, such as privacy breaches, data misuse, cyberattacks, bias or discrimination. • It helps to foster a culture of ethics and integrity in the organisation, which can encourage innovation, creativity and excellence in IT."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 42, "chunk_id": "english_pack_42_0_5da28491", "content": "24 | MODULE 1: IT GOVERNANCE FRAMEWORKS Achieving ethical & legal compliance Some general steps that can be followed are: Identify and understand the legal, regulatory and ethical requirements that apply to the organisation’s IT activities. These may include laws and regulations related to data protection, cybersecurity, intellectual property, consumer rights, human rights or environmental protection. They may also include ethical principles or codes of conduct that reflect the organisation’s values or industry standards. Assess and document the current state of compliance in the organisation’s IT governance framework. This may involve conducting audits, reviews or surveys to evaluate how well the organisation’s IT policies, processes and practices comply with the relevant requirements. It may also involve identifying any gaps, weaknesses or risks that need to be addressed. Develop and implement a plan to improve compliance in the organisation’s IT governance framework. This may involve updating or creating new IT policies, processes or practices that align with the relevant requirements. It may also involve providing training, guidance or support to staff or stakeholders on how to comply with the requirements. It may also involve monitoring, measuring or reporting on the progress or outcomes of compliance efforts. Review and update the compliance plan regularly. This may involve revisiting the legal, regulatory or ethical"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 43, "chunk_id": "english_pack_43_0_0ae44426", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 25 requirements periodically to ensure they are up-to-date and relevant. It may also involve evaluating the effectiveness or impact of compliance efforts on the organisation’s performance or stakeholder satisfaction. It may also involve seeking feedback or input from staff or stakeholders on how to improve compliance. 1.2. IT Compliance & Regulatory Standards Today’s business environment is becoming more complex, and organizations must negotiate the web of regulations and standards. IT compliance and regulatory standards ensure that organizations adhere to a set of guidelines, laws, and best practices. Organizations must define and implement policies that not only facilitate compliance but also engender ethical conduct, responsible innovation, and safeguards against risks. The Regulatory Framework The regulatory framework for IT and cybersecurity compliance and regulatory standards is the set of laws, rules, guidelines and best practices that govern how businesses use, store, process and transmit information technology (IT)."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 44, "chunk_id": "english_pack_44_0_9481acbf", "content": "26 | MODULE 1: IT GOVERNANCE FRAMEWORKS The framework varies depending on the type and nature of the data involved, such as personal data, health data, financial data or government data. The framework also depends on the geographic location of the business and its customers, as different regions and countries have different regulations. IT and Cybersecurity compliance standards include: GDPR: The General Data Protection Regulation (GDPR) is a set of IT regulations that the European Union (EU) enforces. It protects the security and privacy of data belonging to EU citizens and residents. It applies to any business that operates with such data, even if it is not located in the EU. Under the GDPR it is legal to process someone’s data provided: • The data subject has given consent to the processing of his or her personal data, • Contractual obligations with a data subject have been fulfilled, • The data subject has complied with a data controller’s legal obligations, • The vital interests of a data subject are protected, • The processing is done in the public interest or official authority, • The processing is done in the legitimate interests of a data controller unless precedence is taken by the interests of the data subject."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 45, "chunk_id": "english_pack_45_0_8676d881", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 27 For informed consent to be used as the lawful basis for processing, that consent must have been explicitly given for the data concerned. That consent must be a “specific, freely given, plainly worded, and unambiguous affirmation” given by the data subject. It is not acceptable to have consent given by default on a web-form, nor to bundle multiple types of processing into the one affirmation. Under GDPR, data subjects must have the option to withdraw consent at any time. And it must not be harder to do so than it was to opt in. In the case of children less than 16 years, consent must be given by the child’s verified parent or custodian. Data controllers must meet the principles of data protection by design and by default, which means data protection measures are designed into the business processes. This includes the pseudonymising of personal data as soon as possible. When data is collected, data subjects must be unambiguously informed about the extent of the data collection, what is the legal basis for the proposed processing of personal data, how long the data will be retained, whether that data will be communicated to a third-party inside or outside the EU and must disclosure of any automated decision-making that is made on a solely algorithmic basis. Anti-Money Laundering (AML) & Know Your Customer (KYC). The Office of the Australian Information Commissioner (OAIC) prescribe the legal framework"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 46, "chunk_id": "english_pack_46_0_eb152ba9", "content": "28 | MODULE 1: IT GOVERNANCE FRAMEWORKS applicable to the prevention of money laundering and associated Know Your Customer. They refer to the Anti-Money Laundering and Counter- Terrorism Financing Act 2006, and the Anti-Money Laundering and Counter-Terrorism Financing Rules which aim to prevent the practice and the financing of terrorism. They impose certain obligations on “reporting entities” which include the financial sector, gambling sector, remittance (money transfer) services, bullion dealers and other professionals or businesses that provide services with the potential for money laundering. These obligations include collecting and verifying certain ‘know your customer’ (KYC) information about a customer’s identity before providing those services. Entities that are required to comply with the AML/CTF Act are likewise required to comply with the Privacy Act 1988 to safeguard the personal information collected for the purposes of compliance with their AML/CTF Act obligations. The Australian Transaction Reports and Analysis Centre (AUSTRAC) is the Australian Government agency responsible for ensuring compliance with the AML/CTF Act. Privacy obligations of small business ‘reporting entities’. Small businesses (annual turnover of $3 million or less) are generally not covered by the Privacy Act. However, small businesses that are reporting entities for the purposes of AML/CTF Act are required to comply with the Privacy Act when handling personal information collected for the"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 47, "chunk_id": "english_pack_47_0_cf85b19f", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 29 purposes of meeting their obligations under the AML/CTF Act. This includes those small businesses exempt from obligations under the Privacy Act. If a small business is brought into the Privacy Act because they are reporting entities under the AML/CTF Act and then are later exempted from reporting obligations due to rules issued by AUSTRAC under the AML/CTF Act, the small business is still a reporting entity within the meaning of the Privacy Act. Therefore, in relation to activities it carried on for the purpose of complying with the AML/CTF Act or AML/ CTF Rules, the small business continues to have all the Privacy Act obligations it had before the exemption was granted. PCI DSS: The Payment Card Industry Data Security Standard (PCI DSS) governs the security of financial card data, such as credit card or debit card information. It applies to any business that stores, processes or transmits such data. HIPAA: The Health Insurance Portability and Accountability Act (HIPAA) is an IT compliance standard for the health care industry. It regulates how medical organizations protect the sensitive information of their patients. It applies to any business that deals with health data. NIST SP 800-171: The National Institute of Standards and Technology (NIST) Special Publication 800-171 is a set of IT security requirements for businesses that work with federal or state agencies. It ensures that government data is protected from unauthorized access or disclosure. These are the most used IT compliance standards which"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 48, "chunk_id": "english_pack_48_0_99139ebd", "content": "30 | MODULE 1: IT GOVERNANCE FRAMEWORKS includes cybersecurity. There are more that may apply to your business depending on your industry, location and data. Implementing standards To follow the regulatory framework for IT compliance and regulatory standards, you need to: Identify the IT compliance standards that apply to your business. You can do this by researching the laws and regulations of your industry and location, consulting with legal experts or using online resources. Assess your current level of compliance. You can do this by conducting an IT security audit, using tools or services that measure your compliance status or hiring external auditors. Implement security measures to meet the compliance requirements. You can do this by adopting security policies and procedures, using secure software and hardware, training your staff on security best practices or outsourcing security tasks to professionals. Monitor and maintain your compliance status. You can do this by regularly reviewing your security policies and procedures, updating your software and hardware, testing your security systems or reporting your compliance activities. Following standards can help protect business from security threats, legal penalties and reputational damage. It can also help you improve your customer trust and satisfaction."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 49, "chunk_id": "english_pack_49_0_82b94ec8", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 31 IT Compliance Policies IT compliance policies matter for several reasons: They help the organization meet its legal and contractual obligations, such as the Sarbanes-Oxley Act (SOX) for financial reporting, the Gramm-Leach-Bliley Act (GLBA) for financial data protection, or the Payment Card Industry Data Security Standard (PCI DSS) for credit card transactions . They enhance the organization’s reputation and trustworthiness among its customers, partners, and regulators, by demonstrating its commitment to data security and privacy. They reduce the likelihood and impact of cyberattacks, data breaches, and other incidents that could compromise the organization’s data and systems, by implementing preventive and corrective measures. They improve the efficiency and effectiveness of the organization’s IT operations, by streamlining processes, reducing errors, and optimizing resources. Creating IT Compliance Policies To create effective IT compliance policies, an organization should follow these steps: • Identify the applicable laws, regulations, and standards that affect its IT activities, such as SOX,"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 50, "chunk_id": "english_pack_50_0_21dc0c38", "content": "32 | MODULE 1: IT GOVERNANCE FRAMEWORKS GLBA, PCI DSS, HIPAA, GDPR, ISO 27001, NIST 800-53, etc. • Assess the current state of its IT compliance posture, by conducting audits, gap analyses, risk assessments, and maturity assessments. • Define the desired state of its IT compliance posture, by setting goals, objectives, and metrics for each compliance area. • Develop the IT compliance policies that outline the roles, responsibilities, procedures, controls, and tools for achieving compliance in each area. • Implement the IT compliance policies across the organization, by communicating them to all stakeholders, providing training and awareness programs, enforcing them through monitoring and reporting mechanisms, and reviewing them periodically for improvement. Risk Management & Mitigation IT compliance and regulatory standards govern how organizations use, protect, and share information and technology. These standards may come from different sources, such as laws, regulations, industry codes, contracts, or ethical principles. IT compliance and regulatory standards include: The General Data Protection Regulation (GDPR),"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 51, "chunk_id": "english_pack_51_0_94fd5695", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 33 which is a European Union law that protects the privacy and rights of individuals in relation to their personal data. The Payment Card Industry Data Security Standard (PCI DSS), which is a set of security requirements for organizations that process, store, or transmit credit card information. The ISO/IEC 27000 series, which is a family of international standards for information security management systems. Managing Risk in IT Compliance and Regulatory Standards? Managing and mitigating risks in IT compliance and regulatory standards involves a systematic process of identifying, analysing, evaluating, treating, monitoring, and reviewing the risks. Some of the steps involved in this process are: Establishing a governance framework for IT compliance and regulatory standards. This involves defining the roles, responsibilities, policies, procedures, and controls for ensuring that the organization meets its obligations and objectives in relation to information and technology. Conducting a risk assessment for IT compliance and regulatory standards. This involves identifying the sources and causes of potential risks, estimating their likelihood and impact, and prioritizing them based on their severity."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 52, "chunk_id": "english_pack_52_0_9a0f16b3", "content": "34 | MODULE 1: IT GOVERNANCE FRAMEWORKS Implementing risk treatment strategies for IT compliance and regulatory standards. This involves selecting and applying appropriate measures to avoid, reduce, transfer, or accept the risks. Some examples of risk treatment strategies are: Implementing technical safeguards such as encryption, firewalls, antivirus software, or backup systems to protect information and technology from unauthorized access or damage. Implementing administrative safeguards such as training, awareness, policies, procedures, or audits to ensure that staff follow the rules and requirements for information and technology. Implementing legal safeguards such as contracts, agreements, or insurance to transfer or share the responsibility or liability for information and technology with other parties. Monitoring and reviewing the effectiveness of risk management activities for IT compliance and regulatory standards. This involves measuring and reporting on the performance and outcomes of the risk management process, identifying any gaps or weaknesses, and adjusting or improvements as needed. Ethical Considerations in Emerging Technologies Emerging technologies such as artificial intelligence, cloud"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 53, "chunk_id": "english_pack_53_0_199b40db", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 35 computing, big data, and cybersecurity have enormous potential to transform various domains of human activity. Ethical Dilemmas and Principles in Data Retention and Deletion Data retention and deletion involve ethical dilemmas that require careful balancing of competing values and interests. Some of the common ethical dilemmas are: How long should data be retained? Retaining data for too long can increase the risk of data breaches, misuse, or abuse, while deleting data too soon can limit the potential benefits of data analysis or reuse. How should data be deleted? Deleting data securely and completely can prevent unauthorized access or recovery, while retaining some traces of data can facilitate auditing or verification. Who should decide on data retention and deletion? Data controllers and processors may have different incentives or preferences for data retention and deletion than data subjects or stakeholders, who may have different levels of awareness or consent. What are the trade-offs between data retention and deletion? Data retention and deletion may involve trade-offs between efficiency and effectiveness, innovation and protection, individual and collective interests, or short-term and long-term goals."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 54, "chunk_id": "english_pack_54_0_06769f93", "content": "36 | MODULE 1: IT GOVERNANCE FRAMEWORKS Addressing ethical dilemmas To address these ethical dilemmas, some ethical principles can guide the decision-making process. Some of the widely accepted ethical principles are: Respect for human dignity. Data retention and deletion should respect the inherent worth and dignity of every human being, regardless of their characteristics or circumstances. Fairness and justice. Data retention and deletion should ensure equal treatment and opportunity for all data subjects and stakeholders, without discrimination or bias. Beneficence and non-maleficence. Data retention and deletion should maximize the benefits and minimize the harms for data subjects, stakeholders, and society at large. Autonomy and consent. Data retention and deletion should respect the choices and preferences of data subjects, who should be informed and empowered to exercise their rights over their data. Transparency and accountability. Data retention and deletion should be clear, consistent, and explainable to data subjects, stakeholders, and regulators, who should be able to monitor and evaluate their compliance and outcomes. 1.3. Zero trust Principles Zero Trust is a modern approach to cybersecurity. It assumes that no one and nothing should be automatically trusted, even"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 55, "chunk_id": "english_pack_55_0_aecbbe83", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 37 if they are inside the organization’s network. This is different from old security models that trusted everything inside the company’s walls. Key Principles of Zero Trust • Verify Always: Check the identity of every user and device, every time they try to access resources. • Least Privilege Access: Give users only the access they need to do their job, nothing more. • Assume Breach: Act as if your network is already compromised. Always monitor for threats. Implementation Strategies 1. Identity and Access Management (IAM): • Use strong authentication methods like multi-factor authentication (MFA). • Implement single sign-on (SSO) for better user experience and security. 2. Network Segmentation: • Divide the network into smaller parts. • Control access between these parts."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 56, "chunk_id": "english_pack_56_0_3f59428a", "content": "38 | MODULE 1: IT GOVERNANCE FRAMEWORKS 3. Continuous Monitoring and Validation: • Always watch for unusual activities. • Regularly check if users still need their current access levels. 4. Data Classification: • Categorize data based on its importance. • Apply different protection levels to each category. 5. Device Access Control: • Only allow known and approved devices to connect. • Ensure all devices meet security standards before granting access. Challenges of Implementing Zero Trust • Complexity: Zero Trust systems can be complicated to set up and manage. • User Experience: Extra security steps might slow down work or frustrate users. • Legacy Systems: Old technology might not work well with Zero Trust principles. • Cost: Implementing new security measures can be"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 57, "chunk_id": "english_pack_57_0_a74f17d6", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 39 expensive. • Cultural Shift: Employees need to change how they think about security. Benefits of Zero Trust • Improved Security: Better protection against both external and internal threats. • Better Visibility: Clearer view of who is accessing what in your network. • Simplified Management: Consistent security rules across the entire organization. • Compliance Support: Helps meet many regulatory requirements. • Flexibility: Works well with modern technologies like cloud services and remote work. Zero Trust is a powerful approach to cybersecurity. While it has challenges, its benefits make it increasingly popular among organizations. As cyber threats grow more complex, Zero Trust principles help create a strong, adaptable security posture."}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 58, "chunk_id": "english_pack_58_0_bb65c350", "content": "40 | MODULE 1: IT GOVERNANCE FRAMEWORKS 1.4 Case Study: Suspicious behaviour linked to large-scale identity fraud operation A bank teller submitted a report to AUSTRAC* detailing suspicious banking transactions. This report assisted authorities investigating a syndicate allegedly involved in large- scale identity fraud. The report described over-the-counter transactions in which two people were involved – the account owner and the main suspect. The suspect was not connected to the account but controlled the transactions and would not allow the account owner to speak. The pair transferred approximately AUD541,000 from a bank account in Jordan to an Australian account. They then withdrew approximately AUD394,000 from the Australian account in the form of a bank cheque. When the teller requested the account owner undertake this withdrawal, the suspect became agitated and aggressive. The pair also transferred approximately AUD147,000 from the Australian account to a third-party account. These transactions left the account owner with an account balance of just AUD1,000. AUSTRAC information allowed authorities to link the suspect in this matter with the movement of funds to Jordan, the United Arab Emirates and Peru. Authorities continued their investigations and ultimately commenced proceeds of crime action against the"}
{"pack_id": "english_pack", "section_title": "MODULE 1: IT GOVERNANCE FRAMEWORKS", "page": 59, "chunk_id": "english_pack_59_0_a0dcf270", "content": "MODULE 1: IT GOVERNANCE FRAMEWORKS | 41 suspect and members of the syndicate, and restrained approximately AUD1.6 million in assets, including real estate and cash. *AUSTRAC or the Australian Transaction Reports and Analysis Centre is an Australian government financial intelligence agency that monitors financial transactions to detect money laundering, organised crime, tax evasion, welfare fraud and terrorism."}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 60, "chunk_id": "english_pack_60_0_423caa4a", "content": "42 | MODULE 2: CYBERSECURITY & DATA PROTECTION MODULE 2: CYBERSECURITY & DATA PROTECTION Cybersecurity and data protection are essential for any organization that collects, processes, or stores personal or sensitive information. In this chapter, we will explore some of the key concepts and challenges. Data breach notification and communication: How to prepare for and respond to data breaches, and how to communicate effectively with stakeholders, regulators, and the public about the incident and its consequences. Cybersecurity training and ethical hacking: How to educate and empower employees and users about cybersecurity best practices, and how to use ethical hacking techniques to test and improve the security of systems and networks. This module provides a concise overview of these topics, as well as practical examples and recommendations on how to implement them in your organization. By reading this chapter, you will gain a better understanding of the current trends and challenges in cybersecurity and data protection, as well as the"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 61, "chunk_id": "english_pack_61_0_6c8d7bf9", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 43 best practices and standards that can help you achieve a high level of compliance and performance. 2.1 Cybersecurity Body of Knowledge (CyBOK) The CyBOK is an international project aimed at codifying best cyber security knowledge and practice. CyBOK provides the means to fill the rising demand for skilled cybersecurity professionals by defining a common body of knowledge that encompasses various domains within the field. It covers topics such as security management, human factors, software security, network security, and cryptography, among others. The knowledge areas defined in CyBOK serve as the building blocks for developing cybersecurity expertise. The CyBOK framework is focussed on thirteen fundamental knowledge areas: • Access Control and Identity Management – covers the principles and practices involved in managing access to systems, networks, and resources, as well as establishing and maintaining user identities. • Cybersecurity Architecture – addresses the design and implementation of secure systems and networks, considering factors such as threat modelling, security controls, and risk management. • Cybersecurity Governance and Management –"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 62, "chunk_id": "english_pack_62_0_de0c66ce", "content": "44 | MODULE 2: CYBERSECURITY & DATA PROTECTION explores the organizational aspects of cybersecurity, including governance frameworks, policies, regulations, risk assessment, and incident response planning. • Digital Forensics and Incident Response – focuses on the techniques and methodologies employed in the investigation and analysis of cyber incidents, as well as the appropriate response measures. • Human Factors in Cybersecurity – recognizes the critical role of human behaviour and psychology in cybersecurity, covering topics such as security awareness, training, and usability considerations. • Information Assurance – encompasses the principles and practices of safeguarding information assets, ensuring data integrity, confidentiality, availability, and non-repudiation. • Malware and Software Vulnerability Analysis – delves into the identification, analysis, and mitigation of software vulnerabilities and malware threats. • Network Security – explores the concepts, protocols, and technologies used to secure computer networks, including network architecture, encryption, intrusion detection, and firewall implementation. • Operating Systems Security – focuses on securing operating systems, including access control, secure configuration, patch management, and secure administration practices. • Privacy and Online Trust – covers the legal, ethical,"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 63, "chunk_id": "english_pack_63_0_986b693e", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 45 and technical aspects of protecting individual privacy in the digital realm, as well as establishing trust in online interactions. • Resilience and System Recovery – addresses the strategies and techniques for ensuring system resilience, business continuity planning, and disaster recovery. • Secure Systems Engineering – emphasizes secure software development practices, secure coding techniques, and secure software lifecycle management. • Software Security Assurance – explores methods for assuring the security of software systems, including secure testing, code reviews, and vulnerability assessment. The CyBOK framework provides a holistic approach to cybersecurity, covering technical, managerial, and human factors. It serves as a valuable resource for professionals seeking to broaden their knowledge, educators designing cybersecurity curricula, and policymakers shaping cybersecurity policies. In summary, the Cybersecurity Body of Knowledge (CyBOK) is a comprehensive guide that defines the essential knowledge areas within the field of cybersecurity. It covers a wide range of topics and disciplines, providing a structured framework to understand, develop, and apply cybersecurity expertise. By promoting a common understanding of cybersecurity principles, CyBOK contributes to the"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 64, "chunk_id": "english_pack_64_0_1b4d0d7d", "content": "46 | MODULE 2: CYBERSECURITY & DATA PROTECTION advancement of the field, addressing the critical need for skilled cybersecurity professionals in today’s interconnected world 2.2. Cybersecurity & Data Protection Australian laws, such as the Privacy Act of 1988 and the Notifiable Data Breaches (NDB) scheme, mandate the protection of personal data. Organizations are obliged to establish robust cybersecurity policies and practices to safeguard sensitive information, thus ensuring compliance with legal requirements and ethical responsibilities alike. Cybersecurity & Data Protection is therefore the practice of safeguarding your devices, accounts and data from cyber threats such as scams and malware. As a general guide to train organisation staff to resist social engineering attacks, these are considered basic cybersecurity best practices: • Turn on automatic updates for your software, apps and operating systems to fix any vulnerabilities that cybercriminals can exploit. • Use strong passwords and authentication methods for your accounts, such as multi-factor authentication or biometrics, to prevent unauthorized access. • Avoid clicking on pop-ups, unknown emails and links that may contain malware or phishing attempts to steal"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 65, "chunk_id": "english_pack_65_0_535867a0", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 47 your information or money. • Always connect to secure Wi-Fi networks that are encrypted and password-protected and avoid using public Wi-Fi for sensitive activities such as online banking or shopping. • Encrypt your data, especially when it is stored or transmitted over the internet or other networks, to prevent cybercriminals from reading or modifying it. • Collaborate and share information with other organisations, security agencies and law enforcement to improve your cyber resilience and awareness of potential threats. • Manage your assets, such as software and data, by using centralised systems and configuration management to ensure visibility and control of your critical resources. • Implement protective measures and controls for your cyber risks, such as firewalls, antivirus software and backup systems, based on the Australian Signals Directorate’s (ASD) Strategies to mitigate targeted cyber intrusions or equivalent. • Use detection systems and processes to monitor your devices and networks for any signs of cyberattacks, such as unusual activity or anomalies, and use data analytics to integrate sources of threats in real time. • Plan for response and recovery in case of a cyber incident, by having a clear strategy, roles and responsibilities, communication channels and"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 66, "chunk_id": "english_pack_66_0_2109c759", "content": "48 | MODULE 2: CYBERSECURITY & DATA PROTECTION contingency plans. This advice notwithstanding, you should always tailor your approach to your specific context, needs and risks. You should also keep yourself updated on the latest trends and developments in cyber security, as cyber threats are constantly evolving and becoming more sophisticated. Australian laws, such as the Privacy Act of 1988 and the Notifiable Data Breaches (NDB) scheme, mandate the protection of personal data. Organizations are obliged to establish robust cybersecurity policies and practices to safeguard sensitive information, thus ensuring compliance with legal requirements and ethical responsibilities alike. Cybersecurity Policies Cybersecurity policies are considered essential for protecting the digital assets and interests of individuals, organizations and nations from cyber threats and attacks. When formulating cybersecurity policies, the following points should be covered: • Be based on a comprehensive risk assessment and a clear understanding of the cyber threat landscape, as well as the legal, ethical and social implications of cyber activities. • Aim to achieve a balance between security, privacy,"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 67, "chunk_id": "english_pack_67_0_8e0b1eae", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 49 accessibility and innovation, while respecting the rights and responsibilities of all stakeholders in the cyberspace. • Promote the adoption of best practices and standards for cyber resilience, such as zero trust and attack surface management, which can help prevent, detect and mitigate cyber risks. • Address the challenges and opportunities posed by emerging technologies, such as artificial intelligence, cloud computing and quantum computing, which can enhance or undermine cyber security. • Be aligned with national and international laws and regulations, as well as with the norms and values of the global community, to foster cooperation and trust among cyber actors. • Be regularly reviewed and updated to reflect the dynamic nature of cyber threats and technologies, as well as the evolving needs and expectations of the cyber society. These policies, often informed by industry best practices and regulatory mandates, guide organizations in implementing a multi-layered defence strategy to protect critical assets and sensitive data. Such policies should specifically include: • Acceptable Use Policy. • Security Awareness and Training Policy. • Change Management Policy."}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 68, "chunk_id": "english_pack_68_0_1e24040c", "content": "50 | MODULE 2: CYBERSECURITY & DATA PROTECTION • Incident Response Policy. • Remote Access Policy. • Vendor Management Policy. • Password Creation and Management Policy. • Network Security Policy. The Privacy Act & Data Protection The Privacy Act of 1988 is a cornerstone of data protection in Australia. It lays the foundation for safeguarding personal information, ensuring that organizations collect, use, and disclose data in a responsible and ethical manner. The Act sets out strict guidelines that organizations must follow, with serious consequences for violations. This legal framework serves as a reminder of the ethical duty organizations must respect the privacy of individuals and safeguard their personal information. The Privacy Act covers the following: • Know why your personal information is being collected, how it will be used and who it will be disclosed to. • Have the option of not identifying yourself, or of using a pseudonym in certain circumstances. • Ask for access to your personal information (including your health information)"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 69, "chunk_id": "english_pack_69_0_863ace55", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 51 The Notifiable Data Breaches Scheme The Notifiable Data Breaches (NDB) scheme is a legal requirement for organisations and agencies that are covered by the Privacy Act 1988 to report data breaches that are likely to cause serious harm to the individuals whose personal information is involved. A data breach occurs when personal information is lost, accessed or disclosed without authorisation. For example, when a device with customer information is stolen, a database with personal information is hacked, or personal information is mistakenly given to the wrong person. The notification to individuals must include recommendations about the steps they should take in response to the data breach. The notification to the Office of the Australian Information Commissioner (OAIC) must be done using the online Notifiable Data Breach form. The NDB scheme aims to protect the privacy and security of personal information and to enhance public confidence in how organisations handle personal information. The NDB scheme also provides guidance and support for organisations and agencies on how to prevent, prepare for and respond to data breaches, drawing on research and best practice."}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 70, "chunk_id": "english_pack_70_0_abd1e588", "content": "52 | MODULE 2: CYBERSECURITY & DATA PROTECTION Ethical & Legal Considerations Ethical and legal considerations in cybersecurity and data protection are essential to ensure the privacy, security and trust of individuals, organisations and society. You should be aware of and comply with the relevant laws and regulations that apply to your jurisdiction, sector and activities, such as the Privacy Act 1988 (Cth) in Australia, which sets out 13 Australian Privacy Principles for handling personal information. You should also follow the international standards and best practices for data privacy and security, such as ISO 27701, which relates to the way an organisation collects personal data and prevents unauthorised use or disclosure. You should respect the confidentiality, integrity and availability of the data you collect, use, store and disclose, and only do so for legitimate purposes and with consent or authorisation from the data subjects or owners. You should employ reasonable protection efforts in your use of technology to communicate with clients, colleagues and stakeholders, and prevent unauthorized disclosure of sensitive information. You should act ethically and responsibly when dealing with data, especially when using artificial intelligence or machine learning, which present some extraordinary challenges in terms of law, ethics and technical advancement. You should consider the potential impact of your actions on"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 71, "chunk_id": "english_pack_71_0_364be34f", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 53 individuals, organisations and society, and balance the benefits and risks of data use and sharing. You should be transparent and accountable for your data practices and report any breaches or incidents promptly and appropriately. Confidentiality, Integrity, & Availability Confidentiality, integrity, and availability (CIA) are the three main objectives of cybersecurity that aim to protect data and information from unauthorized access, use, and disclosure. Confidentiality ensures that only authorized users and processes can access or modify data. This can be achieved by using encryption, authentication, access control, and other security measures. Integrity ensures that data is maintained in a correct state, and nobody can improperly modify it, either accidentally or maliciously. This can be achieved by using checksums, digital signatures, audit trails, and other security measures. Availability ensures that authorized users can access data whenever they need to do so. This can be achieved by using backup systems, redundancy, load balancing, and other security measures. Cybersecurity and data protection are broader topics that cover the legal, ethical, and technical aspects of ensuring the CIA of data in various contexts and domains. The best advice on the topic of CIA in cybersecurity and"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 72, "chunk_id": "english_pack_72_0_6203c27d", "content": "54 | MODULE 2: CYBERSECURITY & DATA PROTECTION data protection is to follow the relevant standards, guidelines, and best practices that apply to your specific industry, sector, or organization. Some examples are ISO/IEC 27001, NIST SP 800-53, GDPR, HIPAA, etc. . The Evolving Threat Landscape The evolving threat landscape is a perpetual top priority for security and risk management leaders, according to a Gartner survey. The COVID-19 pandemic has created new challenges and opportunities for cyberattackers, who exploit vulnerabilities in remote work environments, digital meeting solutions, and unpatched systems. Cyber resilience is the ability to anticipate, withstand, recover from, and adapt to adverse conditions, stresses, attacks, or compromises on systems that use or are enabled by cyber resources. A Defence in Depth (DiD) architecture is an approach to cybersecurity that uses a series of layered defensive mechanisms to protect valuable data and information. Artificial intelligence (AI) models are becoming effective at enhancing the capabilities of both defenders and attackers in the cyber domain, requiring adaptive strategies to safeguard sensitive data and protect against potential breaches. Security best practices include using strong passwords, enabling multi-factor authentication, updating software and"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 73, "chunk_id": "english_pack_73_0_95018bc8", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 55 firmware, encrypting data, backing up data, avoiding phishing emails, and educating users on cyber hygiene. Collaborative Approach Recognize that cybersecurity risks are global and require a coordinated, collaborative approach. Cyberattacks can affect any country, sector, or organization, and have severe economic and social consequences. Therefore, we need to ensure that risks to cybersecurity, data protection, privacy, and online safety are addressed at all levels and by all stakeholders. Share knowledge, build capacity and expertise, and assess cybersecurity risks at the country level. To cope with the evolving nature and complexity of cyber threats, we need to foster a culture of learning and innovation among cybersecurity and data protection professionals. We also need to conduct regular risk assessments to identify the most critical assets and vulnerabilities and prioritize the appropriate measures to protect them. Provide incentives for the private sector to invest in digital infrastructure and technology. The private sector plays a vital role in developing and deploying secure and resilient digital solutions for various domains, such as health, transport, energy, etc. Therefore, we need to create a favourable environment for private sector participation, such as by providing tax breaks, subsidies, grants, or public-private partnerships."}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 74, "chunk_id": "english_pack_74_0_f0d4676a", "content": "56 | MODULE 2: CYBERSECURITY & DATA PROTECTION Unite data protection and cybersecurity skills. Data breaches can have multiple impacts on an organization’s reputation, operations, finances, and legal compliance. Therefore, we need to ensure that both data protection and cybersecurity specialists work together to prevent and respond to data breaches, by combining their skills in areas such as encryption, authentication, access control, incident response, etc. The Human Element The human element is a crucial factor in cybersecurity and data protection, as humans are both the primary source of risks and the target of attacks. According to ISACA, humans represent a mystery to be deciphered by security/cybersecurity experts because their behaviours, attitudes, beliefs, rituals and decisions constitute a little-understood universe for executives and their heads of security. The human factor in information security can be seen as the weakest link in the chain or as the reliable and resilient factor of the system, depending on how organizations approach the challenge of educating and empowering their employees. Harvard Business Review suggests that better cybersecurity starts with fixing employees’ bad habits, such as using weak passwords, clicking on suspicious links, or sharing sensitive information online."}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 75, "chunk_id": "english_pack_75_0_87b0e928", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 57 The human element of cybersecurity also involves ethical, legal, and social aspects, such as privacy, consent, accountability, and responsibility. To address the human element of cybersecurity and data protection, organizations need to adopt a holistic approach that combines technical, organizational, and behavioural measures, such as encryption, policies, training, and awareness. Some points include: • Keeping software up to date. • Avoiding opening suspicious emails. • Keeping hardware up to date. • Using a secure file-sharing solution to encrypt data. • Using anti-virus and anti-malware. • Using a VPN to privatize your connections. • Check links before you click. • Don’t be lazy with your passwords. 2.3. Data Breach Notification & Communication Data breaches have become an unfortunate reality, posing significant threats to individuals’ privacy and organizations’ sensitive information. In response, data breach notification and communication policies have emerged as vital tools to address these challenges. These policies establish clear guidelines for organizations to"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 76, "chunk_id": "english_pack_76_0_a7bcdbd7", "content": "58 | MODULE 2: CYBERSECURITY & DATA PROTECTION follow when a breach occurs, ensuring affected individuals and relevant authorities are promptly informed. Ethical and legal considerations underscore the importance of transparent and timely communication during data breaches, fostering trust, accountability, and responsible data handling. Transparency, Trust, & Accountability Transparency, trust and accountability are essential principles for managing data breaches involving personal information. Data breach notifications are required by law under the Privacy Act 1988 (Cth) when a breach is likely to result in serious harm to affected individuals and remedial action cannot prevent or mitigate the harm. Data breach notifications should inform the affected individuals and the Office of the Australian Information Commissioner (OAIC) of the following: what happened, what information was involved, what are the risks and impacts, what are the steps taken or planned to address the breach, and what are the options for individuals to protect themselves. Data breach notifications must be timely, clear, concise and easy to understand. They should also be honest, respectful and empathetic. It is not uncommon for organisations to wait weeks or months before notifying those affected. Meanwhile their personal information is being sold on the dark web. Data breach notifications can help reduce the potential harm to individuals, restore trust and confidence in the"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 77, "chunk_id": "english_pack_77_0_6285ad08", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 59 organisation, and demonstrate compliance with legal obligations and ethical standards. Such notifications should be part of a broader data breach response plan that includes preparation, containment, assessment, notification, review and evaluation stages. Data breach response plans must be aligned with best practices and guidance from relevant authorities, such as the OAIC, the Data Protection Commissioner and industry bodies. The Modern Data Landscape The ubiquity of digital systems has led to an unprecedented accumulation of personal and sensitive data. From financial records and healthcare information to personal preferences and online behaviours, data has become an asset, making it an attractive target for cybercriminals. The modern data landscape is dynamic, with data being collected, stored, processed, and shared across multiple platforms, devices, and jurisdictions. Data breaches are therefore a serious threat to the privacy and security of personal information, and can have significant legal, reputational, and financial consequences for organisations and individuals. As mentioned, data breach notification and communication should follow the best practices outlined by the Office of the Australian Information Commissioner"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 78, "chunk_id": "english_pack_78_0_bb298cde", "content": "60 | MODULE 2: CYBERSECURITY & DATA PROTECTION (OAIC) in its Data Breach Preparation and Response Guide, as well as any applicable laws or regulations in the relevant jurisdictions. Some of the best practices for data breach notification and communication are: • Notify the OAIC and affected individuals as soon as practicable after becoming aware of a data breach that is likely to result in serious harm, unless remedial action can prevent or mitigate the risk of harm. • Use multiple communication channels to ensure that all affected individuals are notified, such as email, phone, SMS, website, social media, or postal mail. • Use plain language that is clear, concise, and accurate, and avoid technical jargon or legal terms that may confuse or mislead the recipients. • Provide a comprehensive explanation of what happened, what information was involved, what actions have been taken to contain and resolve the breach, what steps are being taken to prevent future breaches, and what assistance or support is available to the affected individuals. • Use effective headlines that capture the attention and convey the urgency of the message, such as “Important: Data Breach Notification” or “Urgent: Action Required Following Data Breach”. • Inform the affected individuals about the next steps they"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 79, "chunk_id": "english_pack_79_0_b1fd0c7e", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 61 should take to protect themselves from potential harm, such as changing passwords, monitoring accounts, contacting credit reporting agencies, or seeking legal advice. Data breach notification and communication should be tailored to the specific circumstances and context of each breach, considering factors such as the nature and extent of the breach, the type and sensitivity of the information involved, the potential harm to the affected individuals, and the expectations and preferences of the recipients. Data Breach Notification Policies Data breach notification policies are a structured framework that organizations must adhere to when a data breach occurs. These policies outline the necessary steps for identifying, mitigating, and communicating the breach to the affected individuals and relevant authorities. Have a written data breach response plan that outlines the roles and responsibilities of the data breach response team, the steps to contain, assess, notify and review the breach, and the communication strategies for internal and external stakeholders. Consider the safety and privacy of the individuals whose personal information has been compromised and avoid"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 80, "chunk_id": "english_pack_80_0_e0e0dc87", "content": "62 | MODULE 2: CYBERSECURITY & DATA PROTECTION disclosing any confidential or sensitive information that could put them at further risk. Comply with the requirements of the Privacy Act 1988 (Cth) and the Notifiable Data Breaches (NDB) scheme, which mandate notification to the affected individuals and the Office of the Australian Information Commissioner (OAIC) if a data breach is likely to result in serious harm. Provide clear and timely information to the affected individuals about the nature and extent of the breach, the steps taken to mitigate the harm, the actions they can take to protect themselves, and the contact details for further assistance. Review the incident and identify the causes and contributing factors of the breach and implement measures to prevent or reduce the likelihood of future breaches. Timeliness The Ethical Imperative Ethical data breach notification policies stress the urgency of timely communication. Delayed notification can exacerbate the impact of a breach, allowing cybercriminals more time to exploit compromised data. Timeliness is an ethical imperative in data breach notification because it can reduce or prevent the harm to the affected individuals and restore the trust in the organisation that handles their personal information. The Privacy Act 1988 (Cth) requires organisations to notify individuals and the Commissioner of eligible data breaches as"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 81, "chunk_id": "english_pack_81_0_36bfa19d", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 63 soon as practicable after becoming aware of them unless an exception applies. An eligible data breach occurs when there is any unauthorised access, disclosure or loss of personal information that is likely to result in serious harm to any of the individuals to whom the information relates. To determine whether a data breach is likely to result in serious harm, organisations should consider the nature and sensitivity of the personal information involved, the circumstances of the breach, and the potential consequences for the individuals. Timely notification and communication can help individuals to take steps to protect themselves from the harm, such as changing passwords, monitoring accounts, or contacting their financial institutions. Timely notification and communication can also demonstrate that the organisation is taking the data breach seriously, is committed to protecting the privacy of its customers or clients and is transparent and accountable for its actions. To achieve timeliness in data breach notification and communication, organisations should have a data breach response plan that outlines the roles and responsibilities of staff, the steps to contain, assess, notify and review a data breach, and the communication strategies and channels to use. Organisations should also train their staff on how to identify and report a data breach, and regularly review and"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 82, "chunk_id": "english_pack_82_0_d7ec94ec", "content": "64 | MODULE 2: CYBERSECURITY & DATA PROTECTION update their data breach response plan to ensure its effectiveness. Balancing Legal Compliance & Ethical Values Data breach notification policies often align with legal requirements imposed by data protection regulations. However, ethical considerations go beyond legal mandates, emphasizing the moral responsibility of organizations to safeguard individuals’ data and rights. Balancing legal compliance and ethical values in data breach notification and communication is a complex and challenging task that requires careful consideration of various factors, such as: • The applicable laws and regulations in different jurisdictions that may impose different obligations and standards for data breach notification and communication, such as the type, timing, content, and format of the notification. • The ethical values and expectations of the stakeholders that may go beyond the legal requirements and demand more transparency, accountability, and responsiveness from the organization. • The potential risks and benefits of disclosing or withholding certain information about the data breach,"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 83, "chunk_id": "english_pack_83_0_796dda2e", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 65 such as the cause, scope, severity, and consequences of the breach, as well as the remedial measures taken or planned by the organization. Based on research and best practice, some of the general principles and guidelines for balancing legal compliance and ethical values in data breach notification and communication are: • Be proactive and prepared – develop a data breach response plan that outlines the roles, responsibilities, procedures, and resources for data breach notification and communication. Conduct regular training and testing to ensure that the plan is effective and up to date. • Be timely and accurate – notify the affected stakeholders as soon as possible after discovering a data breach, without unreasonable delay. Provide accurate and information about the data breach, without speculation or exaggeration. Update the information as new facts emerge or circumstances change. • Be clear and concise – use plain and simple language that is easy to understand by the intended audience. Avoid technical jargon or legal terms that may confuse or mislead the stakeholders. Use appropriate channels and formats to communicate the information, such as email, phone call, letter, website, social media, etc. • Be respectful and empathetic – acknowledge the impact"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 84, "chunk_id": "english_pack_84_0_73f21c9d", "content": "66 | MODULE 2: CYBERSECURITY & DATA PROTECTION and harm caused by the data breach to the stakeholders. Express sincere apology and regret for the incident. Demonstrate genuine concern and care for the stakeholders’ well-being and security. Offer assistance and support to help them cope with the aftermath of the data breach. • Be honest and accountable – admit responsibility and liability for the data breach, if applicable. Explain the root cause and contributing factors of the data breach. Disclose the actions taken or planned to investigate, contain, recover, and prevent future breaches. Cooperate with relevant authorities and regulators in their inquiries or investigations. Accept feedback and criticism from the stakeholders and address their questions or concerns. Resilient Defences & Responsible Practices Offensive cyber security training involves teaching students how to perform penetration testing, ethical hacking and other techniques to identify and exploit vulnerabilities in systems and networks. This type of training can have many benefits, such as improving the security posture of organizations, enhancing the skills and knowledge of cyber security professionals, and"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 85, "chunk_id": "english_pack_85_0_068518cf", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 67 contributing to the advancement of cyber security research and innovation. However, such training also poses significant ethical risks, such as misuse or abuse of the acquired skills, violation of privacy or confidentiality, damage to systems or data, or harm to individuals or society at large. Therefore, you should follow some ethical principles for designing responsible offensive cyber security training, such as: • Principle 1: Respect for autonomy. You should respect the autonomy of your students and other stakeholders by informing them about the objectives, methods, risks and benefits of the training, and obtaining their consent before engaging in any offensive cyber security activities. • Principle 2: Beneficence and non-maleficence. You should aim to maximize the benefits and minimize the harms of the training for your students and other stakeholders by ensuring that the training is relevant, proportionate, necessary and effective. • Principle 3: Justice. You should ensure that the training is fair and equitable for your students and other stakeholders by avoiding discrimination, bias, favouritism or exploitation, and providing equal opportunities for participation and learning. • Principle 4: Accountability. You should be accountable for your actions and decisions in the"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 86, "chunk_id": "english_pack_86_0_75b0eff8", "content": "68 | MODULE 2: CYBERSECURITY & DATA PROTECTION training by adhering to relevant laws, regulations, standards and codes of conduct, and being transparent, honest and responsible for the outcomes and impacts of the training. • Principle 5: Education. You should educate your students and other stakeholders about the ethical implications of offensive cyber security by raising their awareness, fostering their critical thinking, and encouraging their ethical reasoning and decision- making. In addition to these principles, you should also follow some good practices for cyber resilience that can help you protect your assets, detect threats, respond to incidents and recover from disruptions. Some of these practices are: • Developing a cybersecurity strategy and governance framework that aligns with your organizational goals and objectives and involves board engagement and oversight. • Implementing a cyber risk management process that identifies, assesses, treats and monitors cyber risks, including those related to third parties such as vendors or partners. • Collaborating and sharing information with other organizations, security agencies and law enforcement entities to enhance your situational awareness, threat"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 87, "chunk_id": "english_pack_87_0_8ab5cfac", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 69 intelligence and incident response capabilities. • Managing your assets effectively by maintaining an inventory of your critical internal and external assets (e.g., software and data), and ensuring their visibility, availability and integrity. • Implementing protective measures and controls based on the Australian Signals Directorate’s (ASD) Strategies to mitigate targeted cyber intrusions (or equivalent), as well as additional controls such as encryption for data in transit. • Using detection systems and processes that enable continuous monitoring of your systems and networks, and leverage data analytics to integrate sources of threats in real time. The Human Element in Cybersecurity Elevating the human element in cybersecurity means strengthening the awareness, skills and behaviours of the people who interact with digital systems and data. According to a report by Verizon, human errors and actions accounted for 82% of all cyberattacks in 2022. Therefore, it is crucial to train and educate employees on how to prevent and respond to cyber threats. Some best practices for elevating the human element in cybersecurity are:"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 88, "chunk_id": "english_pack_88_0_43f60c58", "content": "70 | MODULE 2: CYBERSECURITY & DATA PROTECTION • Offering continuous training opportunities for all staff members, from the CEO to the receptionist, on their role in protecting the organization from cyber risks. • Deploying advanced email protections, such as spam filters, phishing simulations and email encryption, to reduce the chances of falling victim to malicious messages. • Revisiting the approach to password security, such as enforcing strong and unique passwords, using password managers and changing passwords regularly. • Updating multifactor authentication controls, such as using biometric or token-based verification methods, to add an extra layer of security for accessing sensitive data or systems. • Using insider threat protection technology, such as user behaviour analytics or data loss prevention tools, to monitor and detect abnormal or suspicious activities by authorized users. Continuous Improvement & Learning Continuous Improvement & Learning (CIL) is a key aspect of data breach notification and communication, as it helps organisations to prevent, prepare for and respond to data breaches effectively. CIL involves reviewing and learning from data breach incidents, identifying the root causes, implementing"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 89, "chunk_id": "english_pack_89_0_86893038", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 71 prevention plans, and updating policies and procedures accordingly. CIL also involves communicating the lessons learned and the actions taken to relevant stakeholders, such as affected individuals, regulators, partners, and employees. CIL can help organisations to reduce the risk of harm to individuals, comply with the Privacy Act 1988 (Cth) and the Notifiable Data Breaches (NDB) scheme, and maintain trust and reputation as data custodians. Some sources of information and guidance on CIL for data breach notification and communication are: • Data breach preparation and response – Office of the Australian Information Commissioner • Part 3: Responding to data breaches – four key steps | OAIC • Data Breach Response: The Continuous Improvement Cycle – Tanner De Witt Solicitors 2.4 Ethical Hacking A Proactive Approach to Security Ethical hacking is the use of hacking skills and techniques with good intentions and with the full consent and approval of the target. Ethical hackers help organizations identify and fix"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 90, "chunk_id": "english_pack_90_0_8a8dc9c2", "content": "72 | MODULE 2: CYBERSECURITY & DATA PROTECTION vulnerabilities in their IT systems, networks, and applications before malicious hackers can exploit them. Ethical hacking is a valued component of cybersecurity, but it is different from cybersecurity. Cybersecurity is a broader term that encompasses all the policies, practices, and tools that protect IT environments from cyber threats. Ethical hacking is a proactive approach that involves system testing to find and address weaknesses. Ethical hacking requires a high level of technical skills, ethical standards, and legal compliance. Ethical hackers must follow certain principles, such as obtaining the target’s consent, defining the scope of their activities, reporting their findings, and respecting the target’s privacy and security. It can benefit organizations in various ways, such as improving their security posture, enhancing their reputation, complying with regulations, and saving costs. Ethical hacking can also benefit society by raising awareness of cyber risks, promoting ethical values, and contributing to cyber resilience. The Responsibility of Responsible Hacking Responsible hacking is the practice of using hacking skills for ethical, legal or beneficial purposes, such as testing the security of systems, finding vulnerabilities, or exposing wrongdoing. Responsible hacking requires adhering to certain principles"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 91, "chunk_id": "english_pack_91_0_8fb6756a", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 73 and standards, such as obtaining consent, respecting privacy, avoiding harm, reporting findings, and complying with laws and regulations. Responsible hacking also entails being aware of the risks and consequences of hacking activities, such as legal liability, reputational damage, or retaliation from malicious actors. As an IT professional, you should advise your clients or employers on how to implement responsible hacking practices in their cybersecurity training and ethical hacking programs. Some of the best practices for responsible hacking include: • Establishing clear policies and procedures for ethical hacking activities, such as defining the scope, objectives, methods, and reporting mechanisms. • Obtaining written authorization from the owners or operators of the systems to be hacked and ensuring that the hacking activities do not violate any contractual or legal obligations. • Conducting regular security assessments and audits to identify and remediate vulnerabilities, and using only approved tools and techniques that minimize the impact on the systems. • Educating and training staff on ethical hacking skills and principles and ensuring that they follow the code of conduct and professional standards of the industry. • Collaborating with other stakeholders, such as law enforcement agencies, regulators, or industry"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 92, "chunk_id": "english_pack_92_0_9f786adf", "content": "74 | MODULE 2: CYBERSECURITY & DATA PROTECTION associations, to share information, best practices, and lessons learned. Some of the sources that you can refer to for more information on responsible hacking are: • Cybersecurity Laws and Regulations Report 2023 Australia, which covers common issues in cybersecurity laws and regulations in Australia. • Cybersecurity. Who is responsible? which discusses the roles and responsibilities of different actors in cybersecurity. • Who is Liable when Business Emails are Hacked? which explains the legal implications of hacking business emails in Australia. Mitigating Legal & Reputational Risks Ethical hacking is a valuable practice that can help organizations improve their cybersecurity posture and prevent malicious attacks. However, ethical hackers also face legal and reputational risks if they do not follow certain principles and guidelines. Some of the best practices for mitigating legal and reputational risks in ethical hacking are: • Obtain written consent from the client or the target"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 93, "chunk_id": "english_pack_93_0_a5e828b6", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 75 organization before conducting any penetration testing or vulnerability assessment. This consent should specify the scope, duration, and objectives of the ethical hacking activity, as well as the roles and responsibilities of both parties. • Follow the principle of least privilege and only access the minimum amount of data and systems necessary to perform the ethical hacking task. Avoid accessing, modifying, or deleting any sensitive or personal information that is not relevant to the security assessment. • Report any findings or incidents to the client or the target organization in a timely and transparent manner. Provide clear and actionable recommendations on how to address the identified vulnerabilities or threats. Do not disclose any information to third parties without prior authorization. • Adhere to the relevant laws, regulations, standards, and codes of ethics that apply to the ethical hacking domain. Respect the privacy, confidentiality, and intellectual property rights of the client or the target organization and their stakeholders. • Maintain a high level of professionalism and integrity throughout the ethical hacking process. Do not engage in any malicious, fraudulent, or illegal activities that could harm the client or the target organization or their reputation."}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 94, "chunk_id": "english_pack_94_0_44fdd72f", "content": "76 | MODULE 2: CYBERSECURITY & DATA PROTECTION 2.5 Cloud Security and Governance Cloud security refers to the measures taken to protect data, applications, and infrastructure associated with cloud computing. As more organizations move their operations to the cloud, ensuring the security of these systems becomes crucial. Key Aspects of Cloud Security • Data Protection. Safeguarding sensitive information stored in the cloud. • Access Control. Managing who can view or modify cloud resources. • Privacy. Ensuring that data is used only for its intended purpose. • Business Continuity. Maintaining operations in case of a cloud service disruption. Cloud Governance Cloud governance involves creating rules and policies for using cloud services. It helps organizations manage risks, reduce costs, and ensure that cloud use aligns with business goals. Important Cloud Governance Practices"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 95, "chunk_id": "english_pack_95_0_5830bc78", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 77 • Policy Management. Creating and enforcing rules for cloud usage. • Compliance. Ensuring cloud services meet legal and industry standards. • Security Management. Implementing and maintaining security measures. • Cost Management. Monitoring and optimizing cloud spending. Challenges in Cloud Security and Governance • Shared Responsibility. Understanding what security aspects the cloud provider handles versus the customer. • Data Location. Knowing where data is stored and ensuring it complies with data protection laws. • Multi-Cloud Environments. Managing security across different cloud platforms. 2.6 IoT (Internet of Things) Security Understanding IoT The Internet of Things (IoT) refers to the network of physical devices embedded with electronics, software,"}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 96, "chunk_id": "english_pack_96_0_0539327f", "content": "78 | MODULE 2: CYBERSECURITY & DATA PROTECTION and network connectivity, which enables these objects to collect and exchange data. IoT Security Concerns • Device Vulnerabilities. Many IoT devices have weak security features. • Data Privacy. IoT devices collect a lot of data, raising privacy concerns. • Network Security. IoT devices can be entry points for network attacks. • Scale. The large number of IoT devices makes security management challenging. Key IoT Security Measures • Device Authentication. Ensuring only authorized devices connect to the network. • Encryption. Protecting data as it moves between devices and servers. • Regular Updates. Keeping device software and firmware up to date. • Network Segmentation. Isolating IoT devices from critical systems."}
{"pack_id": "english_pack", "section_title": "MODULE 2: CYBERSECURITY & DATA PROTECTION", "page": 97, "chunk_id": "english_pack_97_0_56a0c877", "content": "MODULE 2: CYBERSECURITY & DATA PROTECTION | 79 IoT Security Best Practices • Security by Design. Building security features into IoT devices from the start. • Risk Assessment. Regularly evaluating and addressing IoT-related risks. • User Education. Teaching users about IoT security risks and safe practices. • Monitoring. Continuously watching for unusual activity in IoT networks. Future of IoT Security As IoT continues to grow, new security challenges and solutions will emerge. Staying informed about these developments is crucial for maintaining effective IoT security. Both cloud and IoT technologies offer significant benefits but also introduce new security challenges. Understanding these challenges and implementing appropriate security measures is essential for organizations adopting these technologies."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 98, "chunk_id": "english_pack_98_0_0e1045f3", "content": "80 | MODULE 3: DATA BREACH PREPARATION & RESPONSE MODULE 3: DATA BREACH PREPARATION & RESPONSE From 25 May 2018 Australian businesses of any size may need to comply with the GDPR if they have an establishment in the European Union (EU), if they offer goods and services in the EU, or if they monitor the behaviours of individuals in the EU. The GDPR includes requirements that resemble those in the Privacy Act 1988, and additional measures that similarly aim to foster transparent information handling practices and business accountability around data handling. In the lead-up to the commencement of the GDPR requirements, businesses should confirm whether they are covered by the GDPR, and if so, take steps to implement any necessary changes to ensure compliance. 3.1 Part 1 (OAIC) Data breaches & the Australian Privacy Act Key points: • A data breach is an unauthorised access or disclosure of"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 99, "chunk_id": "english_pack_99_0_55aad127", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 81 personal information, or loss of personal information. • Data breaches can have serious consequences, so it is important that entities have robust systems and procedures in place to identify and respond effectively. • Entities that are regulated by the Privacy Act should be familiar with the requirements of the NDB scheme, which are an extension of their information governance and security obligations. • A data breach incident may also trigger reporting obligations outside of the Privacy Act. What is a data breach? A data breach occurs when personal information that an entity holds is subject to unauthorised access or disclosure or is lost. Personal information is information about an identified individual, or an individual who is reasonably identifiable. Entities should be aware that information that is not about an individual on its own can become personal information when it is combined with other information, if this combination results in an individual becoming ‘reasonably identifiable’ as a result. A data breach may be the result of malicious action (by an external or insider party), human error, or a failure in information handling or associated security systems. Examples of data breaches include:"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 100, "chunk_id": "english_pack_100_0_ad117dbb", "content": "82 | MODULE 3: DATA BREACH PREPARATION & RESPONSE • loss or theft of physical devices (such as laptops and storage devices) or paper records that contain personal information. • unauthorised access to personal information by an employee. • inadvertent disclosure of personal information due to ‘human error’, for example an email sent to the wrong person. • disclosure of an individual’s personal information to a scammer, because of inadequate identity verification procedures. Confidentiality breach Technically, confidentiality is breached every time some who does not need to know, comes to know something. It is not just when the consequences make themselves felt. Breaches of this kind can occur in writing, by oral transmission or by electronic means including eavesdropping. Availability breach A breach of availability can occur through Denial-of-Service attacks where the web server is deluged with requests, or when millions of spam emails overwhelm servers, or a virus spread on a network. The latter point is a rather formal argument, based on the literal meaning of the word ‘breach’. While it is normal to use"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 101, "chunk_id": "english_pack_101_0_ab4a504e", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 83 the term for incidents affecting confidentiality and leading to unwanted disclosure of information, temporary unavailability of systems or services is not normally defined as a breach. People prefer to call this an ‘incident’ (based on terms used in the ITIL framework). Integrity breach Whenever the integrity of information or its means of storage are violated. It could be through transmission errors, by intentional manipulation, by unintentional handling errors or by the corruption of file content or structure due to electrical, magnetic or other failures. Consequences of a data breach Data breaches can cause harm in multiple ways. Individuals whose personal information is involved may be at risk of serious harm, whether that is harm to their physical or mental well-being, financial loss, or damage to their reputation. Examples of such harm include: • financial fraud including unauthorised credit card transactions or credit fraud • identity theft causing financial loss or emotional and psychological harm • family violence"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 102, "chunk_id": "english_pack_102_0_dd5e7b01", "content": "84 | MODULE 3: DATA BREACH PREPARATION & RESPONSE • physical harm or intimidation • extortion A data breach can also negatively impact an entity’s reputation for privacy protection, and as a result undercut an entity’s commercial interests. As shown in the OAIC’s long-running national community attitudes to privacy survey, privacy protection contributes to an individual’s trust in an entity. 2 If an entity is perceived to be handling personal information contrary to community expectations, individuals may seek out alternative products and services. An entity can reduce the reputational impact of a data breach by effectively minimising the risk of harm to affected individuals, and by demonstrating accountability in their data breach response. This involves being transparent when a data breach, which is likely to cause serious harm to affected individuals, occurs. Transparency enables individuals to take steps to reduce their risk of harm. It also demonstrates that an entity takes their responsibility to protect personal information seriously, which is integral to building and maintaining trust in an entity’s personal information handling capability. The Australian Privacy Principles The Privacy Act contains 13 Australian Privacy Principles (APPs) listed below that set out entities’ obligations for the management of personal information. The APPs are"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 103, "chunk_id": "english_pack_103_0_7c64e567", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 85 principles-based and technologically neutral; they outline principles for how personal information is handled and these may be applied across different technologies and uses of personal information over time. • APP 1 — Open and transparent management of personal information • APP 2 — Anonymity and pseudonymity • APP 3 — Collection of solicited personal information. • APP 4 — Dealing with unsolicited personal information. • APP 5 — Notification of the collection of personal information. • APP 6 — Use or disclosure of personal information. • APP 7 — Direct marketing. • APP 8 — Cross-border disclosure of personal information. • APP 9 — Adoption, use or disclosure of government related identifiers. • APP 10 — Quality of personal information. • APP 11 — Security of personal information. • APP 12 — Access to personal information. • APP 13 — Correction of personal information Compliance with the APPs will reduce the risk of a data breach occurring because they ensure that privacy risks are either reduced or removed during the process of personal"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 104, "chunk_id": "english_pack_104_0_70601f0f", "content": "86 | MODULE 3: DATA BREACH PREPARATION & RESPONSE information handling, including collection, storage, use, disclosure, and destruction of personal information. For example, APP 3 restricts the collection of personal information. APPs 4.3 and 11.2 outline requirements to destroy or de-identify information if it is unsolicited or no longer needed by the entity. Compliance with these requirements reduces the amount of data that may be exposed because of a breach. Compliance with the requirement to secure personal information in APP 11 is key to minimising the risk of a data breach.3 APP 11 requires entities to take reasonable steps to protect the personal information they hold from misuse, interference and loss, and from unauthorised access, modification or disclosure. The type of steps that are reasonable to protect information will depend on the circumstances of the entity and the risks associated with personal information handled by the entity. In addition, APP 1 requires entities to take reasonable steps to establish and maintain practices, procedures, and systems to ensure compliance with the APPs.5 The Notifiable Data Breaches (NDB) scheme This topic was discussed in some detail in Module 2 – mentioned again here for full disclosure. The NDB scheme in Part IIIC of the Privacy Act requires entities to notify affected individuals and the Commissioner of certain data breaches."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 105, "chunk_id": "english_pack_105_0_55ba88f6", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 87 The NDB scheme requires entities to notify individuals and the Commissioner about ‘eligible data breaches’. Such a breach occurs when the following criteria are met: • There is unauthorised access to, or disclosure of personal information held by an entity (or information is lost in circumstances where unauthorised access or disclosure is likely to occur). • This is likely to result in serious harm to any of the individuals to whom the information relates. • The entity has been unable to prevent the likely risk of serious harm with remedial action. Entities must also conduct an assessment if it is not clear if a suspected data breach meets these criteria. The assessment will determine whether the breach is an ‘eligible data breach’ that triggers notification obligations. Other obligations Entities may have other obligations outside of those contained in the Privacy Act that relate to personal information protection and responding to a data breach. These may include other data protection obligations under state-based or international data protection laws. Australian businesses may need to comply with the European Union’s (EU’s) General Data Protection Regulation (GDPR) if they have an"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 106, "chunk_id": "english_pack_106_0_77e8e8dc", "content": "88 | MODULE 3: DATA BREACH PREPARATION & RESPONSE establishment in the EU, if they offer goods and services in the EU, or if they monitor the behaviour of individuals in the EU. For data breaches affecting certain categories of information, other mandatory or voluntary reporting schemes may exist. For example, entities might consider reporting certain breaches to: • the entity’s financial services provider • police or law enforcement bodies • the Australian Securities & Investments Commission (ASIC) • the Australian Prudential Regulation Authority (APRA) • the Australian Taxation Office (ATO) • the Australian Transaction Reports and Analysis Centre (AUSTRAC) • the Australian Cyber Security Centre (ACSC) • the Australian Digital Health Agency (ADHA) • the Department of Health • State or Territory Privacy and Information Commissioners • professional associations and regulatory bodies • insurance providers. Some entities may have additional obligations to report to the Commissioner under the National Cancer Screening Register Act 2016 (NCSR Act) or have different reporting obligations"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 107, "chunk_id": "english_pack_107_0_09ce9532", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 89 under the My Health Records Act 2012 (My Health Records Act). Under the NCSR Act, current and former contracted service providers of the National Cancer Screening Register must notify the Secretary of the Department of Health (the Secretary) and the Commissioner if they become aware of unauthorised recording, use or disclosure of personal information included in the Register. The Secretary must also notify the Commissioner of certain data breaches, including potential breaches, in connection with the National Cancer Screening Register. The Secretary must also consult the Information Commissioner about notifying individuals who may be affected. Separately, entities with NCSR Act obligations must consider whether the incident also requires notification under the NDB scheme, as the two schemes operate concurrently. Where the test for both schemes have been met, the entity may make a joint notification to the Commissioner. Why do you need a data breach response plan? All entities should have a data breach response plan. Your actions in the first 24 hours after discovering a data breach are often crucial to the success of your response. A quick response can substantially decrease the impact on the affected individuals. High profile data breaches, both in Australia and overseas,"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 108, "chunk_id": "english_pack_108_0_9ef73fc8", "content": "90 | MODULE 3: DATA BREACH PREPARATION & RESPONSE highlight the significant disruption caused by a breach of personal information. Research suggests that the cost to an organisation for a data breach can be significant. Implementing a data breach response plan can assist in mitigating these costs. Having a data breach response plan is part of establishing robust and effective privacy procedures. And having clear roles and responsibilities is part of good privacy governance. A data breach response plan can also help you: • Meet your obligations under the Privacy Act — an entity must take reasonable steps to protect the personal information that it holds; those reasonable steps may include having a data response plan. • Protect an important business asset — the personal information of your customers and clients as well as your reputation. • Deal with adverse media or stakeholder attention from a breach or suspected breach • Instil public confidence in your capacity to protect personal information by properly responding to the breach."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 109, "chunk_id": "english_pack_109_0_7f26b408", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 91 3.2 Part 2 (OAIC) Preparing a data breach response plan Key points • A quick response to a data breach, based on an up-to- date data breach response plan, is critical to effectively managing a breach • your data breach response plan should outline your entity’s strategy for containing, assessing and managing the incident from start to finish • this part will provide practical guidance to help you develop a comprehensive and effective data breach response plan. Why do you need a data breach response plan? All entities should have a data breach response plan. A data breach response plan enables an entity to respond quickly to a data breach. By responding quickly, an entity can substantially decrease the impact of a breach on affected individuals, reduce the costs associated with dealing with a breach, and reduce the potential reputational damage that can result. A data breach response plan can help you:"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 110, "chunk_id": "english_pack_110_0_34091751", "content": "92 | MODULE 3: DATA BREACH PREPARATION & RESPONSE Meet your obligations under the Privacy Act Under the Privacy Act, an entity must take reasonable steps to protect the personal information that it holds. A data breach response plan focussed on reducing the impact of a breach can be one of these reasonable steps. Limit the consequences of a data breach A quick response can reduce the likelihood of affected individuals suffering harm. It can also lessen financial or reputational damage to the entity that experienced the breach. Preserve and build public trust An effective data breach response can support consumer and public confidence in an entity’s respect for individual privacy, and the entity’s ability to manage personal information in accordance with community expectations. What is a data breach response plan? A data breach response plan is a framework that sets out the roles and responsibilities involved in managing a data breach. It also describes the steps an entity will take if a data breach occurs. Your data breach response plan should be in writing to"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 111, "chunk_id": "english_pack_111_0_5466422f", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 93 ensure that your staff clearly understand what needs to happen in the event of a data breach. It is also important for staff to be aware of where they can access the data breach response plan on short notice. You will need to regularly review and test your plan to make sure it is up to date and that your staff know what actions they are expected to take. You can test your plan by, for example, responding to a hypothetical data breach and reviewing how your response could be made more effective. How regularly you test your plan will depend on your circumstances, including the size of your entity, the nature of your operations, the possible adverse consequences to an individual if a breach occurs, and the amount and sensitivity of the information you hold. It may be appropriate in some instances that a review of the plan coincides with the introduction of new products, services, system enhancements, or such other events which involve the handling of personal information. What should the plan cover? The more comprehensive your data breach response plan is, the better prepared your entity will be to effectively reduce the risks and potential damage that can result. Information that your plan should cover includes: A clear explanation of what constitutes a"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 112, "chunk_id": "english_pack_112_0_5caa437f", "content": "94 | MODULE 3: DATA BREACH PREPARATION & RESPONSE data breach This will assist your staff in identifying a data breach should one occur (see What is a data breach? section above). You may also want to include potential examples of a data breach which are tailored to reflect your business activities. A strategy for containing, assessing and managing data breaches This strategy should include the actions your staff, and your response team, will take in the event of a data breach or a suspected data breach. Consider: • potential strategies for containing and remediating data breaches • ensuring you have the capability to implement those strategies as a matter of priority (e.g., having staff available to deal with the breach – see Response team membership section below). Your plan should reflect the capabilities of your staff to adequately assess data breaches and their impact, especially when breaches are not escalated to a response team. • legislative or contractual requirements (such as the requirements of the NDB scheme if they apply to your entity) • a clear and immediate communications strategy that allows for the prompt notification of affected individuals"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 113, "chunk_id": "english_pack_113_0_8156af36", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 95 and other relevant entities. In particular: • who is responsible for implementing the communications strategy. • determining when affected individuals must be notified (refer to Identifying eligible data breaches for further information about mandatory data breach notification requirements under the NDB scheme) • how affected individuals will be contacted and managed. • criteria for determining which external stakeholders should be contacted (for example, law enforcement and cyber security agencies, regulators such as the OAIC, and the media) • who is responsible for liaising with external stakeholders. The roles and responsibilities of staff Your plan should outline the responsibilities of staff members when there is a data breach, or a suspected data breach. Consider: • who staff should inform immediately if they suspect a data breach • the circumstances in which a line manager can handle a data breach, and when a data breach must be escalated to the response team. The following factors may determine when a data breach is escalated to the response team: • the number of people affected by the breach or"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 114, "chunk_id": "english_pack_114_0_b5cf64b8", "content": "96 | MODULE 3: DATA BREACH PREPARATION & RESPONSE suspected breach • whether there is a risk of serious harm to affected individuals now or in the future • whether the data breach or suspected data breach may indicate a systemic problem with your entity’s practices or procedures • other issues relevant to your circumstances, such as the value of the data to you or issues of reputational risk. • who is responsible for deciding whether the breach should be escalated to the response team. One option is for each senior manager to hold responsibility for deciding when to escalate a data breach to the response team. Another option is to have a dedicated role, such as the privacy contact officer. Documentation Your plan should consider how your entity will record data breach incidents, including those that are not escalated to the response team. This will assist you in ensuring you have documentation of how your entity has met regulatory requirements. Response team membership Your data breach response team is responsible for carrying out the actions that can reduce the potential impact of a data"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 115, "chunk_id": "english_pack_115_0_33d5f6b6", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 97 breach. It is important that the staff that make up the response team, as well as their roles and responsibilities, are clearly established and documented before a data breach occurs. Otherwise, your response to the breach may be unnecessarily delayed. Who is in your data breach response team will depend on the circumstances of your entity and the nature of the breach. Different skill sets and staff may be needed to respond to one breach compared to another. In some cases, you may need to include external experts in your team, for example legal advice, data forensics, or media management. You should identify the types of expertise you may need and ensure that this expertise will be available on short notice. You might consider creating a core team and adding other members as they are required. You should keep a current list of response team members and clearly detail their roles, responsibilities, and authorities, as well as their contact details (possibly attached to the data breach response plan). You should ensure these contact details remain updated, particularly in the event of organisational changes. Each role on the response team should have a second point of contact in case the first person is not available. Typical data breach response team roles and skills Your data breach response team may include: • Team leader — who is responsible for leading the"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 116, "chunk_id": "english_pack_116_0_298ee3d9", "content": "98 | MODULE 3: DATA BREACH PREPARATION & RESPONSE response team and reporting to senior management. • Project manager — to coordinate the team and provide support to its members. • Senior member of staff with overall accountability for privacy and/or key privacy officer — to bring privacy expertise to the team. • Legal support — to identify legal obligations and provide advice. • Risk management support — to assess the risks from the breach. • Information and Communication Technology (ICT) support/forensics support — this role can help establish the cause and impact of a data breach that involved ICT systems. • Information and records management expertise – to assist in reviewing security and monitoring controls related to the breach (for example, access, authentication, encryption, audit logs) and to provide advice on recording the response to the data breach. • Human resources (HR) support — if the breach was due to the actions of a staff member. • Media/communications expertise — to assist in communicating with affected individuals and dealing with the media and external stakeholders. If you hold an insurance policy for data breaches, that insurer may have a pre-established panel of external service providers"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 117, "chunk_id": "english_pack_117_0_0c0f26c5", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 99 in many of the roles listed above. You may want to consult with your insurer as to the identity of that panel so they can be included in any response team. Alternatively, the insurer may have a hotline available to assist in the event of a data breach, and that could be noted in the response plan. Which individuals carry out the roles outlined in your response team will depend on your circumstances. For example, in smaller entities it may not be necessary to include steps related to escalating the data breach to the response team, as this may be an automatic process. Depending on the size of your entity or the size of the breach, a single person may perform multiple roles. In smaller entities the owner/principal of the entity could potentially be the person who needs to respond to and act on that breach. It is important that the response team has the authority to take the steps outlined in the response plan without needing to seek permission, as this will enable a faster response to the breach. The role of team leader should be carefully considered, as they should have sufficient ability and authority to effectively manage the various sections within the entity whose input is required and to report to senior management. It may be your senior member of staff with overall accountability for privacy, a senior lawyer (if you have an internal legal function) or another senior manager. If the breach is serious, it may be a senior executive. Actions the response team should"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 118, "chunk_id": "english_pack_118_0_1ca5c37c", "content": "100 | MODULE 3: DATA BREACH PREPARATION & RESPONSE take A data breach response plan should also set out (or refer to) the actions the response team is expected to take when a data breach is discovered. Part 3 of this Guide provides a general framework for responding to a data breach, and Part 4 outlines the requirements of the NDB scheme, which may apply to your entity if they have personal information security obligations under the Privacy Act. The response team will need to consider what information needs to be reported to senior management and at what point. This reporting structure should form part of the plan. The data breach response plan should outline how staff will record how they have become aware of a data breach and the actions taken in response. Keeping records on data breaches and suspected breaches will help you manage the breach and identify risks that could make a breach more likely to occur. Other considerations In developing your plan, you could also consider: • when and how the response team could practice a response to a breach to test procedures and refine them • whether your plan for dealing with personal information data breaches could link into or be incorporated into already existing processes, such as a disaster recovery plan, a cyber security/ICT incident response plan, a"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 119, "chunk_id": "english_pack_119_0_87df7e4d", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 101 crisis management plan or an existing data breach response plan involving other types of information (e.g., commercially confidential information) • whether senior management should be directly involved in the planning for dealing with data breaches and in responding to serious data breaches • any reporting obligations under laws other than the Privacy Act or to other entities • whether you have an insurance policy for data breaches that includes steps you must follow. Data breach response plan quick checklist Use this list to check whether your response plan addresses relevant issues."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 120, "chunk_id": "english_pack_120_0_d4c90bbb", "content": "102 | MODULE 3: DATA BREACH PREPARATION & RESPONSE Information to be included What a data breach is and how staff can identify one Clear escalation procedures and reporting lines for suspected data breaches Members of the data breach response team, including roles, reporting lines and responsibilities Details of any external expertise that should be engaged circumstances How the plan will apply to various types of data breaches and varying risk profiles with c actions An approach for conducting assessments Processes that outline when and how individuals are notified Circumstances in which law enforcement, regulators (such as the OAIC), or other entities ma Processes for responding to incidents that involve another entity A record-keeping policy to ensure that breaches are documented Requirements under agreements with third parties such as insurance policies or servic A strategy identifying and addressing any weaknesses in data handling that contribute Regular reviewing and testing of the plan A system for a post-breach review and assessment of the data breach response and the eff plan"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 121, "chunk_id": "english_pack_121_0_18bffd25", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 103 3.3 Part 3 (OAIC) Responding to data breaches — Four key steps Key points • Each data breach response needs to be tailored to the circumstances of the incident. • In general, a data breach response should follow four key steps: contain, assess, notify and review. Overview Data breaches can be caused or exacerbated by a variety of factors, involve different types of personal information, and give rise to a range of actual or potential harms to individuals and entities. As such, there is no single way of responding to a data breach. Each breach will need to be dealt with on a case-by- case basis, with an understanding of the risks posed by a breach and the actions that would be most effective in reducing or removing these risks. Generally, the actions taken following a data breach should follow four key steps: Step 1: Contain the data breach to prevent any further compromise of personal information."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 122, "chunk_id": "english_pack_122_0_204ba320", "content": "104 | MODULE 3: DATA BREACH PREPARATION & RESPONSE Step 2: Assess the data breach by gathering the facts and evaluating the risks, including potential harm to affected individuals and, where possible, taking action to remediate any risk of harm. Step 3: Notify individuals and the Commissioner if required. If the breach is an ‘eligible data breach’ under the NDB scheme, it may be mandatory for the entity to notify. Step 4: Review the incident and consider what actions can be taken to prevent future breaches. At any time, entities should take remedial action, where possible, to limit the impact of the breach on affected individuals. If remedial action is successful in preventing a likely risk of serious harm to individuals, the NDB scheme notification obligations may not apply. In general, entities should: • take each data breach or suspected data breach seriously and move immediately to contain, assess and remediate the incident. Breaches that may initially seem immaterial may be significant when their full implications are assessed • undertake steps 1 (Contain), 2 (Assess), and 3 (Notify) either simultaneously or in quick succession. In some cases it may be appropriate to notify individuals immediately, before containment or assessment of the breach occurs • determine how to respond on a case-by-case basis."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 123, "chunk_id": "english_pack_123_0_de9fe60c", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 105 Depending on the breach, not all steps may be necessary, or some steps may be combined. In some cases, an entity may take additional steps that are specific to the nature of the breach. The following diagram summarises the data breach response process. The parts of this process that are required by the NDB scheme are coloured red. The NDB scheme is explained in detail in Part 4 of this guide. Step 1: Contain Once an entity has discovered or suspects that a data breach has occurred, it should immediately take action to limit the breach. For example, stop the unauthorised practice, recover the records, or shut down the system that was breached. If it is not practical to shut down the system, or if it would result in loss of evidence, then revoke or change computer access privileges or address weaknesses in physical or electronic security. Addressing the following questions may help you identify strategies to contain a data breach: • How did the data breach occur? • Is the personal information still being shared, disclosed, or lost without authorisation? • Who has access to the personal information? • What can be done to secure the information, or stop the"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 124, "chunk_id": "english_pack_124_0_55d0c70f", "content": "106 | MODULE 3: DATA BREACH PREPARATION & RESPONSE unauthorised access or disclosure, and reduce the risk of harm to affected individuals? At this point, an entity may suspect an eligible data breach under the NDB scheme has occurred, which would trigger assessment obligations. Or the entity may believe the data breach is an eligible data breach, which requires them to notify individuals as soon as practicable. During this preliminary stage, be careful not to destroy evidence that may be valuable in identifying the cause of the breach, or that would enable the entity to address all risks posed to affected individuals or the entity. Step 2: Assess An assessment of the data breach can help an entity understand the risks posed by a data breach and how these risks can be addressed. It should be conducted as expeditiously as possible. Gather and evaluate as much information about the data breach as possible. By creating a complete picture of the data breach, an entity can ensure they understand the risk of harm to affected individuals and identify and take all appropriate steps to limit the impact of a data breach. This assessment should also assist entities in deciding whether affected individuals must be notified. In your assessment of a data breach, consider:"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 125, "chunk_id": "english_pack_125_0_4941db5a", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 107 • the type or types of personal information involved in the data breach • the circumstances of the data breach, including its cause and extent • the nature of the harm to affected individuals, and if this harm can be removed through remedial action. All entities should consider whether remedial action can be taken to reduce any potential harm to individuals. This might also take place during Step 1: Contain, such as by recovering lost information before it is accessed. Entities subject to the NDB scheme are required to assess ‘suspected’ eligible data breaches and take reasonable steps to complete this assessment within 30 days (see Assessing a suspected data breach). Criteria for assessing a data breach, including the risk of harm and remedial action, is explored in Identifying eligible data breaches. Step 3: Notify Notification can be an important mitigation strategy that has the potential to benefit both the entity and the individuals affected by a data breach. The challenge is to determine when notification is appropriate. Sometimes, notifying individuals can cause undue stress or harm. For example, notifying individuals about a data breach that poses very little or no risk of harm can cause unnecessary anxiety. It can also de-sensitise individuals so that they don’t take a notification seriously, even"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 126, "chunk_id": "english_pack_126_0_c2463302", "content": "108 | MODULE 3: DATA BREACH PREPARATION & RESPONSE when there is a real risk of serious harm. Each incident needs to be considered on a case-by-case basis to determine whether breach notification is required. Consider: • the obligations of the entity under the NDB scheme. Entities are required to notify individuals and the Commissioner about data breaches that are likely to result in serious harm. Part 4 of this guide provides further detail about the NDB scheme’s requirements • other circumstances in which individuals should be notified. For example, your entity may not have obligations under the NDB scheme, but have processes in place to notify affected individuals in certain circumstances • how notification should occur, including: • what information is provided in the notification • how the notification will be provided to individuals • who is responsible for notifying individuals and creating the notification? • who else other than affected individuals (and the Commissioner if the notification obligations of the NDB scheme apply) should be notified • where a law enforcement agency is investigating the breach, it may be appropriate to consult the investigating agency before making details of the breach public"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 127, "chunk_id": "english_pack_127_0_91786e78", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 109 • whether the incident triggers reporting obligations to other entities. Effective data breach response is about reducing or removing harm to affected individuals, while protecting the interests of your organisation or agency. Notification has the practical benefit of providing individuals with the opportunity to take steps to protect their personal information following a data breach, such as by changing account passwords or being alert to possible scams resulting from the breach. It is important that staff can engage with individuals who have been affected by a data breach with sensitivity and compassion, in order not to exacerbate or cause further harm. Notification can also help build trust in an entity, by demonstrating that privacy protection is taken seriously. Step 4: Review Once steps 1 to 3 have been completed, an entity should review and learn from the data breach incident to improve its personal information handling practices. This might involve: • a security review including a root cause analysis of the data breach • a prevention plan to prevent similar incidents in future • audits to ensure the prevention plan is implemented • a review of policies and procedures and changes to reflect"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 128, "chunk_id": "english_pack_128_0_0f08fd19", "content": "110 | MODULE 3: DATA BREACH PREPARATION & RESPONSE the lessons learned from the review • changes to employee selection and training practices • a review of service delivery partners that were involved in the breach. In reviewing information management and data breach response, an entity can refer to the OAIC’s Guide to securing personal information When reviewing a data breach incident, it is important to use the lessons learned to strengthen the entity’s personal information security and handling practices, and to reduce the chance of reoccurrence. A data breach should be considered alongside any similar breaches that have occurred in the past, which could indicate a systemic issue with policies or procedures. If any updates are made following a review, staff should be trained in any changes to relevant policies and procedures to ensure a quick response to a data breach. 3.4 Part 4: (OAIC) Notifiable Data Breach (NDB) Scheme The Privacy Act requires certain entities to notify individuals"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 129, "chunk_id": "english_pack_129_0_2267912c", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 111 and the Commissioner about data breaches that are likely to cause serious harm. The requirements of the NDB scheme are contained in Part IIIC of the Privacy Act and apply to breaches that occur on or after 22 February 2018. This part of the guide covers the following topics: • Entities covered by the NDB scheme. • Data breaches involving more than one entity. • Identifying eligible data breaches • Exceptions to the notification obligation • Assessing a suspected data breach • Notifying individuals about an eligible data breach • What to include in an eligible data breach statement • The Australian Information Commissioner’s role in the NDB scheme. Entities covered by the NDB scheme Key points: • Entities that have existing obligations under the Privacy Act to secure personal information must comply with the NDB scheme. • This includes Australian Government agencies, businesses and not-for profit organisations that have an annual turnover of more than AU$3 million, private sector health service providers, credit reporting bodies,"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 130, "chunk_id": "english_pack_130_0_45fcc916", "content": "112 | MODULE 3: DATA BREACH PREPARATION & RESPONSE credit providers, entities that trade in personal information and tax file number (TFN) recipients. • Entities that have Privacy Act security obligations in relation to types of information only (for example, small businesses that are required to secure tax file number information) do not need to notify about data breaches that affect other types of information outside the scope of their obligations under the Privacy Act. Australian Privacy Principles entities The NDB scheme applies to entities that have an obligation under APP 11 of the Privacy Act to protect the personal information they hold (s 26WE(1)(a)).11 Collectively known as ‘APP entities’, these include Australian Government agencies and private sector and not-for-profit organisations with an annual turnover of more than $3 million. The definition of APP entity generally does not include small business operators, registered political parties, state or territory authorities, or a prescribed instrumentality of a state (s 6C). However, some businesses of any size are APP entities, including businesses that trade in personal information12 and organisations that provide a health service to, and hold health information about, individuals (see Is my organisation a health service provider?). For more information about APP entities, see Chapter B of the Australian Privacy Principal Guidelines (APP Guidelines). Exempt acts and practices, including employee records."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 131, "chunk_id": "english_pack_131_0_19d0f2ba", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 113 The NDB scheme only applies to entities and personal information holdings that are already subject to security requirements under the Privacy Act. This means that acts and practices of APP entities that are exempt from the Privacy Act will also be exempt from the NDB scheme. For example, in some circumstances, private sector employers do not have to comply with the APPs in relation to employee records associated with current and former employment relationships (s 7B(3)). If an exempt employee record is subject to unauthorised access, disclosure or loss, the private sector employer does not have to assess the breach or notify individuals and the Commissioner. This exemption does not apply to TFN information that is contained within an employee record. However, given community expectations around the handling of their personal information, it is recommended that employers notify affected individuals where a breach of an employee record is likely to result in serious harm. Doing so will enable affected individuals to take protective action against any potential harms, as well as illustrating to employees that the security of their records is taken seriously. Further information about acts and practices that are exempt from the APPs and, by extension, the NDB scheme can be found in Privacy business resource 13: Application of the Australian Privacy Principles to the private sector."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 132, "chunk_id": "english_pack_132_0_17247681", "content": "114 | MODULE 3: DATA BREACH PREPARATION & RESPONSE Small business operators A small business operator (SBO) is an individual (including a sole trader), body corporate, partnership, unincorporated association, or trust that has not had an annual turnover of more than $3 million in any financial year since 2001 (s 6D). Generally, SBOs (Small Business Operator) do not have obligations under the APPs unless an exception applies (s 6D(4)). In certain circumstances an SBO must comply with the APPs, and therefore with the NDB scheme. That will be the case where the SBO: • holds health information and provides a health service • is related to an APP entity • trades in personal information. That is, the SBO discloses personal information about individuals to anyone else for a benefit, service or advantage; or provides a benefit, service or advantage through the collection of personal information about another individual from anyone else • are a credit reporting bodies • is an employee association registered under the Fair Work (Registered Organisations) Act 2009 • has ‘opted-in’ to APP coverage under s 6EA of the Privacy Act. If an SBO carries on certain activities it must comply with the"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 133, "chunk_id": "english_pack_133_0_b3265593", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 115 APPs, and therefore must comply with the NDB scheme, but only in relation to personal information held by the entity for the purpose of, or in connection with, those activities. Those activities include: • providing services to the Commonwealth under a contract • operating a residential tenancy data base • reporting under the Anti-Money Laundering and Counter-Terrorism Financing Act 2006 • conducting a protected action ballot • information retained under the mandatory data retention scheme, as per Part 5-1A of the Telecommunications (Interception and Access) Act 1979. More information about how to determine whether a business or organisation is an APP entity or subject to the APPs for some of its activities is available at Privacy business resource 10: Does my small business need to comply with the Privacy Act?.16 Credit reporting bodies A credit reporting body (CRB) is a business or undertaking that involves collecting, holding, using, or disclosing personal information about individuals for the purpose of providing an entity with information about the credit worthiness of an"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 134, "chunk_id": "english_pack_134_0_f1c5182f", "content": "116 | MODULE 3: DATA BREACH PREPARATION & RESPONSE individual (s 6P). Credit reporting information is defined as credit information or CRB derived information about an individual (s 6(1)). CRBs (Credit Reporting Body) have obligations under the NDB scheme in relation to their handling of credit reporting information (s 26WE(1)(b)), and in relation to their handling of any other personal information for which they have obligations under APP 11. Credit providers The NDB scheme applies to all credit providers whether they are APP entities. The section of the Privacy Act under which a credit provider is required to comply with the scheme will depend on what kind of information is involved in the data breach. If it is ‘credit eligibility information’ (defined in s 6(1)) the NDB scheme will apply because of the security requirement in s 21S (1) in relation to that information. If the credit provider is also an APP entity the NDB scheme applies in relation to other personal information because of the security requirement in APP 11. The organisations that are credit providers for the purposes of the Privacy Act (s 6G) are: • a bank • an organisation or small business operator if a substantial part of its business is the provision of credit,"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 135, "chunk_id": "english_pack_135_0_6ef35d05", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 117 such as a building society, finance company or a credit union • a retailer that issues credit cards in connection with the sale of goods or services • an organisation or SBO that supplies goods and services where payment is deferred for seven days or more, such as telecommunications carriers, and energy and water utilities • certain organisations or SBOs that provide credit in connection with the hiring, leasing, or renting of goods. An organisation or SBO that acquires the right of a credit provider in relation to the repayment of an amount of credit is also considered a credit provider, but only in relation to that credit (s 6K). For more information about categories of credit-related personal information, see Privacy business resource 3: Credit reporting – what has changed. TFN recipients The NDB scheme applies to TFN recipients18 in relation to their handling of TFN information (s 26WE(1)(d)). A TFN recipient is any person who is in possession or control of a record that contains TFN information (s 11). TFN information is information that connects a TFN with the identity of a particular individual (s 6). A TFN recipient may also be an APP entity or credit"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 136, "chunk_id": "english_pack_136_0_7ace78a9", "content": "118 | MODULE 3: DATA BREACH PREPARATION & RESPONSE provider. In certain circumstances, entities that are not otherwise covered by the Privacy Act, such as state and local government bodies, may also be authorised to receive TFN information and will be considered TFN recipients. The NDB scheme applies to TFN recipients to the extent that TFN information is involved in a data breach. If TFN information is not involved, a TFN recipient would only need to comply with the NDB scheme for breaches of other types of information if they are also a credit provider or APP entity. More information about TFN recipients is available in Privacy business resource 12: The Privacy (Tax File Number) Rule 2015 and the protection of tax file number information.19 Overseas activities Entities with an ‘Australian link’ The NDB scheme generally extends to the overseas activities of an Australian Government agency (s 5B (1)). It also applies to organisations (including small businesses covered by the Act, outlined above) that have an ‘Australian link’ (s 5B (2)). An organisation has an Australian link either because it is, in summary, incorporated or formed in Australia (see s 5B(1A) for more detail), or where: • it carries on business in Australia or an external Territory, and • it collected or held personal information in Australia or"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 137, "chunk_id": "english_pack_137_0_9d61d87a", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 119 an external Australian Territory, either before or at the time of the act or practice (s 5B (3)). Further information about entities that are taken to have an Australian link is available in Chapter B of the APP Guidelines. Disclosing personal information overseas If an APP entity discloses personal information to an overseas recipient, in line with the requirements of APP 8.1, then the APP entity is deemed to ‘hold’ the information for the purposes of the NDB scheme (s 26WC (1)). APP 8.1 says that an APP entity that discloses personal information to an overseas recipient is required to take reasonable steps to ensure that the overseas recipient does not breach the APPs in relation to the information. This means that if the personal information held by the overseas recipient is subject to loss, unauthorised access, or disclosure, the APP entity is still responsible for assessing whether it is an eligible data breach under the Privacy Act, and if it is, for notifying individuals at risk of serious harm and providing a statement to the Commissioner. There are exceptions to the requirement in APP 8.1 to take reasonable steps. APP entities that disclose information overseas under an exception in APP 8.2 are not taken to ’hold’ information they have disclosed overseas under s 26WC. In these circumstances, if the personal information held by the overseas recipient is subject to a data breach, the APP entity"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 138, "chunk_id": "english_pack_138_0_8f10632d", "content": "120 | MODULE 3: DATA BREACH PREPARATION & RESPONSE does not have obligations to notify about the breach under the NDB scheme. More information about APP 8 is available in Privacy business resource 8: Sending personal information overseas.21 Disclosing credit eligibility information If a credit provider discloses credit eligibility information about one or more individuals to a person, a body or a related body corporate that does not have an ‘Australian link’ (s 26WC(2)(a)), the credit provider may also have obligations under the NDB scheme in respect of that information. If credit eligibility information held by the person or related body corporate is subject to loss, unauthorised access, or disclosure, the credit provider is responsible for assessing whether there is an eligible data breach that needs to be notified to individuals at risk of serious harm and the Commissioner. 3.6 Cross-border Data Transfer Regulations Cross-border data transfer refers to the movement of personal data across national borders. In our interconnected world, many organizations transfer data internationally for various business purposes. However, this practice is subject to increasing regulation to protect individuals’ privacy rights."}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 139, "chunk_id": "english_pack_139_0_d416349e", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 121 Key Regulations 1. General Data Protection Regulation (GDPR) • Applies to. European Union (EU) and European Economic Area (EEA) • Key points: • Strict rules for transferring data outside the EU/EEA • Requires “adequate level of protection” for personal data • Uses mechanisms like Standard Contractual Clauses (SCCs) and Binding Corporate Rules (BCRs) 2. California Consumer Privacy Act (CCPA) • Applies to. Businesses dealing with California residents’ data • Key points: • Gives consumers right to know about data transfers • Requires businesses to protect personal information during transfers 3. Other Notable Regulations • Brazil’s General Data Protection Law (LGPD) • China’s Personal Information Protection Law (PIPL) • Japan’s Act on Protection of Personal Information"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 140, "chunk_id": "english_pack_140_0_d9c92bd1", "content": "122 | MODULE 3: DATA BREACH PREPARATION & RESPONSE (APPI) Common Requirements Across Regulations • Transparency. Inform individuals about data transfers • Purpose Limitation. Only transfer data for specified, legitimate purposes • Data Minimization. Transfer only necessary data • Security Measures. Protect data during transfer and storage • Individual Rights. Allow individuals to access, correct, or delete their data Challenges in Complying with Cross-border Data Transfer Regulations • Complexity. Different countries have different rules • Changing Landscape. Laws and interpretations evolve rapidly • Technological Challenges. Ensuring secure transfer methods • Contractual Requirements. Managing agreements with multiple parties • Data Localization Laws. Some countries require data to stay within their borders"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 141, "chunk_id": "english_pack_141_0_80a4ef22", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 123 Best Practices for Cross-border Data Transfers • Data Mapping. Know what data you have and where it goes • Risk Assessment. Evaluate risks associated with data transfers • Use Approved Transfer Mechanisms. Like SCCs or BCRs for GDPR compliance • Regular Audits. Check compliance periodically • Employee Training. Ensure staff understand data transfer rules • Documentation. Keep records of data transfers and compliance efforts Impact on Business Operations • Global Data Strategies. Companies need to rethink how they handle global data • Technology Infrastructure. May need to change IT systems to comply • Vendor Management. Careful selection and monitoring of third-party service providers • Customer Trust. Proper handling of data transfers can build customer confidence Cross-border data transfer regulations are becoming"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 142, "chunk_id": "english_pack_142_0_dd29d5c9", "content": "124 | MODULE 3: DATA BREACH PREPARATION & RESPONSE increasingly important in our global, data-driven economy. Understanding and complying with these regulations is crucial for businesses to operate legally and ethically in the international market. As regulations continue to evolve, staying informed and adaptable is key to maintaining compliance. 3.7 Case Study: Equifax Data Breach In July 2017, credit reporting agency Equifax were the victims of a significant data breach which resulted in an estimated 143 million U.S. records containing customer information being stolen by hackers. This included social security numbers, dates of birth, and the credit card details of over 209,000 Americans. The breach also impacted other countries, with Equifax admitting that 15.2 million records of British citizens and 8000 Canadians were stolen in the breach. There was over a month’s delay in disclosing the data breach. Senior executives were criticized for selling shares in the days before the breach was announced to the public. The intruders managed to gain access to the records using a weakness in a popular back-end website application. The vulnerability was made public in March 2017, but Equifax were slow to fix the bug in their networks, highlighting the importance of maintaining the latest patches. The Equifax hack had the markings of a sophisticated cyber-"}
{"pack_id": "english_pack", "section_title": "MODULE 3: DATA BREACH PREPARATION & RESPONSE", "page": 143, "chunk_id": "english_pack_143_0_24b4d84c", "content": "MODULE 3: DATA BREACH PREPARATION & RESPONSE | 125 attack, leading to speculation about attribution, with some in the cyber security community blaming Chinese-backed groups due to similarities with other attacks such as the U.S. Office of Personnel hack in 2017. The potential for the stolen Equifax data to be used in financial fraud has caused U.S. banks such as Citi Group and Wells Fargo to step up anti-fraud controls."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 144, "chunk_id": "english_pack_144_0_70b4cbbc", "content": "126 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL The SEI’s Incident Management Maturity Model is a practical framework that helps organizations assess and improve their capabilities for responding to security incidents. It is based on two existing models: the Security Incident Management Maturity Model (SIM3) and the ENISA CSIRT (Computer Security Incident Response Team) maturity approach. The SIM3 model was developed by the CSIRT community and has been applied by teams all over the world since 2009. It defines 44 indicators of maturity across four domains: organization, human, tools and processes. The ENISA CSIRT maturity approach was proposed by the European Union Agency for Cybersecurity (ENISA) and provides a three-tier classification of CSIRTs based on their services, cooperation, and quality management. The SEI’s Incident Management Maturity Model combines these two models and aligns them with the requirements of relevant EU policies, such as the NIS Directive. The model"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 145, "chunk_id": "english_pack_145_0_2ce30330", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 127 can be used by organizations to measure their current level of maturity, identify gaps and areas for improvement, and plan their development roadmap. The model also supports benchmarking and comparison among different organizations or sectors. 4.1 Overview SEI’s Incident Management Maturity Model. The Software Engineering Institute (SEI) has been at the forefront of American efforts to counter cyber threats for several decades. To this end, it has produced (in conjunction with others a maturity model that allows organisations to proactively evaluate and improve their ability to manage cyber security incidents. It is intended for process improvement, it does not measure how well a given incident management activity is performed, only that it is performed. The rationale behind this approach is to allow individual organisations to devise their own implementation, having been given sufficient guidance to do so. These incident management capabilities have evolved over many years. They are based on a set of metrics developed by the US Defense Information Systems Agency (DISA) and National Security Agency (NSA) in 2000-2002. The Department of"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 146, "chunk_id": "english_pack_146_0_34918c22", "content": "128 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Homeland Security (DHS) and United States Computer Emergency Readiness Team (US-CERT) funded the initial work to adapt the U.S. Department of Defense (DoD) version for Federal use in 2003–2005. There are multiple aspects to successfully managing computer security incidents. Usually, the primary focus is on the response actions to remedy the incident. As a result, the organization fails to adequately consider that there is more to incident management than reacting when a threatening event occurs. Being proactive is arguably more important than reactive alone; it is the combination of the two that works best. The capabilities listed here provide a baseline of incident management practices. The incident management capabilities—each including a series of indicators—define the benchmark. You can use these guidelines to assess how your current incident management functions are defined, managed, and measured. It provides the basis for improvements to the incident management function. What Are These Capabilities? The capabilities are used to evaluate an incident management function. In any sizeable organization, one or more groups will be involved in incident management. Each group has a set of its own goals, tasks, and activities (their mission) that must be completed to support the overall strategic mission of the"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 147, "chunk_id": "english_pack_147_0_eae17cab", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 129 organization. The capabilities in this report explore different aspects of incident management activities for protecting, detecting, and responding to unauthorized activity in an organization’s information systems and computer networks, as well as for establishing and sustaining the ability to provide those services. Each capability includes a set of indicators, which are used by an assessment team to determine whether a capability has successfully been achieved or met. The results from an assessment can help an organization determine the comprehensiveness of its incident management function. What We Mean by Incident Management Function (IMF) An incident management function is a set of capabilities (the people, processes, technology, etc. that provide an ability or capacity to perform some task) considered essential to protecting, detecting, and responding to incidents, as well as sustaining the incident management function (refer to Alberts and colleagues for more information [Alberts 2004]). These capabilities can be provided internally by security or network operators; be outsourced to managed security service providers (MSSPs); or be provided and managed by a computer security incident response team (CSIRT), security operations centre (SOC), or security team. We recognize that CSIRTs might not always be providing these capabilities."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 148, "chunk_id": "english_pack_148_0_e0c6e592", "content": "130 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL For the sake of simplicity, the term incident management personnel are generally used in this report to refer to the groups (or individuals) performing incident management capabilities. The term incident management function includes everyone who is involved in the performance of incident management activities or the incident management process. The term constituency is used to refer to those who receive the services provided by whoever is performing incident management activities. The term organization is used to refer to the entire group that is composed of the incident management personnel as well as their constituency. Occasionally we use the term CSIRT, which refers to a designated function or group of people to perform a portion of the incident management functions. Incident management capabilities are grouped into the five categories described in Table 1— Prepare, Protect, Detect, Respond, and Sustain. Each category contains a range of subcategories with a set of one or more capabilities. Each capability includes a set of indicators that describe the essential activities leading to adequate performance of that capability. Within the five major categories and many subcategories, each capability is assigned a priority. These priorities can be useful when making decisions about where to focus improvement efforts. • Priority I capabilities are critical services that an incident management function must provide."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 149, "chunk_id": "english_pack_149_0_c22c4045", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 131 • Priority II capabilities are the important services that should ideally be provided. • Priority III constitutes the remaining capabilities. They represent additional best practices that enhance operational effectiveness and quality. Table 1 – Prepare, Protect, Detect, Respond, Sustain – Categories and Su PREPARE PROTECT DETECT RESPOND SUST • Establish • Risk Assessment • Network and • Incident • IM • Prevention Systems Reporting • Function • Operational Security • Analysis • Core Exercises for Monitoring • Incident • Processes Incident • Threat and Response and Tools Management Situational • Training and Awareness Guidance • • Vulnerability • Management • Overview of the Major Categories (Chs 1 & 2) The next few paragraphs provide an overview of the major categories: Prepare, Protect, Detect, Respond, and Sustain."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 150, "chunk_id": "english_pack_150_0_5ef26bca", "content": "132 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Prepare Prepare focuses on establishing an effective, high-quality incident management function. This includes formally recognizing an incident management function, defining roles and responsibilities, and establishing interfaces between the various groups and individuals performing or affected by incident management functions. High-level processes must be defined, and essential tools, such as an incident tracking system, need to be acquired and embedded. Trusted relationships, both internal and external, are established for the purpose of sharing relevant and necessary information. Protect Protect relates to the actions taken to prevent attacks and to mitigate the impact of those that do occur. Preventative actions secure and fortify systems and networks, which helps to decrease the potential for successful attacks against the organization’s infrastructure. In this model, Protect is focused on what changes can be made to the infrastructure as part of the response to contain or eradicate the malicious activity. It also includes taking proactive steps to look for weaknesses and vulnerabilities in the organization while understanding new threats and risks. Such steps can include:"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 151, "chunk_id": "english_pack_151_0_68a7b0e5", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 133 • Performing security audits, vulnerability assessments, and other infrastructure evaluations to address weaknesses before they can be successfully exploited. • Collecting information on new threats and evaluating their impact Mitigation involves making changes in the constituent infrastructure to contain, eradicate, or fix actual or potential malicious activity. Such actions might include. • Making changes in filters on firewalls, routers, or mail servers to prohibit malicious packets from entering the infrastructure. • Updating intrusion-detection system (IDS) or anti-virus (AV) signatures to contain new threats. • Installing patches for vulnerable software Changes to the infrastructure may also be made, based on the process improvement changes and lessons learned that result from a post-mortem review done after an incident is handled. These types of changes are made to ensure that incidents do not happen again or that similar incidents do not occur. Detect In Detect, information about current events, potential incidents, vulnerabilities, or other security or incident management information is gathered proactively and"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 152, "chunk_id": "english_pack_152_0_7db868b3", "content": "134 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL reactively. With reactive detection, information is received from internal or external sources in the form of reports or notifications. Proactive detection calls for action by the designated staff to identify suspicious activity through monitoring and analysis of a variety of logging results, situational awareness, and evaluation of warnings about situations that can adversely affect the organization’s successful operation. Respond Respond includes the steps taken to analyse, resolve, or mitigate an event or incident. Such actions are targeted at understanding what has happened and what needs to be done to enable the organization to resume operation as soon as possible or to continue to operate while dealing with threats, attacks, and vulnerabilities. Respond steps can include: • Analysis of incident impact, scope, and trends. • Collection of computer forensics evidence, following chain-of-custody practices. • Additional technical analysis related to malicious code or computer forensics analysis. • Notification to constituents, stakeholders, and other involved parties of incident status and corresponding response steps. • Development and release of alerts, advisories, bulletins, or other technical documents."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 153, "chunk_id": "english_pack_153_0_77bc0b51", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 135 • Coordination of response actions across the organization and with other involved internal and external parties. • Verification and follow-up to ensure that response actions were correctly implemented, and that the incident has been appropriately handled or contained. Sustain Sustain focuses on maintaining and improving the CSIRT or incident management function itself. It involves ensuring that: • The incident management function is appropriately funded. • Incident management personnel are appropriately trained. • Infrastructure and equipment are adequate to support the incident management services and mission. • Appropriate controls, guidelines, and regulatory requirements are followed to securely maintain, update, and monitor the infrastructure. • Information and lessons learned from the Protect, Detect, and Respond processes are identified and analysed to help determine improvements for the incident management operational processes."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 154, "chunk_id": "english_pack_154_0_25657c95", "content": "136 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Explanation of the Capability Structure The capabilities are formatted in a workbook structure that can be used during an assessment to both conduct the assessment and capture information. The structure for each incident management capability provides two basic sets of information: • the capability itself, presented as a primary capability statement, and a more detailed set of indicators that can be used by the assessor to assess the performance of the capability. • explanatory information and scoring guidance—additional information explaining the significance of the capability and how to assess the performance of that capability. Each capability also includes a set of cross-references to selected regulations or guidance: the Federal Information Security Management Act (FISMA), National Institute of Standards and Technology (NIST) publications, and relevant best practices. Each capability includes indicators to assess the performance of that capability. Within these indicators, when the word personnel are used, it refers to whomever is performing the activities associated with the capability. If other"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 155, "chunk_id": "english_pack_155_0_d8ba3daf", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 137 roles or more specific types of roles are being referenced, the indicator will specify which type of personnel. These indicators are grouped into three areas: Required, Recommended Best Practice, and Institutional and Quality Improvement. All the indicators in the Required area must be met for an organization to successfully meet this capability. The indicators in the Recommended Best Practice area represent additional aspects that are recommended for a more complete or robust capability. The indicators in the Institutional and Quality Improvement area are those needed to ensure this capability can be sustained, that is, those things that would ensure the continuity or resilience of the capability even in the face of personnel changes. In addition, there are four types of indicators, specified by the italicized word occurring before the indicator statement: • Prerequisites must be met before this capability can be performed or be performed adequately. • Controls are available or exist that direct the proper execution of the activities. • Activities are performed as part of this capability (and could be observed by an assessor). • Quality indicators measure effectiveness, completeness, usefulness, institutionalization, and other quality aspects of the activities. To help the assessor use the tables, the following list explains"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 156, "chunk_id": "english_pack_156_0_79158a5a", "content": "138 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL how the information for each capability is organized. Reading the table from left to right, the fields are • Capability subcategory and number (e.g., 2.1 Risk Assessment) • Capability reference number and statement—represents major category number, subcategory number, and specific capability number and statement (e.g., 2.1.1 Security risk assessments (RAs) are performed on the organization.) • Priority—I through III (where priority I is the most important) • Clarification—additional information explaining the purpose and description of the capability team guidance—information to help an assessment team score this capability • References—standards, guidelines, or regulations relating to this capability, including a placeholder for organization-specific references • Organization response—optional field if early information was collected from an organization indicating how they would respond to the capability • Examples of evidence—list of possible evidence the team should look for during interviews, documentation reviews, or observations • Scoring criteria—the indicators (preceded by a unique indicator number), scoring choices (Yes/No), and room"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 157, "chunk_id": "english_pack_157_0_b7babec0", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 139 to list evidence (i.e., the specific criteria the assessors can see or examine during the assessment to help them determine whether the capability is being performed) • Final score— “Met” if all required indicators are met; “Not Met” if any required indicator is not met, Not Applicable—used when capability is excluded from scoring, Not Observed—used when capability was not observed during the assessment • Evidence collected place to identify what documents were reviewed, interviews conducted, or activities observed • Notes—additional notes made by the assessment team either in preparation for the assessment or during the assessment • Suggestions for improvement—additional ideas for an organization to consider if it works to improve this capability beyond implementing the concepts in each indicator 4.2 Performing Assessments A C2M2 capability assessment is a process of evaluating the maturity of an organization’s cybersecurity practices based on a standardized model. The C2M2 stands for Cybersecurity Capability Maturity Model, and it consists of the 10 domains (described earlier), such as Risk Management, Asset Management, Identity and Access Management, etc."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 158, "chunk_id": "english_pack_158_0_499e4ccc", "content": "140 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Each domain has a set of objectives and practices that describe different levels of capability, from 0 (Incomplete) to 3 (Optimized). To perform a C2M2 capability assessment, an organization follows these steps: • Select a facilitator and a team of participants who are familiar with the organization’s cybersecurity activities and processes. • Choose one or more domains to assess, depending on the scope and purpose of the assessment. • Review the C2M2 model and its components, such as the objectives, practices, indicators, and target states. • Conduct a self-assessment using the C2M2 toolkit, which provides a questionnaire and a scoring tool for each domain. • Analyse the results and identify the strengths and gaps in the organization’s cybersecurity capabilities. • Develop an action plan to address the gaps and improve the capabilities based on the priorities and resources of the organization. • Implement the action plan and monitor the progress and outcomes of the improvement efforts. • Repeat the assessment periodically to measure the changes and track the maturity level over time. A C2M2 capability assessment can help an organization to benchmark its cybersecurity performance, identify areas for"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 159, "chunk_id": "english_pack_159_0_033847b6", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 141 improvement, and align its practices with best practices and standards. Using These Capabilities to Assess the Incident Management Function of an Organization This section provides an overview of how the capabilities can be used to assess and improve an organization’s incident management function. This section and the next provide an overview of the assessment methodology and considerations for scoring the capabilities. To generalize, this assessment method centres around using interviews, artefact reviews, and activity observations to determine how completely the incident management activities represented in the capabilities are performed. It is possible to use these capabilities for a broad range of assessments. For example, the entire set of capabilities can be used to assess an organization’s entire incident management function. A subset can be used to focus on only the specific responsibilities of an actual SOC, CSIRT, or security service provider. The extent or scope of an assessment is determined early in the process, based on the goals of the organization or the specific focus of the assessment sponsor. The assumption for this section is that the entire incident management function is being assessed. An assessment with a narrower"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 160, "chunk_id": "english_pack_160_0_d7479a57", "content": "142 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL scope would simply use fewer capabilities and assess fewer groups. Incident management, as a complete function, includes activities that may be performed within a SOC, by a CSIRT, or by other groups across an organization. There may be several groups, each with some distinct or overlapping responsibilities that support management of cybersecurity events and incidents. In the latter case, applying these capabilities against only a designated centralized incident management function or CSIRT may result in an inaccurate or very limited view of the organization’s total ability to effectively manage cybersecurity incidents. An assessment should consider all groups performing incident management activities to produce accurate results. An assessment using these capabilities generally requires: • Assessment planning: establishing points of contact, assessment scope, schedule, and resources and assembling the assessment team and supporting equipment and supplies • Pre-assessment: preparing for on-site assessment activities; gathering information as needed before going onsite; analysing available documents and other artifacts; identifying groups and individuals (e.g., groups involved in Prepare, Protect, Detect, Respond, and Sustain activities) to interview onsite; allocating capabilities to those groups; and finalizing the onsite schedule"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 161, "chunk_id": "english_pack_161_0_2817139c", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 143 • Onsite: conducting interviews, observing activities, reviewing additional artefacts, documenting evidence collected, determining preliminary scores according to evidence rules, and gathering additional information, if possible, to fill any gaps • Post-assessment: performing final analysis and scoring and, optionally, identifying recommendations for improvement, producing a report for stakeholders, and conducting required reviews • Close-out: properly disposing or archiving of gathered information and conducting a “lessons learned” review Some specific guidance for selecting assessment activities follows. Identify the Groups Involved in Incident Management and Allocate the Capabilities There are many techniques for identifying the groups involved in incident management. One technique uses a process model benchmark for incident management, such as that described by Alberts and colleagues (2004). By comparing the organization to this process model of incident management activities, all groups performing such activities can be identified. An alternative is to use some form of work process modelling to map all groups and interfaces associated with incident management activities. Once the groups and activities are identified, capabilities can then be allocated to each group"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 162, "chunk_id": "english_pack_162_0_75a56170", "content": "144 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL (e.g., allocate Detect capabilities to the groups performing network monitoring). Bear in mind that there may not be clearly defined roles that align with the categories, and you may need to ask more than one group about the same set of capabilities to achieve complete coverage. While you can adjust your schedule of interviews and observations when onsite, it is best to keep schedule adjustments to a minimum. Assess Each Group The simplest means of assessing each group against its capabilities is to conduct interviews or group discussions, observe the activity being performed or a demonstration of the activity, and ask the assembled individuals about each capability that is applicable to their group. Artefacts related to the capabilities can be requested and reviewed and, when necessary, additional activities can be observed. The assessment team should use the general scoring guidance in Section 4 of the model and the specific guidance provided with each capability to guide its assessment. (See Section 2 of the model, “Explanation of the Capability Structure,” for a description of the sections and indicators provided for each capability.) When more than one group shares the responsibilities to perform a certain capability, the assessment team should conduct interviews (or group discussions, observations, or process demonstrations, as applicable) with at least two of the involved groups, and then compare and assess the collective"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 163, "chunk_id": "english_pack_163_0_8d2490a6", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 145 results from the different sources. (See Section 3.3 for further guidance about groups that cannot be assessed.) When the results for capabilities or individual indicators differ between groups, the lowest score generally prevails (i.e., if one individual or group indicates “Yes” to an indicator but another individual or group says “No,” the combined score for the organization for that indicator will generally be “No”). All indicators are scored as either Yes or No, and Capabilities are scored at the end as “Met,” “Not Met,” “Not Observed,” or “Not Applicable.” • “Met”—At a minimum, all the required indicators have been met. • “Not Met”—One or more of the required indicators has not been met. • “Not Observed”—A capability cannot be assessed because the assessment team does not have access to the individuals who can provide the correct answer or cannot observe that the activity or capability was performed. • “Not Applicable”—The activity is not included in the assessment, which may mean that it is deliberately not performed by the organization as part of the incident management processes. Capabilities that are not applicable should be identified during assessment scoping."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 164, "chunk_id": "english_pack_164_0_246d3adf", "content": "146 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Determine What to Do About Groups That Cannot Be Assessed Given the complexities and political realities of some organizations, it may not be possible to meet with some groups or obtain access to certain types of information. At the very least, the interface to that group or the way in which those groups interact should be assessed. The organization can then decide if those groups should be assessed later. Alternatively, those groups could assess themselves using applicable information from these capabilities and then provide the results (or feedback) to appropriate individuals. Another option is to use an external or third-party organization to perform the assessment on relevant groups. If part of the incident management function is outsourced and the organization being assessed can provide sufficient evidence to prove that the outsourced contractor or group is performing the capability, the outsourced contractor or group may not need to be assessed. If specific information cannot be reviewed, the assessment team and assessment sponsor will need to decide if the remaining evidence is sufficient to indicate an actual score or if “Not Observed” needs to be used. Use the Results to Decide What to Improve The organization, using the assessment results, has a clear idea of how it is meeting these capabilities with respect to incident"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 165, "chunk_id": "english_pack_165_0_ac7bdb8e", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 147 management. It knows what its strengths and weaknesses are. To improve the processes, the organization can look at the resulting scores and begin to create a strategy for improvement building on its strengths. For example, the candidates for improvement could be sorted by priority order, so that unmet Priority I capabilities come first, and so on. Existing strengths can be used to improve weaker areas. For example, if some capabilities have exceptionally good procedures and policies, use those as a basis for developing policies and procedures for capabilities that are not as robust or are missing. If there is a strong training program for some types of personnel, expand that program to include additional types of training for capabilities that are lacking. A further review of results may be needed when considering improvements in Priority II through Priority III capabilities. For example, improving a Priority III capability from “Not Met” to “Met” might be less critical than improving a Priority II capability from “Not Met” to “Met.” Each organization makes its own determination concerning the order in which to improve scores on any Priority II-III capabilities based on a review of the entire set and by considering the changes that are needed, the required resources, the mission, the goals, and the objectives. Finally, a common type of improvement for all the capabilities can be found by looking at the non-required indicators: Recommended Best Practices and Institutional and Quality Improvement indicators. These types of"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 166, "chunk_id": "english_pack_166_0_f42c9a66", "content": "148 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL improvements go beyond meeting the basic requirements and consider additional aspects that can build an exceptional incident management function. Even those capabilities for which required indicators were successfully met can be improved by implementing the non-required indicators. Each capability should be examined to consider the relative consequences of “doing” or “not doing” the capability or required indicators therein. This examination can provide elemental insight into whether improvement might yield an unexpected result. Look to the suggested improvements for ideas on enhancing performance or identifying ways to improve. When applying the capabilities to identify improvements, use judgment and common sense, respect the budgetary process, and stay abreast of changing regulations and standards in this ever-evolving environment. Ultimately, the end goal for these capabilities (or other types of assessments) is to strive for continuous improvement of the processes, so it is also a recommended best practice to periodically re-assess to see what new “current” state has been achieved. This re-assessment could be done annually or as conditions change (e.g., as new technologies are deployed, the infrastructure changes, or new partnerships or supply chains are adopted). These capabilities should be considered a starting place for identifying improvements. They are not a precisely defined path for every organization to build the perfect incident management function, but they can be used as a guideline for"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 167, "chunk_id": "english_pack_167_0_7c644182", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 149 what to include in an incident management function, based on the organization’s mission and the incident management function’s services. 4.3 Scoring the Capabilities General Guidance for Scoring Capabilities This section discusses scoring issues that the assessment team needs to remember as it is conducting an assessment. Each capability can have a score of “Met” or “Not Met.” To determine the score for a capability, the assessment team applies the rules of evidence against all the information gathered from interviews, demonstrations, observations, and document or artefact reviews. Interviews are question-and- answer sessions with one or more people with peer relationships where the assessment team uses the capabilities as the basis for asking questions. In observations, the assessment team watches one or more people conduct their actual IM activities; the team observes only and does not question or ask for additional actions. In demonstrations, the assessment team interacts with the people performing real or hypothetical IM activities, asking questions, getting demonstrations of what could occur, or how tools might be used in hypothetical situations. Observations and interviews are similar. Document"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 168, "chunk_id": "english_pack_168_0_eb603be7", "content": "150 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL or artefact reviews are conducted by assessment team members to understand relevant parts of IM-related documents. For each capability, all Required indicators must have an answer of “Yes” to obtain a successful or passing score for that capability (i.e., the capability is met). If one or more of the Required indicators has an answer of “No,” the score for the capability is “Not Met.” The Recommended Best Practice indicators and the Institutional and Quality Improvement indicators include those that are not necessarily required to achieve success for the capability but are recommended. These indicators are not included in the final determination of a capability being met or not met. They are currently provided for improvement purposes. See Section 4.3 for alternative scoring ideas. Evidence Collection Requirements Sufficient evidence for establishing a passing score requires more than one document, interview, observation, or demonstration. The indicators listed with each capability are used to assist in the collection of evidence. The Evidence column to the right of each indicator is used to record the type of evidence (e.g., interview, observation, demonstration, or document review) or a description of the evidence that was used to score that indicator. If a capability is to be scored “Met,” all Required indicators for that capability have been determined to be covered (checked “Yes”). The coverage rules for sufficiency of evidence"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 169, "chunk_id": "english_pack_169_0_50bba7f4", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 151 to determine if an indicator can be checked “Yes” are provided in Table 2 below. In summary, it takes at least two different types of sources to confirm an indicator. Note that in the rules for sufficiency, an interview and a demonstration are considered equivalent. An observation, then, needs the confirmation of an interview or demonstration, or a document review. A document review needs the confirmation from either an observation or a demonstration/interview. Also note that it takes at least one document, but in general, more than one document is preferred. Evidence Rules Interview/ Observation Demonstration Interview/ Not Sufficient Demonstration Not Observation Sufficient Document/Artifact Uicons by Flaticon"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 170, "chunk_id": "english_pack_170_0_0272125c", "content": "152 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Check Completeness and Quality of Documented Policies and Procedures When deciding if documented policies and procedures referenced in the indicators are adequate, assessment teams should consider the following: • Does the policy or procedure adequately address the process, technology, requirements, expected behaviours, or another topic it is supposed to address? • Do the procedures reflect what is done by personnel? • Are the policies and procedures easily available to personnel? • Are the policies or procedures being kept up to date? There should be a review and/or revision date or some indication that policies and procedures are reviewed and changed as needed. Also look for • a defined process and periodicity for reviewing and revising • established criteria for when to review (e.g., change in organization structure, major technology installation) • defined roles and responsibilities for review and update • a defined process for communicating changes and revisions throughout relevant parts of the organization • a change log history • indications the date was simply changed to make it look up to date5"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 171, "chunk_id": "english_pack_171_0_0937c9aa", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 153 It may also be useful to ask for any documents that are currently being revised to help evaluate their process for keeping documents up to date or to at least demonstrate that they are in the process of improving a current gap. Such findings will be useful when the organization decides what to improve. In most cases, policies (and processes) are included in the Required indicators, and documented, formal procedures are included in the Institutional and Quality Improvement indicators. Determine Personnel Knowledge of Procedures and Successful Training The assessment team should be able to determine from discussions with the personnel whether they understand the process (e.g., they are able to describe it intelligently and consistently). More importantly, the personnel should be able to easily show how they perform that work (e.g., show the forms that they fill in, describe the process they use to take information from an incident report that is displayed and extract information to feed into summary or other organizational or regulatory reports, or demonstrate how they perform analysis on a set of logs). A process can be consistently known and followed even without a formal, documented procedure. If a documented procedure does exist, the assessment team needs to determine if the procedure is followed. Training can range from formal training that has complete"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 172, "chunk_id": "english_pack_172_0_2c39983c", "content": "154 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL packages with materials and dedicated instructors to informal, on-the-job mentoring by more senior, experienced personnel. The assessment team seeks to determine whether training is provided, that the training is sufficient to meet the needs of the organization, and, as shown in the Institutional and Quality Improvement indicators, that the personnel are knowledgeable and perform the procedures consistently. During demonstrations, the assessment team can ask personnel to discuss the process they are following to show a level of understanding that supports knowledge of their capabilities about the activities being conducted. The observation of personnel performing tasks can also provide an indication of the maturity of their operations and training. For example, observation can show that personnel know the following: • how to use the tools that support the capabilities • where reports or data are archived • what types of information are contained in reports or alerts or other documents and products • where procedures, policy, or guidance documents are kept and how to access them if needed Scoring Variations It is possible for the assessment team and assessment sponsors to determine a different scoring algorithm (e.g., all the Required and Recommended Best Practice for a “Met” score)."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 173, "chunk_id": "english_pack_173_0_36cebd6a", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 155 The only caution would be to use a consistent scoring algorithm over time to allow for accurate determination of improvement from one assessment to the next or for accurate comparison between assessed groups. In addition to the “Met,” “Not Met,” “Not Observed,” or “Not Applicable” scores for a capability, some assessors have used a “Partial” score. “Partial” in this case would mean that some of the Required indicators have been met, but not all. “Partial” scores can be difficult to use as it becomes more subjective as to what percentage or number of Required indicators is needed to reach a “Partial” as opposed to a “Not Met” score. Some assessment teams have also found it useful to use “Not Observed,” or “Not Applicable” for the indicators as well as the capability. In that case, on the worksheet, the indicator can be scored as either a “No,” and the evidence column used to state the rationale for it being not observed, or scored as a “Yes,” with the rationale for it not being applicable in the evidence column. 4.4 The Capabilities The Incident Management Capabilities The remainder of this document contains Version 3.0 of the capabilities, split into five sections: • Prepare: Section 1 of the capabilities"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 174, "chunk_id": "english_pack_174_0_93824edc", "content": "156 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL • Protect: Section 2 of the capabilities • Detect: Section 3 of the capabilities • Respond: Section 4 of the capabilities • Sustain: Section 5 of the capabilities These capabilities are a living document. Periodic changes may be made to these capabilities, and new versions may be released. PREPARE: SECTION 1 OF INCIDENT MANAGEMENT CAPABILITIES Prepare is getting the incident management function up and operational. This includes getting the incident management function established, creating and implementing the necessary plans, defining the key work processes that will be essential to the smooth functioning of an incident management function, and establishing the necessary working relationships with both internal and external experts and groups who will provide needed assistance and expertise. Getting formal recognition and designation as an incident management function, regardless of whether it is a formal CSIRT, is essential to ensuring that the other parts of the organization understand and agree to accept the services provided and provide the required information to the incident management function. If that does not happen, the IM function may not be able to perform effectively. Defining roles, responsibilities, and interfaces among groups of people"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 175, "chunk_id": "english_pack_175_0_ce4ffc6e", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 157 performing incident management capabilities is needed to ensure everyone knows what their job is and how to work efficiently with other groups to detect, analyse, and respond to incidents. The plans that are developed will establish and sustain the incident management function in terms of how it will function, communicate, and deal with incidents when they occur. The core processes are needed to define how the various key activities will be carried out, and the essential tools needed by the incident management function must be acquired. Chief among these tools is the incident repository where all the information relevant to incidents will be retained. This repository allows not only the immediate analysis of current incidents but also later analysis for trends and patterns, forensic analysis, and so forth. Finally, no incident management function can be effective if it operates in isolation. IM personnel must establish trusted relationships with other experts to be aware of events and other types of attacks going on outside the organization and to reach back for additional expertise and help when faced with a new or unprecedented form of incident or the need for new tools. It takes time to get these relationships established and maintain them. This needs to be done as part of preparing. Within the Prepare category, the subcategories and their capabilities include the following:"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 176, "chunk_id": "english_pack_176_0_8d20a4bb", "content": "158 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Establish IM Function 1.1Establish IM Function—Establishing the IM function requires formal recognition and acceptance of its existence and its mission, who the people are who perform the activities and what they do and defining how it works with other groups. 1.1.1An incident management function or CSIRT has been officially designated by the organization head or chief information officer (CIO). 1.1.2An incident management plan has been developed and implemented for the organization. 1.1.3Roles and responsibilities are documented for key incident management activities throughout the organization and followed. 1.1.4Formal interfaces for conducting organizational incident management activities are defined and maintained. 1.1.5Trusted relationships are maintained with experts who can give technical and nontechnical advice and information. Core Processes and Tools 1.2Core Processes and Tools—An incident management function needs to establish the core practices and the basic tools that will be required for effective performance of incident management activities. That includes understanding how"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 177, "chunk_id": "english_pack_177_0_dceebd60", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 159 work will be managed, incident information will be retained, and how the potential for insider threat can be controlled. 1.2.1A communication plan for incident management activities has been established and disseminated. 1.2.2An IM information management plan is established and followed. 1.2.3An inventory exists of mission-critical systems and data. 1.2.4Workflow management processes and/or systems are implemented. 1.2.5A central repository exists for recording and tracking security events and incidents. 1.2.6Security events and incidents are categorized and prioritized according to organizational guidance. 1.2.7An insider threat program exists within the organization Refer to Incident Management Capability Assessment Workbook. December 2018 TECHNICAL REPORT CMU/SEI-2018-TR-007"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 178, "chunk_id": "english_pack_178_0_9415577a", "content": "160 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL 4.5 Incident Management Capabilities List of Incident Management Capabilities"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 179, "chunk_id": "english_pack_179_0_f75ed41e", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 161 A simple list of all the capability statements contained in the SEI-CMU’s Cybersecurity Maturity Model. Capabilities Priority Prepare Establish IM Function 1.1.1 An incident management function or CSIRT has been officially designated by the II organization head or chief information officer (CIO). 1.1.2 An incident management plan has been developed and implemented for the I organization. 1.1.3 Roles and responsibilities are documented for key incident I management activities throughout the organization and followed. 1.1.4 Formal interfaces for conducting organizational incident management I activities are defined and maintained. 1.1.5 Trusted relationships are maintained with experts who can give technical and III nontechnical advice and information. Core Processes and Tools 1.2.1 A communication plan for incident management activities has II been established and disseminated."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 180, "chunk_id": "english_pack_180_0_b2a5ad59", "content": "162 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL 1.2.2 An IM information management plan is II established and followed. 1.2.3 An inventory exists of mission-critical I systems and data. 1.2.4 Workflow management processes and/or III systems are implemented. 1.2.5 A central repository exists for recording and tracking security events I and incidents. 1.2.6 Security events and incidents are categorized and prioritized according to II organizational guidance. 1.2.7 An insider threat program exists within I the organization. Protect Risk Assessment 2.1.1Security risk assessments (RAs) are performed on the I constituents’ organization. 2.1.2The constituents get help correcting problems identified through security risk II assessment (RA) activities. Prevention 2.2.1The organization has an institutionalized I malware prevention program."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 181, "chunk_id": "english_pack_181_0_f32394b1", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 163 Operational Exercises for Incident Management 2.3.1Operational exercises are conducted to assess the IM function II of the organization. Training and Guidance 2.4.1 Guidance is provided to constituents on best practices for II protecting their systems and networks. 2.4.2 Constituents are provided with security education, training, and awareness I (ETA). Vulnerability Management 2.5.1 A patch management and alert program I exists. 2.5.2 Proactive vulnerability assessment is performed on constituent networks and I systems. 2.5.3 Constituents receive help to correct problems identified by vulnerability II assessment activities. Detect Network and Systems Security Monitoring 3.1.1 Security monitoring is continuously performed on all constituent networks and I systems."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 182, "chunk_id": "english_pack_182_0_9f5f18ae", "content": "164 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL External Sources of Incident Information 3.2.1 Events and incidents are reported from I outside the organization. Threat and Situational Awareness 3.3.1 Public monitoring of external security websites and other trusted sources of I information is conducted. 3.3.2 Trend analysis is supported and II conducted. 3.3.3 Network and system configurations or rule sets are reviewed and updated in response to changes in the threat I environment, and constituents are notified of the updates. 3.3.4 Penetration testing is conducted on I organizational networks and systems. Respond Incident Reporting 4.1.1 Events and incidents are reported from the I constituency. 4.1.2 Incidents are reported to appropriate management in I accordance with organizational guidelines."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 183, "chunk_id": "english_pack_183_0_d187da90", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 165 4.1.3 Incidents are reported to and coordinated with the appropriate external organizations or groups in I accordance with organizational guidelines. 4.1.4 Incident management is supported for restricted information, networks, and I systems. Analysis 4.2.1 Incident management personnel conduct I triage of events and incidents. 4.2.2 Incident analysis is performed on declared I incidents. 4.2.3 Incident correlation is performed to II identify similar activity. 4.2.4 Impact of an incident is determined. II 4.2.5 Incident root cause analysis is conducted. II 4.2.6 Fusion analysis is performed to identify concerted attacks and shared III vulnerabilities. 4.2.7 Retrospective analysis is conducted. III 4.2.8 Media analysis is performed on II constituent networks and systems. 4.2.9 Artifact or malware analysis is conducted. II"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 184, "chunk_id": "english_pack_184_0_f51755d0", "content": "166 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Incident Response 4.3.1 General incident response guidance and procedures are II distributed to constituents. 4.3.2 Incidents are resolved. I 4.3.3 Incident management personnel coordinate incident I response across stakeholders. 4.3.4 Incident management personnel create alerts and warnings and distribute I them as needed. 4.3.5 Incident management personnel verify that a response is implemented, as I appropriate, and that the incident is closed, in accordance with organizational guidance. 4.3.6 Postmortem reviews of significant incidents are conducted, and lessons I learned are identified and acted upon, as appropriate. Sustain MOUs and Contracts 5.1.1 A list of incident management services provided by the designated incident II management function is documented. 5.1.2 The constituency provides advance notification of all changes or planned III outages to their networks."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 185, "chunk_id": "english_pack_185_0_39c2bac3", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 167 5.1.3 Formal agreements exist for managing IM activities with third parties across the I supply chain. Project/Program Management 5.2.1 A financial plan exists for incident III management activities. 5.2.2 A workforce plan exists for incident II management personnel. 5.2.3 A personnel security plan exists for I incident management personnel. 5.2.4 A quality assurance (QA) program exists to ensure the quality of provided II products and services. 5.2.5 An established plan exists to ensure continuity of operations for incident I management. 5.2.6 The effectiveness of the incident management function in meeting its III mission is routinely evaluated and improved. IM Technology Development, Evaluation, and Implementation 5.3.1 The incident management function has I the tools it needs to meet its mission. 5.3.2 Software tools are tested for use within the incident II management environment."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 186, "chunk_id": "english_pack_186_0_090070fb", "content": "168 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL 5.3.3 The IT infrastructure for incident management is adequate to I support incident management operations. Personnel 5.4.1 A training program exists for incident I management personnel. 5.4.2 Support for professional development exists for incident III management personnel. Security Administration 5.5.1 Physical protective measures are in place to protect incident management IT systems, I facilities, and personnel. 5.5.2 An operations security (OPSEC) program I exists. IM Information Systems 5.6.1 An inventory exists of mission-critical incident management systems, data, and I information. 5.6.2 Defense-in-depth strategies and methodologies exist for hardening the I incident management computer networks and systems. 5.6.3 Processes and technologies exist to support the confidentiality, integrity, and I availability of incident management data and information."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 187, "chunk_id": "english_pack_187_0_448ffb21", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 169 5.6.4 Network security monitoring is performed on all incident-management- I related networks and systems. 5.6.5 Security risk assessments (RAs) are performed on the incident management I function. 5.6.6 Vulnerability assessments are performed on incident management I systems and networks. 5.6.7 A patch management program is in place for the incident management I systems. 5.6.8 More than one communications system or mechanism (other than email) exists for receiving and distributing II notifications, information about new viruses, incidents, vulnerabilities, threats, and other kinds of warnings. 4.6 Cybersecurity Capability Maturity Model (C2M2) The Cybersecurity Capability Maturity Model (C2M2) is a tool developed by the US Dept of Energy (DOE) to give organizations the means to consistently assess their cybersecurity capabilities. The assessment highlights way to improve an organisation’s cybersecurity capability. In this regard, the model helps organizations identify their"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 188, "chunk_id": "english_pack_188_0_9ce00cfe", "content": "170 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL current level of cybersecurity maturity and develop a roadmap for improving their cybersecurity posture over time. The C2M2 is based on the SEI’s Capability Maturity Model Integration (CMMI) framework, which is widely used in software engineering and other industries to assess and improve organizational processes. The basic concept of the ‘capability maturity models’ has been applied across various industries and professions owing to its simple conceptual design and adaptability. They simply establish the comprehensive range of processes that must be performed for given discipline, then measures how well a particular process is being performed. One of the advantages of the CMM (Capability Maturity Model) concept is that they are ‘process model’ that describes the process, but not how to execute the process, leaving that for the organisation to devise their own ways and means on the assumption that they know their business best. A ‘one size fits all’ model that prescribes the ‘how’ would not work in practice. With this flexibility of application, but encompassing all required activities, a maturity model becomes an excellent process improvement tool. The concept was originally devised in the 1980’s by Watts Humphrey at the Software Engineering Institute at Carnegie-Mellon University in Pittsburgh. There was a need to establish the means for the US Dept of Defense to evaluate the software development capabilities of suppliers"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 189, "chunk_id": "english_pack_189_0_b9e9bb9e", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 171 of software intensive products to the DoD. And to provide those suppliers with the means to improve their capability. The key rationale behind the design of CMMs (Capability Maturity Model) can be summarized as follows: Assessing Current Capabilities – CMMs aim to provide a systematic and standardized way of evaluating an organization’s current capabilities in a specific area. By assessing their capabilities against predefined maturity levels, organizations can gain insights into their strengths, weaknesses, and areas for improvement. This assessment helps organizations identify gaps and set realistic goals for enhancing their performance. Establishing a Common Language – CMMs create a common language and shared understanding within an organization and across industries. They define key concepts, processes, and practices related to a specific domain, enabling organizations to communicate and collaborate effectively. This common language facilitates knowledge sharing, benchmarking, and comparison among different organizations. Providing a Roadmap for Improvement – CMMs offer a structured roadmap for organizations to enhance their capabilities incrementally. By defining maturity levels and associated practices, CMMs provide organizations with a clear progression path. This roadmap allows organizations to prioritize and focus their efforts on areas that require"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 190, "chunk_id": "english_pack_190_0_d45eed9a", "content": "172 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL improvement, ensuring a systematic and step-by-step approach to maturity enhancement. Encouraging Continuous Improvement – CMMs emphasize the importance of continuous improvement and ongoing development. They recognize that maturity is not a static state but rather a journey of constant growth and evolution. CMMs encourage organizations to adopt a culture of learning, innovation, and adaptation, fostering a mindset of continuous improvement in their practices, processes, and performance. Enabling Benchmarking and Best Practices – CMMs facilitate benchmarking against industry best practices and standards. They provide organizations with a reference point to compare their capabilities with peers and industry leaders. This benchmarking allows organizations to identify areas where they lag and learn from others’ successes. It promotes knowledge sharing and collaboration, ultimately driving overall industry advancement. Supporting Decision-Making and Resource Allocation – CMMs help organizations make informed decisions and allocate resources effectively. By providing a structured assessment of capabilities and areas for improvement, CMMs enable organizations to prioritize investments, allocate resources efficiently, and address critical gaps. This data-driven approach ensures that resources are allocated based on identified needs and strategic objectives. In summary, the design of capability maturity models is"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 191, "chunk_id": "english_pack_191_0_c620409b", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 173 grounded in the principles of assessment, improvement, common understanding, roadmap development, continuous learning, benchmarking, and resource allocation. CMMs serve as valuable tools for organizations to enhance their capabilities, establish industry best practices, and achieve higher levels of performance in a structured and systematic manner. 4.7 C2M2 Maturity Levels The C2M2 consists of five maturity levels, each with a set of capabilities that organizations must demonstrate to achieve that level. The five levels are: Initial (Level 1) At this stage, cybersecurity practices are ad hoc and unorganized. The organization has limited awareness of cybersecurity risks and lacks a formal strategy. There may be a reactive approach to security incidents, and the focus is primarily on resolving immediate issues rather than implementing preventive measures. The primary goal at this level is to establish a foundation for a structured cybersecurity program. Managed (Level 2) At the managed level, the organization starts implementing"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 192, "chunk_id": "english_pack_192_0_dc725dce", "content": "174 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL basic cybersecurity controls and processes. There is a defined and documented cybersecurity policy and strategy. The organization has a better understanding of its critical assets and associated risks. Incident response plans and procedures are established, and regular vulnerability assessments are conducted. The focus at this level is on establishing a management framework for cybersecurity. Defined (Level 3) The defined level signifies a higher level of cybersecurity maturity. At this stage, the organization has a well-defined and documented set of cybersecurity processes and controls. Policies, procedures, and standards are in place and communicated throughout the organization. Risk management processes are established, and cybersecurity responsibilities are clearly defined. Security awareness training programs are conducted for employees, and regular audits and assessments are performed to ensure compliance. Quantitatively Managed (Level 4) At this level, the organization focuses on quantifying and measuring its cybersecurity capabilities. The organization"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 193, "chunk_id": "english_pack_193_0_d5bdbad3", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 175 collects and analyses security metrics to assess the effectiveness of its controls and processes. Risk assessments are performed regularly, and security incidents are tracked and monitored using advanced tools and technologies. Continuous improvement is a key aspect at this level, with the organization using data-driven insights to enhance its cybersecurity capabilities. Optimized (Level 5) The optimized level represents the highest level of cybersecurity maturity. At this stage, the organization has a proactive and adaptive approach to cybersecurity. It continually monitors emerging threats and incorporates them into its security strategy. The organization actively participates in information sharing and collaboration with industry peers and government entities. It leverages advanced technologies, such as artificial intelligence and machine learning, to detect and respond to cyber threats in real-time. Regular testing, simulations, and exercises are conducted to ensure the effectiveness of cybersecurity controls and response plans."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 194, "chunk_id": "english_pack_194_0_0999c7df", "content": "176 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL 4.8 Progressing up levels Once the organisation has implemented all the processes and controls associated with one level they can proceed to the next. And not before. In this structured way, the Cybersecurity Capability Maturity Model lays out a definitive roadmap for organizations to identify their current maturity level, to set goals for improvement, and continuously improve their cybersecurity capabilities. The assessment of an organization’s maturity level is typically conducted through an assessment of its existing cybersecurity practices, policies, procedures, and technical controls. This assessment involves interviews, documentation reviews, and technical assessments. The results are then mapped against the maturity levels defined in the model to determine the organization’s current level and identify areas for improvement. Evidence that processes are being performed is required when doing assessments. This structured approach to building a robust cybersecurity program brings alignment with industry best practices and regulatory requirements. Customers may be interested to know a potential supplier’s maturity level and might prescribe a minimum level as a condition of doing business and integrating the organisation into a supply chain."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 195, "chunk_id": "english_pack_195_0_7493797a", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 177 4.9 The C2M2 Domains The C2M2 has comprehensive list of 10 domains that must be addressed to achieve each maturity level: • Asset Management • Access Control • Awareness and Training • Data Security • Incident Response • Maintenance • Protective Technology • Risk Management • Situational Awareness • System and Communications Protection Each domain is further divided into the maturity levels discussed above, which represent the degree to which the organization has implemented the associated cybersecurity practices. Each of the domains listed above will have a maturity rating determined by the assessment. Typically, an organisation will have varied results across the domains, with some being performed more rigorously than others. Again, the maturity levels in brief are: • Initial: The organization has not yet implemented any"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 196, "chunk_id": "english_pack_196_0_25484d1a", "content": "178 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL cybersecurity practices in this domain. • Repeatable: The organization has implemented some cybersecurity practices in this domain, but they are not consistently applied. • Defined: The organization has defined cybersecurity practices in this domain, and they are consistently applied. • Managed: The organization has established a process for managing cybersecurity in this domain. • Optimized: The organization has continuously improved its cybersecurity practices in this domain. Organizations therefore use the C2M2 to consistently measure their cybersecurity capabilities over time, to identify target maturity levels based on risk, and to prioritize the actions and investments that allow them to meet their targets. It is advisable to present the C2M2 as a useful tool for improvement, not as a kind of audit like the tax man might do to uncover wrongdoing. People become defensive if the wrong perception of this valuable tool for any organization that wants to improve its cybersecurity posture. The C2M2 is aligned with internationally recognized cyber standards and best practices. 4.10 Benefits In summary, the benefits of using the C2M2:"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 197, "chunk_id": "english_pack_197_0_cd827b58", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 179 • Identify and prioritize cybersecurity risks. • With a roadmap for improving cybersecurity capabilities. • Measure their progress over time. • Align cybersecurity with business objectives. • Comply with cybersecurity regulations. For more about the C2M2, you can visit the website: https://c2m2.doe.gov/ . The website provides a wealth of information about the model, including the model documentation, case studies, and resources for implementation. 4.11 DevSecOps and Continuous Security Integration DevSecOps is a practice that combines software development (Dev), security (Sec), and IT operations (Ops). It aims to make everyone responsible for security at every stage of the software development process. Key Principles of DevSecOps • Shift Left. Move security earlier in the development process • Automate Security. Use tools to automatically check"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 198, "chunk_id": "english_pack_198_0_9920aed9", "content": "180 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL for security issues • Continuous Monitoring. Always watch for new security threats • Collaboration. Encourage teamwork between developers, security experts, and operations staff Benefits of DevSecOps • Faster, Safer Releases. Catch and fix security issues early • Improved Security Posture. Build security into the product from the start • Cost Reduction. Fixing security issues early is cheaper than fixing them later • Better Compliance. Easier to meet regulatory requirements Continuous Security Integration Continuous Security Integration is a key part of DevSecOps. It means constantly checking for and fixing security issues throughout the development process. Key Components of Continuous Security Integration • Automated Security Testing. Use tools to automatically test code for security flaws • Security in CI/CD Pipelines. Include security checks"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 199, "chunk_id": "english_pack_199_0_9c9409c2", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 181 in the continuous integration and deployment process • Regular Vulnerability Scans. Frequently check systems for known vulnerabilities • Dependency Checks. Make sure all software components and libraries are secure and up-to-date Implementing DevSecOps and Continuous Security Integration • Choose the Right Tools. Select tools that fit your development process and security needs • Train Your Team. Ensure everyone understands security principles and practices • Start Small. Begin with a few key security checks and gradually add more • Measure and Improve. Track security metrics and continuously improve your processes Challenges in Adopting DevSecOps • Cultural Change. Getting everyone to think about security can be difficult • Tool Integration. Making all the security tools work together smoothly • Balancing Speed and Security. Ensuring security doesn’t slow down development too much • Skill Gaps. Team members may need to learn new skills"}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 200, "chunk_id": "english_pack_200_0_22c7d457", "content": "182 | MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL Best Practices for DevSecOps • Use Infrastructure as Code. Manage and provision infrastructure through code • Implement Least Privilege. Give users only the access they absolutely need • Conduct Regular Security Training. Keep the team updated on the latest security threats and practices • Perform Threat Modelling. Identify potential threats early in the development process Future Trends in DevSecOps • AI and Machine Learning. Using smart technologies to predict and prevent security issues • Cloud-Native Security. Adapting DevSecOps practices for cloud environments • Shift-Right Testing. Extending security testing into production environments DevSecOps and Continuous Security Integration are changing how organizations approach software development and security. By integrating security throughout the development process, companies can create safer products faster and more efficiently. As cyber threats continue to evolve, these practices will become increasingly important in the world of cybersecurity."}
{"pack_id": "english_pack", "section_title": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL", "page": 201, "chunk_id": "english_pack_201_0_cb8a26ed", "content": "MODULE 4: CYBERSEC INCIDENT MANAGEMENT MATURITY MODEL | 183 References Alberts, Chris; Dorofee, Audrey; Killcrece, Georgia; Ruefle, Robin; & Zajicek, Mark. Defining Incident Management Processes for CSIRTs: A Work in Progress. CMU/ SEI-2004-TR-015 ADA453378. Software Engineering Institute, Carnegie Mellon University. 2004. https://resources.sei.cmu.edu/library/asset- view.cfm?assetid=7153"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 202, "chunk_id": "english_pack_202_0_796db84f", "content": "184 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES Meta-cognition in the context of ethical decision making as is the conscious awareness and control of one’s thinking processes. It involves thinking about thinking, understanding how you learn, and making adjustments to improve learning outcomes. It’s the foundation for effective study skills and problem-solving, allowing individuals to adapt their strategies and monitor their own comprehension and learning progress. Ethical decision-making is the process of evaluating and choosing between moral dilemmas. It’s a vital skill for IT professionals facing complex ethical choices. This process involves considering the ethical principles and values that guide one’s actions, as well as the consequences of those actions on individuals and society. Ethical decision-making helps individuals navigate difficult moral issues with integrity and responsibility. Ethical theories provide structured frameworks for"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 203, "chunk_id": "english_pack_203_0_e0de210d", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 185 understanding what is morally right or wrong. These theories offer different approaches to evaluating ethical questions. Understanding these ethical theories enables individuals to engage in informed ethical debates and make well-reasoned moral judgments. Each theory offers unique perspectives on how to address ethical dilemmas and make principled decisions. 5.1 How do we define ethics? Ethics is the general name for the branch of moral philosophy that deals with behaviour that increases people’s well-being. Ethics in the context of this book is therefore about how technologists should behave to increase people’s well-being. Ethics is not about religion or being slavishly law-abiding, nor is it about going along with the majority view held by the people around you. Ethics is having your own moral compass. Technology is ethical when it helps people reach their fullest potential; when it improves their quality of life, makes them happier and more fulfilled, and gives them the freedom to choose what they want to be. We consider the interests of people living here and now, but also the interests of future generations, other living creatures, and the preservation of the environment. Technology is unethical when it dehumanises; when it makes a person less human than they were. It forces people to engage in behaviour that diminishes them or the environment"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 204, "chunk_id": "english_pack_204_0_30283faa", "content": "186 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES in some way or creates a problem for future generations. Simply put, ethics is a guide to how to live well, how to be in the world in a way that creates benefit and minimises harm. Why do we need ethics? Ethics allows us to live in harmony and cooperation with others. When people are ethical, we can trust one another. We can build communities and organisations that can achieve outcomes that a single, self-interested individual would be incapable of. Without ethics, we would lack loyalty and be unable to trust others and form cooperative communities of interest. Long- term relationships would be difficult if not impossible. We could not have the economies that now exist in the developed world where wealth and a high standard of living are enjoyed by most. Life without ethics would likely be nasty, short, and brutish. Levels of ethics Ethics or right behaviour has three broad levels of application: Personal ethics guide how you live, what you do, and how you interact with others. It helps you to develop a sense of personal responsibility by making you think, both before and after you act. It considers how your behaviour impacts on others. As a rational being with free will, you choose how"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 205, "chunk_id": "english_pack_205_0_40e95ca5", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 187 you behave on a day-to-day basis with full awareness of the consequences of your actions. Organisational ethics is an aspect of organisational culture. It is how the organisation behaves and how it interacts with people. This level of ethics has explicit and implicit components. The explicit is clearly stated by management, written down and understood to be ‘correct’ behaviour. The implicit is not written down but is nonetheless understood to be the ‘way things are done’. As with personal ethics, this middle level should cultivate a sense of responsibility for how the organisation’s actions impact on the world. System ethics is concerned with how the overall economic and social systems behave, how it interacts with people. Ethics at the system level is codified into laws and codes of acceptable conduct; cultural practices that by consensus are widely understood and practices. As with the previous two levels, systems ethics cultivates a sense of responsibility for how the system impacts on the world in general. System ethics tries to create a system that best serves the interests of the greatest number of people. As a citizen, you have a right to vote and to have your voice heard. You are free to argue for a more humane society. Values & ethics Values feed into ethics in four broad ways; (a) how to get along"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 206, "chunk_id": "english_pack_206_0_7825e6fd", "content": "188 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES with each other, (b) what is a ‘good life’, (c) what are our obligations to each other, and (d) what are my rights? If ethics is about behaviour, values are about what you believe to be important, and what you would like to see more of by means of more ethical behaviour. For example, in western-style democracies, values are codified into ‘rights’. Freedom of speech, freedom of religious worship, the pursuit of happiness and many other values are all considered to be our birth right as human beings. Values come before ethics. The ethical standards of a society will reflect these pre-existing values. Values come from many sources; one’s family, the media, religion, the community, one’s education and life experiences. Values change over time with the evolution of societies and culture. While it is true that much of our value system is created through our childhood experiences, they can nonetheless be changed through a process of conscious self- reflection and external influences. Roles & ethics The roles we play have a strong determining effect on our ethics and on our behaviour generally. A role is simply a set of relationship responsibilities and expectations that we have adopted either voluntarily, or because they have been placed upon us through circumstance. The first experience of roles for many is within the early"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 207, "chunk_id": "english_pack_207_0_8cf7059d", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 189 environment where a child has a role in relation to their parent(s) or carers. Later we adopt a variety of roles by choice; we choose to get married, have children, and enter an occupation or profession. We might join a faith community and attend worship. We might become a volunteer for a worthy cause, or indeed any number of possible roles. Each role has a set of responsibilities and expectations that belong to it and which we must fulfil if we are not be sanctioned in some way. Roles can come into conflict with each other, for example a member of a religious community might find a role conflict if s/he were to perform military service. The obligations that go along with a role can form the basis of ethical conduct for that person. 5.2 Ethics is meta-consciousness Ethics is the general name for the branch of moral philosophy that deals with behaviour that increases people’s well-being. Ethics in the context of this book is therefore about how technologists should behave to increase people’s well-being. Ethics is not about religion or being slavishly law-abiding, nor is it about going along with the majority view held by the people around you. Ethics is having your own moral compass. Meta-cognition involves actively engaging the recently evolved parts of the brain, the places where higher, rational"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 208, "chunk_id": "english_pack_208_0_9882b80c", "content": "190 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES thought occurs, the place where you can recognise the causal links. This state of mind contrasts with the semi-conscious autopilot that people commonly use as their default setting. Reacting to situations in a habit-driven, stimulus-response manner based on prior learning. Conditioned responses to specific situations have been acquired over time from social learning. Thus, a poorly programmed autopilot is why people continue to make the same mistakes time and again. Meta- cognition is the only remedy to lift oneself out of this semi- conscious mode into a fully conscious state where a person responds to situations in a rational way that is based on the needs of the situation at hand. This rational, meta-cognitive ability is what sets humans apart from intelligent animals. The neural infrastructure of the evolved human brain is said by neuroscience to be the most complex biological structure ever to have existed on this planet. Our brains and the abstract thinking that it is capable of is what has made humanity the most adaptable creature living on this planet. 5.3 Codes of ethical conduct Computer societies are working towards licensing its members so that like doctors, lawyers, teachers, accountants and other professions, it is not lawful to work unless you are licensed. To be licensed, a practitioner must have completed an approved"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 209, "chunk_id": "english_pack_209_0_1b6745ac", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 191 study program that includes instruction on professional ethics. They must agree to abide by the code of conduct. This chapter presents typical code of conduct, based on the Australian Computer Society’s (ACS) code. This code is used since The Ethical Technologist is the textbook used in an ethics course at an Australian university. We might just as well have a code from North America, the United Kingdom, Japan, Germany, France or any country in the developed world. The underlying code is the same. The ACS Code is summarised into six core ethical values that it expects its members to always practice in their professional life (source ACS): The Primacy of the Public Interest. You will place the interests of the public above those of personal, business or sectional interests. The Enhancement of Quality of Life. You will strive to enhance the quality of life of those affected by your work. Honesty. You will be honest in your representation of skills, knowledge, services and products. Competence. You will work competently and diligently for your stakeholders. Professional Development. You will enhance your own professional development, and that of your staff. Professionalism. You will enhance the integrity of the ACS and the respect of its members for each other."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 210, "chunk_id": "english_pack_210_0_c8119e16", "content": "192 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES The Primacy of the Public Interest The term ‘Primacy’ indicates that this is the core ethical value that takes precedence over any personal, private, or sectional interests that you might have. Where a conflict exists, it must be resolved in favour of the public interest. There is no room for self-interest, looking after ‘number one’. As you go about your work, you act in the interests of your employer so long as this does not conflict with your duty to the public interest. This means that you should not be developing technology that will adversely affect public health, public safety and the natural or built environment. You identify those who will be impacted by your work and actively consider their interests to avoid harming them. If you become aware of conflicts between your professional work and any legal or social factors, you work with the stakeholders to resolve the conflict before the problem becomes more serious. These can include problems the stakeholder(s) might have with what you are doing, or any conscientious objections you yourself might have. Your duty to the public interest includes preserving the integrity and public image of the profession, respect for other people’s intellectual property and for the confidentiality of any information that might come into your possession."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 211, "chunk_id": "english_pack_211_0_c7cc3dc9", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 193 The Enhancement of Quality of Life Information and Communication Technology (ICT) has the potential to create both harm and benefit. The ethical technologist considers the impact that technology has on society and individuals and actively works to minimise the negative effects while maximising the positive. The ethical technologist cultivates an equity of access attitude that gives the under-privileged members of society the same access that the more privileged already have. As an ethical technologist, you develop an awareness of the many ways that ICT can enhance people’s quality of life, particularly those less advantaged people in society and the world generally (for example in the developing world). The technology you develop should promote the health and safety of the people who use it or are affected by it. At the very least it should not harm anyone. At a more abstract level, the use of technology should create a positive perception and a deeper sense of personal satisfaction in people. It should help people become a fuller expression of their human potential by allowing them to do what they were previously unable to do, and which gives them great satisfaction to do. This core ethical value is an extension of the Public Interest value discussed in the previous section."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 212, "chunk_id": "english_pack_212_0_40d95fad", "content": "194 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES Honesty It is imperative that you do nothing to undermine public trust in the profession, or the trust of the stakeholders in a situation (i.e., your employer, the users etc.). Trust is a valuable but fragile commodity. It requires much time and effort to build, and yet it can be destroyed the moment deception is detected. Trust can only be maintained in the long-term by being consistently honest in your dealings with people. You must be perceived as a person who can be relied upon to act with integrity, someone who avoids deception even when there is little risk of discovery. As an ethical technologist, you therefore avoid offering or receiving inducements (favours, bribes, gratuities) or place yourself in a position where you can be coerced. Any situation intended to bring favour to one stakeholder at the expense of another. Neither shall you mislead anyone as to the suitability of a product or service. You keep your professional life separate from your personal or sectional interests. It is not uncommon for IT practitioners to act as agents for a commercial organisation without disclosing that conflict of interest to their employer or customer. Any estimates you give will be accurate and unbiased, you qualify a professional opinion that is based on limited expertise, you give credit where credit is due for the work of"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 213, "chunk_id": "english_pack_213_0_7e0de714", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 195 others, nor do you attempt to build your own reputation at the expense of other(s). Competence Given the complex nature of technology as a global industry, no single technologist can possibly know everything about everything. Yet it is common for IT practitioners to pretend to know more than they do and knowingly accept work that they are unqualified to perform. This is done on the assumption that they can learn the required skill at short notice or as they go along. In this they are little more than a trainee masquerading as a competent professional. It is a practice commonly seen when people “pad their CV’s” with skills they do not possess. The client has a right to know that the technologist they engage is competent to perform the work, so as an ethical technologist you only accept work that you know you are competent to perform and avoid over-stating your skills and capabilities. You deliver products and services that meet your clients’ operational needs and respect their proprietary interests. If you are aware of issues in relation to a project that are not in the clients’ interests, you make the client aware of these issues even if it might be in your personal interests to say nothing (for example, allowing you to stay employed on a project for longer)."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 214, "chunk_id": "english_pack_214_0_ff5cb913", "content": "196 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES Competency also means taking responsibility for your work, avoiding putting the blame on others when things go wrong. Professional Development In the age of exponentially advancing technology, finding the tie to stay up to date in your field can be a major challenge. It is tempting to let recent developments slip by when you realise that the work you did to learn the latest technology not so long ago is now redundant. The instinct we all must conserve energy suggests ‘don’t bother’. You must resist this ‘economy of effort’ mind-set, it is a major contributing factor to the burn-out and cynicism of mid and late-career members of the profession. Professional development for the ethical technologist means taking the time and making the effort to not only stay abreast of the latest developments, but also to pass on your knowledge and experience to colleagues, particularly those in more junior roles. In the spirit of win-win, you understand that by helping others advance, you are ultimately benefiting everyone, including yourself. Win-win thinking benefits the profession. So, the ethical technologist makes it their business to acquaint themselves with the technological issues having impact on the world, they encourage their colleagues and subordinates to do the same, and support educational"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 215, "chunk_id": "english_pack_215_0_df5f548c", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 197 initiatives aimed at the professional development of themselves and others. Professionalism The computer industry, while global, is relatively new and does not yet have an established set of ethical standards. It takes time for the profession to mature. As an ethical technologist, you can help to establish these standards by always being professional and so improving the perception and image of the profession in the eyes of the public. The challenge is to build public confidence in the profession, particularly in the workplace. The public has mixed feelings about computer technology; on the one hand they enjoy the convenience that it affords them, but on the other they do not understand it and sometimes fear that it might do them harm. To dispel this fear, the ethical technologist takes a calm, objective and well-informed approach to their professional work. As an ethical technologist, you encourage other practitioners to behave in accordance with the code and do nothing to tarnish the image of the profession. This includes ensuring that properly qualified people are not excluded from employment through unfair discrimination. You also do what you can to extend public knowledge and appreciation of ICT, taking pride in being an IT professional."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 216, "chunk_id": "english_pack_216_0_24ddea7e", "content": "198 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES A final word Professional societies around the world provide real assistance to practitioners in time of need. The excerpt below is from the Australian Computer Society, though every society will be offering the same service, should you need it: ‘All people have a right to be treated with dignity and respect. Discrimination is unprofessional behaviour, as is any form of harassment. Members should be aware that the ACS can help them resolve ethical dilemmas. It can also provide support for taking appropriate action, including whistleblowing, if you discover an ACS member engaging in unethical behaviour’. For more detail, visit: www.acs.org.au or the equivalent society in your country. 5.4 Ethical decision model (EDM) For the purpose of resolving ethical dilemmas, we define a dilemma as a complex problem for which there is no obvious solution. A solution exists but is obscured by the complexity. Common sense would suggest that the best way to deal with a complex problem is to simplify it. You can do this by breaking it down into more comprehensible pieces. Here we outline the Ethical Decision Model (EDM), a general-purpose model for analysing complex situations in a range of domains including IT. It helps you to reveal optimal"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 217, "chunk_id": "english_pack_217_0_120091fe", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 199 solution(s), ones that might be described as ethical, and be defended as such. Appendix A is an example of how the EDM can be applied to an IT-related case study. The solutions in the example are indicative, not definitive. The model has three main stages: analysis, prioritisation, decision. Analysis is getting the facts and categorising them into extrinsic factors (legal, professional, employment, social, personal) and intrinsic (a person’s individual attributes). Prioritisation involves ranking the elements into order of importance by means of a priority table. A Decision is made by rationally weighing up the relative importance of the elements. No two people who approach a complex situation will perceive the various factors the same. Their perceptions are filtered through the lens of their personal experience and intrinsic leanings. The precise nature of what reaches their cognitive centre will be different for every person and might even differ for the same person on different occasions. Applying the defined process of the EDM helps to remove the subjectivity from the situation and gives us an objective, process-based approach to the solving of ethical dilemmas."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 218, "chunk_id": "english_pack_218_0_97222ca8", "content": "200 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES Step 1: Analysis In preparing for the ethical analysis, there are some questions that you should ask: • What are the relevant facts of this case? • What do we know, what do we not know that we need to know before deciding? • Who are the stakeholders? • Is this a legal matter for which a prescribed course of action already exists? Every effort must be made to obtain satisfactory answers to these questions before proceeding. It is the nature of ethical dilemmas that they are a complex mix of factors for which there is no obvious solution. Maybe there are two or more obligations that conflict with each other, or the outcomes of anything you do will be undesirable, or even that the cost of doing the right thing is too high. The factors that comprise a given situation can be broadly categorised as Extrinsic and Intrinsic; those that exist in the outside world, and those that exist within the individual. The Extrinsic factors include Legal, Professional, Employment, Social and Personal. The Intrinsic factors have been grouped together under a single heading. Extrinsic factors Legal factors take precedence over the others since"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 219, "chunk_id": "english_pack_219_0_103b3bd5", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 201 breaking the law will get you into serious trouble, even loss of liberty. There will be no conflict between Legal and Professional factors since professional bodies are in the business of creating a solid, respectable public image for its members and will never advocate acting illegally. Professional factors are the obligations you have to the profession, as prescribed in their code of practice. These take precedence over the obligations you have to your employer since it is possible that your employer will ask or demand that you do something unprofessional (unethical) in the profitability interests of the employer. Many dilemmas stem from this source. Employment factors. Most employers have their own code of ethical conduct, as prescribed in their mission statement and other documents that define the values of the organisation. This code sets standards of ethical conduct. These will be generally compatible with the legal, professional and social standards, since no organisation, particularly commercial ones, will want to be seen as deviating from the standards of society. There will be some exceptions to this in the case of organisations on the periphery of society, ones that do not share its mainstream ideals, one’s with an extreme political agenda. Social Factors. The society in which the employer operates will have its inherent standards that are reinforced by the family, at school, in the community generally and in the media and other institutions. All the ways a society"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 220, "chunk_id": "english_pack_220_0_3d74c21b", "content": "202 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES communicates with itself. Society is complex, so standards will not always be unanimously agreed upon. Some members of society will agree, and others disagree on the rightness of various issues. We see this often in polarised political debate. Legal, professional and employment factors take precedence over social factors where there is disagreement. Personal factors include those aspects of your make-up that psychologists categorise as coming from the ‘Nurture’ side of the ‘Nature-Nurture’ theory (of what makes us what we are). These are the factors that you acquire from the environment, your family, close friends and associates, your peer group, sporting association or faith community. While these are undoubtedly within you, they have their origin from outside of you. Personal factors account for much of a person’s ethics, their morality. When there is variance between one’s personal morality and that of the Social, Professional and Legal environments, a person will have the greatest difficulty resolving this ethical conflict. How does one remain true to oneself and still behave ethically in a professional sense? The unpleasant truth for some is that one’s professional obligations must take precedence over any personal qualms you might have about what is ethical. To be a member of a profession means to accept its standards and practice them. To act otherwise will exclude you from the profession. Intrinsic Factors"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 221, "chunk_id": "english_pack_221_0_90cc40bd", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 203 Intrinsic factors include what psychologists categorise as the ‘Nature’ side of ‘Nature-Nurture’. It is your set of innate qualities, the behavioural disposition with which you were born, the disposition that your genetic make-up has equipped you with. People are born with differing degrees of a wide variety of personality traits. These are summed up in The Big Five Personality Traits which are extraversion, agreeableness, openness, conscientiousness, and neuroticism. Each trait represents a continuum. Individuals fall anywhere on the continuum for each trait. For example, with the extraversion/ introversion continuum you can be anywhere on the bell curve from very extraverted to very introverted or somewhere in between. The Big Five remain relatively stable throughout most of one’s lifetime. So, people’s Nature can vary widely within the broad definition of being human. This is a complex area well beyond the scope of this chapter and this book. In addition to The Big Five, you might also google the Myers-Briggs personality profile to learn more on this fascinating subject. Jonathan Haidt’s Moral Foundation Theory. On a more general level, Haidt’s Moral Foundation Theory suggests that there are six innate moral foundations that all humans are born with, the innate moral code that we all share: • Care/harm, • Fairness/cheating, • Liberty/ oppression,"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 222, "chunk_id": "english_pack_222_0_0ad593ec", "content": "204 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES • Loyalty/betrayal, • Authority/subversion, and • Sanctity/degradation (discussed in later chapter). Personal factors (previous section) and intrinsic attributes often exert the strongest yet most idiosyncratic influence on the process of ethical decision-making. While this is a potential problem, someone with the kind of personal and intrinsic attributes that makes them uncomfortable with what is generally accepted for an IT developer is unlikely to last long working in this capacity. Applying the analysis to an example. Consider the case of the market research company that collects demographic information from the broader community and sells contact lists to interested parties who want to do targeted direct marketing. The market research company obtains people’s informed consent to collect and store this information. But now the company changes hands and the new owner wants to increase profits. The owner instructs their web programmers to implement deceptive strategies aimed at gathering information for which they have no informed consent. This instruction contravenes privacy legislation, and the professional code of conduct. It is also contrary to community expectations on privacy. In this instance, when we prioritise the factors, it is clear what the developers should do – refuse to comply, even at the risk of losing their job."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 223, "chunk_id": "english_pack_223_0_e213d141", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 205 For example, if the new owner agreed to supply a gay hate group with the names and addresses of people known to have an interest in gay culture. While it is clearly wrong from a legal, professional, and social perspective, if an IT developer working there is intrinsically homophobic, their disposition will influence their thinking on whether it is right to supply the names. The developer may well perceive this as an ethical dilemma, when the developer sitting in the next cubicle clearly sees it as a wrong act. Step 2: Prioritisation Prioritisation is most easily performed by the making of a list that shows each factor in descending order of importance. It can be helpful to include a column that outlines related matters beside each factor. This has the same common-sense value as the Ben Franklin decision-making method of listing Pro’s and Con’s on a sheet of paper with a single vertical line drawn down the middle. The format with the EDM is somewhat different, but the principle is the same. As a rule, the Legal and Professional factors take precedence since there is an obligation on everyone to abide by the law with no exceptions. This is a long-standing principle that was established for the benefit of the greatest number. The rule recognises personal freedom but says that there is a point where personal freedom ends and the public interest begins. A person can have their personal freedom curtailed by society if"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 224, "chunk_id": "english_pack_224_0_4868b8be", "content": "206 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES it is believed that such freedom is not in the public interest or the greater good. Related to the obligation to abide by the law is the obligation to know the rules laid down by law. Ignorance of the law is not a defence in court for breaking the law. Within the legal framework that governs society we have the various professions, medicine, law, accounting for example. All professions have a Code of Professional Conduct. It is always incumbent upon members to know it and practice it. Membership of that profession is conditional on a sincere undertaking that as a member you will do your utmost to follow the code. Codes of Professional Conduct have relevance to professional standards legislation that exists in many jurisdictions. Breaches of the Code can be used as grounds for a claim of professional negligence. In legal proceedings, the Code can be quoted by an expert witness giving an assessment of professional conduct. The Australian Computer Society’s code of ethics can be summarised as follows; always act in the public interest, your work should enhance people’s quality of life, you should be honest, hard-working, competent and stay current with the latest developments, and finally do what you can to enhance the reputation of the profession. If a conflict occurs between these values, the deciding factor is what is in the public interest, otherwise known as the ‘greater good’. Codes of conduct of professional computer societies in"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 225, "chunk_id": "english_pack_225_0_8daf4d20", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 207 other countries will not be much different. The way in which they are expressed may be outwardly different but the essential, underlying meaning will be similar. Codes of professional conduct and the larger laws of society are certain to be consistent with each other. For example, the first item in the ACS code clearly states that you should always act in accordance with the public interest, which by default is governed by law. Professional groups will never advocate behaviour that even hints at being unlawful or not consistent with the values of the society in which it operates. They want to establish a respectable place for themselves in society. Social factors will also be broadly consistent with legal and professional factors. There is room for disagreement here because as society evolves, its values change, but the law, which is inherently conservative, does not change as quickly. There may be some gap between the two, with the legal taking precedence over the social. The process of law reform will take its course in time and the law will come to reflect community values. Professional codes maintain a safe legal position. Extended debate within professional forums will perform the same role as the law reform bodies in larger society. Prioritising the factors inherent in a situation should always have the legal, professional, and social factors at the top of the list. Most likely to conflict with these are Work factors. The goals, policies and culture of an organisation are at the discretion of the owners who may well perceive their first"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 226, "chunk_id": "english_pack_226_0_85d84da5", "content": "208 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES responsibility as being to their own financial interests and those of the shareholders. It is not being overly cynical to suggest that some business owners are more concerned with the question of whether they will get caught, not whether something is legal. Beyond the question of being caught, there is also the issue of how likely it is the state will prosecute, given that the law lags the pace of technological change. And given the expense of legal proceedings, Prosecutors will usually only pursue cases of significance that are likely to result in a conviction. A commercial organisation’s reason to exist is to make a profit or at least to survive and continue to trade. Despite outward appearances, many companies operate on the verge of collapse, delaying payment of their debts for as long as possible while trying to extract payment from debtors as quickly as possible. In desperate circumstances even a normally honest business owner has been known to resort to unethical if not illegal strategies if they can get away with it. Most organisations are honest and ethical, but it is not difficult to see how a technologist working in some organisations are going to find themselves told to do ‘questionable’ things. Step 3: Decision Having drawn up a prioritised list that shows each factor in order of importance, you are now able to decide, based on rational choice, what will be the most ethical course of action."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 227, "chunk_id": "english_pack_227_0_1503bd2f", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 209 In deciding, you might take into consideration which course of action: does the most good or the least harm, respects stakeholder rights, treats people justly, best serves the public interest (not just some members), and which allows me to be the best kind of person I can be? If called upon, you should be able to make a strong argument, citing evidence as to why you chose as you did. Imagine that you have been called to explain yourself to the board of directors, or the ethics committee of a professional society or even the police/prosecutor. Your case should be strong enough that you could deliver it with confidence and a clear conscience. 5.5 Theories of Ethical Behaviour This section summarises the major philosophical theories that have bearing on ethics, the branch of philosophy that deals with morality. The list is a representative sample, not exhaustive. This level of detail is appropriate for a discussion on ethics in IT. For balance, the list covers both the philosophies of the West, starting with the classical Greeks, and then those of the East, including Buddhist, Confucist and Taoist philosophies. It should be noted that Buddhism, Confucism and Taoism are rightly called philosophies not religions since they concern themselves with how to think and behave correctly and recognize no deity. These Eastern"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 228, "chunk_id": "english_pack_228_0_d91e4d32", "content": "210 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES philosophies are a kind of applied psychology which might explain their popularity in contemporary Western culture. Each philosophy is useful, yet none are complete all the time in every situation. No one philosophy can be all things to all people. Therefore, the rational course of action is to consider them together and look for underlying common factors that may be present. We make allowances for superficial differences in the way they are expressed, since each is a product of the culture that created it. Some discretion and judgment are required to know how best to apply them. As you will see, they can contradict each other, for example moral relativism and universalism. On the one hand Relativism says that right action is determined by circumstances, while Universalism says that right action is determined by principle, regardless of circumstance. Relativism Relativism holds that moral or ethical propositions do not reflect objective and/or universal moral truths, but instead make claims relative to social, cultural, historical or personal circumstances. Right action is determined on a case-by-case basis, being dependent on who is involved and a host of situational factors. Relativism is differentiated into subjective and cultural relativism."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 229, "chunk_id": "english_pack_229_0_a65d310c", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 211 Subjective Relativism A personal and subjective moral core lies or ought to lie at the foundation of a person’s moral acts. This is essentially an inward-looking approach to morality, with each person being their own ultimate authority on what is right action. In the subjective view, public morality is merely a reflection of social convention. Only personal, subjective morality expresses true authenticity. The French philosopher Jean-Paul Sartre is a foremost exponent of this approach to morality. Cultural Relativism In contrast to the subjective approach, in Cultural Relativism a person’s beliefs and activities are understood in the context of his or her culture. Right action is defined by cultural convention and exists as a commonly understood principle in that culture. Since morality varies from culture to culture, with each culture having an equal claim as to what constitutes right action. This approach to morality grew out of the work of th anthropologist Franz Boaz in the early 20 century. Anthropologists, if they are to properly understand a culture must not impose moral judgments on their practices even if they differ from the anthropologist’s own cultural beliefs. A criticism of both subjective and cultural relativism is that they differ fundamentally and take no account of the other. Arguably, both approaches have merit, and both deserve to be"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 230, "chunk_id": "english_pack_230_0_86371875", "content": "212 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES recognised, but not to the exclusion of the other. A blended approach that could simply be called Relativism is proposed, which takes both subjective and objective factors into account and tries to reconcile them. This would lead to a more balanced understanding of a given situation. Kantianism Immanuel Kant (1724–1804) was a notable German philosopher who argued with good reason that morality be based on a standard of rationality that he dubbed the Categorical Imperative (CI). Immorality is therefore a violation of the CI and is irrational. The importance of being rational is a consistent theme in Western philosophy. The Stoic philosophers of classical Greece emphasises the use of logic and rationality to overcome the tendency to act emotionally and irrationally. Kant’s position can be summed in his categorical imperatives which form the foundation of his work. Categorical Imperative (First Formulation): Act only according to that maxim whereby you can at the same time will that it should become a universal law. Ask yourself, if I do this, would be all right if everyone did it? Categorical Imperative (Second Formulation): Act so that you always treat both yourself and other people as ends in themselves, and never only to an end. Ask yourself, am I exploiting someone to get what I want?"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 231, "chunk_id": "english_pack_231_0_45a0c737", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 213 The first formulation is the foundation of the Universalist view of morality that if something is right, then it is always right, all the time. To make a special case exception is little more than a sense of selfish entitlement. The second formulation lies at the heart of much of what the Ethical Technologist is about; the importance of helping people to come to a fuller expression of their potential. This position maintains that whatever you do must not harm other people or diminish them by treating them to an end. Kant’s theory belongs to the broader category of non- Consequentialist theories that determines whether an action is right or wrong by considering the underlying rule or principle that motivates the action. Social Contract theory is another member of this category. Utilitarianism Utilitarianism asserts that moral behaviour is that which promotes happiness or pleasure; that which creates the greatest good and/or does the least harm. A wrong act is one which produces unhappiness or suffering. The degree of ‘wrongness’ is determined by how much harm the act has caused. Therefore, the guiding principle in Utilitarianism is to do the thing that brings the greatest good to the greatest number. Utilitarianism is sometimes known as a Consequentialist approach; if the outcome or consequence of an act is good,"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 232, "chunk_id": "english_pack_232_0_2c0a5d10", "content": "214 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES then the act itself is good. It is often used in the world of business and politics to achieve desired ends, sometimes incurring damage along the way. The ends justify the means. Though, the ends do not justify the means if significant harm is caused by doing so. Act Utilitarianism With Act-utilitarianism the principle of utility is applied directly to each alternative act in a situation of choice. The right act is defined as the one which brings about the best results, or the least amount of harm. Criticisms of this viewpoint to the difficulty of having full knowledge of the consequences of our actions. Act-utilitarianism has been used to justify barbaric acts, for example suppose you could end a war by torturing children whose fathers are enemy soldiers to find out where the fathers are hiding. Act utilitarianism is supremely pragmatic as it confines itself to a simple moral calculus; for example, if I can save 10,000 lives by killing one innocent person, the killing is a moral act. Rule Utilitarianism With Rule-utilitarianism the principle of utility is used to determine the validity of the rules of conduct, the moral principles that underlie. For example, if we have a rule about keeping promises, it is"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 233, "chunk_id": "english_pack_233_0_ebfc178a", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 215 because we have considered what the world would be like if people broke promises when they feel like it, compared with a world where people keep their promises. Moral behaviour is therefore defined by whether we follow the rules. There are limits to how far Rule utilitarianism can be applied. When more and more exceptions to the rule are applied, it collapses into Act utilitarianism. More general criticisms of this view argue that it is possible to generate unjust rules by resorting to the principle of utility. For example, slavery in ancient Greece might have been right if it led to an overall achievement of cultivated happiness at the expense of some mistreated individuals. Social Contract theory Philosopher Thomas Hobbes argued that everybody living in a civilised society has implicitly agreed to (a) establish a set of moral rules to govern relations among citizens, and (b) establish a government capable of enforcing these rules. This is called the social contract. In practical terms, Social Contract theory might also be construed to be a kind of reciprocal social obligation, society to the individual, and the individual to society. When individuals live in a society and enjoy the benefits of doing so (a place to live, meaningful work, the chance to raise a family in safety and so on), they have a reciprocal obligation to contribute to that society in whatever way they are best able to do. A person who"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 234, "chunk_id": "english_pack_234_0_b168a2fc", "content": "216 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES takes and refuses to give according to their ability is little more than a parasite. Social Contract theory belongs to the broader category of non-Consequentialist theories that determines whether an action is right or wrong by considering the underlying rule or principle motivating the action. Kant’s theory is another member of this category. Marcus Aurelius and the Stoics Marcus Aurelius (full name Marcus Aurelius Antoninus Augustus, 121 – 180 AD) was an exceedingly rare individual; a genuine philosopher-king. His leadership is based on the often-misunderstood Stoic philosophy. The power and relevance of this philosophy is as potent today as it was when he was Roman Emperor (161 to 180AD). Marcus Aurelius might have been a Roman, but his thinking had been shaped by the classical period of ancient Greece. Even today, classical Greek thinking is still at the foundations of Western civilisation. Influenced by the earlier work of Socrates and Diogenes of Sinope, the Stoic school of philosophy was founded around 300 BC by Zeno of Citium. Speaking from beneath a painted portico (Stoa Poikilē) in Athens, signifying openness to anyone passing by Zeno taught that a wise person should not allow their emotions to rule them; instead, they should master their emotions and use logic to think rationally about how to"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 235, "chunk_id": "english_pack_235_0_ac310459", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 217 behave in life. He urged his followers to carefully study the laws of Nature and to live in harmony with them. In this respect his ideas coincide with those of far distant Lao Tzu, the ancient Chinese philosopher who wrote the Tao Te Ching. A central point in Stoic philosophy is the active relationship between the laws of Nature that rule the Cosmos, and human free will. A wise person derives maximum benefit and happiness in life by bringing his or her will into harmony with Nature. They come to know themselves, recognising that their inner nature (microcosm) is a representation of the outer macrocosm, or universe; it the same nature in both, only differing in scale. Stoics conceived of the universe as being governed by Logos, what we today would think of as the Laws of Physics. Pure, abstract, these laws pervade the universe and make it behave in the way it does. The same informing principle resides in humans. Virtue is therefore gained by recognising this and working to harmonise one’s inner self with the qualitatively similar outer world. The Greek founders of Stoicism conceived of three interrelated elements that collectively make Philosophy. These are logic, physics, and ethics. Logic allows us to recognise truth when we see it, and to avoid making mistakes. Logic allows us to understand Physics, which is the way the world operates, the laws of Nature. Together, Logic and Physics allows us to practice Ethics, or moral behaviour that brings benefit. Ethical behaviour is that which is in harmony with the"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 236, "chunk_id": "english_pack_236_0_afc841d2", "content": "218 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES unfolding laws of Nature. This unfolding is the cause of both pleasure and suffering in people. If we are to stay in accord with it, we must discipline our minds to become indifferent to suffering, accepting with grace that it is necessary and inevitable to suffer sometimes. This state of mind is called apatheia. Likewise, we must not become so attached to pleasure that we cannot relinquish it when it passes. The goal is to become self-sufficient, or autarcheia. The Stoic therefore becomes equally indifferent to good fortune or bad, whether they are rich or poor, well-respected or despised. They understand that the approval or disapproval of others can exert undue influence to conform to values that may not be true. The Stoic does his or her duty in accordance with Nature as revealed by careful observation and logical enquiry. They do their duty regardless of whether it is easy or hard. With its emphasis on duty and right action, Stoicism is therefore well-suited to the needs of those who would lead. It was used as a guide by the ruling class of Rome for centuries. Buddhism & the four noble truths About the same time as the classical Greek philosophers were formulating their ideas a revolution in thought was taking place in northern India. Siddhartha Gautama, the man who would become the Buddha, or Awakened One, was formulating some ideas of his own. It is remarkable how similar in structure and meaning the philosophies of East and"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 237, "chunk_id": "english_pack_237_0_11a8fa1e", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 219 West at this time were. It is almost as if it was a good idea whose time had come to be brought into the world. Buddhism is thought by many to be a religion, yet it recognizes no deity. In its basic form is an applied psychology expressed in the language of the time. It outlined a formula for how to become self-actualized. The foundation of Buddhist philosophy is the so-called Four Noble Truths and the Noble Eight-Fold Path. The eight-fold path aims to improve your (a) Wisdom by practicing right view and intention, (b) Ethical conduct by practicing right speech, action and livelihood, and (c) Mental capabilities by practicing right effort, mindfulness and concentration. We shall examine more closely the three aspects of ethical conduct. Right Speech Words are powerful. Words can make or break a person’s life, start wars or bring peace. Words can indeed be mightier than the sword, as great orators through the ages have proven. Right speech (including written words) is therefore the principle of expressing oneself in a way that enhances the quality of people’s lives and does no harm. It means to refrain from (a) lies and deceit, (b) malicious language (including slander), (c) angry or offensive language, and (d) idle chit-chat (including gossip). Notice the correspondence between this principle and the prime ethical value in the ACS code of conduct to act in ways that improves people’s quality of life. Therefore, tell the truth, speak with warm gentleness when"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 238, "chunk_id": "english_pack_238_0_8d24abac", "content": "220 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES you do speak, and refrain from speaking when you have nothing important to say. Right Action Right action can be defined open-endedly by prescribing what a person should not do. That then leaves the field wide open for choice. Broadly, right action means refraining from (a) harming any sentient creature, (b) stealing, and (c) sexual misconduct. Doing no harm to others covers a very broad range of behaviours. The worst a person can do is to take the life of sentient creatures, hence many Buddhists are vegetarians. Not stealing includes all forms of robbery, theft, deceit and fraud; essentially taking what you have not earned the right to have. The ethical person is therefore kind and compassionate in their dealings with the world. They respect other people’s property, and do not engage in sexual behaviour that harms another either at a physical or emotional level. Right Livelihood Right livelihood is about earning one’s living in ways that does no harm to others. Of all the possible ways a person might earn money, they should avoid those that exploit people’s weaknesses. Right livelihood means one should refrain from any employment that is contrary to the principles of right action"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 239, "chunk_id": "english_pack_239_0_6a3739c3", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 221 and right speech, including but not limited to (a) trading in weapons, (b) trading in living beings, including slavery, prostitution and raising animals for slaughter, (c) butchery and meat processing, and (d) trading in drugs and poisons, including alcohol and recreational drugs. Lao Tzu & the Tao Te Ching The Tao Te Ching is said to have been written by Lao Tzu (604 – 531 BCE), the philosopher and Custodian of the Imperial Archives in the time of the Chou Dynasty in ancient China. It is uncertain when Lao Tzu was born or died, but he is said to be a contemporary of Confucius (551–479 BCE). Central to Taoist philosophy is the avoidance of extremes, to always seek the middle way on our journey through life. Find the middle ground between the extremes and occupy that space and in doing so have the fewest consequences to deal with. The principle at work here is that extreme action always results in an equal and opposite reaction. As a pendulum swings to one extreme, it will always swing to the other extreme in equal measure. Following the middle path reduces the “swing” to a minimum. Only through this practice can harmony in society be achieved. We are encouraged to sense the world around us directly and to contemplate our impressions deeply. It advises against relying on the structures and belief systems that have been created by others and put forward as orthodox truth. Such"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 240, "chunk_id": "english_pack_240_0_5924d0cf", "content": "222 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES ideologies remove us from a direct experience of life and effectively cut us off from our intuition. The middle path requires us to develop an awareness of the physical forces that shape our world. Such forces operate uniformly at all levels from the largest to the smallest. They operate in the universe as a whole and in the minds and lives of individual people. An understanding of these natural laws and the forces they direct give us the power to influence events in the world without force. Influence is achieved through guiding rather than coercion. The objective is always to avoid taking action that will elicit strong counter-reactions. In Nature, an excessive force in a particular direction always triggers the growth of an opposing force, and therefore the use of force cannot be the basis for establishing an enduring social condition. We come to understand that everything in the universe is impermanent, in a state of change. The emotional and intellectual structures that we build for ourselves to feel secure are likewise subject to change by external forces that are largely beyond our control. The challenge is to accept the inevitability of change and not waste our energy trying to prop up these impermanent structures, defending them against criticisms, and trying to convince others to believe in them so that they might become recognised as permanent truth. Lao Tzu wrote the Tao Te Ching from the point of view of the “superior man”, the person who is transcending their"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 241, "chunk_id": "english_pack_241_0_237309f2", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 223 base nature by consciously improving their lives through wise choices. The Ethics of Confucius Confucius (551 BC – 479 BC) established a system of personal and governmental morality that has endured for 2,500 years. It concerns itself with correctness in social relations during a time of great disturbance. The work of Confucius and Lao Tzu are both aimed at achieving social harmony and coherence to remedy the rampant chaos of the times. Three key principles are emphasized in Confucius’ teachings: the principles of Li, Jen and Chun-Tzu. The term Li has several meanings; it is often translated as propriety, reverence, courtesy, ritual or ideal conduct. It is what Confucius believed to be the ideal standard of religious, moral, and social conduct. The second principal Jen is the fundamental virtue of Confucian teaching, the virtue of goodness and benevolence. It is expressed through recognition of value and concern for others, no matter their rank or class. Jen is summarised as the Silver Rule: Do not do to others what you would not like them to do to you. (Analects 15:23) Li provides the structure for social interaction. Jen makes it a moral system. The third principle, Chun-Tzu describes the idea of the true gentleman (should not be seen as gender-specific). This is the person who lives according to the highest ethical standards."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 242, "chunk_id": "english_pack_242_0_fce72a0d", "content": "224 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES The gentleman displays five virtues: self-respect, generosity, sincerity, persistence, and benevolence. As a son, he is always loyal; as a father, he is just and kind; as an official, he is loyal and faithful; as a husband, he is righteous and just; and as a friend, he is faithful and tactful. In today’s world, the words she, mother and wife could be substituted for he, father and husband. The Universal Moral Code To identify underlying moral principles across cultures, Kent W. Keith puts forward these two lists, one expressed in ‘do this’ form and the other in the ‘do not do this’ form. These principles are found embedded in the moral codes of diverse cultures. The first list, do no harm, essentially says, whatever you do, do not do these. The list can be seen as the foundation upon which a positive set of behaviours can be based, the do-good list. Do no harm. Do not do to others what you would not like them to do to you, do not lie, do not steal, do not cheat, do not falsely accuse others, do not commit adultery, do not commit incest, do not physically or verbally abuse others, do not murder, do not destroy the natural environment upon which all life depends. Do good. Do to others what you would like them to do to you, be honest and fair, be generous, be faithful to your family and friends, take care of your children when they are young, take care"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 243, "chunk_id": "english_pack_243_0_53e87f09", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 225 of your parents when they are old, take care of those who cannot take care of themselves, be kind to strangers, respect all life. The Golden Rule Perhaps the most often quoted moral absolute is the so-called Golden Rule. Beyond the religious or even the philosophical, this principle is recognisable in Physics as Newton’s second law of motion; the mutual forces of action and reaction between two bodies are equal, opposite and collinear. What we do elicits an equal and opposite reaction. As humans, we are not separate from the laws of Physics. If we take the position that we are not masochists and we want good things to happen to us, then we have the Golden Rule: Christianity. Therefore, all things whatsoever ye would that men should do to you, do ye even so to them: for this is the law and the prophets. Matthew 7:12 Confucianism. Do not do to others what you would not like yourself. Then there will be no resentment against you, either in the family or in the state. Analects 12:2 Buddhism. Hurt not others in ways that you yourself would find hurtful. Udana-Varga 5,1 Hinduism. This is the sum of duty; do naught onto others what you would not have them do unto you. Mahabharata 5,1517 Islam. No one of you is a believer until he desires for his brother that which he desires for himself. Sunnah"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 244, "chunk_id": "english_pack_244_0_c36c83ff", "content": "226 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES Judaism. What is hateful to you, do not do to your fellowman. This is the entire Law; all the rest is commentary. Talmud, Shabbat 3id Taoism. Regard your neighbour’s gain as your gain, and your neighbour’s loss as your own loss. Tai Shang Kan Yin P’ien Zoroastrianism. That nature alone is good which refrains from doing another whatsoever is not good for itself. Dadisten-I-dinik, 94,5 Comparison of knights’ codes The Japanese Samurai and the chivalric knights of medieval Europe were separated by a great distance, and likely had no contact with each other. Yet independently they arrived at noticeably similar codes of ethical conduct as seen below. Interestingly, there is correspondence with the Australian Computer Society’s code of professional conduct too."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 245, "chunk_id": "english_pack_245_0_b2ec128d", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 227 Samurai Knight’s ACS Code of Prof Conduct Code Code Objectivity and Independence Courage Courage Integrity Loyalty Loyalty Confidentiality Subordinates Honor Nobility Responsibility to your Client The Public Interest Honesty/ Defense The Image of the Profession Trust Justice Promoting Information Technology Prowess Competence Franchise / Keeping Up To Date replicate Rectitude Faith Right action Respect Humility Respect for stakeholders Do what is in best interests of client Benevolence Generosity and public Digital Ethics & Responsible AI Artificial intelligence (AI) is transforming the world in many ways, from improving health care and education to enhancing productivity and innovation. However, AI also poses significant challenges and risks, such as potential bias,"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 246, "chunk_id": "english_pack_246_0_9f24c43d", "content": "228 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES discrimination, privacy breaches, security threats, and ethical dilemmas. How can we ensure that AI is used for good and not evil? How can we design and implement AI systems that are fair, transparent, accountable, reliable, and respectful of human values? Follow the AI Ethics Principles Many countries and organizations have developed ethical principles or guidelines for AI, such as Australia’s 8 AI Ethics Principles, the IEEE’s Ethically Aligned Design [PDF], or the Berkman Klein Centre’s report on ethical principles in eight categories. These principles provide a common framework and a shared language for understanding and addressing the ethical issues of AI. They also help to build public trust and consumer loyalty in AI-enabled services. The principles cover various aspects of AI, such as human wellbeing, human-centred values, fairness, privacy protection and security, reliability and safety, transparency and explainability, contestability, and accountability. By following these principles and committing to ethical AI practices, you can achieve safer, more reliable and fairer outcomes for all stakeholders."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 247, "chunk_id": "english_pack_247_0_f2c07d75", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 229 5.6. Ethical AI & Algorithmic Bias Ethical AI & Algorithmic Bias Artificial intelligence (AI) is a powerful technology that can enhance decision-making, optimize processes, and create new value in various domains. However, AI also poses ethical challenges that need to be addressed by IT professionals who design, develop, deploy, or use AI systems. One of the most pressing ethical issues in AI is algorithm bias, which is a kind of error or unfairness that can arise from the use of AI. What is algorithm bias and why does it matter? Algorithm bias is a situation where an AI system produces outcomes that are systematically skewed or inaccurate, often resulting in unfair or discriminatory treatment of individuals or groups based on their characteristics, such as race, gender, age, or disability. Algorithm bias can have negative impacts on human rights, such as the right to equality, privacy, dignity, and justice. Algorithm bias can occur for several reasons, such as: • The data used to train or test the AI system is not representative of the target population or context,"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 248, "chunk_id": "english_pack_248_0_5817cd65", "content": "230 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES leading to overfitting or underfitting. • The algorithm design or implementation is flawed or contains hidden assumptions or preferences that favour certain outcomes or groups over others. • The interpretation or application of the AI results is influenced by human biases or prejudices, either intentionally or unintentionally. Some examples of algorithm bias in real-world scenarios are: • A facial recognition system that performs poorly on people of colour, resulting in false positives or negatives that can affect security, access, or identification. • A hiring system that screens candidates based on their resumes but excludes qualified applicants who have non- traditional backgrounds or names that indicate their ethnicity or gender. • A credit scoring system that assigns lower scores to people who live in certain neighbourhoods or have certain occupations, affecting their access to loans or insurance. How can IT professionals address algorithm bias? As IT professionals who are involved in the development or use of AI systems, we have a responsibility to ensure that our AI systems are ethical and aligned with human rights"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 249, "chunk_id": "english_pack_249_0_bbf638e4", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 231 principles. We can do this by following some best practices, such as: • Conducting a thorough analysis of the data sources, algorithms, and outcomes of the AI system, and identifying potential sources and impacts of bias. • Applying appropriate methods and tools to mitigate or reduce bias in the data collection, processing, analysis, and validation stages of the AI system. • Implementing transparency and accountability mechanisms to explain how the AI system works, what data it uses, what assumptions it makes, and what results it produces. • Engaging with relevant stakeholders, such as users, customers, regulators, and experts, to solicit feedback, address concerns, and ensure compliance with ethical standards and legal requirements. • Monitoring and evaluating the performance and impact of the AI system on an ongoing basis and updating or correcting it as needed. Algorithm bias is a serious ethical challenge that can undermine the trustworthiness and value of AI systems. IT professionals have a key role to play in ensuring that our AI systems are ethical and respect human rights. By following some best practices, we can create AI systems that are fair, accurate, and beneficial for all."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 250, "chunk_id": "english_pack_250_0_376b0ee8", "content": "232 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES The Importance of Ethical AI Policies AI poses significant challenges and risks, such as potential bias, discrimination, privacy breaches, and accountability gaps. Therefore, it is essential to develop and implement ethical AI policies that can ensure the safe, secure, and responsible use of AI for the benefit of individuals, society, and the environment. What are ethical AI policies? Ethical AI policies are guidelines or principles that aim to align the design, development, and deployment of AI systems with human values and rights. Ethical AI policies can help to: • Achieve safer, more reliable, and fairer outcomes for all stakeholders affected by AI applications. • Reduce the risk of negative impacts or harms caused by AI systems. • Build public trust and confidence in AI systems and their providers. • Encourage innovation and competitiveness in the AI sector. • Comply with existing laws and regulations related to AI. Ethical AI policies can be developed and implemented by various actors, such as governments, businesses, researchers, civil society, and international organizations. Ethical AI"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 251, "chunk_id": "english_pack_251_0_ce9679d4", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 233 policies can also vary in their scope, level of detail, and enforceability. Examples of ethical AI policies Several countries and regions have developed or are developing ethical AI policies to guide their AI strategies and initiatives. For example: Australia has published its AI Ethics Framework, which includes eight voluntary AI Ethics Principles that cover human, social, and environmental wellbeing; human-centred values; fairness; privacy protection and security; reliability and safety; transparency and explainability; contestability; and accountability. The European Union has proposed its Artificial Intelligence Act, which is a comprehensive legal framework that aims to regulate high-risk AI systems and promote trustworthy AI based on four ethical principles: respect for human dignity and autonomy; prevention of harm; fairness; and democratic values. The United States has issued its Executive Order on Maintaining American Leadership in Artificial Intelligence, which directs federal agencies to foster public trust and confidence in AI technologies by promoting reliable, robust, trustworthy, secure, portable, and interoperable AI systems. In addition to governments, many private sector companies have also adopted their own ethical AI policies or principles"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 252, "chunk_id": "english_pack_252_0_c34dc995", "content": "234 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES to demonstrate their commitment to responsible AI practices. For example: Microsoft has established its Responsible AI Standard, which is a set of requirements and processes that help its teams design, develop, deploy, and operate AI systems in a manner consistent with its six ethical principles: fairness; reliability and safety; privacy and security; inclusiveness; transparency; and accountability. Google has published its Responsible AI Practices, which is a collection of best practices and tools that help its engineers build AI systems that are aligned with its seven principles: socially beneficial; avoid creating or reinforcing unfair bias; be built and tested for safety; be accountable to people; incorporate privacy design principles; uphold high standards of scientific excellence; and be made available for uses that accord with these principles. Mitigating Bias Identify and Assess Potential Sources of Bias The first step to mitigate bias is to identify and assess the potential sources of bias in the IT system or decision. This can be done by conducting a thorough analysis of the data, algorithms, processes and outcomes involved in the system or decision. Some questions to ask are:"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 253, "chunk_id": "english_pack_253_0_bda6b586", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 235 • What are the objectives and criteria of the system or decision? • What are the data sources, methods and quality of the data collection and processing? • What are the assumptions, limitations and trade-offs of the algorithms and models used? • How are the results interpreted, communicated and acted upon? • Who are the stakeholders, beneficiaries and potential victims of the system or decision? • What are the ethical, legal and social implications of the system or decision? Some tools that can help with this step are: • IBM’s AI Fairness 360 toolkit, which provides a set of metrics, algorithms and visualizations to detect and mitigate bias in datasets and machine learning models. • IBM’s AI Factsheets, which provide a standardized way to document the characteristics, capabilities and limitations of AI systems. • IBM Watson OpenScale, which provides a platform to monitor, explain and improve AI performance, fairness and compliance. Implement Bias Mitigation Strategies The second step is to implement bias mitigation strategies that"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 254, "chunk_id": "english_pack_254_0_75a4c28b", "content": "236 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES address the identified sources of bias. This can be done by applying various techniques, such as: • Data augmentation, transformation or sampling to improve the representativeness, diversity and balance of the data. • Algorithm selection, modification or regularization to reduce the complexity, opacity or sensitivity of the models. • Human review, feedback or intervention to provide oversight, validation or correction of the results. • Stakeholder engagement, consultation or participation to ensure transparency, accountability and inclusiveness of the system or decision. Some examples of bias mitigation strategies are: Conflicts and Biases in the Boardroom, which provides guidance on how to address conflicts of interest and common biases that impact board decisions. Algorithmic bias detection and mitigation: Best practices … – Brookings, which provides policy recommendations on how to detect and mitigate algorithmic bias in consumer harms. AI Ethics Part 2: Mitigating bias in our algorithms – CMO, which provides best practices on how to build fairness and bias metrics and run a model governance process."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 255, "chunk_id": "english_pack_255_0_dab009d8", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 237 Evaluate and Monitor Bias Mitigation Outcomes The third step is to evaluate and monitor the outcomes of the bias mitigation strategies. This can be done by measuring, testing and reporting on the performance, fairness and trustworthiness of the system or decision. Some questions to ask are: • How effective are the bias mitigation strategies in achieving the objectives and criteria of the system or decision? • How fair are the system or decision outcomes for different groups of stakeholders? • How trustworthy are the system or decision processes and results for different audiences? • How robust are the system or decision against changes in data, algorithms or contexts? • How adaptable are the system or decision to new requirements, feedback or challenges? Some tools that can help with this step are: • IBM Watson OpenScale, which provides a platform to monitor, explain and improve AI performance, fairness and compliance. • IBM Watson Discovery, which provides a service to analyse text data for sentiment, emotion, tone and"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 256, "chunk_id": "english_pack_256_0_8dfb9b28", "content": "238 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES personality insights. • IBM Watson Assistant, which provides a service to build conversational agents that can interact with users and provide feedback. Mitigating bias in IT governance is a complex and ongoing challenge that requires a holistic and proactive approach. By following these three steps – identify and assess potential sources of bias, implement bias mitigation strategies, and evaluate and monitor bias mitigation outcomes – IT leaders can ensure that their systems and decisions are more ethical, fair and trustworthy. Ethical AI in Critical Domains Certain domains, such as criminal justice and healthcare, hold significant ethical ramifications for AI usage. Biased algorithms in predictive policing can lead to unjust targeting, while healthcare AI biased against certain demographics might exacerbate health disparities. Ethical AI policies should emphasize thorough evaluation and validation of algorithms in these critical contexts. Identify the ethical principles for AI The first step to build ethical AI is to identify the ethical principles that should guide its development and use. There"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 257, "chunk_id": "english_pack_257_0_aeb25638", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 239 are many sources of ethical principles for AI, such as the OECD Principles on AI, the World Economic Forum’s 9 Ethical AI Principles for Organizations, or the Ethics of Artificial Intelligence course by Coursera. These principles usually include values such as fairness, transparency, accountability, privacy, security, human oversight, and social good. However, these principles are not enough by themselves. They need to be translated into concrete norms and practices that can be implemented and governed in specific contexts and domains. For example, what does fairness mean for an AI system that diagnoses diseases or recommends treatments? How can transparency be achieved for an AI system that predicts criminal behaviour or assesses legal risks? How can accountability be ensured for an AI system that controls autonomous vehicles or drones? To answer these questions, we need to conduct a thorough ethical analysis of the AI system and its impacts and implications for the stakeholders involved. Conduct an ethical analysis of the AI system The second step to build ethical AI is to conduct an ethical analysis of the AI system and its impacts and implications for the stakeholders involved. This analysis should consider the following aspects:"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 258, "chunk_id": "english_pack_258_0_33e2f543", "content": "240 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES • The purpose and goals of the AI system. What problem does it aim to solve? What benefits does it provide? What risks does it entail? • The data and algorithms of the AI system. What data is used to train and test the AI system? How is it collected, processed, stored, and shared? What algorithms are used to analyse the data and generate outputs? How are they designed, validated, and updated? • The outputs and outcomes of the AI system. What outputs does the AI system produce? How are they interpreted and used? What outcomes do they lead to? How are they measured and evaluated? • The stakeholders of the AI system. Who are the stakeholders of the AI system? How are they affected by its outputs and outcomes? What are their needs, preferences, values, and expectations? • The ethical issues of the AI system. What ethical issues arise from the AI system’s purpose, data, algorithms, outputs, outcomes, and stakeholders? How can they be identified, prioritized, and addressed? To conduct this analysis, we need to use critical skills and methods that can help us clarify and ethically evaluate the AI system in different domains of life. We also need to consult with relevant experts and stakeholders to ensure that we capture their perspectives and concerns."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 259, "chunk_id": "english_pack_259_0_227878fb", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 241 Implement ethical solutions for the AI system The third step is to implement ethical solutions for the AI system that can address the ethical issues identified in the previous step. These solutions may include: • Ethical design. Applying ethical principles and values in the design process of the AI system, such as user- cantered design or value-sensitive design. • Ethical development. Applying ethical standards and guidelines in the development process of the AI system, such as code of ethics or best practices. • Ethical testing. Applying ethical criteria and methods in the testing process of the AI system, such as audits or impact assessments. • Ethical deployment. Applying ethical rules and regulations in the deployment process of the AI system, such as policies or laws. • Ethical governance. Applying ethical mechanisms and structures in the governance process of the AI system, such as oversight boards or ethics committees. To implement these solutions, we need to use appropriate tools and techniques that can help us operationalize ethics in practice. We also need to monitor and evaluate the impacts and outcomes of the AI system on a regular basis. Ethical AI is not only a moral duty but also a strategic"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 260, "chunk_id": "english_pack_260_0_3a77dacd", "content": "242 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES advantage for organizations that want to create value and trust with their customers, employees, partners, regulators, and 5.7 Ethics in Emerging Technologies (Quantum Computing, 5G) As new technologies develop, they bring both opportunities and ethical challenges. It’s important to consider the potential impacts of these technologies on society, privacy, and security. Quantum Computing Quantum computing uses principles of quantum mechanics to process information. It has the potential to solve complex problems much faster than traditional computers. Ethical Considerations: • Cryptography. Quantum computers could break current encryption methods, threatening privacy and security. • Inequality. Access to quantum computing might create a technological divide between countries or organizations. • Dual-use Concerns. Quantum computing could be used for both beneficial and harmful purposes."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 261, "chunk_id": "english_pack_261_0_63a7e46a", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 243 Ethical Approaches: • Develop quantum-resistant encryption methods. • Ensure equitable access to quantum computing resources. • Establish international guidelines for quantum technology use. 5G Technology 5G is the fifth generation of cellular network technology, offering faster speeds and more connections than previous generations. Ethical Considerations: • Privacy. 5G enables more data collection, raising concerns about personal privacy. • Health Concerns. Some worry about potential health effects of 5G radiation, though current evidence doesn’t support these concerns. • Digital Divide. Unequal 5G access could widen gaps between urban and rural areas. • Security. More connected devices mean more potential entry points for cyberattacks. Ethical Approaches:"}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 262, "chunk_id": "english_pack_262_0_0b5a4ff7", "content": "244 | MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES • Implement strong data protection measures in 5G networks. • Conduct ongoing research on potential health impacts. • Develop policies to ensure widespread, equitable 5G access. • Integrate robust security measures into 5G infrastructure. General Ethical Principles for Emerging Technologies • Transparency. Be open about how the technology works and its potential impacts. • Accountability. Establish clear responsibility for the consequences of using the technology. • Fairness. Ensure the benefits and risks of the technology are distributed fairly. • Human Rights. Protect and promote human rights in the development and use of new technologies. • Sustainability. Consider the long-term environmental and social impacts of the technology. Ethical Decision-Making Framework for Emerging Technologies • Identify Stakeholders. Determine who will be affected by the technology."}
{"pack_id": "english_pack", "section_title": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES", "page": 263, "chunk_id": "english_pack_263_0_de05eb6c", "content": "MODULE 5: META-COGNITION, ETHICAL DECISION MAKING, ETHICAL THEORIES | 245 • Assess Impacts. Evaluate potential positive and negative effects. • Consider Alternatives. Explore different approaches or technologies. • Apply Ethical Principles. Use established ethical frameworks to guide decisions. • Monitor and Adjust. Continuously evaluate the technology’s impact and make changes as needed. Challenges in Ethical Governance of Emerging Technologies • Rapid Development. Technologies often advance faster than regulations can keep up. • Uncertainty. It’s hard to predict all potential impacts of new technologies. • Global Nature. Technologies often cross-national boundaries, making regulation complex. • Balancing Innovation and Caution. Encouraging progress while managing risks. As emerging technologies like quantum computing and 5G continue to develop, it’s crucial to consider their ethical implications. By applying ethical principles and decision- making frameworks, we can work to ensure these technologies benefit society while minimizing potential harms."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 264, "chunk_id": "english_pack_264_0_c3726655", "content": "246 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT Intellectual property and copyright are legal concepts that protect the rights of creators and owners of original works, such as books, music, software, and inventions. They grant them exclusive control over how their works are used, distributed, and modified by others. Digital rights management (DRM) is a technology that restricts the access and use of digital content, such as e-books, movies, and games. DRM aims to prevent unauthorized copying, sharing, or modifying of protected content. However, DRM also raises ethical and technical issues, such as limiting the fair use rights of consumers, interfering with the interoperability of devices and platforms, and creating security vulnerabilities. Open-source software licensing is a type of software licensing that allows anyone to access, use, modify, and distribute the source code of a software program. Open-source software is often developed collaboratively by a community"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 265, "chunk_id": "english_pack_265_0_57bc3481", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 247 of developers who share a common vision and values. Open- source software licensing promotes innovation, transparency, and freedom of choice for users and developers. 6.1. Intellectual Property & Copyright What is intellectual property? Intellectual property (IP) is any creation of the mind that has commercial value. It includes inventions, designs, artistic works, symbols, names and images. IP can be protected by law through patents, trademarks, copyrights and trade secrets. Why is IP important for cybersecurity? IP is one of the most valuable assets of any business. It gives a competitive edge, attracts customers and investors, and generates revenue. However, IP is also vulnerable to cyberattacks, theft, misuse, and infringement. Cybersecurity is the process of protecting IP from unauthorized access, use, disclosure, modification, or destruction. How to protect IP from cyber threats? Here are some tips to keep IP safe from cyber threats:"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 266, "chunk_id": "english_pack_266_0_27c7bab5", "content": "248 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT • Identify your IP assets and their value. Conduct an IP audit to find out what IP you have, who owns it, where it is stored and how it is used. • Implement appropriate security measures for your IP assets. Use encryption, authentication, access control, backup and recovery systems to safeguard your IP data. • Educate your employees and partners about IP protection. Provide training and awareness programs on IP policies, procedures and best practices. Monitor and enforce compliance with IP rules and agreements. • Register your IP rights where possible. Apply for patents, trademarks or copyrights to secure legal protection for your IP assets. Use notices and labels to indicate your ownership and rights. • Monitor your IP environment and respond to incidents. Use tools and services to detect and prevent IP breaches, such as firewalls, antivirus software, intrusion detection systems and threat intelligence. Report and resolve any IP issues as soon as possible. The Essence of Intellectual Property & Copyright IP is the intangible creation of the human mind, such as inventions, artistic works, designs, symbols, names and images. IP is protected by laws that grant exclusive rights to the creators"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 267, "chunk_id": "english_pack_267_0_a5a367d9", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 249 or owners of IP, such as patents, trademarks, designs and copyright. Why IP matters in cyberspace Cyberspace is the virtual environment where people communicate and interact through computer networks. Cyberspace is becoming a hub for IP infringement, as it is easy to copy, distribute and modify digital content without the owner’s consent. IP infringement can harm the owner’s reputation, revenue, and competitive advantage. It can also expose the infringer to legal risks and liabilities. Some common examples of IP infringement in cyberspace are: • Using another person’s logo, brand name or domain name without permission • Copying or downloading another person’s software, music, video, e-book or game without a licence • Making a profit by using another person’s creation without paying royalties or fees • Modifying or adapting another person’s work without authorisation • Selling counterfeit or pirated goods online How to protect your IP in cyberspace Take the following steps to protect your IP in cyberspace:"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 268, "chunk_id": "english_pack_268_0_698ee326", "content": "250 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT • Identify and audit your IP assets. Know what IP you have, who owns it, how it is used and how it is protected. • Register your IP rights. Apply for patents, trademarks and designs to secure your exclusive rights in Australia and overseas • Monitor your IP online. Use tools and services to detect and prevent unauthorised use of your IP on the internet. • Enforce your IP rights. Act against infringers by sending cease and desist letters, filing complaints or initiating legal proceedings. • Commercialise your IP. Negotiate and draft licensing, technology transfer, distribution and content agreements to generate income from your IP Where to find more information Visit the following websites: • Intellectual Property Lawyers Gilbert + Tobin : A leading Australian law firm that provides advice on all aspects of IP law • Intellectual Property, Technology & Cyber Security HopgoodGanim: A market-leading team of lawyers with scientific or technical qualifications in IP, technology and cyber security • Intellectual Property in Cyberspace GeeksforGeeks: A website that explains the basics of IP in cyberspace with"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 269, "chunk_id": "english_pack_269_0_1499caac", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 251 examples • Intellectual Property Crime Australian Federal Police: A website that provides information on how to report IP crime and what actions the AFP can take Cybersecurity Risks One of the main challenges of digital transformation is ensuring the security of your data and software systems. Data breaches are becoming more frequent and costly, exposing sensitive information, damaging reputations and causing legal liabilities. According to a report by Norton Rose Fulbright, there were 4,100 publicly disclosed data breaches in 2022 alone, comprising some 22 billion records that were exposed. Moreover, software systems are becoming more complex and vulnerable, especially with the rise of artificial intelligence and generative AI, which can create realistic but fake content that can deceive or manipulate users. Therefore, it is important to adopt a proactive and comprehensive approach to cybersecurity, that includes: • Developing a framework that aligns your technology strategy with your business goals and risk appetite. • Implementing zero trust architectures that assume all systems can or will be compromised and require continuous verification of users, devices and data. • Applying encryption, authentication and access control"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 270, "chunk_id": "english_pack_270_0_1937686b", "content": "252 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT measures to protect your data at rest and in transit. • Monitoring and auditing your systems for any anomalies or suspicious activities. • Updating and patching your software regularly to fix any vulnerabilities or bugs. • Educating and training your employees and customers on cybersecurity best practices and awareness Privacy Risks Another challenge of digital transformation is respecting the privacy rights of your customers, employees and partners. Privacy laws are becoming more stringent and diverse across jurisdictions, requiring you to comply with various rules and regulations on how you collect, use, store and share personal information. For example, the General Data Protection Regulation (GDPR) in the European Union imposes strict obligations on data controllers and processors, such as obtaining consent, providing transparency, ensuring data minimization and enabling data portability. Failing to comply with privacy laws can result in hefty fines, lawsuits and reputational damage. Therefore, you need to adopt a privacy- by-design approach that incorporates privacy principles into every stage of your digital transformation process, such as: • Conducting privacy impact assessments to identify and mitigate any potential privacy risks or harms. • Implementing privacy-enhancing technologies that"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 271, "chunk_id": "english_pack_271_0_2e082788", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 253 anonymize, pseudonymize or encrypt personal data. • Establishing privacy policies and notices that inform your data subjects about their rights and choices. • Obtaining valid and informed consent from your data subjects before processing their personal data • Responding to data subject requests to access, correct or delete their personal data. • Reporting any data breaches or incidents to the relevant authorities and data subjects within the prescribed time frames. Intellectual Property Rights Finally, one of the most important aspects of digital transformation is protecting your intellectual property rights. Intellectual property rights are the legal rights that grant you exclusive ownership and control over your creations, such as inventions, designs, trademarks, logos, slogans, software code, content etc. Intellectual property rights are essential for fostering innovation, competitiveness, and differentiation in the digital economy. However, digital transformation also poses new threats to your intellectual property rights, such as: • Copying or stealing your software code or content by hackers or competitors. • Infringing or violating your patents, trademarks or copyrights by using them without authorization or paying royalties."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 272, "chunk_id": "english_pack_272_0_f2bce5f5", "content": "254 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT • Diluting or tarnishing your brand image or reputation by creating confusingly similar or disparaging products or services. • Challenging or invalidating your intellectual property rights by claiming prior art or public domain status. Therefore, you need to adopt a strategic and proactive approach to intellectual property protection that includes: • Registering your intellectual property rights with the relevant authorities and agencies. • Enforcing your intellectual property rights against any infringers or violators through legal action or alternative dispute resolution. • Licensing your intellectual property rights to others for mutual benefit or collaboration. • Monitoring the market for any potential infringements or violations of your intellectual property rights. • Updating your intellectual property portfolio to reflect any changes or improvements in your products or services. Digital transformation comes with significant risks that can jeopardize your cybersecurity, privacy and intellectual property rights. Navigating the Australian Copyright"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 273, "chunk_id": "english_pack_273_0_85c742cf", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 255 Act What is copyright? Copyright is a legal right that gives the creator of an original work the exclusive right to control how it is used, reproduced, communicated, or performed. It covers a wide range of works, such as books, music, films, software, databases, artworks, photographs and more. It also covers some types of online content, such as websites, blogs, podcasts, and social media posts. Why is it important? Protecting your intellectual property is important for many reasons. It can help you: • Reward your creativity and innovation. • Prevent others from copying or exploiting your work without your permission. • Generate income from licensing or selling your work. • Enhance your reputation and brand recognition. • Contribute to the cultural and economic development of society."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 274, "chunk_id": "english_pack_274_0_04b1ef44", "content": "256 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT How does it work in Australia? Australia has a complex and evolving legal framework for copyright protection. Some of the key features are: • You do not need to register or apply for copyright protection. It is automatic once you create an original work in a material form. • You do not need to use the © symbol or any other notice to indicate your ownership. However, it may be helpful to do so as a reminder to others. • You have the right to take legal action against anyone who infringes your copyright, such as by copying, distributing, displaying, or modifying your work without your consent. • You may also have some moral rights, such as the right to be attributed as the author and the right to object to any derogatory treatment of your work. • You may grant or transfer some or all your rights to others through a licence or an assignment agreement. You should always read and understand the terms and conditions before signing any contract. • You may also allow others to use your work for free under certain circumstances, such as for fair dealing purposes (e.g. research, study, criticism, review, parody or satire) or under a Creative Commons licence. • You must respect the rights of other creators when you use their works. You should always seek permission or"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 275, "chunk_id": "english_pack_275_0_dfbfd2ff", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 257 rely on a valid exception before using any copyrighted material. • You must comply with any applicable laws and regulations that affect your online activities, such as the Online Safety Act 2021 (Cth), the Surveillance Legislation Amendment (Identify and Disrupt) Act 2021 (Cth) and the Security Legislation Amendment (Critical Infrastructure) Bill 2021 (Cth). These laws aim to enhance the security and safety of online platforms and services and may impose new obligations and responsibilities on you as a user or provider. Digital Rights Management (DRM) Balancing Rights & Access Digital Rights Management (DRM) is a set of technologies and protocols that protect digital content from unauthorized access, reproduction, and distribution. It is used to enforce copyright protection, licensing agreements, and access control for various forms of digital media, such as music, videos, eBooks, software, and more. Benefits of DRM DRM provides a crucial layer of protection for content creators and owners. It helps them safeguard their intellectual"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 276, "chunk_id": "english_pack_276_0_f5a6e45e", "content": "258 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT property rights and prevent piracy or data breaches. By using DRM, content creators can: • Control how their content is used, shared, or modified by authorized users. • Generate revenue from their content by charging fees or subscriptions. • Monitor the usage and performance of their content. • Enhance the user experience by providing high-quality and secure content. DRM also benefits content consumers by ensuring that they receive legitimate and quality content. It also helps them respect the rights and wishes of the content creators. Challenges of DRM DRM is not without its limitations. Some of the common issues that DRM faces are: • Compatibility. Different platforms and devices may use different DRM systems, which can cause problems for users who want to access the same content across multiple devices • Usability. DRM may impose restrictions or requirements that can affect the user experience, such as requiring internet connection, limiting the number of devices or downloads, or preventing offline access"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 277, "chunk_id": "english_pack_277_0_7bfec797", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 259 • Privacy. DRM may collect personal or behavioural data from users, which can raise concerns about data protection and consent • Fair use. DRM may interfere with the rights of users to use the content for legitimate purposes, such as education, research, criticism, or parody. Best Practices for DRM To balance the rights and access of both content creators and consumers, it is important to follow some best practices when implementing or using DRM solutions. Some of these are: • Choose a suitable DRM system that meets your needs and goals. There are different types of DRM systems available, such as encryption-based, watermark-based, or fingerprint-based. You should consider factors such as cost, complexity, security level, compatibility, and scalability when selecting a DRM system. • Use a multi-DRM strategy to protect your streams on all platforms with strict licensing rules. A good DRM vendor will allow you to do all the following to protect your streams: • Prevent screen capture. • Prevent downloading of the streams by using the strictest variants of the DRM available. • Ensure a strict expiration date in the license beyond which the stream will be inaccessible."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 278, "chunk_id": "english_pack_278_0_87053ef6", "content": "260 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT • Provide the option to rotate the DRM keys during the live streams to frustrate hackers. • Communicate clearly with your users about the terms and conditions of your DRM policy. You should inform them about what they can and cannot do with your content, how long they can access it, what data you collect from them, and how you protect their privacy. • Respect the fair use rights of your users and allow them some flexibility in using your content for legitimate purposes. You should also provide them with options to contact you or request permission if they have any questions or issues with your DRM policy. • Keep up to date with the latest developments and trends in DRM technology and legislation. You should monitor the changes in the market and the legal environment and adjust your DRM strategy accordingly. Open source & Licensing Considerations OSS is software that uses publicly available source code that anyone can see, modify, and distribute. OSS can offer many benefits, such as affordability, flexibility, and quality, but it also comes with some risks and challenges that you need to be aware of."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 279, "chunk_id": "english_pack_279_0_77674d84", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 261 Types of Open-Source Licenses One of the main challenges of using OSS is complying with the terms and conditions of the open-source licenses. These are legal agreements that specify what you can and cannot do with the OSS and its derivatives. There are two main types of open- source licenses: permissive and copyleft. Permissive licenses are the more business-friendly ones, as they allow you to use, modify, and distribute the OSS for any purpose if you give proper attribution to the original authors. Some examples of permissive licenses are the MIT license, the Apache license, and the BSD license. Copyleft licenses are the more restrictive ones, as they require you to share your modifications and derivatives under the same or compatible license as the original OSS. This means that if you use copyleft OSS in your proprietary software, you might have to disclose your source code and allow others to use it for free. Some examples of copyleft licenses are the GNU General Public License (GPL), the GNU Lesser General Public License (LGPL), and the Mozilla Public License (MPL). Risks and Best Practices Using OSS can introduce some risks to your cybersecurity projects, such as: • Excessive access. Open access means that anyone can see"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 280, "chunk_id": "english_pack_280_0_883c8e71", "content": "262 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT and manipulate the source code, which creates opportunities for malicious actors to introduce vulnerabilities or backdoors. • Lack of verification. There are no guarantees that the OSS is tested and reviewed by qualified experts, which can make it prone to errors and security flaws. • Lack of support. Most OSS does not have dedicated support teams, which means that updates and patches may not be available or timely. This can leave your software exposed to known or unknown vulnerabilities. To mitigate these risks, you should follow some best practices when using OSS, such as: • Conduct a thorough due diligence. Before using any OSS, you should check its license type, terms, and conditions, and make sure they are compatible with your intended use and distribution. You should also check its reputation, quality, security, and maintenance status. • Use a software composition analysis tool. This is a tool that can help you identify and manage the OSS components in your software. It can help you track their licenses, versions, dependencies, vulnerabilities, and compliance status. • Implement a security policy. You should have a clear and consistent policy for using OSS in your projects. This policy should define the roles and responsibilities of"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 281, "chunk_id": "english_pack_281_0_2dd9c137", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 263 your team members, the criteria for selecting OSS components, the processes for reviewing and updating them, and the procedures for reporting and resolving any issues. Using OSS can be a great way to enhance your cybersecurity projects with high-quality software components. However, you need to be careful about the legal and security implications of using OSS. By following the types of open-source licenses, understanding their risks, and applying best practices, you can use OSS safely and effectively. Fair Use & Flexibility What is Fair Use and Flexibility? Fair use and flexibility are legal doctrines that allow the use of copyrighted material without permission or payment under certain circumstances. They are essential for promoting creativity, innovation, education, research, and public interest. In the context of cybersecurity, fair use and flexibility can enable security professionals to access, analyse, test, and improve the security of digital systems and data. For example, fair use and flexibility can allow security researchers to reverse engineer software, conduct vulnerability assessments, disclose security flaws, and develop patches or workarounds."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 282, "chunk_id": "english_pack_282_0_cd2eebd3", "content": "264 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT Why is Fair Use and Flexibility Important for Cybersecurity? Fair use and flexibility are important for cybersecurity because they can help: • Enhance the security posture of organizations and individuals by allowing them to identify and mitigate risks, protect their assets, and respond to incidents. • Foster a culture of security awareness and collaboration by allowing security professionals to share their findings, insights, and best practices with others. • Support the development of new security technologies and solutions by allowing security professionals to experiment with different methods, tools, and techniques. • Advance the state of the art in cybersecurity by allowing security professionals to contribute to the scientific knowledge and innovation in the field. What are the Challenges and Risks of Fair Use and Flexibility in Cybersecurity? Fair use and flexibility are not absolute rights. They are subject to limitations and exceptions depending on the jurisdiction, context, purpose, nature, amount, and effect of the use. They are also balanced against the rights and interests of the copyright holders."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 283, "chunk_id": "english_pack_283_0_09e9353a", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 265 Therefore, fair use and flexibility in cybersecurity can pose some challenges and risks, such as: • Legal uncertainty and liability. Security professionals may face legal challenges or lawsuits from copyright holders who claim that their use of the material was unauthorized or infringing. Security professionals may also face criminal charges or penalties if their use of the material violates other laws or regulations. • Ethical dilemmas and conflicts. Security professionals may encounter ethical dilemmas or conflicts when deciding whether, how, when, and with whom to use or share the material. Security professionals may also face criticism or backlash from their peers, employers, clients, or the public for their use or disclosure of the material. • Operational difficulties and costs. Security professionals may face operational difficulties or costs when obtaining, storing, processing, or transmitting the material. Security professionals may also face technical challenges or limitations when using or modifying the material."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 284, "chunk_id": "english_pack_284_0_79e37a25", "content": "266 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT Digital Commons & Collaborative Creation What are digital commons and collaborative creation? Digital commons are resources that are shared by a community of users online, such as open-source software, open data, open educational resources, and creative commons licenses. Collaborative creation is the process of producing digital content or knowledge by working together with others, such as through wikis, blogs, podcasts, or social media platforms. Why are they important? Digital commons and collaborative creation can foster innovation, creativity, education, and social inclusion. They can also reduce costs, increase efficiency, and improve quality of digital products and services. For example, Wikipedia is a collaborative encyclopedia that anyone can edit, which provides free and reliable information to millions of users around the world. Linux is an open-source operating system that powers many servers, devices, and applications, which benefits from the contributions of thousands of developers and users."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 285, "chunk_id": "english_pack_285_0_a69cdd8d", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 267 What are the cybersecurity risks? However, digital commons and collaborative creation also pose cybersecurity risks that need to be addressed. These risks include: • Unauthorized access or modification of digital resources by hackers, competitors, or malicious insiders • Theft or leakage of sensitive or personal data by cybercriminals, spies, or whistleblowers • Infringement or violation of intellectual property rights by copycats, pirates, or trolls • Disruption or sabotage of digital services or infrastructure by activists, terrorists, or state actors • Misinformation or manipulation of digital content or users by propagandists, fraudsters, or bots How to protect them? These include: • Implementing strong authentication and authorization mechanisms to verify the identity and access rights of users and contributors. • Encrypting data in transit and at rest to prevent unauthorized interception or extraction. • Applying digital signatures or watermarks to prove the origin and integrity of digital resources."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 286, "chunk_id": "english_pack_286_0_c3aeb95a", "content": "268 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT • Monitoring and auditing the activity and performance of digital systems and networks to detect and respond to anomalies or incidents. • Educating and engaging the community of users and contributors to raise awareness and foster trust and cooperation. Where to learn more? If you want to learn more about digital commons and collaborative creation, you can visit the following websites: • Rebuilding digital trust for a cyber-inclusive future – World Economic Forum • Cybersecurity, cybercrime and cybersafety: a quick guide to key internet links – Parliament of Australia • The Digital World Is Changing Rapidly. Your Cybersecurity Needs to Keep Up Digital rights management (DRM) is a term that encompasses the methods and technologies used to protect and control the access and use of digital content, such as books, music, videos, software, and data. DRM aims to prevent unauthorized copying, sharing, modification, or distribution of digital content by applying various restrictions and encryption techniques to the content or the devices that can access it."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 287, "chunk_id": "english_pack_287_0_bb611d2f", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 269 Why is DRM important? DRM is important for several reasons. First, it helps content creators and owners to safeguard their intellectual property rights and their financial and creative investments in their work. By limiting what users can do with their content, DRM ensures that content creators and owners can benefit from their work and prevent others from exploiting it without permission or compensation. Second, DRM helps users to respect the legal and ethical boundaries of using digital content. By complying with the terms and conditions of DRM, users can avoid infringing on the rights of content creators and owners and avoid potential legal consequences or penalties. Third, DRM helps to maintain the quality and integrity of digital content. By preventing unauthorized modification or alteration of digital content, DRM ensures that users can access and enjoy the original and authentic version of the content as intended by the content creators and owners. How does DRM work? DRM works by using various technologies and tools to implement different types of restrictions and encryption on digital content. Some of the common DRM methods are: Copy protection. This method prevents or limits users"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 288, "chunk_id": "english_pack_288_0_0aeeacd5", "content": "270 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT from making copies of digital content or transferring it to other devices or platforms. Access control. This method requires users to have a valid license, password, or authentication to access digital content or certain features or functions of it. Expiration. This method sets a time limit, or several uses for accessing digital content, after which the content becomes inaccessible or unusable. Geolocation. This method restricts access to digital content based on the user’s location or IP address. Watermarking. This method embeds a visible or invisible mark on digital content that identifies the source or owner of the content. Encryption. This method scrambles the data of digital content using a secret key that only authorized users can decrypt. What are some examples of DRM? DRM is widely used across various types of digital content and industries. Some examples are: E-books. Many e-books use DRM to prevent users from copying, printing, sharing, or modifying them. Some e-books also use DRM to limit the number of devices or platforms that users can read them on. Music. Many music files use DRM to prevent users from copying, sharing, or converting them to other formats. Some"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 289, "chunk_id": "english_pack_289_0_83fb4b0b", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 271 music files also use DRM to limit the number of devices or platforms that users can play them on. Videos. Many videos use DRM to prevent users from copying, sharing, or editing them. Some videos also use DRM to limit the resolution, quality, or playback speed of them. Software. Many software programs use DRM to prevent users from installing, copying, sharing, or modifying them. Some software programs also use DRM to require online activation, registration, or subscription to use them. Data. Many data sets use DRM to prevent users from accessing, copying, sharing, or analysing them. Some data sets also use DRM to require payment, permission, or attribution to use them. Benefits and Drawbacks of DRM DRM can have positive effects for both content providers and users. For content providers, DRM can help them: • Protect their intellectual property rights and prevent revenue loss from piracy and unauthorized copying. • Enhance their reputation and brand image by ensuring the quality and authenticity of their content. • Increase their market share and customer loyalty by offering different options and incentives for accessing their content. • Innovate and create new business models and revenue"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 290, "chunk_id": "english_pack_290_0_57a092a6", "content": "272 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT streams by leveraging the potential of digital technologies. For users, DRM can help them: • Access a wide range of digital content at affordable prices and convenient formats. • Enjoy a better user experience and quality of service by avoiding malware, viruses, glitches and errors. • Support their favourite content creators and contribute to the development of the digital economy. • Exercise their rights to privacy, security, anonymity and fair use by choosing the content providers and platforms that respect these rights. However, DRM can also have negative effects for both parties. For content providers, DRM can: • Increase their costs and complexity of developing, maintaining and updating their DRM systems. • Reduce their flexibility and adaptability to changing market conditions and customer preferences. • Expose them to legal risks and liabilities if their DRM systems violate users’ rights or infringe on other parties’ intellectual property rights. • Damage their reputation and customer satisfaction if their DRM systems are perceived as intrusive, restrictive"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 291, "chunk_id": "english_pack_291_0_4e1dbfef", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 273 or unfair. For users, DRM can: • Limit their access, use and enjoyment of digital content by imposing technical or contractual restrictions. • Interfere with their legitimate activities and expectations, such as sharing, lending, reselling or modifying the content. • Violate their rights to privacy, security, anonymity and fair use by collecting, storing or disclosing their personal data or monitoring their online behaviour. • Harm their devices or data by introducing malware, viruses, glitches or errors. Challenges & controversies of DRM DRM is not without its challenges and controversies. Some of the common issues are: User rights. Some users argue that DRM violates their fair use rights or their right to own and control their purchased digital content. They claim that DRM restricts their ability to make personal copies, backups, modifications, or adaptations of digital content for their own purposes. User experience. Some users complain that DRM negatively affects their user experience by making digital content less accessible, convenient, compatible, or functional."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 292, "chunk_id": "english_pack_292_0_b98ab02b", "content": "274 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT They claim that DRM causes technical problems, errors, glitches, or incompatibilities with their devices or platforms. User privacy. Some users worry that DRM invades their privacy by collecting their personal information, tracking their online activities, or exposing them to security risks. They claim that DRM requires them to share their personal data with third parties, such as content providers, service providers, or advertisers. User activism. Some users resist or challenge DRM by circumventing it using various tools or techniques, such as cracking codes, hacking systems, or creating alternative platforms. They claim that they are exercising their civil disobedience rights or their freedom of expression rights. Design principles for DRM Systems Given these benefits and drawbacks, how can we design and implement DRM systems that balance the interests of both content providers and users? Here are some principles and guidelines that I suggest: Respect the law. DRM systems should comply with the relevant laws and regulations in the jurisdictions where they operate. They should not infringe on other parties’ intellectual property rights or violate users’ rights to privacy, security, anonymity or fair use. Respect the ethics. DRM systems should follow the ethical standards and values of the society where they operate."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 293, "chunk_id": "english_pack_293_0_93a41275", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 275 They should not harm or exploit users or other stakeholders. They should promote social justice, human dignity and public interest. Respect the users. DRM systems should consider the needs, preferences and expectations of the users. They should provide clear information about the terms and conditions of accessing the content. They should offer choices and options for different user groups. They should ensure a high quality of service and user experience. Respect the content. DRM systems should protect the integrity and authenticity of the content. They should not degrade or distort the content. They should not interfere with the artistic or creative expression of the content creators. Respect the innovation. DRM systems should foster innovation and creativity in the digital economy. They should not stifle or hinder the development of new technologies, products or services. They should not create artificial barriers or monopolies in the market. Ethical Implications of DRM Digital rights management (DRM) applies to the copying, sharing, or modifying of digital content such as music, movies, software, or e-books. DRM can be seen to protect the rights and revenues of the creators and distributors of digital content, but it can also raise ethical issues for the users and consumers of such content."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 294, "chunk_id": "english_pack_294_0_6d4bfda4", "content": "276 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT What is Restrictive DRM? Restrictive DRM is a type of DRM that imposes strict limitations on how users can access, use, or transfer digital content. For example, restrictive DRM may: • Stop a cell phone from working with a different wireless provider. • Make a DVD from a certain region unplayable in other regions of the world. • Encrypt software to prevent copying or installing on multiple devices. • Prevent children from accessing adult content. • Require online verification or authentication to use certain products or services. Restrictive DRM can be implemented through hardware, software, or legal means. Some examples of restrictive DRM technologies are: • Region codes on DVDs or Blu-ray discs. • Activation codes or serial numbers for software products. • Digital locks or encryption keys on e-books or music files. • Online platforms or services that require subscription or registration."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 295, "chunk_id": "english_pack_295_0_d09cea6f", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 277 Restrictive DRM can also be enforced through legal measures such as the Digital Millennium Copyright Act (DMCA) in the United States, which prohibits the circumvention of DRM technologies or the distribution of tools or devices that can bypass them. Why is Restrictive DRM Ethical? Some of the arguments in favour of restrictive DRM are: • It protects the intellectual property rights and interests of the creators and distributors of digital content, who invest time, money, and effort to produce and deliver quality products and services. • It prevents piracy and illegal use of digital content, which can harm the revenues and reputation of the content industry and reduce the incentives for innovation and creativity. • It enables new business models and revenue streams for the content industry, such as subscription-based services, pay-per-view models, or dynamic pricing strategies. • It provides users with features and benefits that they want or need, such as parental controls, trial versions, or personalized recommendations. Some of the sources that support restrictive DRM are: • The Entertainment Software Association of Canada"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 296, "chunk_id": "english_pack_296_0_64bdadfa", "content": "278 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT (ESAC), which represents the video game industry in Canada. ESAC argues that DRM is necessary to protect the investments and innovations of game developers and publishers, and to provide consumers with diverse and high-quality gaming experiences. • The Alliance of Canadian Cinema, Television and Radio Artists (ACTRA), which represents performers in the audiovisual media sector in Canada. ACTRA advocates for DRM to ensure fair compensation and recognition for artists whose works are distributed digitally. • Microsoft Corporation, which is one of the leading developers and providers of software products and services in the world. Microsoft uses DRM technologies to secure its products and platforms, such as Windows operating system, Office suite, Xbox console, or Azure cloud service. Why is Restrictive DRM Unethical? Some of the arguments against restrictive DRM are: • It infringes on the rights and freedoms of users and consumers of digital content, who may face restrictions or barriers to access, use, or share content that they have legally acquired or paid for. • It creates technical and legal challenges for users and consumers of digital content, who may encounter"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 297, "chunk_id": "english_pack_297_0_898e28e7", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 279 compatibility issues, performance problems, privacy risks, or legal liabilities when using or transferring content across different devices, platforms, or regions. • It stifles innovation and competition in the content industry, as it creates entry barriers for new entrants or alternative providers who may offer better quality or lower prices for digital content. • It reduces the social and cultural value of digital content, as it limits the possibilities for remixing, reusing, or transforming content into new forms of expression or knowledge. Some of the sources that oppose restrictive DRM are: • The Electronic Frontier Foundation (EFF), which is a non-profit organization that defends civil liberties in the digital world. EFF campaigns against DRM as a threat to user rights, fair use, privacy, security, accessibility, and innovation. • The Canadian Library Association (CLA), which is a national association that represents libraries and librarians in Canada. CLA opposes DRM as an obstacle to access to information, education, culture, and democracy. • The Free Software Foundation (FSF), which is a non- profit organization that promotes free software and free culture. FSF rejects DRM as a form of digital restriction"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 298, "chunk_id": "english_pack_298_0_22a0d2cf", "content": "280 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT management that violates user freedom and autonomy. Restrictive DRM is a controversial topic that involves ethical dilemmas for both producers and consumers of digital content. On one hand, restrictive DRM can be seen as a legitimate and necessary way to protect the rights and interests of the content industry, and to provide users with features and benefits that they want or need. On the other hand, restrictive DRM can be seen as an illegitimate and unnecessary way to infringe on the rights and freedoms of users and consumers, and to create technical and legal challenges, stifle innovation and competition, and reduce the social and cultural value of digital content. The ethical implications of restrictive DRM depend on the perspective, values, and interests of the stakeholders involved, as well as the context, purpose, and effects of the DRM technologies or measures used. 6.3. Open-Source Software Licensing The Essence of Open source Open-source software is software that allows anyone to use, modify, and share its source code. The source code is the set of instructions that tells the computer what to do. By making"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 299, "chunk_id": "english_pack_299_0_71efb7ff", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 281 the source code available, open-source software enables collaboration, innovation, and transparency. Why Open-source Matters Open-source software has many benefits for users, developers, and society. Some of these benefits are: Users can choose from a variety of software options that suit their needs and preferences. They can also inspect the source code to verify its quality, security, and functionality. Developers can learn from other developers’ work, improve existing software, or create new software based on existing code. They can also contribute to the development of software that they use or care about. Society can benefit from the collective knowledge and creativity of the open-source community. Open-source software can also promote social good by addressing common problems or serving public interests. How Open-source Works Open-source software is governed by licenses that define the terms and conditions for its use, modification, and distribution. There are many different open-source licenses, but they generally fall into two categories: permissive and copyleft. Permissive licenses allow users to do whatever they want with the software, provided they give credit to the"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 300, "chunk_id": "english_pack_300_0_e6773f3e", "content": "282 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT original author. Examples of permissive licenses are the MIT License and the Apache License. Copyleft licenses require users to share their modifications of the software under the same or compatible license as the original. This ensures that the software remains open-source and accessible to everyone. Examples of copyleft licenses are the GNU General Public License and the Mozilla Public License. How to Choose an Open-source License Choosing an open-source license depends on your goals and preferences as a software developer. Some factors to consider are: • How much control do you want to have over your software and its derivatives? • How much credit do you want to receive for your work? • How compatible do you want your license to be with other open-source licenses? • How important is it for you to protect your software from potential legal risks? Choose a License website to compare different open-source licenses and find one that matches your needs. You can also consult a lawyer or an expert in IT governance, policy, ethics, and law if you have specific questions or concerns. By choosing an open-source license, you can define how"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 301, "chunk_id": "english_pack_301_0_77b49f5d", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 283 others can use, modify, and distribute your software. You can also join a community of developers who collaborate on creating and improving open-source software. The Ethics of Collaboration & Innovation Open-Source Software Licensing Open-source software (OSS) is software that is distributed with a license that allows anyone to use, study, change, or share its source code, without restrictions on how the software is used or by whom. OSS has become ubiquitous across all areas of software development, as it enables developers to reuse existing code and create more functionality at greater speed. OSS also promotes the adoption of transparent standards and makes applications more interoperable. However, OSS also raises some ethical questions about how the software is used and who benefits from it. Some developers do not want their work to be used for harm, such as military or surveillance purposes, while others think that restricting OSS is contradictory or impractical. Moreover, some OSS licenses may impose obligations on the users or distributors of the software, such as disclosing the source code, providing attribution, or sharing modifications."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 302, "chunk_id": "english_pack_302_0_e8714b4d", "content": "284 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT The Hippocratic License One example of an ethical OSS license is the Hippocratic License, created by Coraline Ada Ehmke in 2019. This license is based on the MIT license but adds a condition that the software may not be used for systems or activities that violate the United Nations Universal Declaration of Human Rights. The Hippocratic License aims to give developers more control over how their software is used and to prevent it from being used for evil. However, the Hippocratic License is not approved by the Open-Source Initiative (OSI), which governs the most widely used OSS licenses. The OSI argues that the Hippocratic License is not conformant with the Open-Source Definition (OSD), which requires that OSS licenses do not discriminate against persons, groups, or fields of endeavour. The OSI also claims that the Hippocratic License is vague and subjective, as it relies on the interpretation of human rights by different users and jurisdictions. The OpenChain Project Another approach to address the ethical issues of OSS licensing is the OpenChain Project, which is an initiative by the Linux Foundation to establish best practices for OSS compliance. The OpenChain Project provides a specification and a certification program for organizations that use OSS in their products or services. The OpenChain Project aims to"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 303, "chunk_id": "english_pack_303_0_ac3c4191", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 285 ensure that OSS users respect the rights and obligations of OSS developers and licensors, and that they provide clear and consistent information about the OSS components they use. The OpenChain Project does not impose any ethical restrictions on how OSS is used, but rather focuses on improving the transparency and accountability of OSS usage. The OpenChain Project also helps organizations to avoid legal risks and reduce costs associated with OSS compliance. Developers who create or use OSS should be aware of the different types of OSS licenses and their implications for collaboration and innovation. Developers should also respect the intentions and expectations of other developers who contribute to or depend on OSS. By following best practices and standards for OSS compliance, developers can ensure that they use OSS in a responsible and ethical manner. What is an Open-Source License? An open-source license is a type of software license that complies with the Open-Source Definition. In brief, it allows software to be freely used, modified, and shared by anyone for any purpose, if the license terms are respected. There are many different open-source licenses, and they vary based on the restrictions or conditions they impose on the software users."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 304, "chunk_id": "english_pack_304_0_dfbd9445", "content": "286 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT Choosing an Open-Source License Some general factors to consider are: Compatibility. Some open-source licenses are compatible with each other, meaning that you can combine or distribute software under different licenses without violating any terms. Some licenses are incompatible with each other, meaning that you cannot do so without obtaining additional permissions or agreements. You should check the compatibility of your chosen license with other licenses that you may want to use or interact with in the future. Copyleft. Some open-source licenses are copyleft, meaning that they require any modified or derived versions of the software to be distributed under the same or equivalent license. This ensures that the software remains open-source and preserves the original author’s rights and intentions. Some licenses are permissive, meaning that they do not impose such a requirement and allow more flexibility for the software users. You should decide whether you want your software to be copyleft or permissive, depending on your preferences or objectives. Popularity. Some open-source licenses are more popular or widely used than others, meaning that they have more recognition or acceptance in the open-source community. This can affect how easy it is to find or collaborate with other projects that use the same or similar licenses. You should"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 305, "chunk_id": "english_pack_305_0_dce82776", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 287 consider whether you want your software to use a popular or less popular license, depending on your needs or expectations. Ethical Considerations OSS comes with several ethical challenges and responsibilities for both contributors and users. Why Ethics Matter for OSS OSS is not just a technical matter; it is also a social and political one. OSS can have positive or negative impacts on society, depending on how it is used and by whom. For example, OSS can be used for military purposes, surveillance, misinformation, or discrimination. OSS can also be vulnerable to security breaches, bugs, or malicious code. Therefore, OSS contributors and users should consider the ethical implications of their actions and decisions. Avoiding Ethical Dilemmas OSS also poses some ethical challenges for developers and users, such as: • How to respect the human rights and dignity of those who may be affected by the software? • How to ensure the quality and security of the software"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 306, "chunk_id": "english_pack_306_0_1ab06ce9", "content": "288 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT and prevent harm or misuse? • How to balance the freedom of OSS with the responsibility of its creators and contributors? • How to deal with ethical conflicts or dilemmas that may arise from the use of OSS in different contexts or for different purposes? Respect the Hippocratic Principle: Do No Harm The Hippocratic principle is a moral principle that states that one should do no harm or avoid doing harm. It is derived from the Hippocratic oath, a code of ethics for physicians that dates to ancient Greece. The Hippocratic principle can be applied to OSS development and use, as a way of ensuring that the software does not cause harm to individuals, groups, or society at large. One way of respecting the Hippocratic principle is to adopt an ethical license for OSS, such as the Hippocratic License, which was created by Coraline Ada Ehmke, a software developer from Chicago. The Hippocratic License is a license that puts ethical restrictions on the use of OSS code, such as prohibiting its use for violating human rights or dignity. The Hippocratic License aims to give developers more control over how their software is used and to prevent its use for harmful purposes."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 307, "chunk_id": "english_pack_307_0_ea0d7fe6", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 289 6.4 Non-Fungible Tokens (NFTs) and Digital Ownership Non-Fungible Tokens, or NFTs, represent a revolutionary approach to digital ownership and asset management. Unlike cryptocurrencies such as Bitcoin or Ethereum, which are fungible (meaning each unit is interchangeable), NFTs are unique digital assets. Each NFT has distinct properties and values, making it non-interchangeable with any other token. How NFTs Work NFTs operate on blockchain technology, most commonly on the Ethereum network. They are created or “minted” using smart contracts, which are self-executing contracts with the terms of the agreement directly written into code. When an NFT is created, it’s assigned a unique identifier on the blockchain, ensuring its authenticity and ownership can be verified. Types of Digital Assets as NFTs NFTs can represent ownership of a wide variety of digital assets. These include digital art, music, videos, virtual real estate in online worlds, in-game items, and even tweets or memes. Essentially, any digital content that can be uniquely identified can be tokenized as an NFT."}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 308, "chunk_id": "english_pack_308_0_11a68440", "content": "290 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT NFTs and Copyright It’s important to note that owning an NFT doesn’t necessarily mean owning the copyright to the underlying work. The rights transferred with an NFT sale depend on the terms set by the creator. In many cases, the NFT represents a claim of ownership to a specific copy of a digital work, similar to owning a signed print of a photograph, rather than owning the rights to reproduce that photograph. Benefits of NFTs NFTs offer several potential benefits. For creators, they provide a way to monetize digital works directly, potentially cutting out middlemen. They can also enable ongoing royalties for creators every time the NFT is resold. For buyers, NFTs offer verifiable ownership and provenance of digital assets, which was previously difficult to establish for digital goods. Challenges and Criticisms Despite their potential, NFTs face several challenges and criticisms. Environmental concerns have been raised due to the energy-intensive nature of blockchain transactions. There are also issues of market volatility, with NFT values fluctuating wildly. Fraud and copyright infringement are ongoing problems, with some people minting NFTs of works they"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 309, "chunk_id": "english_pack_309_0_7a984e2c", "content": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT | 291 don’t own. Additionally, there are concerns about the long- term value and permanence of NFTs, especially if the platforms hosting the linked content cease to exist. Legal Implications The legal landscape surrounding NFTs is still evolving. Questions about intellectual property rights, taxation, and regulatory compliance are being debated in jurisdictions around the world. For example, how should NFT sales be taxed? What happens if an NFT links to illegal content? These and many other legal questions are yet to be fully resolved. NFTs and the Future of Digital Ownership Despite the challenges, many see NFTs as a glimpse into the future of digital ownership. They could potentially be used for things like digital identity verification, ticketing for events, or proof of ownership for physical assets. Some envision a future where most forms of ownership, both digital and physical, are represented by NFTs on a blockchain. Impact on Creative Industries NFTs have already begun to reshape parts of the creative industry. They’ve opened up new revenue streams for digital"}
{"pack_id": "english_pack", "section_title": "MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT", "page": 310, "chunk_id": "english_pack_310_0_702e7a1a", "content": "292 | MODULE 6: INTELLECTUAL PROPERTY & COPYRIGHT artists and have the potential to change how we think about ownership in the digital realm. However, they’ve also raised questions about the nature of art and value in the digital age. Non-Fungible Tokens represent a significant development in the concept of digital ownership. While they offer exciting possibilities for creators and collectors, they also present complex challenges in terms of regulation, environmental impact, and long-term viability. As the technology and its applications continue to evolve, it will be crucial for policymakers, technologists, and users to work together to address these challenges and harness the potential benefits of NFTs."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 311, "chunk_id": "english_pack_311_0_56568088", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 293 MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE Agencies in Australia that Investigate Cybercrime. Australia has established several key agencies dedicated to investigating cybercrime. These include the Australian Cyber Security Centre (ACSC), the Australian Federal Police (AFP), and the Australian Criminal Intelligence Commission (ACIC). These agencies work collaboratively to combat cyber threats, protect national interests, and maintain cybersecurity. Understanding their roles and cooperation is vital in the fight against cybercrime within the country. Cyber forensics is a crucial discipline in the realm of cybersecurity. It involves the collection, preservation, analysis, and presentation of digital evidence to uncover cybercrimes. Cyber forensic experts use their skills to track down hackers, investigate data breaches, and support legal proceedings. This field plays a pivotal role in maintaining the security and integrity of digital information and is essential for both cybersecurity professionals and law enforcement agencies."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 312, "chunk_id": "english_pack_312_0_b52c2c5c", "content": "294 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE Cyber intelligence is the collection and analysis of data related to cyber threats and vulnerabilities. It provides insights into potential cyberattacks, helping organizations and governments take proactive measures to protect their information systems. In Australia, agencies like the Australian Signals Directorate (ASD) engage in cyber intelligence activities to safeguard national security. Cyber intelligence is a critical component of modern cybersecurity, enabling timely responses to emerging threats and ensuring the resilience of digital infrastructure. Understanding its principles is vital for cybersecurity professionals and policymakers alike. 7.1 Agencies that Investigate Cyber Crime The Commonwealth of Australia has a National Plan to Combat Cybercrime that includes a wide variety of agencies and stakeholders. The list that follows shows the spectrum of government agencies whose combined efforts amount to Australia’s response to cybersecurity and cybercrime prevention. • Attorney-General’s Department (AGD): formulates Commonwealth criminal law policy for parliament to enact. It includes such matters as personal identity security, privacy and wire-tapping policy. • Australian Criminal Intelligence Commission"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 313, "chunk_id": "english_pack_313_0_d2bd9060", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 295 (ACIC): Australia’s national criminal intelligence agency provides independent advice to government on current and developing risks of organised crime. ACIC has wide-ranging investigative capabilities from which it produces strategic intelligence assessments. It coordinates the effort to disrupt the impact of organised crime in Australia. • Australian Federal Police (AFP): Enforcement of federal criminal law and the proactive protection of Australia’s interests from crime at home and overseas. The AFP has high capability to investigate, disrupt and apprehend cyber-criminals. • Australian Transaction Reports and Analysis Centre (AUSTRAC) is the Australian government’s financial intelligence agency that monitors financial transactions to detect money laundering, organised crime, tax evasion, welfare fraud and terrorism. • Commonwealth Director of Public Prosecutions (CDPP): Works with AFP and ACIC to prosecute offenders. Also provides advice to other prosecuting and investigating agencies at the State level in relation to cybercrime offences. • State and Territory law and justice agencies: Concerned with criminal law policy at the state and Territory level. • State and Territory police: Enforcement of State and Territory law. Police cybercrime units investigate all"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 314, "chunk_id": "english_pack_314_0_c32b58cd", "content": "296 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE cyber offences against the person, business, and state, territory and local government. Other related agencies: • CERT Australia: The initial point of contact for cyber security incidents occurring in or impacting on Australian networks. • Australian Communications and Media Authority (ACMA): Notifies Internet Service Providers of transient threats such as malware identified among their customers. Also provides a channel of communication for reporting illegal online content. • Australian Competition and Consumer Commission (ACCC): Disrupts scams and prosecutes under the Competition and Consumer Act 2010 (Cth). • Australian Security Intelligence Organisation (ASIO): Concerned with cyber activity for the purpose of espionage, sabotage, terrorism or other forms of politically motivated violence. Works with other investigatory agencies to prevent efforts directed against Australia. • Australia New Zealand Policing Advisory Agency (ANZPAA): A trans-Tasman advisory and coordinating body that provides policy advice on cross-jurisdictional issues. • CrimTrac: A national database aimed at disseminating"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 315, "chunk_id": "english_pack_315_0_99c62c2e", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 297 timely advice to state and federal agencies and stakeholders. • Department of Broadband, Communications and the Digital Economy (DBCDE): Responsible for the provision of internet services to government, industry and the community. • Department of Defence’s Cyber Security Operations Centre (CSOC): Concerned with identifying sophisticated cyber threats against Australia. • Department of Foreign Affairs and Trade (DFAT): Protects Australia’s interests by combating cybercrime internationally. • Department of the Prime Minister and Cabinet (PM&C): Central coordinator of cyber policy. 7.2 Cyber Forensics Cyber forensics is an extensive discipline in its own right — a fit topic for a course all its own. This section gives an overview of the discipline, highlighting methods and important considerations. While it will not make the average cyber- security professional a forensics expert, it will nonetheless acquaint them with the principles, and equip them to communicate with forensics consultants in a meaningful way."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 316, "chunk_id": "english_pack_316_0_bbfd0914", "content": "298 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE Legal Issues While a data breach cause might be determined through the application of forensic techniques, certain legal issues might complicate matters. For example, the ‘Trojan Defence’ which allows an apparent perpetrator to argue that it was not they, but a piece of malicious computer code, or Trojan, that performed the actions unbeknown to them. A competent forensic investigator could anticipate this defence and obtain evidence to dismiss the argument. Scene of the Crime Information systems, as any cybersecurity professional will agree can be the ‘scene of a crime’ when a data breach has occurred. There will be evidence left behind of the perpetrators in the form network logs and other traces. Organisations in recent years have employed forensics to investigate cases of: • Hacking of commercially sensitive material • Intellectual Property (IP) theft • Fraud • Forgery • Bankruptcy • Improper or illegal system use in the workplace • Regulatory compliance"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 317, "chunk_id": "english_pack_317_0_b33e6fe8", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 299 Evidence must be admissible in court Admissibility is a key consideration, and this means the evidence is accurate, not prejudicial and was legally obtained. To ensure admissibility: • Data that may be subsequently relied upon in court must not have been changed during collection. • Persons with access to said data must be competent and have a legitimate reason for access. • Access logs are kept providing an audit trail of access, complete with details of who, what, where when and how access occurred, and any actions performed. • The chief investigator has oversight and is responsible for ensuring the law is always respected. The forensic investigator will use a “write-blocker” to make an exact copy of an original hard disk, thus preserving the original in unchanged form. Investigatory Stages Broadly speaking, the process can be divided into six stages: • Readiness – a proactive stance that ensures a system is in a state of functional readiness for forensic investigation. There are two aspects; the IT staff have"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 318, "chunk_id": "english_pack_318_0_8d909969", "content": "300 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE been briefed and knows what needs to happen in the case of a breach, and secondly the investigator must be trained and competent. • Evaluation – in the event of an incident, it must be clear to all concerned what their role is and what the impact of the incident is likely to be. • Collection – the process of collecting evidence in a way that ensures admissibility in a court of law. This includes placing items in tamper-resistant bags and labelling them properly, conveying them to a secure environment as designated by law enforcement. Is also likely to involve interviewing various people. • Analysis — must be accurate, thorough, impartial, recorded, repeatable and completed within the time- scales available and resources allocated. • Presentation – preparation of a report on findings written in plain language that non-forensic experts would understand. This would be in accordance with the initial instructions, plus any other relevant information. • Review – performed afterwards as a kind of lessons learned, process improvement exercise that identifies how the process might be done more efficiently in the future."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 319, "chunk_id": "english_pack_319_0_534b2ee8", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 301 Countermeasures Criminals engage in an on-going game of cat and mouse in which they constant seek loopholes in existing defences to exploit. Encryption is one such way; to prevent forensic analysis data may be over-written to render it unrecoverable. A files metadata can be changed, or the file subjected to “obfuscation” to disguise it. 7.3 Data Breach Intelligence Data breach intelligence forms a subset of a larger threat intelligence landscape. There are categories of threat intelligence that agencies of all kinds (government and private) use to gather information that might be useful in proactively managing threat. If you are a commercial organisation, or government department not directly concerned with legally sanctioned intelligence gathering, some of these methods will not be legally available. Intelligence Sources Cyber Security Intelligence analyses and disseminates tactical information about cyber threats, actors, and incidents. Cyber Security Intelligence can help organizations improve their cyber defence, response, and resilience."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 320, "chunk_id": "english_pack_320_0_1bad9d07", "content": "302 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE Here are nine sources of Cyber Security Intelligence that can provide valuable insights and data: Primary sources of Cyber Intelligence Cyber intelligence (CYBINT) is the collective name for data derived from a variety of intelligence-collection disciplines, as discussed below. CYBINT often gathers data from SIGINT (Signals intelligence), OSINT (Open-source intelligence) and ELINT (Electronic Intelligence). Less often it is derived from SOCMINT (Social Media Intelligence), HUMINT, GEOINT (discussed after this section). • Signals intelligence (SIGINT) derived from having listened into or intercepted the signals of persons of interest. In civil society, this is likely to be illegal, though in the defence of national interest, such methods are legally employed. • Tech intelligence (TECHINT) relates to information on the hardware and software capabilities of adversaries, allowing proper countermeasures. • AlienVault Open Threat Exchange. Categorised as Open-Source Intelligence (OSINT). This is one of the largest and most popular free open-source intelligence platforms, with over 100,000 participants sharing threat data and indicators of compromise (IOCs). • ACSC Annual Cyber Threat Report. Open-Source Intelligence (OSINT). This is an official report by the"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 321, "chunk_id": "english_pack_321_0_cf66051f", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 303 Australian Cyber Security Centre (ACSC), which provides an overview of key cyber threats impacting Australia, how the ACSC is responding to them, and crucial advice for Australian individuals and organisations to protect themselves online. • CrowdStrike Global Threat Report. Open-Source Intelligence (OSINT). This is an annual report by CrowdStrike, a leading cybersecurity company, that provides in-depth analysis of threat trends, adversary tactics, techniques, and procedures (TTPs), and recommendations for enhancing security posture. • Threat Intelligence Communities. Open-Source Intelligence (OSINT). Groups of individuals or organizations that share threat intelligence information and collaborate on cyber security issues. Threat intelligence communities can be formal or informal, public, or private, and have different levels of trust and access. • Endpoint Devices. These are the devices that connect to a network, such as computers, smartphones, tablets, and IoT devices. Endpoint devices can store useful data about user activity, system configuration, installed applications, and potential malware infections. • Network Traffic. This is the data that flows through a network, such as packets, protocols, ports, and IP addresses. Network traffic can reveal information about network topology, device communication, data"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 322, "chunk_id": "english_pack_322_0_bf5be26c", "content": "304 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE exfiltration, and malicious activity. • Threat Intelligence Platforms. These are software tools that aggregate, correlate, and analyse threat data from multiple sources, such as feeds, reports, endpoints, and networks. Threat intelligence platforms can help automate threat detection, prioritization, and response. • Threat Intelligence Providers. Organizations that offer threat intelligence services or products to customers, such as reports, feeds, alerts, or analysis. Threat intelligence providers can have different areas of expertise, such as industry-specific threats, regional threats, or threat actor profiles. Secondary sources of Cyber Intelligence • Market intelligence (MARKINT) helps in understanding the commercial environment of an adversary. • Human intelligence (HUMINT) through direct or indirect contact with people likely to have useful information. Might also be gathered through observation. • Geospatial intelligence (GEOINT) derived from sources such as GPS data and maps. • Financial intelligence (FININT) is information relating to the finances, or financial capabilities of adversaries. FININT is a principle tool in the fight"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 323, "chunk_id": "english_pack_323_0_48b5e007", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 305 against money laundering. Create a Cyberthreat Intelligence Program (CIP) As a complement to your Incident Response (IR) a Cyberthreat Intelligence Program (CIP) is an aspect of organisational risk management working in conjunction with the security operations centre (SOC) and producing information on request from management and board. The CIP allows for the prioritization of attacks and the necessary updating of protective measures. It facilitates the early detection of incidents. It includes operational and strategic components. The operational component identifies and investigates incidents and fine-tunes the protection and detection processes. The strategic component allows for networking with external parties who might be helpful, for example information sharing and analysis centres (ISACs) and other threat-sharing communities as well as specialist information providers. This networking allows for the identification of evolving threats, and of new and possibly disruptive technologies. When setting up your CIP, the following points will be useful to consider. • Identify from where you will be getting your data – this is a pre-requisite of properly defining the threat"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 324, "chunk_id": "english_pack_324_0_efd984d2", "content": "306 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE landscape. • Concentrate your efforts on your specific business or sector because collecting intelligence that is not relevant will deplete your resources and divert attention. • Create your table of priorities early and be disciplined in giving proper focus to the higher priorities, not allowing peripheral matters to deflect your efforts into less productive areas. • Think of your CIP as a work-in-progress and deliberately build in the kind process improvement feedback loops that will allow the plan to evolve strategically over time. • As far as possible automate the processing and dissemination of intelligence, as relying on manual processing is time consuming and limited in capability. 7.4 Legal Aspects of Cyber Risk: State, National & International The International Legal Guide group based in London publish an excellent up-to-date country-by-country resource of the legal statutes applicable to cybersecurity at a state and national level. The information available at their website is written in layperson’s language but expressed with the precision that is the hallmark of legal writing. I would not attempt to"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 325, "chunk_id": "english_pack_325_0_4b01836d", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 307 summarise at the risk of misunderstanding and misrepresenting an issue in a small but significant way. Peruse the entries for Australia to gain a view of the laws currently in force in relation to cybersecurity in Australia. It is segmented as follows: 1. Cybercrime 2. Cybersecurity Laws 3. Preventing Attacks 4. Specific Sectors 5. Corporate Governance 6. Litigation 7. Insurance 8. Investigatory and Police Powers 7.5 Cryptocurrency and Blockchain Forensics Cryptocurrency forensics involves tracking and analyzing transactions on blockchain networks to investigate financial crimes or verify compliance. Key Aspects of Cryptocurrency Forensics • Transaction Tracing. Following the flow of funds across the blockchain. • Wallet Analysis. Identifying and examining cryptocurrency wallets."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 326, "chunk_id": "english_pack_326_0_46090087", "content": "308 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE • Exchange Investigations. Analyzing transactions involving cryptocurrency exchanges. • Blockchain Analytics. Using specialized tools to visualize and interpret blockchain data. Challenges in Cryptocurrency Forensics • Anonymity. Many cryptocurrencies offer some level of anonymity. • Mixing Services. Tools that obscure the origin of cryptocurrency funds. • Cross-chain Transactions. Tracking funds across different blockchain networks. • Rapid Technological Changes. New cryptocurrencies and features constantly emerging. Forensic Techniques • Clustering. Grouping addresses likely owned by the same entity. • Taint Analysis. Tracing the percentage of funds from a specific source. • Pattern Recognition. Identifying common transaction patterns associated with illegal activities."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 327, "chunk_id": "english_pack_327_0_d6cdef33", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 309 Legal and Ethical Considerations • Privacy Concerns. Balancing investigation needs with individual privacy rights. • Admissibility of Evidence. Ensuring blockchain evidence is accepted in court. • International Cooperation. Dealing with cross-border nature of cryptocurrency transactions. 7.6 Dark Web Investigations The Dark Web is a part of the internet that is intentionally hidden and requires specific software or configurations to access. Key Aspects of Dark Web Investigations • Anonymity Networks. Understanding tools like Tor and I2P. • Marketplace Analysis. Investigating illegal online marketplaces. • Forum Infiltration. Gathering intelligence from dark web forums. • Cryptocurrency Tracing. Linking dark web activities to cryptocurrency transactions."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 328, "chunk_id": "english_pack_328_0_cd37926d", "content": "310 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE Investigation Techniques • OSINT. Using Open Source Intelligence to gather information. • Undercover Operations. Creating and maintaining covert online personas. • Network Analysis. Mapping connections between dark web entities. • Linguistic Analysis. Identifying individuals through writing style. Challenges in Dark Web Investigations • Technical Barriers. Overcoming anonymity technologies. • Legal Jurisdictions. Navigating international legal frameworks. • Ethical Concerns. Balancing privacy rights with law enforcement needs. • Rapid Changes. Adapting to quickly evolving dark web landscapes. Tools and Technologies • Specialized Browsers. Tools for accessing and navigating the dark web. • Crawler Software. Automated tools for indexing dark"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 329, "chunk_id": "english_pack_329_0_dd743fe3", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 311 web content. • Forensic Suites. Integrated tools for data collection and analysis. • Cryptocurrency Analytics. Tools for tracing dark web financial activities. Legal and Ethical Framework 1. Warrant Requirements. Understanding when and how to obtain legal authorization. • Chain of Custody. Properly handling and documenting digital evidence. • Privacy Protections. Respecting individual rights during investigations. • International Cooperation. Working with global partners on cross-border cases. Cryptocurrency forensics and dark web investigations are crucial areas in modern cybercrime fighting. They require a combination of technical skills, legal knowledge, and ethical considerations. As technologies evolve, investigators must continually adapt their techniques to effectively combat online criminal activities while respecting legal and ethical boundaries."}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 330, "chunk_id": "english_pack_330_0_8332eb87", "content": "312 | MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE 7.7 Case Studies Case Study 1: X, the sales manager of Company A gives 4 weeks’ notice. Soon after he leaves, Company A receives advice from several clients that they received emails from an unknown Hotmail account containing defamatory information about Company A. Computer Forensics NZ Ltd (CFNZ) is instructed to search for evidence on X’s PC that the emails originated from it. During the briefing CFNZ suggests that the PC be examined for any evidence of any confidential data being copied to removable external media during the preceding 4 weeks. Every bit and byte on the PC’s hard disk is acquired and preserved using rigorous procedures as employed by NZ Police, the Serious Fraud Office, NZ Customs etc. The data is then meticulously analysed and various data (deleted) and system files are recovered showing that email data was created at the date and time that X was known to be operating the PC. Detailed analysis also shows that during the last 3 days of X’s employment 1 MYOB data file and 1 Microsoft Access file were copied to a USB drive. The files and detailed report are provided to Company A and appropriate discussions are held with the company’s legal advisors for recommended action. Case Study 2: Computer Forensics – Cyber CrimeIt was noticed by her manager that C’s work output had been dropping over the previous 3 weeks, which coincided with"}
{"pack_id": "english_pack", "section_title": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE", "page": 331, "chunk_id": "english_pack_331_0_f68d6f9d", "content": "MODULE 7: LEGAL GOVERNANCE, CYBER FORENSICS, CYBER INTELLIGENCE | 313 the provision of broadband Internet to her department. It is visually established that she is spending many hours Internet ‘surfing’, which is specifically banned under her terms of employment. She is cautioned appropriately but she continues with the unauthorised activity. Workmates also note that pornographic images are seen on her PC after the second caution. The company subsequently dismisses her and within 14 days the company receives formal advice that it would be served with a charge of unjustified dismissal. The manager convinces Management that all correct procedures were followed and that the Internet use was clearly beyond any amount or type that could be considered reasonable. Management decides to contest the action, especially as a significant amount of money is at risk and instructs CFNZ to analyse her PC for evidence of excessive Internet activity and deliberate entry to pornographic sites. Analysis of her PC by CFNZ shows that incontestable evidence exists proving conclusively that the company’s assertions were correct. Finally, costs are awarded to the employer."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 332, "chunk_id": "english_pack_332_0_36d89285", "content": "314 | MODULE 8: IMPACT OF IT ON SOCIETY MODULE 8: IMPACT OF IT ON SOCIETY Social media and online behaviour. Social media platforms such as Facebook, Twitter and Instagram allow us to connect with people around the world, share our opinions and interests, and access information and entertainment. However, they also pose some challenges and risks, such as cyberbullying, fake news, privacy breaches and addiction. We need to be aware of these issues and use social media responsibly and ethically. Technology for social good. IT can also be used to address social problems and improve the lives of people in need. For example, IT can help with disaster relief, health care, education, environmental protection and human rights. There are many initiatives and organizations that use IT for social good, such as the United Nations, the Red Cross, Khan Academy and Wikipedia. We should support and participate in these efforts to make a positive difference in the world. Accessibility and inclusion. IT can also help to reduce barriers and inequalities for people with disabilities, minorities, women and other marginalized groups. For example, IT can provide assistive devices, adaptive software, online learning and remote work opportunities for people"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 333, "chunk_id": "english_pack_333_0_12971534", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 315 with disabilities. IT can also promote diversity, inclusion and empowerment for people from different backgrounds, cultures and perspectives. We should respect and celebrate the diversity of people in the IT field and society at large. 8.1. Social Media & Online Behaviour Social media platforms have become an integral part of our lives, connecting us with people, information and entertainment. There are significant risks for individuals and organisations, such as cyberattacks, privacy breaches, misinformation and ethical dilemmas. How can we use social media responsibly and safely, while enjoying its benefits? Protect Your Data & Devices One of the main threats of social media is that hackers can exploit the data you share online to launch cyberattacks, steal your identity or access your accounts. To prevent this, you should: • Use strong passwords and change them regularly. • Enable two-factor authentication for your accounts. • Avoid clicking on suspicious links or attachments. • Update your software and antivirus regularly."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 334, "chunk_id": "english_pack_334_0_15beb865", "content": "316 | MODULE 8: IMPACT OF IT ON SOCIETY • Review your privacy settings and limit what you share publicly. • Be careful when using public Wi-Fi or devices. Be Respectful & Ethical Another challenge of social media is that it can amplify negative emotions, opinions and behaviours, such as anger, hatred, discrimination and harassment. To avoid this, you should: • Think before you post or comment. • Respect the views and feelings of others. • Avoid spreading rumours or false information. • Report or block abusive or offensive content. • Follow the rules and guidelines of each platform. • Seek help if you experience cyberbullying or distress. Learn and Grow Social media can also be a valuable source of learning and growth, if used wisely and critically. You can: • Follow reputable sources of information and news. • Verify the accuracy and credibility of what you read or watch. • Seek diverse perspectives and opinions."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 335, "chunk_id": "english_pack_335_0_a5745a12", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 317 • Engage in constructive and respectful dialogue. • Explore new topics and interests. • Share your knowledge and skills with others. Social media is a powerful tool that can have positive or negative impacts on society, depending on how we use it. By following these tips, you can make the most of social media, while protecting yourself and others from its risks. The Need for Ethical Policies Ethical policies are not only beneficial for individuals and society, but also for IT professionals and organizations. They can help to foster trust, reputation, innovation, and competitiveness in the IT sector. They can also prevent or mitigate legal, financial, and reputational damages that may result from unethical IT practices. Some examples of ethical policies that can be adopted or implemented in the IT field are: Data protection and privacy policies. These policies aim to protect the personal data of users and customers from unauthorized access, use, disclosure, or deletion. They also specify the rights and obligations of data subjects and data controllers regarding data collection, processing, storage, and transfer. Cybersecurity policies. These policies aim to ensure the security and integrity of IT systems and networks from"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 336, "chunk_id": "english_pack_336_0_d375fa88", "content": "318 | MODULE 8: IMPACT OF IT ON SOCIETY malicious attacks or threats. They also define the roles and responsibilities of IT staff and users regarding cybersecurity measures, such as encryption, authentication, backup, and incident response. Social responsibility policies. These policies aim to promote the positive social impact of IT and to minimize its negative effects on society and the environment. They also encourage the involvement of IT stakeholders in social issues, such as digital inclusion, education, health care, and sustainability. Professional ethics policies. These policies aim to uphold the ethical standards and principles of the IT profession. They also provide guidance and codes of conduct for IT professionals regarding their duties, rights, and responsibilities towards their clients, employers, colleagues, and society. Ethical policies are not static or universal. They need to be updated and adapted to the changing IT landscape and to the diverse cultural and legal contexts. They also need to be communicated and enforced effectively to ensure compliance and accountability. Moreover, they need to be supported by ethical education and awareness programs that foster a culture of ethics among IT stakeholders. Ethical policies are not a burden or a constraint for IT. They are an opportunity and a necessity for IT to contribute positively to society and to achieve its full potential."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 337, "chunk_id": "english_pack_337_0_5c3b8626", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 319 Privacy Protection & Data Sharing As an IT professional, you have a responsibility to protect the privacy of your clients, customers, and users. Privacy is a fundamental human right, and it is also essential for trust, innovation, and competitiveness in the digital economy. However, privacy protection is not always easy or straightforward, especially when it comes to data sharing. Data sharing can have many benefits, such as improving efficiency, quality, and collaboration, but it can also pose significant risks, such as data breaches, identity theft, and discrimination. How can you balance the need for data sharing with the respect for privacy? Know the law. Different countries and regions have different laws and regulations regarding privacy and data protection. You should be aware of the legal requirements and obligations that apply to your data processing activities and comply with them accordingly. For example, if you are dealing with personal data from the European Union, you should follow the General Data Protection Regulation (GDPR), which sets high standards for data protection and gives individuals more rights and control over their data. Know your data. Before you share any data, you should know what kind of data you have, where it came from, how it was collected, what it is used for, and who has access to it. You should also classify your data according to its sensitivity and value and apply appropriate security measures to protect it."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 338, "chunk_id": "english_pack_338_0_a32ad69b", "content": "320 | MODULE 8: IMPACT OF IT ON SOCIETY For example, you should encrypt sensitive data such as health records or financial information, and limit access to authorized personnel only. Know your purpose. You should only share data for a specific and legitimate purpose that is compatible with the original purpose of collection. You should not share data for purposes that are unrelated, incompatible, or harmful to the individuals or groups involved. For example, you should not share customer data with third parties for marketing or advertising purposes without their consent. Know your partners. You should only share data with trustworthy and reliable partners who have a similar or higher level of privacy protection than you. You should also establish clear and transparent agreements with your partners that specify the terms and conditions of data sharing, such as the purpose, scope, duration, security, and accountability of data processing. You should also monitor and audit your partners’ compliance with the agreements and the applicable laws. Know your limits. You should only share the minimum amount of data that is necessary to achieve the purpose of data sharing. You should also respect the rights and preferences of the individuals or groups whose data you are sharing and give them choices and control over their data. For example, you should inform them about the data sharing activities, obtain their consent when required, allow them to access, correct, or delete their data when possible, and respond to their complaints or requests promptly."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 339, "chunk_id": "english_pack_339_0_177eefe0", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 321 Online Harassment & Cyberbullying Online harassment and cyberbullying are serious issues that affect many people, especially children and adolescents. They can cause emotional, psychological and even physical harm to the victims, as well as damage their reputation and relationships. Online harassment & cyberbullying Online harassment and cyberbullying are forms of bullying that use digital technologies, such as social media, messaging platforms, gaming platforms and mobile phones, to intimidate, humiliate, threaten or harm someone else. They can include: • Spreading lies, rumours or embarrassing photos or videos about someone online • Sending or requesting nude or nearly nude images or videos (also known as sexting) • Excluding someone from online groups or conversations • Making fun of someone’s appearance, identity, beliefs or abilities • Stalking someone online or offline • Impersonating someone online or hacking their accounts • Sending hateful or violent messages or threats"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 340, "chunk_id": "english_pack_340_0_bbee1c04", "content": "322 | MODULE 8: IMPACT OF IT ON SOCIETY Online harassment and cyberbullying can happen to anyone, but some groups are more vulnerable than others, such as girls, LGBTQ+ youth, ethnic minorities and people with disabilities. Online harassment and cyberbullying can have negative effects on the victims’ mental health, self-esteem, academic performance and social skills. They can also increase the risk of depression, anxiety, loneliness, self-harm and suicide. Preventing harassment & cyberbullying The best way to prevent online harassment and cyberbullying is to promote a culture of respect, kindness and empathy online. Here are some tips to help you do that: • Be aware of what you post online and how it might affect others. Think before you share something that could be hurtful, offensive or inappropriate. • Respect other people’s privacy and boundaries. Do not share personal or private information about someone else without their consent. Do not send or ask for nude or nearly nude images or videos. • Be a positive role model for others. Use positive language and compliments online. Support those who are being harassed or bullied online. Report any abusive or harmful content or behaviour you see online. • Educate yourself and others about online safety and digital citizenship. Learn how to protect your personal"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 341, "chunk_id": "english_pack_341_0_0cd1a8a4", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 323 information, passwords and devices online. Learn how to recognize and avoid scams, phishing and malware. Learn how to use privacy settings and blocking features on different platforms. Learn about your rights and responsibilities online. Coping with harassment & cyberbullying If you are experiencing online harassment or cyberbullying, you are not alone and you do not deserve it. Here are some steps you can take to cope with it: • Do not respond or retaliate to the harasser or bully. This might only make things worse or escalate the situation. Instead, ignore them or block them if possible. • Save the evidence of the harassment or bullying. Take screenshots or record the messages, posts or comments that are abusive or harmful. This can help you report them later or seek legal action if needed. • Report the harassment or bullying to the platform where it happened. Most platforms have policies and tools to deal with online abuse and hate speech. You can also report the harasser or bully to their school, employer or authorities if they are breaking the law. • Seek support from someone you trust. Talk to a friend, family member, teacher, counsellor or helpline about what you are going through. They can offer you emotional support, advice and resources to help you"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 342, "chunk_id": "english_pack_342_0_2d20e981", "content": "324 | MODULE 8: IMPACT OF IT ON SOCIETY cope. • Take care of yourself. Online harassment and cyberbullying can affect your physical and mental health. Try to do things that make you happy and relaxed, such as hobbies, exercise, meditation or music. Avoid drugs and alcohol as they can worsen your mood and health. 8.2. Technology for Social Good Ethical Innovation & Positive Impact Ethical Innovation & Positive Impact: How to Use Technology for Social Good How can we ensure that our innovations are aligned with our values and contribute to positive social impact? Define your purpose and vision Before you start developing or implementing any technology solution, you need to have a clear idea of what problem you are trying to solve, who you are serving, and what impact you want to achieve. This will help you set your goals, measure your progress, and communicate your value proposition to your stakeholders."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 343, "chunk_id": "english_pack_343_0_ba8d0d73", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 325 Engage with your users and beneficiaries Technology for social good should be designed with and for the people who will use it and benefit from it. You need to understand their needs, preferences, expectations, and feedback. You also need to respect their rights, dignity, privacy, and autonomy. Engaging with your users and beneficiaries will help you create solutions that are relevant, accessible, inclusive, and empowering. Consider the broader context and implications Technology for social good should not operate in isolation, but in relation to the social, cultural, economic, environmental, and political context in which it is deployed. You need to consider how your solution will interact with other systems, actors, and norms. You also need to anticipate the potential positive and negative consequences of your solution, both intended and unintended, and mitigate any risks or harms. Adopt ethical principles and standards Technology for social good should be guided by ethical principles and standards that reflect your values and commitments. You need to define what ethical innovation means for you and your organization, and how you will"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 344, "chunk_id": "english_pack_344_0_0423ba2d", "content": "326 | MODULE 8: IMPACT OF IT ON SOCIETY operationalize it in your processes, practices, and policies. You also need to align your solution with the relevant laws, regulations, codes of conduct, and best practices in your field. Evaluate your impact and learn from your experience Technology for social good should be continuously monitored and evaluated to assess its impact and effectiveness. You need to collect data and evidence that show how your solution is performing, what outcomes it is producing, and what impact it is having on your users, beneficiaries, and society at large. You also need to learn from your experience, reflect on your successes and failures, and improve your solution accordingly. The Essence of Technology for Social Good Technology needs to be guided by ethical principles, aligned with social values, and informed by evidence-based practices. Technology for social good Technology for social good is the use of technology to address social problems, such as poverty, inequality, health, education, environment, and human rights. Technology for social good can take many forms, such as:"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 345, "chunk_id": "english_pack_345_0_66f8d6bb", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 327 • Digital platforms that connect people, resources, and information across borders and sectors. • Mobile applications that provide access to essential services, such as health care, education, and banking. • Data analytics that help measure and improve the impact of social interventions. • Artificial intelligence that enhances human capabilities and supports decision making. • Blockchain that enables transparency and accountability in transactions and governance. • Internet of things that enables smart and sustainable solutions for energy, water, and waste management. Policies Guiding Ethical Innovation It is important to have policies that guide ethical innovation and ensure that it aligns with the values and needs of the people it serves. Ethical Framework for Innovation One possible ethical framework for innovation is based on the Principles for Digital Development, which are nine guidelines that help integrate best practices into technology-enabled programs. They include: • Design with the user. Involve the user throughout the"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 346, "chunk_id": "english_pack_346_0_843b51ba", "content": "328 | MODULE 8: IMPACT OF IT ON SOCIETY design process and test the solution in real contexts. • Understand the existing ecosystem. Assess the strengths and weaknesses of the current system and identify potential partners and stakeholders. • Design for scale. Plan for growth and sustainability from the start and consider how to reach more users over time. • Build for sustainability. Secure long-term funding and support and ensure that the solution can operate independently of external resources. • Be data driven. Collect, analyse and use data to inform decision making and improve performance. • Use open standards, open data, open source, and open innovation. Adopt interoperable and transparent approaches that facilitate collaboration and sharing of knowledge and resources. • Reuse and improve. Learn from existing solutions and adapt them to the local context and needs. • Do no harm. Assess and mitigate the risks and harms that the innovation may cause to the users, communities and environment. • Address privacy and security. Protect the data and information of the users and respect their rights and preferences. Ethical innovation is about ensuring that technology is human-centered, inclusive, responsible and impactful. By"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 347, "chunk_id": "english_pack_347_0_1adab345", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 329 following these principles, innovators can design solutions that are more likely to achieve social good and avoid unintended consequences. Data Privacy & Security Data privacy is the right of individuals to control how their personal data is collected, used, shared and stored by others. Data security is the protection of data from unauthorized access, use, modification or destruction. Data privacy and security are closely related, but not the same. Data privacy focuses on the rights and choices of individuals, while data security focuses on the technical and organizational measures to safeguard data. Implementing best practices for data privacy and security Here are some of the best practices for data privacy and security for technology for social good: Assess and classify data. First, assess your business data comprehensively to understand what types of data you have. Then, classify your data according to its sensitivity and the value it adds to your business. Practice minimal data collection. A rule of thumb when collecting data is to only collect what you need. Avoid collecting unnecessary or excessive data that may increase the risk of exposure or misuse."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 348, "chunk_id": "english_pack_348_0_683683c2", "content": "330 | MODULE 8: IMPACT OF IT ON SOCIETY Get consent and be transparent. Before collecting or using someone’s data, get a clear go-ahead from the user. And this shouldn’t be buried in jargon; it should be as clear as day. Let them know why and how you are collecting their data, how you will use it, who you will share it with and how long you will keep it. Practice robust data security. Use encryption, authentication, access control and other technical measures to protect your data from unauthorized access or loss. Also implement policies, procedures and training to ensure that your staff and partners follow the best practices for data security. Encourage education and awareness. Privacy can become a way to engage with your customers and show them you respect their data. Educate them about their rights and choices regarding their data, and provide them with easy ways to access, update or delete their data if they wish. Create achievable policies and SLAs with third parties. If you work with third parties who handle your data, such as cloud providers, vendors or contractors, make sure they adhere to the same standards of data privacy and security as you do. Establish clear policies and service level agreements (SLAs) that define the roles, responsibilities and expectations of each party. By following the best practices outlined in this article, you can ensure that your technology respects the rights and interests of your users and beneficiaries, while also creating value and impact for your organization and"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 349, "chunk_id": "english_pack_349_0_e826b392", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 331 Addressing Ethical Dilemmas Identify the stakeholders and their values. Who are the people or groups that are affected by the technology, directly or indirectly? What are their needs, preferences, rights, and responsibilities? How do they value the benefits and risks of the technology? Analyse the ethical issues and principles. What are the moral values or principles that are relevant to the technology and its use? For example, privacy, autonomy, justice, transparency, accountability, etc. How do they conflict or align with each other and with the stakeholders’ values? Evaluate the alternatives and consequences. What are the possible actions or decisions that can be taken regarding the technology and its use? What are the potential outcomes and impacts of each alternative on the stakeholders and their values? How likely and how severe are they? Choose the best option and justify it. Based on the analysis and evaluation, what is the most ethical option that balances the interests and values of all stakeholders? How can you explain and defend your choice using ethical reasoning and evidence? Monitor and revise as needed. How can you monitor the implementation and effects of your choice? How can you identify and address any new or unforeseen ethical issues that may arise? How can you learn from your experience and improve your ethical decision-making in the future?"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 350, "chunk_id": "english_pack_350_0_da2d93fb", "content": "332 | MODULE 8: IMPACT OF IT ON SOCIETY 8.3. Accessibility & Inclusion The Digital Divide The digital divide is the gap that exists between those who have access to digital technology and the internet, and those who do not. It affects millions of people in Australia, especially in remote and regional areas, low-income households, older people, and people who speak a language other than English at home. The digital divide can limit people’s ability to participate in society, access essential services, communicate with others, learn new skills, and find opportunities. It can also increase social isolation, disadvantage, and inequality. There are ways to bridge the digital divide and promote digital inclusion. Digital inclusion means ensuring that everyone can access, afford, and use digital technology and the internet effectively. It also means helping people develop their digital ability, which is the knowledge, skills, and confidence to use digital technology safely and creatively. How to Measure Digital Inclusion One way to measure digital inclusion is to use the Australian Digital Inclusion Index (ADII). The ADII is a tool that uses survey data to measure digital inclusion across three dimensions: access, affordability, and digital ability. The ADII"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 351, "chunk_id": "english_pack_351_0_5434328b", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 333 also explores how these dimensions vary across the country and across different social groups. The latest ADII report shows that digital inclusion at the national level is improving, but there are still significant gaps and challenges. For example, 11 per cent of Australians are “highly excluded” from digital services, meaning they do not have access to affordable internet or don’t know how to use it. That equates to about 2.8 million people. The report also shows that the divide between metropolitan and regional areas has narrowed but remains marked. People in capital cities are more likely to be online than those in regional areas, and unsurprisingly, low-income earners struggle to connect. There are different reasons for the digital divide – many older Australians lack online literacy, while in some areas a lack of infrastructure limits options. Bridging the Digital Divide Bridging the digital divide requires a collaborative effort from various stakeholders, including governments, businesses, community organisations, educators, researchers, and users themselves. Some of the strategies that can help bridge the digital divide are: • Improving the availability and quality of internet infrastructure and services in remote and regional areas • Providing affordable and flexible internet plans and devices for low-income households"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 352, "chunk_id": "english_pack_352_0_e526ddec", "content": "334 | MODULE 8: IMPACT OF IT ON SOCIETY • Offering free or subsidised access to public internet facilities such as libraries, community centres, or Wi-Fi hotspots • Developing and delivering digital literacy programs that cater to the needs and preferences of different groups of users • Supporting online safety and security awareness and education • Encouraging and facilitating online participation and engagement in social, cultural, economic, and civic activities • Promoting innovation and creativity in using digital technology for personal and professional development Legal & Ethical Imperatives Accessibility and inclusion are not only good practices, but also legal and ethical obligations for organisations that provide products, services or information to the public. What is accessibility and inclusion Accessibility involves designing systems to optimise access for people with disability or other diverse needs. Inclusion is about giving equal access and opportunities to everyone wherever possible, and respecting and valuing diversity. Accessibility and inclusion benefit not only people with"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 353, "chunk_id": "english_pack_353_0_9967ed77", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 335 disability, but also other groups such as older people, people from different cultural backgrounds, people with low literacy or digital skills, and people in remote areas. What are the legal and ethical frameworks for accessibility and inclusion? There are several laws and standards that require organisations to provide accessible and inclusive products, services or information. These include: The Disability Discrimination Act 1992 (DDA), which makes it unlawful to discriminate against people with disability in various areas of public life, such as employment, education, accommodation, access to premises, goods, services and facilities. The Web Content Accessibility Guidelines (WCAG), which are internationally recognised standards for making web content accessible to people with disability. The Australian Government has adopted WCAG as the minimum level of accessibility for all government websites. The United Nations Convention on the Rights of Persons with Disabilities (CRPD), which is an international treaty that promotes and protects the human rights of people with disability. Australia ratified the CRPD in 2008 and has obligations to ensure that people with disability can access information, communication, technology, education, health, employment, justice and other services on an equal basis with others."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 354, "chunk_id": "english_pack_354_0_f20d8bf9", "content": "336 | MODULE 8: IMPACT OF IT ON SOCIETY Apart from legal compliance, accessibility and inclusion are also ethical imperatives for organisations that want to demonstrate social responsibility, respect for human dignity, and commitment to diversity and innovation . Implementing accessibility and inclusion To implement accessibility and inclusion effectively, organisations need to adopt a holistic approach that covers all aspects of their operations, such as: Developing an Accessibility Action Plan that outlines the organisation’s vision, goals, strategies, actions, responsibilities, timelines and measures for improving accessibility and inclusion for people with disability as employees, customers and stakeholders. Making workplace adjustments that anticipate the needs of people with disability and provide reasonable accommodations for individuals, such as ergonomic equipment, assistive technology, flexible working hours and locations. Communicating and marketing in accessible ways that ensure that all communication channels, such as websites, social media, emails, brochures, videos and podcasts are accessible to people with disability and can be adjusted for individual preferences. Designing products and services that value people with disability as customers, clients or service users and"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 355, "chunk_id": "english_pack_355_0_f01b9ece", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 337 address their needs when developing and delivering products or services. Recruiting and retaining people with disability as employees at all levels of the organisation and providing them with career development opportunities. Engaging suppliers and partners that reflect and enable the organisation’s commitment to accessibility and inclusion and expect them to follow best practices. Innovating practices and processes that continually strive to do better in accessibility and inclusion and seek feedback from people with disability to improve outcomes. Promoting Inclusivity Why is inclusivity important? Inclusivity is not only a moral duty, but also a strategic advantage for organizations. By promoting inclusivity, organizations can: • Enhance their reputation and trust among customers, employees, partners and regulators. • Increase their innovation and creativity by tapping into diverse perspectives and experiences. • Reduce their legal and ethical risks by complying with relevant laws and standards. • Improve their efficiency and effectiveness by avoiding"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 356, "chunk_id": "english_pack_356_0_1dd8901a", "content": "338 | MODULE 8: IMPACT OF IT ON SOCIETY bias, errors and waste. Promoting inclusivity Promoting inclusivity requires a holistic approach that involves all stakeholders in the IT governance, policy, ethics and law domains. Here are some best practices that I recommend based on my research and experience: • Establish a clear vision and strategy for inclusivity that aligns with the organization’s mission, values and goals. • Define and communicate the roles and responsibilities of each stakeholder in ensuring inclusivity throughout the IT lifecycle. • Conduct regular assessments and audits to measure the level of inclusivity and identify gaps and opportunities for improvement. • Provide training and education to raise awareness and skills on inclusivity issues and solutions. • Implement policies and standards that support inclusivity principles and practices. • Adopt tools and methods that enable inclusive design, development, testing and evaluation of IT solutions. • Engage with diverse groups of users, customers, experts and communities to solicit feedback and input on IT solutions. • Monitor and review the impacts and outcomes of IT solutions on different groups of people and society at"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 357, "chunk_id": "english_pack_357_0_6e695d47", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 339 large. 8.4 Digital Wellbeing and Tech Addiction Digital wellbeing refers to the impact of digital technologies and media on people’s mental, physical, social, and emotional health. Key Aspects of Digital Wellbeing • Screen Time Management. Balancing time spent on digital devices with other activities. • Digital Mindfulness. Being aware and intentional about technology use. • Online-Offline Balance. Maintaining a healthy balance between digital and real-world interactions. • Digital Stress. Managing stress related to constant connectivity and information overload. Tech Addiction Tech addiction refers to the compulsive use of digital devices and platforms, often interfering with daily life. Signs of Tech Addiction"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 358, "chunk_id": "english_pack_358_0_f959c97d", "content": "340 | MODULE 8: IMPACT OF IT ON SOCIETY • Loss of Control. Inability to limit time spent on devices. • Neglecting Responsibilities. Prioritizing device use over work, school, or relationships. • Withdrawal Symptoms. Feeling anxious or irritable when unable to use devices. • Tolerance. Needing to spend increasing amounts of time on devices to feel satisfied. Strategies for Promoting Digital Wellbeing • Digital Detox. Taking regular breaks from technology use. • Setting Boundaries. Establishing rules for device use in different settings. • Mindful Usage. Being conscious of how and why we use technology. • Digital Literacy Education. Teaching healthy technology habits from an early age. Ethical Considerations • Design Ethics. Responsibility of tech companies in creating non-addictive products. • Data Privacy. Ensuring user data collected for wellbeing purposes is protected."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 359, "chunk_id": "english_pack_359_0_9118e377", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 341 • Accessibility. Ensuring digital wellbeing tools are available to all users. 8.5 Misinformation and Deep Fakes Misinformation refers to false or inaccurate information that is spread, regardless of intent to deceive. Types of Misinformation • Disinformation. Deliberately created and spread false information. • Malinformation. Genuine information shared with intent to cause harm. • Fake News. False stories presented as news, often for political or financial gain. Deep Fakes Deep fakes are highly realistic and often misleading digital manipulations of audio or video, created using artificial intelligence. Impact of Deep Fakes • Political Manipulation. Potential to influence elections"}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 360, "chunk_id": "english_pack_360_0_0a14e539", "content": "342 | MODULE 8: IMPACT OF IT ON SOCIETY and public opinion. • Personal Harm. Used for harassment, blackmail, or revenge. • Financial Fraud. Creating fake videos for scams or market manipulation. • Erosion of Trust. Undermining faith in visual and audio evidence. Combating Misinformation and Deep Fakes • Fact-Checking. Developing and using reliable fact- checking tools and services. • Media Literacy. Educating people to critically evaluate information sources. • Detection Technology. Developing AI tools to identify deep fakes and misinformation. • Legal Frameworks. Creating laws to address the creation and spread of harmful misinformation. Ethical Considerations • Free Speech vs. Regulation. Balancing freedom of expression with the need to combat harmful misinformation. • Platform Responsibility. Role of social media and tech companies in moderating content."}
{"pack_id": "english_pack", "section_title": "MODULE 8: IMPACT OF IT ON SOCIETY", "page": 361, "chunk_id": "english_pack_361_0_9cd89d2d", "content": "MODULE 8: IMPACT OF IT ON SOCIETY | 343 • Privacy Concerns. Balancing the need for attribution with user privacy rights. • Unintended Consequences. Ensuring efforts to combat misinformation don’t inadvertently suppress legitimate speech. Digital wellbeing, tech addiction, misinformation, and deep fakes present significant challenges in our increasingly digital world. Addressing these issues requires a combination of technological solutions, education, policy measures, and ethical considerations. As technology continues to evolve, it’s crucial to stay informed and adapt our approaches to ensure a healthy, informed, and trustworthy digital environment."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 362, "chunk_id": "english_pack_362_0_db9fc9eb", "content": "344 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE Trends in Cyber Loss Processes. The environment of cyber threats has led to a shift in how organizations manage cyber losses. This dynamic field is experiencing trends in incident response, mitigation, and recovery. Cyber loss processes now focus on swift detection, effective containment, and resilient recovery strategies. Understanding these trends is crucial for organizations to adapt and safeguard their digital assets. Cyber Insurance. With the increasing frequency and sophistication of cyberattacks, cyber insurance has emerged as a critical tool in risk management. This insurance provides coverage for losses resulting from data breaches, ransomware attacks, and other cyber incidents. It plays a vital role in helping organizations recover financially from the fallout of cyberattacks. As a key component of a comprehensive cybersecurity strategy, cyber insurance is essential for mitigating the financial risks associated with cyber threats. Understanding its principles and benefits is crucial for businesses in today’s digital landscape."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 363, "chunk_id": "english_pack_363_0_67fa0e91", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 345 9.1 Trends in Cyber Loss Processes Several trends have emerged in cyber loss processes as organizations seek to improve their cyber risk management and response capabilities. These trends include: • Incident Response Automation – organizations are adopting automation tools and technologies to improve their incident response capabilities. Automated incident response systems can detect and respond to cyber threats in real-time, reducing response times, minimizing the impact of attacks, and improving overall cyber resilience. • Threat Intelligence Sharing – collaboration and sharing of threat intelligence between organizations, industries, and even across national borders has become crucial. By sharing information on emerging threats, attack techniques, and vulnerabilities, organizations can proactively defend against cyber threats and better protect their systems and networks. • Cyber Insurance – demand for cyber insurance has been on the rise as organizations recognize the financial risks associated with cyber incidents. Cyber insurance policies help mitigate potential financial losses by covering costs such as incident response, legal expenses, customer notifications, and business interruption. • Focus on Cyber Resilience – rather than solely relying"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 364, "chunk_id": "english_pack_364_0_917bc7d4", "content": "346 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE on prevention measures, organizations are shifting their focus towards building cyber resilience. This involves implementing strategies and technologies to enable quick recovery and continuity of operations in the face of a cyber incident. Cyber resilience includes measures such as regular data backups, incident response planning, and robust business continuity management. • Regulatory Compliance and Data Privacy – with the increasing number of data protection regulations worldwide (e.g., GDPR, CCPA), organizations are prioritizing compliance efforts. This includes implementing strong data privacy practices, conducting privacy impact assessments, and ensuring secure data handling and processing. • Cloud Security and Third-Party Risk Management – as organizations embrace cloud computing and rely on third-party vendors for various services, managing cloud security risks and assessing third-party cyber risk have become critical. Organizations are implementing robust security measures, conducting thorough due diligence on vendors, and establishing clear contractual agreements to mitigate potential risks. • Cybersecurity Training and Awareness – recognizing that human error is a significant factor in cyber incidents, organizations are investing in cybersecurity training and awareness programs for their employees. These programs aim to educate staff about common"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 365, "chunk_id": "english_pack_365_0_e07a772b", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 347 cyber threats, best practices for data protection, and the importance of maintaining good cyber hygiene. • Cyber Exercise and Simulation – organizations are conducting regular cyber exercises and simulations to test their incident response plans and identify potential gaps. These exercises involve simulating realistic cyber- attack scenarios to assess the effectiveness of response processes, train incident response teams, and improve overall preparedness. By staying informed and adapting to these trends, organizations can enhance their cyber loss processes, better mitigate cyber risks, and respond effectively to cyber incidents. Data Exfiltration Data exfiltration is the unauthorized extraction or theft of data from a computer network, system, or device. It involves the unauthorized transfer of sensitive, confidential, or valuable data from an organization’s internal network to an external location or unauthorized recipient. Data exfiltration can occur through various methods, such as exploiting vulnerabilities in the network, using malware or malicious software, leveraging social engineering techniques, or unauthorized physical access to devices. The stolen data can include intellectual property, financial information, personally identifiable information (PII), trade secrets, or any other"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 366, "chunk_id": "english_pack_366_0_08854f21", "content": "348 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE valuable data assets. Data exfiltration is a significant risk to organizations, including financial loss, reputational damage, regulatory compliance issues, and potential legal repercussions. Data exfiltration continues to be the predominant cause of insured losses, with individual companies suffering significant data breaches. While the frequency of smaller data breaches has reduced in United States, incidences are increasing in most other countries. The sizes of successful breaches are increasing, and breaches are becoming costlier in many jurisdictions. There has been a significant shift towards large scale data breaches occurring outside of the U.S., particularly in Asia. Record-breaking size of data exfiltration events In May 2017, one of the largest data breaches ever recorded occurred in China, where 2 billion phone records were stolen from the popular Chinese call-blocking tool DU Caller. The U.S. has still suffered from large scale and high-profile data breaches. Equifax, the U.S. based credit reporting agency, was subjected to a high-profile data breach, which resulted in an estimated 143 million U.S. customers personal and financial information stolen. Yahoo’s parent company Verizon, which officially acquired Yahoo in June 2017, announced in a statement that the 2013 data breach has resulted in all 3 billion email accounts being compromised. Evidence that the data is being sold on the black market by an Eastern European"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 367, "chunk_id": "english_pack_367_0_ff8719de", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 349 hacking collective may result in an increase in email fraud and account takeovers. The disclosure of further data loss and evidence of fraudulent use of this data could increase financial liabilities in the future. Decreasing incidence rates of data breaches Data exfiltration events in U.S. increased rapidly during the period 2009 to 2014. Events since 2014 have continued to occur at a similar incidence rate, with variation year-on-year, but have not continued the rapid rate of increase of the previous five years and show signs of declining. This correlates with major increases in investment in cyber security across many of the companies at risk, and a focus on prevention and awareness in staff that is reducing the number of accidental data loss incidents and smaller breaches. It may also reflect the decreasing ‘return on effort’ for hackers as black-market prices fall for stolen data. Cyber-criminals are finding easier ways to make money, including ransomware and extortion. Hackers are making less money out of data exfiltration, as the black-market sale price of stolen records from data breaches has fallen with the abundant supply of stolen personal data now being offered for sale. Cyber attackers may instead be turning to less secure targets in other countries, and to other forms of cybercrime, such as extortion. Data exfiltration remains a very lucrative form of crime for the more professional cyber criminals, who focus on"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 368, "chunk_id": "english_pack_368_0_d59ec603", "content": "350 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE larger scales of thefts from their targets. The median size of successful data exfiltration attacks has continued to increase over time. Increasing magnitude of global large-scale data breaches While the overall numbers of data breaches has fallen due to improved methods of prevention, the severity of data incidents has nonetheless grown. The number of records stolen per breach of P3 and higher (greater than 1000 records) has tripled over the past three years. Severity of large-scale data breaches have generally increased over time, with the data being skewed by a few extremely large data loss events. Professional hackers are becoming more sophisticated in their approaches to data exfiltration."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 369, "chunk_id": "english_pack_369_0_6953dd7e", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 351 Number Company Country of Date Severity Records Du Group DBA Du China 2 Billion 2017 P8 Caller United 1.37 River City Media 2017 P8 States Billion 1,22 Netease, Inc. China 2017 P8 Billion 01/ 268 Emailcar China 01/ P8 Million 2017 United 200 Deep Root Analytics 2017 P8 States Million United 143 Equifax Inc. 2017 P8 States Million National Social 01/ Assistance Programme 135 India 11/ P8 (NSAP), Government of Million 2016 India Tencent Holdings 130 China 2017 P8 Limited DBA Million Reliance Jio Infocomm 120 India 2017 P8 Ltd Million Youku China 91 Million 2017 P7 United Edmodo 77 Million 2017 P7 States"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 370, "chunk_id": "english_pack_370_0_fefe678d", "content": "352 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE Jigsaw Holdings (Pty) South 60 Million 2017 P7 Ltd Africa 13/ United Uber Technologies, Inc. 57 Million 10/ P7 States 2016 Republic of The 11/ Philippines Commission Philippines 55 Million 01/ P7 On Elections 2017 01/ Altel Communications Unknown 50 Million 01/ P7 2014 United Dun & Bradstreet 33 Million 2017 P7 States Yahoo Inc. UK 32 Million 2017 P7 Sina Corporation Dba China 31 Million 2017 P7 01/ Unitebook Smart China 30 Million 01/ P7 Microblogging 2017 Selected Recent Large Data Breaches Companies are holding more data “Data is the new gold”: Companies are harvesting data from their customers and mining it for insights in ever increasing volumes. The total amount of business data being stored is estimated to be doubling every 12 to 18 months. This means that the potential for data exfiltration of sensitive information"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 371, "chunk_id": "english_pack_371_0_0bcf2dd2", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 353 is increasing rapidly. The size of datasets, and the aspects of people’s lives and behaviours that could potentially be exfiltrated, is a constantly upward trend. The magnitude of data exfiltration losses can be expected to increase in the future. Data breaches by business sector Other the past eight years, data exfiltration incidences have been most frequent in organizations involved in public sector, education and healthcare. Certain types of data are worth more than others and personal health records (PHI (Private Health Insurance)) and personal identifiable information (PII) are worth more on the black market, relative to credit cards and other personal finance records. The fact that these organizations hold more of these types of data, combined with potentially lower security standards, make these sectors more attractive targets. Recent incidence rates of data loss for different business sectors remain broadly consistent with previous patterns. Data breach rates have increased in IT services, manufacturing sectors, and have doubled in retail. An emerging recent target for data breaches has been offshore legal firms in tax havens, with a string of incidences of whistle-blower tax filings, including another exfiltration, following on from the Panama Papers in 2016, of the so- called ‘Paradise Papers’ where 1.4TB of sensitive financial and legal information about clients of offshore legal firm Appelby was leaked to the public."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 372, "chunk_id": "english_pack_372_0_3de04da9", "content": "354 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE Cost trends in data breaches There has been an increasing trend in the average cost per record of data loss for incidents over 100,000 records. This is attributed to the regulatory costs, escalating legal complexity and growing cost of compensation. Costs of data exfiltration attacks vary significantly between countries and increases in countries with lower compensation costs have resulted in average costs worldwide apparently decreasing, but costs are generally increasing over time in many countries, as regulations tighten. The highest cost per record remains in the U.S. due to the increasing notification costs. Average costs per record are reported to have decreased recently in Western Europe, particularly in the U.K., Austria and Denmark. Costs of data breaches are expected to increase in Europe with the implementation of GDPR. Costs in other countries are likely to rise, such as Asia-Pacific countries as they move towards tougher data breach laws including the new Cyber Security Laws introduced in China. Cyber insurers are increasingly moving their larger insured accounts to ‘managed response’ relationships, where they control the claim costs when they occur, and this is managing to reduce the cost of data breaches in those client accounts. The business impact of a data breach has reduced, with some of the consequences having diminished, such as churn (number of customers lost due to a data breach) which has reduced in Western Europe."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 373, "chunk_id": "english_pack_373_0_a6aa1df7", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 355 Data loss mainly caused by external outsiders The main cause of data breaches is attacks from malicious outsiders rather than accidental losses or ‘whistle-blower’ leaks from employees. While external actors remain the most pertinent threat, internal threats are still a concern to most corporations. The escalating use of third-parties such as sub- contractors is responsible for a growing proportion of loss events. Contractor-breaches result from businesses being granted, access to vital systems within a company’s network. One of the higher profile ‘contractor- breaches’ was from National Security Agency (NSA), which demands the highest level of vetting for employees. Accidental data loss remains significant Unintended disclosure of data remains a significant loss process. While the forensic costs are often less when data is unintentionally disclosed, cost to insurers can still be substantial due to the high notification and credit monitoring costs. Contagious Malware Malware that can replicate and spread through networks of communication has been one of the longest-standing cyber threats. Recent events have shown that malware remains a"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 374, "chunk_id": "english_pack_374_0_9b2491ec", "content": "356 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE potent trigger for loss, even in companies with high standards of security. Most significantly WannaCry and NotPetya demonstrated that contagious malware can scale and to cause systemic loss to thousands of companies. WannaCry and NotPetya WannaCry and NotPetya demonstrated the disruptive capabilities of viruses, worms, and trojan horses to spread through populations of organizations, see case studies. Many of these infections affected organizations of different geographical location, industry and size. Cyber contagion and cyber physical These contagious cyber attacks have had significant effects on physical operating environments. They have affected critical infrastructure and public services, imperilling public safety. Previous extortion attacks, for example on hospitals remained compartmentalized to an individual hospital or specific department. The WannaCry event threatened public safety across large numbers of hospitals. WannaCry affected 81 out of 236 National Healthcare System Trusts throughout United Kingdom, and 603 primary care providers. The disruption locked up important medical equipment such as MRI scanners, and caused the diversion of patients, the cancelling of appointments and surgeries, and forced a reversion to manual record keeping."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 375, "chunk_id": "english_pack_375_0_fa732e8d", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 357 WannaCry affected over 300,000 machines, many critical to national infrastructure such as power stations and transportation hubs, localized and international banking systems, global manufacturing networks and logistics and delivery centres. Ransomware attacks on the rise The use of ransomware, where malware is infiltrated into the networks of a company and disables servers or locks up data until a ransom is paid, has become one of the most pressing concerns for cyber security specialists. Attempts to extort major companies using cyber-attacks have grown in frequency, scope and ambition. Many companies have developed contingencies for ransomware attacks in the future. Some commentators have suggested that companies stockpiling BitCoin in case of extortion attacks may have fuelled the recent surges in BitCoin demand. Estimates of ransomware extorted in 2017 exceed five billion dollars, a 15-fold increase over the previous two years. Ransomware has historically afflicted personal computers and small and medium sized enterprises, but recent developments have seen large multinational corporations affected, with security companies seeing some 42 percent of all ransomware infections in the first half of 2017 targeting organizations in an interconnected and networked environment."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 376, "chunk_id": "english_pack_376_0_ae5cb1cb", "content": "358 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE Cyber extortion from large companies Ransomware is not the only method of cyber-attacks that has been used for extortion. There have been several high-profile instances where data exfiltration attacks have resulted in ransom demands. In the July 2017 HBO breach, hackers threatened to release upcoming episodes of hit shows if a price was not met. Another targeted attack, utilizing the ransomware Erebus against a South Korean web hosting company, Nayana, in which all its servers were encrypted, resulted in a $1 M ransom being paid and the bankruptcy of the company. Increasingly, the interconnectedness of things has been exploited by cyber criminals. The past year has seen a rise in targeted attempts to extort major multinational corporations, often compromising thousands of machines across these organizations Financial Theft Financial theft has continued to be a major source of cyber- attacks and cyber-enabled fraud. Compromising networks of trust to misappropriate financial transfers remains a significant threat, despite major efforts to improve security. Cyber-attacks on customer systems continue to be a major cause of loss."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 377, "chunk_id": "english_pack_377_0_860b6d79", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 359 Customer side financial theft Cyber attacks on the customer side of financial institutions continue to dominate, with online fraud plaguing the e- commerce, airline and retail industries. Physical fraud on ATM’s and point-of-sale (POS) terminals also remain a key threat. An emerging threat is complex attacks on the financial institutions and their company’s internal systems (back-end systems) and key counterparty networks of trust, involving sophisticated threat actors. This is evident from the Bangladeshi and Taiwanese SWIFT attacks (see case study) and the Polish financial regulator attack in early 2017. which are both linked to the North Korean hacking group Lazarus.85 Cyber-attacks for financial theft and fraud are still a more significant element of cyber loss than ransomware, with 2.5 times the annual detection of cyber-attacks involving financial malware. Muted EMV implementation in the U.S. The U.S. remains a key location for credit card fraud, accounting for 24 percent of total credit card use, but 47 percent of global credit card fraud. In 2016, Visa, Mastercard, and Europay credit card companies introduced new rules in the U.S. requiring retailers to upgrade their point-of-sale terminals to accept EMV-chip enabled cards. These rules are accompanied by an EMV fraud liability shift requiring retailers"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 378, "chunk_id": "english_pack_378_0_9b95f243", "content": "360 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE to bear the costs for card-present and other point-of-sale (POS) fraudulent card transactions if merchants did not upgrade their systems. Implementation of the EMV post-liability shift has been slow, with only 52% of U.S. card-accepting merchants upgraded to EMV technology88 compared with 84.9% of European vendors. Sluggish rollout of EMV in the U.S. has been attributed to the cost of implementing EMV technology, regulatory confusion, and lack of awareness of the risk of cyber-fraud, particularly for small-medium sized enterprises. U.S. continues to see many types of card-present and point-of- sale fraud, including cashing counterfeit EU payment cards. Digital currency and financial theft Cyber-attacks have increased against third-party cryptocurrency wallets to steal digital currency, exploiting weaknesses in factor security verification in wallets. Reports of financial theft from wallets is wide-spread, with at least 36 major heists on cryptocurrency exchanges since 2011. In July 2017, three separate cyber-attacks occurred across cryptocurrency platforms, including 153,000 Ethereum worth $30 million stolen from the widely used Parity Wallet. Cyber-attacks in cryptocurrency markets undermines attempts to validate digital currency and impedes the introduction of insurance against digital financial theft. Financial transaction theft remains key"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 379, "chunk_id": "english_pack_379_0_74bf91c2", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 361 threat A major source of large loss from cyber-attacks is the emergence of cyber criminals targeting financial institutions by penetrating banks internal systems, including inter-bank transaction networks. The Lazarus SWIFT financial theft in early 2016 was one of the most audacious cyber bank heists of its kind, which could have resulted in a theft of more than a billion dollars. The 2016 campaign successfully stole $81 million, with dozens of banks and central banks compromised including the U.S. Federal Reserve. The hackers hit the SWIFT network by repeatedly using specially-crafted software which allowed them to gather information on standard practices and send fraudulent requests for funds across the network. In response to the cyber-attack, SWIFT in 2017 announced an updated security protocol. The vulnerability was not in the SWIFT technology itself, but a weakness in the security of some of the member banks, so SWIFT introduced the Customer Security Control policy which gives advice on how to segregate SWIFT and critical systems from a member bank’s general framework. Further security measures include a new real-time payment controls service to reinforce existing fraud controls and cyber-crime prevention. The security update in 2017 has become more pertinent because of a further attack on the SWIFT network involving Taiwanese banks (see case study). Although the amount stolen"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 380, "chunk_id": "english_pack_380_0_3c936ad1", "content": "362 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE was smaller, the risk of large losses from compromises of financial transaction systems remains significant High standards of cybersecurity in financial companies Banks and financial service companies are fully aware of their susceptibility to attempted hacks and are leaders in the implementation of security systems and measures for preventing cyber theft. Expenditure on cybersecurity by banks has been high profile and extensive; the banking industry is the single largest sector of cybersecurity expenditure. Bank of America disclosed that it spent $400 million on cybersecurity in 2015 and, in January 2016, its CEO said that its cybersecurity budget was unconstrained. JP Morgan Chase and Co. announced the doubling of its cybersecurity budget from $250 million in 2015 to $500 million. Financial services continue to be the largest investors in cyber security. Cloud Outage Cloud computing is being adopted increasingly rapidly. The failure of a cloud service provider, while very unlikely, represents a potential cyber insurance systemic exposure as many cyber policies include coverage for outages. Failures of individual services or availability regions have the potential to cause losses to thousands of users."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 381, "chunk_id": "english_pack_381_0_fafb9b89", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 363 Cloud computing has successfully inundated the global markets, creating a utility-like service for over 90% of companies.103 Adoption rates for use of the public cloud reached an estimated 18% with up to $246 Billion in revenue worldwide. Large numbers of companies depend on the cloud, particularly in the ecommerce sector which accounts of 8.9% of total sales in the U.S. This represents a significant exposure to a potential failure of cloud service providers in cyber- affirmative IT insurance portfolios. Concentration risks in big four cloud service providers (CSPs) The global market of CSPs continues to be dominated by Amazon Web Services (AWS) at 47%, followed by Microsoft Azure at 10%, Google Cloud Platform with 4%, & IBM Softlayer with 3%. While Amazon’s position of market leader has yet to be seriously threatened by its competitors, the highest cloud adoption rates went to Microsoft Azure, particularly in application workloads. Azure adoption grew from 20 to 34 percent in a single year, while AWS maintained a steady 57 percent. While this could be due to the size of AWS relative to Microsoft Azure, Azure’s marketability to companies aiming to work in hybrid cloud may have begun to tip the scales. Azure’s infrastructure is marketed to support data within a company’s data centre and within the Azure cloud, which may catch the attention of prospective clients. 67% of cloud users"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 382, "chunk_id": "english_pack_382_0_8350d67b", "content": "364 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE currently report using a hybrid cloud strategy which allows processes in-house and on the cloud. High resilience standards of Cloud Service Providers To be competitive in the public CSP (Cloud Service Providers) market, providers need to minimize downtime and deliver on promised reliability ratings. While companies can state that their products are designed to deliver ‘99.999999999% durability’, the Service Level Agreements (SLAs) for AWS’ compute service ‘EC2’, and Microsoft Azure’s cloud services, dictate an official commitment to their customers of 99.95% reliability for each region. To maintain such high levels of reliability, the architecture of CSPs focuses on strategic isolation to protect the spread of malicious software and geographic redundancies for datacentres to reduce downtime. With plans for continued growth across the industry, the AWS Cloud operates 44 Availability Zones within 16 geographic Regions around the world, Microsoft with 36 regions, Google with as 13 regions, 39 zones, and IBM with 60 IBM Cloud data centres. Potential disruption from CSP failure While agreements of 99.95% reliability are impressive, anything less than 100% translates to damaging downtime. The critical minutes or hours of downtime have proven to"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 383, "chunk_id": "english_pack_383_0_07962769", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 365 be costly to both the CSPs and their clients. The committed 99.95% reliability of the top 4 CSPs would legally allow for roughly four and a half hours of downtime for customers. The cost of downtime for 98% of organizations for a single hour totals $100,000, with 33% of those enterprises reporting that one hour of downtime costs their firms $1-5 million. Downtime for a CSP rarely translates to a shutdown of the entire cloud. Rather, CSP downtime often manifests in service interruption to a single service, or, in the case of interdependent services, all those associated with the single service. Interruption to ‘compute’ and ‘storage’ services have the potential to cause greatest impact on customers as interdependencies within the cloud are often traced back to these essential services. Isolation between CSP availability zones limits the impact of the down service(s) – aiming to prevent global interruption. Denial of Service Attacks Distributed Denial of Service (DDos) attacks continue to be a major component in the cyber risk landscape. A third of all organizations reportedly experience DDoS attacks, twice as many as a year ago. This trend of growing likelihood of attack is likely to continue across sectors, geographies, and activity areas, as the firepower capacity of attackers increases, and they seek out new targets."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 384, "chunk_id": "english_pack_384_0_4a1698e3", "content": "366 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE Increasing complexity of DDoS attacks A Distributed Denial of Service attack uses internet traffic to overwhelm servers forcing a shut-down of the system or a slowing of services. This increased traffic denies access and limits usability to legitimate users or systems. Not only is the number of DDoS attacks increasing, but so too is the complexity. Instead of tactics focused on single aspect of a company’s infrastructure, DDoS attacks are taking a more diversified approach, alternating targets within a single attack including web application servers, firewalls, and other infrastructure components. Additionally, by varying the modes within of attack, an additional layer of complexity can be added. Attack types are broadly categorized into Volume Based Attacks, Protocol Attacks, and Application Layer Attacks each with a different method of overwhelming site bandwidth. The increased complexity of a multi-modal attack makes these attacks difficult for a company to defend its networks both proactively and reactively. Pulse DDoS attacks The typical attack pattern of DDoS attacks has also grown in complexity. While previously a DDoS attack pattern was pictured as a prolonged wave leading to a peak in activity followed by a rapid descent, a new tactic known as the ‘pulse wave attack’ has changed the timing of attacks."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 385, "chunk_id": "english_pack_385_0_e8bbbdcc", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 367 A pulse wave attack is a rapid succession of attacks with the interval between each attack being used to mount the next attack on a different target. It may take attackers only minutes to bring down a server which will take hours to reinstate. Pulse DDoS attacks can extend for days at a time and thus pose a significant risk to the accessibility of a company’s network. The significance of complex successive attacks is that large commercial servers designed to deal with high traffic volumes are resilient against attacks of low intensity, but very-high intensity attacks with frequently changing targets within a network’s infrastructure can bring down even the strongest websites. It is possible that no web server will be resilient to disruption from DDoS attacks if the intensity of attacks continues to scale up. Repeated attacks on targets Repeat attacks on targets are a common characteristic of DDoS attacks. The average number of DDoS attacks per target is increasing. Over 75% of targets are reportedly hit multiple times, an increase from 43.2% in 2016.118 There is a wide variation in number of attacks per target, with some companies reporting several hundreds of attacks. Internet of Things: A technology for DDoS attacks Much of the firepower from recent DDoS attacks has been"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 386, "chunk_id": "english_pack_386_0_ada739c8", "content": "368 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE drawn from Internet of Things (IoT) devices connected to the web. In addition, IoT devices can also become vulnerable targets for DDoS attacks: computers, mobile devices, tea- kettles, fish tanks, all being used in recent DDoS attacks. IoT devices serve as an ideal platform for DDoS attacks. Networks for IoT devices are notoriously vulnerable and offer high speed connections on a consistently switched on network. Until manufacturers of IoT devices address network security, these devices will continue to pose an increasingly large threat as a platform for DDoS attacks as IoT devices are projected to account for more than two-thirds of the 34 billion internet connected devices by 2020. Political use of DDoS attacks The motivations for recent DDoS attacks have been evolving, with politically-motivated DDoS attacks gaining the focus of the media globally. DDoS attacks accompanied the Qatar Crisis, with an attack on Al Jazeera, the largest news network in the area, the presidential elections in France where Le Monde and Le Figaro websites were targeted, and voter registration for Brexit in U.K. among others. Sectoral preferences in DDoS targeting Profiling the business sectors that experience the highest number of DDoS attacks has consistently indicated that the Gaming Industry, with its need for reliable, high-speed"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 387, "chunk_id": "english_pack_387_0_77903537", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 369 connections, is a preferred target for DDoS cybercriminals. Other popular targets for DDoS attacks for 2017 included the Software & Technology Sector as well as Internet & Telecom and Financial Services. Other sectors including Media & Entertainment, Retail & Consumer Goods, and Education sectors have all reported frequent DDoS attacks. Business disruption from DDoS attack For most competitive companies, internet access is as essential as basic utilities. A DDoS attack, regardless of platform threatens the accessibility of network traffic from legitimate customers and thereby the bottom line of web-based sales. Business interruption loss poses one of the most severe financial outcomes of a DDoS attack as without reliable access to internet functionality, significant financial losses can result. A DDoS attack which is designed to cause such disturbances to essential network infrastructure has recently been estimated to cost companies up to $2.5 million per attack. Insurance agencies have paid out Business Interruption claims specifically for DDoS and DDoS extortion attacks with pay- outs nearing half a million dollars. DDoS protection Many cyber security companies offer DDoS protection and tracking software which create intelligent resilience solutions for customers. These solutions include protective firewalls,"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 388, "chunk_id": "english_pack_388_0_33e9fd4a", "content": "370 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE large networks which can absorb DDoS attacks, and monitoring software to keep track of network traffic. By monitoring the internal and external network traffic, and defining ‘normal’ traffic patterns, companies can be alerted when they deviate from the norm. DDoS traffic can usually be traced to bots or hijacked web-browser rather than personnel, so it is important to monitor signatures and identifiable attributes of network traffic. The best protection for a company is to diversify protection techniques. An internal understanding of the norm for a company’s network, paired with the software to monitor and protect this norm allows for expedited mitigation techniques from emergency response services in the event of a DDoS attack. Case Study: The Return of Lazarus: More SWIFT Financial Thefts in 2017 Sophisticated cyber-attacks continued to enable financial thefts from the SWIFT inter-banking financial transaction system, following on from the major attacks in 2016. The victim of the 2017 attack was Far Eastern International Bank (FEIB) based in Taiwan. The gang used a vulnerability in the bank’s security, which allowed the group to secretly implant their malicious malware onto the bank’s computers and servers.97 This lead to a SWIFT terminal operated by the bank becoming compromised. Once the group gained access to the SWIFT network and"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 389, "chunk_id": "english_pack_389_0_5a773060", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 371 acquired the credentials necessary for payment transfers, the group attempted to fraudulently transfer $60 million to accounts in United States, Cambodia and Sri Lanka.98 Due to a mistake by the criminals causing an error in the specific fields of the SWIFT transfer, banking officials were alerted and all but $500,000 was recovered. As with previous attacks on the SWIFT network, the attackers used a specifically-crafted malware with many layers of subterfuge to avoid discovery. The sophistication of the attack is highlighted due to the incorporation of ransomware in the attack, which is likely to have been used to mislead the cyber security community. However, the money laundering process was less sophisticated than in previous attacks on the SWIFT network, and two ‘money mules’ were arrested attempting to physically withdraw stolen funds from a bank account in Sri Lanka. Some have attributed this attack to the North Korean state- sponsored hacking group Lazarus due to the similarities in the method of attack.100 This group is a sophisticated advanced persistent threat (APT) group which has been associated with many high-profile financial thefts including Bangladeshi SWIFT attack in 2016 and the 2017 attack on Polish banks. The continuation of attacks on financial network highlights that these are attractive targets offering big rewards to cyber criminals. Systems in place continue to manage to stop the criminals extracting the full potential from the initial penetration, although other attacks are known to succeed"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 390, "chunk_id": "english_pack_390_0_3d73d49f", "content": "372 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE 9.2 Cyber Insurance The growing cyber insurance market is continuing to be profitable but has had some near misses that could have substantially impacted the industry loss ratio. Growth is coming from new sectors and markets. Implementing growth and loss control strategies is a major priority. Rapid Growth The cyber insurance market continues to demonstrate consistent growth at around 30% year on year. Estimates for 2020 range from between $5 to 10 billion, with several analysts expecting by 2025 the market could be as large as $20 billion. While this represents substantial growth, it remains modest in comparison with the overall commercial insurance market of $247 billion. It is also relatively small in comparison with the overall corporate cyber risk management spend, with Gartner reporting worldwide cybersecurity spending at over $75.4 billion. Drivers of growth A review of many cyber insurance policies seen by RMS suggests the growth in the U.S. has been driven by increased take up from non-traditional purchases of cyber insurance (outside healthcare, technology and retail), as well as additional"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 391, "chunk_id": "english_pack_391_0_6b90a1d1", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 373 premiums generated from the availability of larger limits. International growth has also played a key part, with several markets demonstrating strong growth including Australia, Japan, and the United Kingdom. Looking more long term, RMS expects substantial growth for the industry driven by not just cyber but a broader category of digital risks. Businesses are becoming increasingly reliant on technology to run their operations and while this brings obvious benefits, it also means they are increasingly vulnerable to system failures, data losses and cyber-attacks. As the rate of technology change continues apace, the digital environment is likely to become even more complex and the amount of digital information will grow exponentially. Corporate risk managers need to develop comprehensive digital risk management strategies that involve a range of mitigations with risk transfer solutions through insurance being critical. Given the pervasive nature of technology as the foundation of the modern economy, digital risk provides a once in a generation opportunity for the insurance industry. Market participants and INCREASED COMPETITION The market continues to see a substantial concentration of premium within a handful of insurers. In the U.S. just 4 domestic writers and one Lloyd’s insurer generate almost 60% of all premium, according to an analysis of the NAIC statutory filings. This market leading position has allowed these"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 392, "chunk_id": "english_pack_392_0_ed65b119", "content": "374 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE organizations to develop a wealth of experience and data, affording them a substantial competitive advantage. However, a key trend observed over the last two years has been the entrance of many new carriers. There are now more than insurers reporting cyber premiums, although their participation remains limited. In 2016, 68 insurers reported premiums greater than a million dollars, and of these only 28 had more than $5 million. The increased competition is having an impact, with rates reportedly down over the last 12 months as well as a general loosening of coverage terms. Despite high profile systemic cyber events over the last 12 months, the limited impact on the cyber insurance industry has likely only exacerbated this issue. International growth While most premiums emanate from the U.S., there are substantial signs of growth internationally, with Europe, Japan and Australia all seeing significant rises in GWP, albeit from a relatively small base. New data protection regulations coming in to place in Australia appear to be stimulating the market, and it is expected that GDPR will have a similar impact for the EU. Profitability of Cyber Lines RMS estimates the industry loss ratio for 2016 at 54.6%. This"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 393, "chunk_id": "english_pack_393_0_a42f181b", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 375 is based on an extensive review into the occurred events and insurance penetration for 2016. This is slightly higher than the 47.6% reported from the admitted business in the U.S.132 However, it is still healthy return compared with more mature insurance markets. Loss processes RMS analysis shows that breach of privacy events (such as data exfiltration) continues to contribute the largest financial impact to losses. As has been widely reported, the proliferation of ransomware (see previous section) has resulted a large spike in the frequency of extortion and BI claims. To date the costliest losses have been driven by individual large loss events rather than more systemic events. This has had the impact of spreading the losses unevenly across insurers, with loss ratios varying substantially between carriers, with writers of larger corporates seeing volatile losses. Some have been fortunate enough to return single digit loss ratios while others have ratios greater than 150%. Near misses But it is fair to say it could have been a very different picture had the WannaCry and NotPetya events played out differently. An analysis of the WannaCry incident carried out by RMS calculated that with just a few small variations in the way it"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 394, "chunk_id": "english_pack_394_0_ddba0d55", "content": "376 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE played out, insured losses for the industry would have exceeded $3 billion. Cyber Reinsurance The cyber reinsurance market has continued to develop over the last 12 months. Insurers are now more aware of the potential for systemic incidents to trigger substantial losses and are looking to the reinsurance market to transfer some of this risk off their balance sheets. Most reinsurance contracts remain as per risk quota share with some aggregate stop loss terms adding additional protection for the reinsurer. However, over the last 12 months RMS is seeing several brokers structuring more complex treaties including excess of loss. Managing Cyber Exposure Driven by increased regulatory pressures and improved awareness at the board level, insurers have looked to implement practices to manage cyber risk. However, substantial challenges exist in providing the clear visibility required. As many commentators have stated, cyber coverage can be found in numerous other lines of business, including property, general liability, crime, kidnap and ransom, and potentially many others. This is either through endorsements or silent ‘non-affirmative’ coverage."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 395, "chunk_id": "english_pack_395_0_a1b1defd", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 377 Consistent approaches Implementing a consistent approach to managing risk across these diverse classes of business is a challenge for many insurers. Some of the main challenges are with the inconsistency in policy wordings, ambiguity in the strength of exclusions, and varying data quality approaches to data capture across multiple often legacy systems. The clear need for visibility into cyber risk has led insurers to tackle these challenges head on. RMS has worked with many insurers over the last 12 months to implement robust but practical exposure management approaches leading to significantly improved visibility. Pricing Cyber Risk Approaches to pricing cyber risk have yet to come to a consensus across the industry. A review of the rate fillings provided to insurance commissioners in the U.S. highlight the challenges of pricing cyber risk given the limited historical data and the relatively dynamic peril. Among the approaches documented includes borrowing from other classes; “we chose to use fiduciary liability data because it has a similar limit profile and expected development pattern [as cyber losses]”, and “factors are taken from our Miscellaneous Professional Liability product”– a less than ideal approach."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 396, "chunk_id": "english_pack_396_0_f0106625", "content": "378 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE Risk capital allocation At the portfolio level, the potential impact of cyber catastrophe risk is predominantly monitored through deterministic models. This has led to increased awareness of the potential for systemic risk to have a material impact on a cyber portfolio and provides insurers with an approach to identify and mitigate risk accumulations. However, approaches to assigning return periods to losses, and thereby supporting the inclusion of modelled results within capital modelling applications have to date been limited. These challenges highlight the need for improved data and risk models to support the industry’s growth in a resilient manner. 9.3 Ransomware Insurance and Recovery Strategies Ransomware is a particularly malicious form of software that encrypts a victim’s files, holding them hostage until a ransom is paid. As this threat has grown, so too has the need for specialized protection and recovery strategies. Ransomware insurance has emerged as a subset of cyber insurance, specifically designed to cover losses from these types of attacks."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 397, "chunk_id": "english_pack_397_0_93db4798", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 379 Understanding Ransomware Insurance Ransomware insurance typically covers a range of costs associated with an attack. This may include the ransom payment itself, expenses related to recovering data and systems, and even losses due to business interruption. However, obtaining this insurance often requires companies to demonstrate that they have certain security measures in place. Many policies also provide access to expert help during an attack, which can be invaluable in navigating the complex process of response and recovery. Pros and Cons of Ransomware Insurance While ransomware insurance can provide important financial protection, it’s not without controversy. Some argue that the existence of insurance may actually incentivize attackers by guaranteeing ransom payments. Additionally, these policies can be expensive, particularly for organizations in high-risk industries. Organizations must carefully weigh these pros and cons when deciding whether to invest in ransomware insurance."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 398, "chunk_id": "english_pack_398_0_b8307a28", "content": "380 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE Prevention Strategies Regardless of whether an organization has insurance, having strong recovery strategies is crucial. Prevention is the first line of defense. This includes regular software updates and patch management, employee training on recognizing phishing and social engineering attempts, use of robust antivirus and anti- malware software, and implementation of network segmentation to limit the spread of any potential infection. Backup and Recovery Processes Backup and recovery processes are also critical. Organizations should maintain regular, comprehensive backups stored offline or in secure cloud locations. It’s not enough to simply have backups – the restoration process should be tested regularly to ensure it works when needed. Care must be taken to ensure that backup copies themselves remain free from malware. Incident Response Planning When an attack occurs, having a well-prepared incident response plan can make a significant difference. This plan should detail the steps to be taken in the event of an attack, including roles and responsibilities for various team members. Regular drills and updates to this plan help ensure it remains effective."}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 399, "chunk_id": "english_pack_399_0_1fa1825a", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 381 Immediate Actions During an Attack If a ransomware attack is successful, quick action is essential. Infected systems should be isolated immediately to prevent the malware from spreading further. Non-critical network connections should be shut down to contain the threat. If negotiation with the attackers becomes necessary, it should only be done through experienced negotiators. Organizations must carefully consider the legal and ethical implications of paying a ransom, and understand that payment doesn’t guarantee successful data recovery. Data Recovery Efforts Data recovery efforts may involve using decryption tools if they’re available for the specific ransomware variant. More often, it involves restoring from clean backups after ensuring the threat has been completely eliminated. Following an incident, a thorough investigation should be conducted to understand how the attack occurred and what additional security measures can be implemented to prevent future incidents. Evolving Landscape of Ransomware The landscape of ransomware attacks and defenses continues to evolve. Attacks are becoming increasingly sophisticated,"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 400, "chunk_id": "english_pack_400_0_f127686f", "content": "382 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE often targeting backup systems to make recovery more difficult. In response, organizations are placing greater emphasis on ransomware prevention in their overall cybersecurity strategies. There’s also ongoing debate about potential government regulations on ransomware payments and insurance. Ransomware poses a significant threat to organizations of all sizes, and managing this risk requires a comprehensive approach. While ransomware insurance can provide a financial safety net, it’s crucial to implement strong prevention and recovery strategies. This includes technical measures, employee training, and well-prepared incident response plans. By combining these elements, organizations can better protect themselves against the devastating impact of ransomware attacks. 9.4 Case Study: WannaCry Malware Attack WannaCryptor ransomware spread via file-sharing network protocols on computers using outdated Windows XP and v8 OS. It resulted in 300,000 infections of computers across 150 countries. WannaCry used a NSA exploit codenamed EternalBlue (released the previous August by ShadowBrokers). It mainly affected personal users, public sector organizations, and SME-sized companies, affecting unpatched boxes and equipment on dedicated older operating systems. Several"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 401, "chunk_id": "english_pack_401_0_e6d9445e", "content": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE | 383 dozens of large companies also reported disruption and losses from infections of their systems. Of the roughly 400 million actively-used Windows computers running version 8 or earlier operating system, approximately 0.1 percent were infected. The great majority of the Windows computers running version 8 or earlier were protected by a Microsoft patch MS17-010 issued two months earlier, in March 2017. The event highlighted the issue of equipment software latency, i.e. that machines and sub-networks within organizations may rely on specific versions of operating system that render them vulnerable. In these cases, although most systems within organizations ran more up-to-date operating systems, certain departments and activities were maintaining the older versions that contained the vulnerability. Machines such as medical MRI scanners and X-Ray machines that were certified on XP and v8 and maintained on those operating systems, were among those that were crippled by the attack.73 Businesses reported substantial losses from lock- outs of systems around the world, such as manufacturing processes, dispatch and ordering systems, gas pump payment applications, and telephone exchange equipment. We estimate the direct costs and indirect business disruption losses from WannaCry to be around half a billion dollars. If the WannaCry malware was created to generate ransom payments then it was remarkably unsuccessful. The BitCoin accounts that it requested payments into received less than $150,000 in payments and may not have been claimed by the"}
{"pack_id": "english_pack", "section_title": "MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE", "page": 402, "chunk_id": "english_pack_402_0_b01292cc", "content": "384 | MODULE 9: CYBER LOSS PROCESS & CYBER INSURANCE criminals. No company that paid a ransom got its data back. The motivation was more likely to sabotage some of the affected companies, rather than generate funds for the hackers. It is possible that the widespread economic disruption was collateral damage to mask a targeted destructive attack. The propagation of WannaCry was stopped after four days by a researcher finding a kill-switch within the software. Otherwise the infection could have spread to many more machines and had a more severe impact. RMS counterfactual analysis suggests that if the kill-switch had not been triggered, and if the attack had occurred prior to the issuing of the MS17-010 patch for Windows 8, the infection rates and losses could have been an order of magnitude higher, perhaps reaching $3 to $6 billion."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 403, "chunk_id": "english_pack_403_0_1436f4a6", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 385 MODULE 10: E-GOV & DIGITAL TRANSFORMATION E-government and digital transformation are two related concepts that aim to improve the quality of life and the efficiency of public services. E-government refers to the use of information and communication technologies (ICTs) to deliver government services, information and participation to citizens, businesses and other stakeholders. Digital transformation is the process of rethinking and redesigning how government operates, interacts and innovates using digital tools and data. One of the main goals of e-government and digital transformation is to enhance citizen engagement, which means involving citizens in the decision-making and policy- making processes of government. Citizen engagement can take various forms, such as online consultations, feedback mechanisms, crowdsourcing, co-creation and participatory budgeting. Citizen engagement can increase the transparency, accountability and legitimacy of government actions, as well as the satisfaction and trust of citizens. Another goal of e-government and digital transformation is"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 404, "chunk_id": "english_pack_404_0_25105f43", "content": "386 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION to create smart cities and ethical urbanization. Smart cities are urban areas that use ICTs to collect, analyse and use data to improve the management and planning of various aspects of urban life, such as transportation, energy, waste, health, education and security. Ethical urbanization is the principle that smart cities should respect the human rights, dignity and diversity of their inhabitants, as well as promote social inclusion, environmental sustainability and economic development. A third goal of e-government and digital transformation is to enable remote work and privacy. Remote work is the practice of working from a location other than the traditional office, such as home, co-working spaces or public places. Remote work can offer benefits such as flexibility, productivity, cost savings and work-life balance. However, remote work also poses challenges such as communication, collaboration, security and privacy. Privacy is the right of individuals to control their personal information and how it is used by others. Privacy is essential for protecting the identity, reputation and autonomy of remote workers, as well as their personal and professional data. 10.1. E-Government & Citizen Engagement E-government initiatives are the use of information and communication technologies (ICTs) to deliver public services,"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 405, "chunk_id": "english_pack_405_0_1d4bec1f", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 387 improve government efficiency and transparency, and enhance citizen participation and trust. Governments worldwide are adopting technological advancements to create more efficient and accessible public services through e-government initiatives. This is an on-going process. As these initiatives take shape, it becomes necessary to have policies cover a range of considerations, including: • Data privacy, • Security, • Accessibility and, • Citizen engagement. Alongside these policies, a range of ethical considerations play a central role in ensuring that e-government efforts are not only efficient and convenient but also uphold. • Democratic principles, • Respect individual rights, and • Promote transparency. Identify and map your stakeholders based on their interest in and influence on your objectives. Determine the issues on which you need stakeholder input and develop strategies for engagement. Be clear about what you are trying to achieve, be open about"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 406, "chunk_id": "english_pack_406_0_cd8bc03a", "content": "388 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION your limitations and constraints, tell people where their input is going, and manage expectations around the outcome and decision-making process. Use information and communication technologies to facilitate the daily administration of government, improve citizen access to government information, services and expertise, ensure citizen participation in and satisfaction with the government process, and enhance cost-effectiveness and efficiency. Foster civic engagement through interactive, easy-to- understand data publishing and visualizations. Provide context for your data and help citizens understand what it signifies. Consult the public on which capital improvement projects to prioritize, update citizens on the progress of projects, and report and communicate the impact of a capital project. Value information as a national resource and a national asset. Ensure information security, privacy, integrity, accountability, innovation and improvement across all the processes of government. The Rise of E-Government Initiatives The rise of e-government initiatives is driven by various factors, such as the increasing demand for online services, the availability of digital infrastructure and data, the pressure to"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 407, "chunk_id": "english_pack_407_0_54db2af9", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 389 reduce costs and improve performance, and the opportunities to foster innovation and collaboration. The benefits of e-government initiatives include improved service quality and accessibility, increased citizen satisfaction and empowerment, reduced administrative burden and corruption, enhanced policy making and accountability, and greater social inclusion and cohesion. The challenges of e-government initiatives include technical issues, such as interoperability, security, privacy, and digital divide; organizational issues, such as leadership, culture, change management, and human resources; and legal and ethical issues, such as data protection, transparency, accountability, and participation rights. The best advice on the topic of the rise of e-government initiatives is to adopt a holistic and strategic approach that considers the needs and expectations of all stakeholders, the goals and objectives of the government, the opportunities and risks of ICTs, and the legal and ethical implications of e- government. Some of the key steps are: • Conduct a situational analysis to assess the current state of e-government in terms of strengths, weaknesses, opportunities, and threats. • Develop a vision and a roadmap for e-government that defines the desired outcomes, priorities, indicators, and milestones. • Establish a governance framework for e-government that"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 408, "chunk_id": "english_pack_408_0_fbadc727", "content": "390 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION clarifies the roles and responsibilities of different actors, the decision-making processes, the coordination mechanisms, and the monitoring and evaluation systems. • Implement e-government projects that are aligned with the vision and roadmap, follow user-centric design principles, ensure interoperability and security standards, involve stakeholder participation and feedback, and evaluate the impacts and outcomes. • Foster a culture of innovation and learning for e- government that encourages experimentation, collaboration, knowledge sharing, and continuous improvement. Policies for Data Privacy & Security Since e-government involves the collection, storage, and processing of citizen data, there must be robust policies for data privacy and security. Data privacy and security are essential for e-government and citizen engagement, as they ensure trust, transparency and accountability in the use of personal and public information. E-government policies should comply with relevant laws and regulations, such as the General Data Protection Regulation (GDPR) in Europe and the California Consumer Privacy Act (CCPA) in the US, that protect consumer rights and choices about how their data are used."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 409, "chunk_id": "english_pack_409_0_0d1a4520", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 391 E-government policies should also follow best practices and standards, such as the Information and Data Governance Framework of the National Archives of Australia, that promote data interoperability, quality and value across government agencies and services. E-government policies should involve citizen participation and feedback, as well as collaboration with other stakeholders, such as private sector, civil society and academia, to ensure data are used for public good and innovation. Accessibility Bridging the Digital Divide Ethical e-government policies extend to accessibility, ensuring that digital services are available to all citizens, including those with disabilities or limited technological access. Governments must prioritize designing platforms that adhere to accessibility standards, making sure that no citizen is excluded from utilizing vital services due to physical or digital barriers. This commitment to accessibility reflects an ethical imperative to create inclusive and equitable digital landscapes. Engagement & Inclusivity A primary consideration of e-government is to enhance citizen engagement and participation in governance."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 410, "chunk_id": "english_pack_410_0_1fedd2dd", "content": "392 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION Ethical considerations demand that these initiatives be inclusive, providing avenues for all citizens to voice their opinions, provide feedback, and influence decision-making processes. Policies should therefore outline mechanisms for soliciting public input, fostering meaningful dialogue, and ensuring that diverse perspectives are considered when shaping policies and services. E-government initiatives must also respect and uphold individual rights, both online and offline. Ethical policies should ensure that citizens’ rights to privacy, freedom of expression, and access to information are not compromised. Measures should be in place to prevent the misuse of citizen data, surveillance abuses, or any actions that could infringe upon fundamental rights. Transparency & Accountability Transparency is a cornerstone of ethical governance. E- government initiatives should promote transparency by giving citizens access to relevant information about government activities, decisions, and processes. Policies should mandate the publication of data, budgets, reports, and other pertinent information in formats that are easily accessible and understandable to the public. This transparency works towards proper accountability and empowers citizens to hold governments to ethical standards."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 411, "chunk_id": "english_pack_411_0_5b20bf4b", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 393 The Digital Divide While e-government initiatives aim to enhance efficiency and accessibility, they also raise concerns about exacerbating existing digital divides. Ethical policies should address these concerns by prioritizing initiatives that bridge these divides, such as providing digital literacy training and ensuring that marginalized communities have access to necessary technology. This approach ensures that the benefits of e-government are distributed equitably. Convenience vs Consent E-government services often require citizens to share personal information for authentication and verification. Ethical policies must need to balance the convenience of streamlined services with the necessity of obtaining informed consent from citizens. Clear communication about how data will be used and the ability to opt-out should be integral to these policies, respecting citizens’ autonomy over their personal information. Digital Literacy & Informed Participation Citizen engagement in e-government initiatives is most"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 412, "chunk_id": "english_pack_412_0_50d755e6", "content": "394 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION effective when citizens are digitally literate and well-informed and have a willingness to engage with e-government. Ethical considerations extend to providing educational resources that train citizens to navigate digital platforms, understand their rights and responsibilities, and actively participate in governance processes. Policies should encompass strategies for promoting digital literacy and awareness. E-government has great potential to revolutionize the relationship between citizens and governments, fostering transparency, accessibility, and engagement. However, realizing this potential requires a foundation of ethical policies that prioritize data privacy, security, inclusivity, transparency, and respect for individual rights. E-government, guided by ethical considerations, becomes a force for positive change, bridging gaps, enhancing accountability, and ultimately strengthening the democratic fabric of society. 10.2. Smart Cities & Ethical Urbanization Smart cities are urban areas that use digital technologies to improve the quality of life of their inhabitants. They can also help address some of the challenges that cities face, such as congestion, pollution, crime, and inequality. However, smart cities also raise some ethical issues and concerns that need to"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 413, "chunk_id": "english_pack_413_0_031358a5", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 395 be considered and addressed by policymakers, developers, and citizens. Network Infrastructure One of the key features of smart cities is the network infrastructure that connects various devices, sensors, and systems to collect, store, and analyse data. This data can be used to optimize urban services, such as transportation, energy, waste management, and public safety. However, this also poses some risks of control, surveillance, data privacy, and ownership. Who owns the data generated by smart city technologies? How is it protected from unauthorized access or misuse? How is it shared among different stakeholders and for what purposes? How can citizens have a say in how their data is used and by whom? Post-Political Governance Another aspect of smart cities is the post-political governance model that relies on data-driven decision-making and public- private partnerships. This model can enhance efficiency, transparency, and accountability in urban management. But it can also undermine democratic participation, deliberation, and representation. How are the interests and values of different groups and communities considered in"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 414, "chunk_id": "english_pack_414_0_7168a478", "content": "396 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION smart city projects? How are the trade-offs and conflicts among them resolved? How are the roles and responsibilities of public authorities and private actors defined and regulated? How can citizens have a voice and a choice in shaping their smart city? Social Inclusion A third dimension of smart cities is the social inclusion of citizens in the benefits and opportunities offered by smart city technologies. This includes ensuring access, affordability, usability, and literacy of digital services for everyone. It also involves promoting citizen participation, engagement, and empowerment in co-creating and co- governing their smart city. However, this also requires addressing the challenges of inequality, discrimination, and exclusion that may arise or persist in smart cities. How are the needs and preferences of diverse and marginalized groups considered and met in smart city design and implementation? How are the potential harms and disadvantages of smart city technologies for some groups prevented or mitigated? How can citizens have a sense of belonging and identity in their smart city?"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 415, "chunk_id": "english_pack_415_0_b8745af2", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 397 Sustainability A fourth dimension of smart cities is the sustainability of their environmental impact and long-term development. This entails using smart city technologies to reduce greenhouse gas emissions, conserve natural resources, enhance resilience to climate change, and improve environmental quality. It also implies aligning smart city goals with the broader agenda of sustainable development that encompasses social, economic, and cultural aspects. This also demands balancing the costs and benefits of smart city technologies for different generations and regions. How are the environmental impacts of smart city technologies measured and monitored? How are they aligned with the global commitments and targets on climate action? How are they integrated with the local contexts and cultures of different cities? The Rise of Smart Cities Redefining Urban Living Smart cities are urban areas that use digital technologies to enhance the quality of life, efficiency of services, and sustainability of the environment. They aim to solve the challenges of urbanization, such as congestion, pollution, waste, and social inequality."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 416, "chunk_id": "english_pack_416_0_7ddb74ad", "content": "398 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION Involve the Stakeholders Smart cities are not just about technology, but also about people. It is important to engage the citizens, businesses, civil society, and public sector in the planning and decision-making process of smart city initiatives. This can foster trust, collaboration, and innovation among the stakeholders, as well as ensure that the solutions are tailored to the local needs and preferences. The Holistic Approach Smart cities are complex systems that require coordination and integration across different domains, such as transportation, energy, health, education, and security. It is essential to adopt a holistic approach that considers the interdependencies and trade-offs among these domains, as well as the potential impacts on the economy, society, and environment. A holistic approach can also help to avoid silos, duplication, and fragmentation of resources and efforts. Ethical & Legal Compliance Smart cities rely on data collection, analysis, and sharing to provide intelligent services and solutions. However, this also raises ethical and legal issues, such as privacy, security, accountability, and transparency. It is crucial to ensure that the data collection and use are compliant with the relevant"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 417, "chunk_id": "english_pack_417_0_96c7531d", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 399 laws and regulations, as well as respect the rights and interests of the data subjects. Moreover, it is advisable to adopt ethical principles and guidelines that can guide the design and implementation of smart city technologies and policies. Promote Innovation & Learning Smart cities are dynamic and evolving entities that need to adapt to the changing needs and expectations of the citizens and the environment. It is important to promote a culture of innovation and learning that encourages experimentation, creativity, and risk-taking. This can help to foster new ideas, solutions, and practices that can improve the performance and outcomes of smart city initiatives. Furthermore, it is beneficial to establish mechanisms for monitoring, evaluation, and feedback that can provide evidence-based insights and lessons for continuous improvement. Ethical Dimensions of Smart Cities Smart cities are urban areas that use digital technologies to improve the quality of life, efficiency of services, and sustainability of resources. They can offer many benefits, such as reducing traffic congestion, enhancing public safety, and promoting social inclusion. However, smart cities also pose ethical challenges that need to be addressed, such as privacy, security, accountability, and participation."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 418, "chunk_id": "english_pack_418_0_b306f548", "content": "400 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION Privacy Smart cities collect and process large amounts of data from various sources, such as sensors, cameras, mobile devices, and social media. This data can reveal sensitive information about the behaviour, preferences, and activities of citizens. How can we ensure that this data is used in a transparent, fair, and respectful way, without violating the rights and dignity of individuals? Security Smart cities rely on complex and interconnected systems that are vulnerable to cyberattacks, natural disasters, or human errors. These systems can affect critical infrastructure, such as transportation, energy, or health care. How can we protect these systems from malicious or accidental threats, while ensuring their resilience and reliability? Accountability Smart cities involve multiple actors, such as governments, businesses, civil society, and citizens. These actors have different roles, responsibilities, and interests in the design, implementation, and evaluation of smart city initiatives. How can we ensure that these actors are accountable for their actions and decisions, and that they comply with ethical standards and legal regulations?"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 419, "chunk_id": "english_pack_419_0_2e9bd086", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 401 Participation Smart cities aim to improve the well-being and empowerment of citizens by providing them with more choices, opportunities, and services. However, not all citizens have equal access to or influence on these benefits. How can we ensure that smart city initiatives are inclusive, participatory, and responsive to the needs and expectations of diverse and marginalized groups? By applying ethical principles and values to smart city projects, we can ensure that they are not only smart but also fair, responsible, and human-centered. Data Privacy & Security Data privacy and security laws aim to protect citizens from the misuse, loss, unauthorized access or disclosure of their personal information by government agencies or private organisations. Why is data privacy & security important? Data privacy and security are important because they respect the fundamental human right to privacy and dignity of individuals. They foster trust and confidence in the digital economy and society and enable citizens to exercise control and choice over their personal information."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 420, "chunk_id": "english_pack_420_0_f4ce1a38", "content": "402 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION They also prevent identity theft, fraud, cybercrime and other harms that can result from data breaches or misuse, and support innovation and competitiveness by creating a level playing field for data-driven businesses. Best practice for data privacy and security Data privacy and security best practices are based on the following principles: Lawfulness, fairness and transparency. Personal information should be collected and processed only for legitimate, specified and explicit purposes, with the consent or authorization of the individuals concerned, and in a clear and open manner. Data minimization. Personal information should be adequate, relevant and limited to what is necessary for the purposes for which it is processed. Accuracy. Personal information should be accurate, complete and up-to-date, and corrected or deleted if inaccurate or outdated. Storage limitation. Personal information should be kept only for as long as necessary for the purposes for which it is processed, and securely disposed of when no longer needed. Integrity and confidentiality. Personal information should be protected from unauthorized or unlawful processing, accidental loss, destruction or damage, using appropriate technical and organisational measures. Accountability. Data controllers and processors should be"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 421, "chunk_id": "english_pack_421_0_9022c59c", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 403 responsible for complying with data privacy and security laws and regulations, and demonstrate their compliance through documentation, audits, reporting and other means. Data privacy & security laws: the world Data privacy and security laws vary from country to country, depending on their legal systems, cultures and values. However, there is a growing trend towards harmonization and convergence of data protection standards across regions and jurisdictions. Some of the major data privacy and security laws around the world are: The General Data Protection Regulation (GDPR). This is the most comprehensive and influential data protection law in the world, which applies to the European Union (EU) and the European Economic Area (EEA), as well as to any organisation that offers goods or services to, or monitors the behaviour of, individuals in the EU or EEA. The GDPR grants individuals a set of rights over their personal information, such as the right to access, rectify, erase, restrict, port or object to its processing. It also imposes strict obligations on data controllers and processors, such as obtaining valid consent, conducting data protection impact assessments, appointing data protection officers, notifying data breaches, implementing data protection by design and by default, and transferring data only to countries with adequate levels of protection. The GDPR also empowers national data protection authorities to enforce the law and impose fines of"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 422, "chunk_id": "english_pack_422_0_54b63f0f", "content": "404 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION up to 20 million euros or 4% of global annual turnover, whichever is higher. The California Consumer Privacy Act (CCPA). This is the first comprehensive data protection law in the United States (US), which applies to California residents as well as to any business that collects or sells their personal information. The CCPA grants individuals a set of rights over their personal information, such as the right to know what is collected, shared or sold; the right to access, delete or opt out of its sale; and the right to non-discrimination for exercising their rights. It also imposes obligations on businesses to provide notice, transparency and choice to consumers; to implement reasonable security measures; to honour consumer requests; and to avoid selling personal information of minors without consent. The CCPA also authorizes the California Attorney General to enforce the law and impose civil penalties of up to $7,500 per violation. The Privacy Act 1988. This is the main data protection law in Australia (AU), which applies to Australian Government agencies (and the Norfolk Island administration) and organisations with an annual turnover more than $3 million. The Privacy Act gives individuals a set of rights over their personal information, such as the right to access or correct it; the right to complain about its mishandling; the right to stop receiving unwanted direct marketing; and the right to be notified of data breaches. It also imposes obligations on agencies."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 423, "chunk_id": "english_pack_423_0_31a4c2f1", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 405 Responsible Use of AI and Automation Ethical AI in government The Australian Government has developed four principles for the ethical use of AI in government, based on interim guidance from the Digital Transformation Agency (DTA) and the Department of Industry, Science and Resources (DISR) . These principles are: • Support the responsible and safe use of technology. • Minimise harm, and achieve safer, more reliable and fairer outcomes for all Australians. • Reduce the risk of negative impact on those affected by AI applications. • Enable the highest ethical standards when using AI. • Increase transparency and build community trust in the use of emerging technology by government. These principles should guide the design, development, deployment, and evaluation of any AI or automation project in the public sector. They should also be aligned with the agency’s ICT obligations and policies, as well as relevant laws and regulations. Using generative AI Generative AI platforms are third-party AI platforms, tools or"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 424, "chunk_id": "english_pack_424_0_62fe322c", "content": "406 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION software that can create new content or data based on existing data or inputs. Examples of such platforms are ChatGPT, Bard AI or Bing AI. These platforms can offer new and innovative opportunities for government, such as generating summaries, reports, or responses to queries. However, they also involve potential risks, such as data quality, security, privacy, accountability, and bias. The DTA and DISR have provided some tactical guidance for Australian Public Service (APS) staff who want to use publicly available generative AI platforms. Some of the key points are: • Assess the potential risks and benefits for each use case. • Use generative AI platforms only for low-risk purposes that do not involve personal or sensitive information, decision making, or official communication. • Do not rely solely on generative AI outputs without human verification or quality assurance. • Clearly disclose the use of generative AI platforms to stakeholders and users • Monitor and evaluate the performance and impact of generative AI platforms regularly. Best practices for digital transformation Digital transformation is not just about using digital technologies to automate or augment existing processes. It is"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 425, "chunk_id": "english_pack_425_0_2c70601d", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 407 also about reimagining how government can deliver value to citizens and businesses in new ways. According to BCG , some of the best practices for digital transformation in government are: • Define a clear vision and strategy for digital transformation that aligns with the agency’s mission and goals. • Establish a dedicated digital team or unit that can drive innovation and collaboration across the agency. • Adopt agile methods and tools that enable rapid experimentation and iteration. • Leverage data and analytics to generate insights and improve decision making. • Engage with stakeholders and users throughout the design and delivery process to ensure user-centricity and feedback. • Foster a culture of learning and change that supports continuous improvement and adaptation. By following these principles, government agencies can use AI and automation responsibly and effectively in E-Gov & Digital Transformation. This can lead to better outcomes for citizens, businesses, and society."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 426, "chunk_id": "english_pack_426_0_866381bc", "content": "408 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION 10.3. Remote Work & Privacy Flexibility, Productivity, & Individual Rights The rise of remote work has revolutionized the way we work, offering unprecedented flexibility and accessibility. However, as organizations embrace this new paradigm, concerns about privacy in remote work environments have come to the forefront. Remote work offers many benefits for both employers and employees, such as increased flexibility, productivity, and satisfaction. However, remote work also poses some challenges for individual rights and privacy, such as blurred boundaries between work and personal life, potential surveillance and monitoring by employers, and increased cyber risks. To address these challenges, it is important to establish clear policies and guidelines for remote work that respect the rights and preferences of workers, while ensuring accountability and security. Some best practices for remote work policies include: • Setting realistic and measurable goals and outcomes for remote workers, rather than focusing on hours or attendance. • Providing adequate training and support for remote"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 427, "chunk_id": "english_pack_427_0_22060317", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 409 workers to use the necessary tools and technologies effectively and safely. • Communicating regularly and transparently with remote workers to maintain trust, collaboration, and feedback. • Respecting the autonomy and flexibility of remote workers to choose their preferred work location, schedule, and style, if they meet their obligations and expectations. • Protecting the privacy and data of remote workers by implementing appropriate security measures, such as encryption, VPNs, firewalls, etc.. • Avoiding excessive or intrusive monitoring or surveillance of remote workers that may violate their rights or harm their well-being. The Remote Work Revolution Advances in technology have paved the way for remote work to become a mainstream practice. Develop standard security rules and procedures for your remote teams that cover regulatory compliance, remote access control, backup and media storage, data protection, remote system management, system ownership and return, and information disposal. Define PII standards that meet the obligations for personally"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 428, "chunk_id": "english_pack_428_0_76b5a883", "content": "410 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION identifiable information compliance in all territories in which your organization operates. Train and educate team members on how to protect themselves and others from the latest cybersecurity threats, especially those related to remote work, such as physical theft of devices, packet sniffers on public Wi-Fi networks, email scams, and spoof sites. Don’t leave your electronic devices unattended in public or in an unsecured office. Set laptops and mobile devices to automatically lock after a period of inactivity. Do not leave passwords written down or visible to others. Use a password manager to generate and store strong, unique passwords for each account and service you use. Change your passwords regularly and avoid using the same password for multiple accounts. Use a VPN and 2-factor authentication whenever possible to encrypt your online traffic and add an extra layer of security to your login credentials. Avoid using public Wi-Fi networks or shared computers to access sensitive data or perform online transactions. Perform all transactions on a secure, password-protected network. Even if you are using a VPN, it’s better safe than sorry. Look for the padlock icon and the https prefix in the address bar of your browser before entering any personal or financial information."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 429, "chunk_id": "english_pack_429_0_1821411f", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 411 The Privacy Puzzle Remote work introduces a unique set of privacy challenges. As employees work from home, the boundaries between professional and personal life blur, potentially leading to privacy infringements. The Privacy Puzzle is a term that refers to the challenges and risks of protecting personal and confidential information in a remote or hybrid work environment. Remote work has increased the exposure of sensitive data to potential threats such as unsecured networks, phishing attacks, device theft, and visual hacking. To address these challenges, IT governance, policy, ethics and law experts recommend the following best practices: • Implementing robust security technologies such as incident response platforms, anti-virus software, identity management and authentication systems, and encryption tools. • Developing and enforcing clear privacy policies that specify the responsibilities and expectations of remote or hybrid workers, as well as the consequences of non- compliance. • Providing regular training and awareness programs that educate employees on the importance of data privacy, the common threats they may face, and the mitigation methods they should use."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 430, "chunk_id": "english_pack_430_0_4fea63df", "content": "412 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION • Adopting privacy-enhancing solutions such as privacy screens, webcam covers, secure file sharing platforms, and VPNs. • Monitoring and auditing the compliance and performance of remote or hybrid workers, as well as the security and privacy of the data they handle. Remote Work & Privacy Policies Remote work poses unique challenges and opportunities for privacy protection. As a professional in IT governance, policy, ethics and law, you should be aware of the legal requirements, best practices and ethical principles that apply to remote work and privacy policies. According to the Privacy Act 1988, you may need to have a clear and up-to-date privacy policy that details how you collect, store, use and disclose personal information of your employees, customers and other stakeholders. You should also comply with the Australian Privacy Principles, especially if you handle sensitive personal information or operate across borders. You should also ensure that your remote work policy covers the following aspects: • Communication. You should establish clear and consistent communication channels and protocols for remote workers, such as email, phone, video"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 431, "chunk_id": "english_pack_431_0_6a64e3eb", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 413 conferencing, instant messaging and collaboration tools. You should also inform remote workers of their rights and responsibilities regarding privacy and confidentiality and provide them with regular feedback and support. • Position and employee eligibility. You should determine which positions and employees are suitable for remote work, based on their roles, skills, performance, availability and preferences. You should also consider the impact of remote work on their wellbeing, productivity, collaboration and career development. • Documentation. You should document your remote work policy and procedures and make them accessible and transparent to all relevant parties. You should also keep accurate records of remote work arrangements, such as hours worked, tasks completed, expenses incurred, and outcomes achieved. • Remote work expectations. You should set clear and realistic expectations for remote workers, such as work hours, deliverables, quality standards, deadlines, reporting requirements and performance indicators. You should also monitor and evaluate their work outcomes and provide them with constructive feedback and recognition. • Remote equipment and tools. You should provide remote workers with the necessary equipment and tools to perform their work effectively and securely, such as"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 432, "chunk_id": "english_pack_432_0_89deb3fe", "content": "414 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION laptops, smartphones, software applications, VPNs and cloud services. You should also ensure that they have adequate internet connection and technical support. • Cybersecurity and internet connection. You should implement appropriate cybersecurity measures to protect your data, systems and networks from unauthorized access, use or disclosure. You should also educate remote workers on how to prevent and respond to cyber threats, such as phishing, malware, ransomware and data breaches. You should also ensure that they use secure internet connections and devices when working remotely. • Adapting existing policies. You should review and update your existing policies to reflect the changes brought by remote work, such as health and safety, leave entitlements, expense reimbursements, travel allowances and insurance coverage. You should also consult with your employees, managers, unions and legal advisors on any policy changes or issues. • Training. You should provide remote workers with adequate training on how to use the equipment and tools provided by you, how to comply with your privacy policy and procedures, how to manage their time, workload and stress levels, how to communicate effectively with their colleagues and customers, how to maintain their professional image and reputation online."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 433, "chunk_id": "english_pack_433_0_49b132de", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 415 By following these best practices for remote work and privacy policies, you can ensure that your business operates efficiently, ethically and legally in the digital age. You can also enhance your employee satisfaction, engagement and retention rates by offering them flexibility, autonomy and trust. Privacy in Digital Communication Privacy in digital communication is a crucial issue for remote workers, as they may share sensitive information with their employers, clients, colleagues, or other parties over various platforms and devices. Remote workers should be aware of the data privacy regulations that apply to their location, industry, and type of data, such as GDPR or CCPA, and follow the best practices to comply with them. Remote workers should also take steps to protect their own privacy and security, such as using strong passwords, encryption, VPNs, anti-virus software, and identity management tools. Remote workers should communicate clearly and respectfully with their managers and co-workers about their expectations, boundaries, and preferences regarding privacy and data sharing. Remote workers should seek advice from IT governance, policy, ethics, and law experts if they encounter any challenges or dilemmas related to privacy in digital communication."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 434, "chunk_id": "english_pack_434_0_3e0adc19", "content": "416 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION Data Security in Remote Work Data security in remote work is the practice of protecting sensitive information and systems when employees work from home or in remote locations. Data security in remote work involves encrypting data at rest and during transit, safeguarding it from interception, compromise, or theft. It also involves preventing data loss or leakage, which can happen when employees use personal devices, unsecured networks, or unauthorized applications. Data security in remote work requires a strong security policy that covers the roles and responsibilities of remote workers, the acceptable use of devices and applications, the encryption and backup of data, and the reporting of incidents. It also requires ongoing education and training for remote workers, so they are aware of the proper security protocols, the importance of data security, and how to look for potential cyber threats. Security can be enhanced by embracing cloud technology, which can provide more flexibility, scalability, and resilience for data storage and access. However, cloud technology also introduces new challenges, such as ensuring compliance with data privacy regulations, managing access rights and permissions, and monitoring cloud activity. This is a critical issue for businesses that want to maintain their competitive edge, reputation, and customer trust. It is also a shared responsibility that requires collaboration and"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 435, "chunk_id": "english_pack_435_0_23dac183", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 417 communication between IT teams, managers, and remote workers. Balancing Monitoring & Trust Balancing monitoring in remote work and privacy is a challenging but important task for employers and employees alike. Monitoring can have benefits such as improving productivity, ensuring compliance, and mitigating risks, but it can also have drawbacks such as eroding trust, harming job satisfaction, and increasing stress. To monitor employees effectively and ethically, employers should follow some best practices, such as: • Choosing metrics that are relevant, fair, and transparent, and involving all stakeholders in the process. • Communicating clearly with employees about what is being monitored, why, and how. • Offering incentives and feedback as well as consequences for performance. • Recognizing that employees may face challenges and distractions in their remote work environment and being flexible and supportive. • Monitoring their own systems to ensure that they are not biased or discriminatory against certain groups of employees."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 436, "chunk_id": "english_pack_436_0_5efe4985", "content": "418 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION • Decreasing monitoring when possible and respecting employees’ privacy rights. Trust is essential for remote work and privacy, as it fosters collaboration, innovation, and well-being. Employers should build trust with their employees by: • Providing them with the tools, resources, and training they need to work remotely. • Empowering them to make decisions and manage their own work schedules. • Encouraging them to communicate openly and frequently with their managers and peers. • Appreciating their contributions and celebrating their achievements. • Respecting their personal lives and boundaries. Remote Work Equipment & Privacy Remote work equipment and privacy are closely related issues that affect both employers and employees in a distributed work environment. Employers have a duty to ensure the health and safety of their workers, as well as the security and compliance of their data and systems, when they work from home or elsewhere. Employees have a right to expect reasonable privacy and"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 437, "chunk_id": "english_pack_437_0_2062a421", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 419 autonomy when they use their own or employer-provided equipment for work purposes. To balance these interests, employers and employees should follow some best practices, such as: • Providing adequate and ergonomic equipment for remote workers that meets their individual needs and preferences. • Establishing clear policies and procedures on providing equipment for remote workers, including who owns, pays for, maintains, repairs, replaces, and returns the equipment. • Implementing effective technologies and tools for protecting privacy and security in a remote or hybrid work environment, such as incident response platforms, anti-virus/anti-malware software, big data analytics for cybersecurity, identity management and authentication. • Educating and training remote workers on how to use the equipment safely and securely, as well as their rights and responsibilities regarding data privacy. • Monitoring and auditing the use of equipment for work purposes only when necessary and proportionate, and respecting the personal use of equipment when allowed."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 438, "chunk_id": "english_pack_438_0_dac7de78", "content": "420 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION Consent & Transparent Practices Consent and transparent practices are essential for ensuring the privacy and trust of employees who work remotely. Employers should follow the Australian Privacy Principles (APPs) when collecting, storing, using and disclosing personal information of their remote workers. Employers should have a clear privacy policy that explains what information they collect, why they collect it, how they use it, who they share it with, and how employees can access or correct it. Employers should seek consent from their remote workers before monitoring their activities, such as their emails, social media accounts, or workspaces. Employers should be transparent with their remote workers about the purpose and scope of monitoring, and the benefits and risks involved. Employers should offer incentives and feedback to their remote workers based on their performance, not on their compliance with monitoring. Employers should respect the diversity and individual circumstances of their remote workers, and avoid any discrimination or bias based on personal information. Employers should review and update their privacy practices regularly and consult with their remote workers and other stakeholders on any changes."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 439, "chunk_id": "english_pack_439_0_d0c50790", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 421 Individual Privacy vs. Organizational Needs Individual privacy vs. organizational needs is a key challenge for remote work, especially in the post-pandemic era. Remote workers may face different expectations and norms than on-site workers, which can affect their sense of belonging, trust, and performance. Organizations should consider the following best practices to balance privacy and needs in remote work: • Establish clear and consistent policies for remote work that address issues such as working hours, communication tools, data security, and performance evaluation. • Communicate frequently and transparently with remote workers to foster a shared culture and identity, and to avoid misunderstandings or isolation. • Provide adequate support and resources for remote workers to ensure their well-being, productivity, and engagement. • Respect the boundaries and preferences of remote workers and avoid micromanaging or intruding on their personal space. • Involve remote workers in decision making and feedback processes and recognize their contributions and achievements."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 440, "chunk_id": "english_pack_440_0_92fd30a6", "content": "422 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION Flexible Working Hours & Privacy Flexible hours are arrangements that allow employees to adjust their work schedules and locations to suit their personal and professional needs. This can benefit both employers and employees by increasing productivity, engagement, retention, diversity, and well-being. Flexible hours can also pose some challenges, such as communication difficulties, performance management, security risks, and legal compliance. To implement flexible working hours successfully, employers need to establish clear policies and guidelines, consult with employees and stakeholders, provide adequate technology and support, and monitor and evaluate the outcomes. And to make the most of flexible working hours, employees need to communicate effectively, manage their time and tasks, balance their work and personal responsibilities, and maintain their health and safety. Managing Sensitive Information Managing sensitive information in a remote work environment is crucial for protecting your data and intellectual property, as well as complying with legal and ethical obligations."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 441, "chunk_id": "english_pack_441_0_1d7bbc3b", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 423 Management (you) should set up and communicate clear policies and guidelines for your employees on how to handle sensitive information, such as personal, financial, health, or confidential data, when working remotely. You should use secure tools and platforms that encrypt your data at rest and in transit, such as Microsoft Teams, which also allows you to apply data loss prevention and sensitivity labelling to prevent unauthorized access or sharing of sensitive information. You should monitor and mitigate insider risks, such as accidental or malicious disclosure of sensitive information by your employees, by using incident response platforms, big data analytics, identity management, and authentication systems. You should provide regular training and awareness programs for your employees on the importance of visual privacy, VPN security, personal device regulation, and communication channel security when working remotely. Cultural and Legal Diversity Remote work can enhance workplace diversity by allowing access to a wider pool of talent, reducing geographic and social barriers, and accommodating different needs and preferences of employees. However, remote work also poses some challenges for diversity and inclusion, such as potential isolation, exclusion,"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 442, "chunk_id": "english_pack_442_0_b487ebe1", "content": "424 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION or misunderstanding of employees from different backgrounds, identities, or locations. To address these challenges, remote workers and managers need to be aware of the cultural differences that can impact global teams, such as communication styles, decision-making processes, conflict resolution strategies, and feedback preferences. Remote workers and managers also need to be mindful of the legal diversity that can affect remote work and privacy, such as data protection laws, employment laws, tax laws, and anti- discrimination laws that may vary across countries or regions. Therefore, it is advisable for remote workers and managers to follow some best practices for cultural and legal diversity in remote work and privacy, such as: • Developing workplace policies and training that promote cross-cultural awareness and respect. • Holding regular virtual meetings and events that celebrate workplace diversity and encourage employees to share their cultures and experiences. • Using clear and inclusive language and communication tools that suit the needs and preferences of different employees. • Seeking feedback and input from diverse employees on important decisions and projects. • Ensuring compliance with relevant laws and regulations in different jurisdictions where remote workers are"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 443, "chunk_id": "english_pack_443_0_510acf41", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 425 located. • Providing support and resources for remote workers to deal with any legal or cultural issues that may arise. Addressing Burnout & Overwork Addressing burnout and overwork in remote work is a crucial challenge for many hard-working IT professionals, who often face high demands, tight deadlines, and complex tasks. Burnout can have a range of negative consequences for individual well-being, team performance, and organizational outcomes, such as increased turnover, reduced productivity, and lower customer satisfaction. To prevent and reduce burnout in remote work, IT professionals should follow some evidence-based strategies, such as: • Creating an environment for communication. Remote workers may feel isolated, disconnected, or misunderstood by their colleagues and managers. To foster a sense of belonging and trust, IT professionals should communicate frequently, clearly, and empathetically with their team members and leaders. They should also seek feedback, share achievements, and celebrate successes. • Lifting morale — genuinely. Remote workers may lack the motivation, engagement, or recognition that"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 444, "chunk_id": "english_pack_444_0_600c593a", "content": "426 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION they would receive in a physical office. To boost morale and enthusiasm, IT professionals should find meaningful and enjoyable aspects of their work, express gratitude and appreciation to others, and participate in social activities that foster camaraderie and fun. • Simplifying remote work systems. Remote workers may struggle with the complexity, ambiguity, or inefficiency of their work processes and tools. To streamline remote work systems, IT professionals should use reliable and user-friendly technology platforms, establish clear and consistent expectations and guidelines, and prioritize and delegate tasks effectively. • Reducing or eliminating meetings. Remote workers may experience meeting fatigue, which can drain their energy, attention, and creativity. To minimize meeting overload, IT professionals should only attend meetings that are relevant, necessary, and productive. They should also limit the duration and frequency of meetings, prepare agendas and objectives beforehand, and follow up with action items afterward. • Addressing the elephant. Remote workers may face personal or professional challenges that are specific to their situation, such as juggling caregiving responsibilities, coping with mental health issues, or dealing with technical difficulties. To address these challenges, IT professionals should be honest and proactive about their needs and concerns, seek support"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 445, "chunk_id": "english_pack_445_0_2bbc60d5", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 427 from their managers or peers, and access available resources or services. • Investing time and attention in themselves. Remote workers may neglect their own well-being by working long hours, skipping breaks, or ignoring physical or emotional signs of stress. To take care of themselves, IT professionals should set healthy boundaries between work and life, practice self-care activities that enhance their mood and energy, and take regular recovery time to relax and recharge. Education & Training Ensure that you comply with IP, ethics and privacy policies and procedures in ICT environments, as outlined in the relevant training packages. Locate and access the organisation’s IP, ethics and privacy policy and procedures, and determine how they apply to your remote work situation. Analyse legislation and standards that relate to IP, ethics and privacy in ICT, such as the Privacy Act 1988 (Cth), the Australian Privacy Principles, the Copyright Act 1968 (Cth), the Code of Ethics for Professional Conduct by the Australian Computer Society, etc. Contribute to policy and procedures improvements in code of ethics and privacy policy documents in the ICT industry, by"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 446, "chunk_id": "english_pack_446_0_7b549499", "content": "428 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION providing feedback, suggestions and recommendations based on your experience and expertise. Use technology competently and securely to deliver education and training remotely, such as using encryption, passwords, firewalls, antivirus software, VPNs, etc. Uphold your professional and ethical obligations while working remotely, such as maintaining supervision, client confidentiality, communication, quality of service, etc. 10.4 Digital Identity and Authentication Systems Digital identity refers to the online or networked identity adopted by an individual, organization, or electronic device. In our increasingly digital world, managing these identities securely and efficiently has become crucial for both public and private sectors. Components of Digital Identity Systems Digital identity systems typically consist of three main components: identification (proving who you are), authentication (verifying that proof), and authorization (determining what you’re allowed to do). These systems aim"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 447, "chunk_id": "english_pack_447_0_4b50dc20", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 429 to provide a secure, convenient, and privacy-preserving way for individuals to prove their identity online. Authentication Methods Authentication methods have evolved significantly over the years. Password-based systems, while still common, are increasingly seen as inadequate on their own. Multi-factor authentication (MFA) has become a standard practice, combining something you know (like a password), something you have (like a phone), and something you are (like a fingerprint). Biometric authentication, using physical characteristics like fingerprints, facial recognition, or iris scans, is becoming more prevalent in both consumer devices and high-security environments. Challenges in Digital Identity Despite advancements, digital identity systems face several challenges. Privacy concerns are paramount, as these systems often involve collecting and storing sensitive personal information. Security is another major concern, with identity theft and fraud remaining persistent threats. Inclusivity is also a challenge, as digital identity systems must be accessible to all members of society, including those with limited tech literacy or access."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 448, "chunk_id": "english_pack_448_0_7383a5f6", "content": "430 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION Future Trends Emerging trends in digital identity include decentralized identity systems, which give users more control over their personal data, and continuous authentication, which constantly verifies a user’s identity based on behavioral patterns. There’s also growing interest in “self-sovereign identity,” where individuals have sole ownership of their digital and offline identities without relying on any centralized authority. 10.5 Blockchain in Government and Public Services Blockchain technology, best known as the foundation of cryptocurrencies, has potential applications far beyond finance. Governments worldwide are exploring how this decentralized, transparent, and tamper-resistant technology can improve public services and operations. Potential Applications One key area of interest is in record-keeping. Blockchain could provide a secure, transparent way to store and manage public records like property deeds, business licenses, or birth certificates. In voting systems, blockchain could potentially increase security and transparency, though this application"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 449, "chunk_id": "english_pack_449_0_e51223bb", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 431 remains controversial due to various technical and social challenges. Improving Government Operations Blockchain could streamline government operations by reducing paperwork, automating processes, and improving data sharing between departments. For example, it could facilitate more efficient distribution of social benefits or streamline supply chain management for government procurement. Enhancing Transparency and Trust By providing a tamper-resistant record of transactions, blockchain could enhance government transparency and accountability. This could be particularly valuable in areas like public spending, where citizens could potentially track how their tax money is being used in real-time. Challenges and Considerations While promising, the implementation of blockchain in government faces several challenges. Technical issues include scalability and energy consumption. There are also legal and regulatory hurdles, as existing laws may not be equipped to handle blockchain-based systems. Privacy concerns are"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 450, "chunk_id": "english_pack_450_0_92e151c3", "content": "432 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION significant, as the immutable nature of blockchain could conflict with data protection regulations like the right to be forgotten. Real-World Examples Several governments are already experimenting with blockchain. Estonia, often cited as a digital leader, uses blockchain technology to protect national data, e-services, and smart devices. The United Arab Emirates has launched a blockchain strategy aiming to conduct 50% of government transactions using blockchain technology by 2021. Future Outlook As the technology matures and governments become more familiar with its potential and limitations, we’re likely to see more blockchain applications in the public sector. However, it’s important to approach these innovations critically, ensuring that they truly serve the public interest and don’t exacerbate existing inequalities or create new vulnerabilities. Digital identity systems and blockchain technology represent significant opportunities for improving government services and operations. However, their implementation must be approached thoughtfully, with careful consideration of security, privacy, and accessibility. As these technologies continue to evolve, ongoing dialogue between technologists,"}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 451, "chunk_id": "english_pack_451_0_8d72b17d", "content": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION | 433 policymakers, and citizens will be crucial to ensure they are used in ways that truly benefit society."}
{"pack_id": "english_pack", "section_title": "MODULE 10: E-GOV & DIGITAL TRANSFORMATION", "page": 452, "chunk_id": "english_pack_452_0_b6f08514", "content": "434 | MODULE 10: E-GOV & DIGITAL TRANSFORMATION"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 453, "chunk_id": "english_pack_453_0_92df67d9", "content": "APPENDIX A: LIST OF ACRONYMS | 435 APPENDIX A: LIST OF ACRONYMS"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 454, "chunk_id": "english_pack_454_0_fda3261e", "content": "436 | APPENDIX A: LIST OF ACRONYMS Appendix A: List of Acronyms"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 455, "chunk_id": "english_pack_455_0_81c6edde", "content": "APPENDIX A: LIST OF ACRONYMS | 437 Acronym Meaning A&O analysis and operations ACL access control list ADS anomaly detection system A/V audio/video AV anti-virus AVS anti-virus software C&A certification and accreditation Continuous Asset Evaluation, Situational CAESARS Awareness, and Risk Scoring Reference Architecture Report CBK Common Body of Knowledge CBT computer-based training CCV Cybersecurity Capabilities Validation CD compact disc CERT/CC CERT Coordination Center"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 456, "chunk_id": "english_pack_456_0_92fc7180", "content": "438 | APPENDIX A: LIST OF ACRONYMS CIA confidentiality, integrity, and availability CIO chief information officer CISO chief information security officer CISSP Certified Information Systems Security Professional CM continuous monitoring CMMI Capability Maturity Model Integration CMU Carnegie Mellon University CND computer network defense CNDSP computer network defense service provider Control Objectives for Information and related COBIT Technology CONOPS concept of operations COOP continuity of operations COP common operational picture CP contingency planning"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 457, "chunk_id": "english_pack_457_0_110a0a45", "content": "APPENDIX A: LIST OF ACRONYMS | 439 CSIRT computer security incident response team CVE Common Vulnerabilities and Exposures D/A department/agency DDOS distributed denial of service DHS Department of Homeland Security DISA Defense Information Systems Agency DMZ demilitarized zone DNS domain name system DoD Department of Defense DoS denial of service ETA education, training, and awareness F-CND Federal-Computer Network Defense FAX facsimile FCD Federal Continuity Directive"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 458, "chunk_id": "english_pack_458_0_c1b633f4", "content": "440 | APPENDIX A: LIST OF ACRONYMS FCMR Federal Cybersecurity Maturity Roadmap FE framework extension FFIEC Federal Financial Institutions Examination Council FIPS Federal Information Processing Standards FIRST Forum of Incident Response and Security Teams Federal Information Security Management Act of FISMA 2002 FNR Federal Network Resilience FNS Federal Network Security FOUO for official use only FYI for your information Government Forum of Incident Response and GFIRST Security Teams GnuPG GNU Privacy Guard GRS General Records Schedule HR human resources"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 459, "chunk_id": "english_pack_459_0_a221fc00", "content": "APPENDIX A: LIST OF ACRONYMS | 441 IA information assurance IC intelligence community IDPS Intrusion Detection and Prevention System IDS intrusion detection system IEC International Electrotechnical Commission IETF Internet Engineering Task Force IG inspector general IM incident management IMF Incident Management Function IP internet protocol IPS intrusion prevention system IR incident response ISAC Information Sharing and Analysis Center International Information Systems Security (ISC)2 Certification Consortium"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 460, "chunk_id": "english_pack_460_0_9747b769", "content": "442 | APPENDIX A: LIST OF ACRONYMS ISCM information system continuous monitoring ISCP Information System Contingency Plan ISF Information Security Forum information security officer OR ISO International Organization for Standardization ISP internet service provider IT information technology ITGI Information Technology Governance Institute ITIL IT Infrastructure Library Joint Worldwide Intelligence Communications JWICS System LE law enforcement LOA letter of agreement MEF mission essential function MIME Multipurpose Internet Mail Extensions MO modus operandi (mode of operation)"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 461, "chunk_id": "english_pack_461_0_e6feab5a", "content": "APPENDIX A: LIST OF ACRONYMS | 443 MOA memorandum of agreement MOU memorandum of understanding MSSP managed security service provider NARA National Archives and Records Administration NDA non-disclosure agreement NEF national essential function NFAT network forensics analysis tools NIC network information centre NIST National Institute of Standards and Technology NIST SP NIST Special Publication NITTF National Insider Threat Task Force NOC network operations centre NSA National Security Agency NVD National Vulnerability Database"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 462, "chunk_id": "english_pack_462_0_02676246", "content": "444 | APPENDIX A: LIST OF ACRONYMS Operationally Critical Threat, Asset, and OCTAVE Vulnerability Evaluation OGC Office of Government Commerce OLRC Office of the Law Revision Counsel OMB Office of Management and Budget OPSEC operations security OS operating system PC personal computer PE physical and environmental PGP Pretty Good Privacy PII personally identifiable information PKI public key infrastructure PMEF primary mission essential function POC point of contact QA quality assurance"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 463, "chunk_id": "english_pack_463_0_bb885ae1", "content": "APPENDIX A: LIST OF ACRONYMS | 445 RA risk assessment RDF resource description framework RFC request for comments RSS RDF Site Summary SA situational awareness SCIF Sensitive Compartment Information Facility SDLC system development lifecycle SEI Software Engineering Institute SEIM security event and incident management SIPRNET Secret Internet Protocol Router Network SKiP Security Knowledge in Practice SLA service level agreement S/MIME Secure/Multipurpose Internet Mail Extensions SME subject matter expert"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 464, "chunk_id": "english_pack_464_0_c116cae8", "content": "446 | APPENDIX A: LIST OF ACRONYMS SMS short message service SOC security operations centre SOP standard operating procedure SP special publication SSP system security plan STE secure terminal equipment SWO senior watch officer Trans-European Research and Education TERENA Networking Association TICAP Trusted Internet Connection Access Provider TS top secret TT&E testing, training, and exercise United States Computer Emergency Readiness US-CERT Team VPN virtual private network VS vulnerability scanning"}
{"pack_id": "english_pack", "section_title": "Appendix A: List of Acronyms", "page": 465, "chunk_id": "english_pack_465_0_8080c2cf", "content": "APPENDIX A: LIST OF ACRONYMS | 447 XML Extensible Markup Language"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 466, "chunk_id": "english_pack_466_0_91d0f960", "content": "448 | APPENDIX B: APPLYING THE ETHICAL DECISION MODEL APPENDIX B: APPLYING THE ETHICAL DECISION MODEL Appendix B: Applying the EDM This sample case study illustrates how the Ethical Decision Model can be applied in practice. Case Scenario. Luke Vandenberg always wanted to be an interactive web designer since he was about halfway his High School years. He started working as an intern for a design studio while he was in his final year and got a lot of good experience from this. After he finished High School, Luke enrolled in a multimedia degree at Altiora University. The degree required him to do a year work experience after completing the second year. After this year, he would return to full time study and complete the final year. This extended a three year degree to four years, but Luke was OK about this because he was able to land an intern job with a leading interactive design studio (TT INTERACTIVE) that worked closely with advertising agencies to develop the web component of major ad campaigns. During his time at TT INTERACTIVE, Luke worked on"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 467, "chunk_id": "english_pack_467_0_b83dcc60", "content": "APPENDIX B: APPLYING THE ETHICAL DECISION MODEL | 449 a big project that involved the design and implementation of a web portal for Altiora University who wanted to re-badge their image, and market itself more effectively while improving the scope and functionality of their existing web portal. It was a big job for a high profile client and TT INTERACTIVE wanted someone who not only had the design skills but also had some familiarity with the culture at Altiora, so Luke got the job. Luke was part of a three person project team. It was a demanding but highly satisfying project that was almost finished by the time he finished his intern year, and was due to return to university for the final year. TT were a hard task- master. Though he was being paid the bare minimum, he was expected to work long hours. He was told the experience he was getting was worth a lot. Nonetheless, he had mixed feelings; on the one hand he was indeed getting some great experience, but he could not help feeling somewhat exploited by TT management who seemed to care more about project outcomes than the people making the project happen. Luke was a careful person by nature, and routinely made backups of the project files that he stored off-site for safe- keeping. He had lost hours of work before and was in the habit of making off-site backups. He did not think or realize that this constituted a breach of the agreement he had signed with TT to not remove any intellectual copy off-site without the permission of a TT Director. And so it was that when he left TT INTERACTIVE he happened to have a more-or-less"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 468, "chunk_id": "english_pack_468_0_49516474", "content": "450 | APPENDIX B: APPLYING THE ETHICAL DECISION MODEL complete set of project files for the new Altiora University web portal. He did not at this stage intend to do anything with this material. During his final year at Altiora, Luke was gratified, in fact delighted to see the new web portal come on-line. He felt like he was making a difference in the world. Despite some teething problems, and some disparaging comments from his fellow students, the new portal was a success. He felt justifiably proud of himself. When he finished his degree he had been told there would probably be a job for him at TT but they were by this time fully staffed. Luke was a free agent in the world, looking for a way to apply his talents. With the experience he now had as a co-developer of a major new portal he was able to secure a similar position at a rival studio, Tangential. A year goes by, and Luke has settled in to his new job at Tangential. He becomes a team leader in due course. One day his boss calls him in for a conversation. How would Luke like to take the lead on a project to develop a web portal for Charleston Technical University? Tangential have successfully tendered for the project. This is important. Tangential management is quite excited about it. The boss hints that anything Luke might have “learned” while at TT INTERACTIVE would be helpful. Something in the Boss’s manner strikes Luke as a bit odd. Then he remembers having mentioned in passing over drinks some months earlier that he still had the backup project files from his earlier job working on the Altiora portal for TT. Luke feels"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 469, "chunk_id": "english_pack_469_0_e40f4d3a", "content": "APPENDIX B: APPLYING THE ETHICAL DECISION MODEL | 451 a little uneasy at what his boss seems to be suggesting, but is nonetheless excited by the project, and is keen to do a good job and perhaps get ahead in the industry, maybe one day open his own studio. Why not? Luke has recently married, and his wife and he had been discussing only the other day how good it would be to find a bigger place to live, maybe even start a family. The upshot of the meeting is that Luke enthusiastically accepts the challenging new team lead role on the Charleston Technical University web portal project. Without any discussion with anyone, he digs out his old backups of the Altiora project (done while at TT) and dusts them off. Yes, he decides, this could be useful; this could be the framework that will save a lot of time and effort. He wonders briefly whether he is doing the right thing, but finishes by telling himself that “This is my own work. Don’t I have a right to use it? Why reinvent the wheel?” So Luke takes his earlier work (and that of his fellow developers at TT who worked on the Altiora project) and modifies it so that it is superficially different from the original (a user would not notice much similarity), but under the skin, at a programming level, it was largely based on his earlier work. The Charleston Technical University web portal is ultimately delivered. The client is happy, Tangential is happy, and Altiora and TT INTERACTIVE do not allege the theft of their intellectual property. Years pass. Luke becomes a senior project manager and"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 470, "chunk_id": "english_pack_470_0_bfbf9239", "content": "452 | APPENDIX B: APPLYING THE ETHICAL DECISION MODEL continues to build a successful career, culminating in the opening of his own studio. One day, as Director of his own studio, he receives a phone call from Altiora University. They flatter him as a successful alumnus, and ask if he would consider taking on an intern or two for the following year. He thinks for a moment, and somewhere in the back of his mind, a troubling thought takes shape … End of Case Study Factors & related issue 1. Intellectual Property Theft (Legal). Creating Backups off site without Directors permission. 2. Breach of Agreement with TT (Legal). Creating Backups off site. 3. Privacy Legislation(Legal). Discussed his past job (work files) of the web portal from TT with the Tangential Manager. 4. Confidentiality (Legal). Discussed his prior jobs completed at a different organisation with his new employer. 5. Piracy (Legal). When Luke used the backup files for assistance from TT, the programming & framework 6. Code of Ethical conduct (Legal). Luke has minimised his integrity since the moment he made that first backup offsite. 7. Acknowledged Backups (Professional). When Luke mentioned to his manager at Tangential about the work"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 471, "chunk_id": "english_pack_471_0_33d93fa0", "content": "APPENDIX B: APPLYING THE ETHICAL DECISION MODEL | 453 he stole from TT. 8. Manager Signifying theft (Professional). When Luke was in his manager’s office getting offered the lead role & felt uneasy about using the work from TT by the Tangential Manager. 9. Project outcomes (Employment & Social). No gratitude was acknowledged to the workers only keeping the project outcomes in sight at TT. 10. Work Experience (Employment & Social). Luke’s work Experience was highly valuable for his forthcoming. 11. Existing Student (Employment & Social). The fact TT hired Luke as an intern for their upcoming project would be valuable. 12. Long Hours (Employment & Social). Luke felt under appreciated. 13. Insufficient money (Employment & Social). Luke felt he was not getting paid enough. 14. Comments (Employment & Social). When Luke’s fellow students were not happy about the portal at Altiora by voicing disparaging comments. 15. Get ahead (Personal). Luke was ignorant of the law to get ahead in his career. 16. Married (Personal). Luke got married. 17. Bigger Place (Personal). Luke & his wife need a bigger place to live in. 18. Family (Personal). Luke and his wife want a family one day."}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 472, "chunk_id": "english_pack_472_0_3619c230", "content": "454 | APPENDIX B: APPLYING THE ETHICAL DECISION MODEL 19. Own Studio (Personal). Luke wants to have his own studio. 20. Lead Role (Personal). Did Lead role for a web portal for a high end client while at Tangential. 21. Senior Director (Personal). Had the job description as Senior Director. 22. 1 year work experience (Intrinsic). Luke did one year work experience at TT to achieve his degree. 23. Co-developer (Intrinsic). During that year of experience Luke gained co-developer skills 24. Multimedia degree (Intrinsic). Luke received his multimedia degree. 25. Team leader (Intrinsic). Luke became a team leader at Tangential. 26. Senior project manager (Intrinsic). Luke achieved the job description senior project manager at tangential for his efforts. 27. Own studio (Intrinsic). Luke owns his own studio. 28. Director (Intrinsic). Luke is the director at his own studio. Sample Solution 1 Legal Factors Factor 1, Contract Breach In the case study, Luke Vandenberg did some work experience for TT INTERACTIVE working"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 473, "chunk_id": "english_pack_473_0_7851b0af", "content": "APPENDIX B: APPLYING THE ETHICAL DECISION MODEL | 455 on the Altiora web portal project. Luke Vandenberg had signed a contract with TT INTERACTIVE which stated that he was not to remove any intellectual property off-site without the expressed permission of the TT director. However, Luke did make off-line backups. This is a legal factor because there are Australian laws which govern contracts. Factor 2, Copyright infringement In the case study, Luke had been working at a second company, Tangential Software. Luke was working on a web portal project like the one he had worked on with TT INTERACTIVE but for Charleston technical university. Luke used the work from the Altiora project and altered it superficially, effectively copying the work he and others had worked on at TT INTERACTIVE. This is a legal factor because the work Luke used was not his own. This is a breach of Australian copyright law Professional Factors Factor 3, Professional Contract Obligations As described earlier Luke Vandenberg made a breach of his contract with TT INTERACTIVE. This is not only a legal factor but also a professional factor as is stated in the Australian computer society’s code of professional conduct and professional practice under H3. Factor 4, Professional Confidentiality Luke copied the intellectual property of his previous employer, TT INTERACTIVE without permission. Copying the Altiora"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 474, "chunk_id": "english_pack_474_0_ab2cd1a5", "content": "456 | APPENDIX B: APPLYING THE ETHICAL DECISION MODEL project is a professional factor as well as a legal one as is stated in the Australian Computer Society’s code of conduct under A3. Employment & Social Factors Factor 5, Luke cheated TT INTERACTIVE As mentioned Luke copied the Altiora web portal he worked on at TT INTERACTIVE and used it in a similar project for rival company Tangential software. Society does not like cheaters and thieves. By copying the Altiora project without crediting TT INTERACTIVE, Luke effectively stole the project and cheated TT INTERACTIVE and its employees. Luke Vandenberg is a member of society; he should have contributed to society. Instead, Luke behaved like a parasite, taking without giving credit or giving back to the organization he was once a part of. Factor 6, Luke was treated poorly at TT INTERACTIVE Luke was made to work long, hard hours whilst working at TT INTERACTIVE. Luke had mixed feelings about his time at TT INTERACTIVE. Luke was getting good experience but he felt that he was worked too hard for his minimum wage. TT INTERACTIVE worked Luke very hard which could have made Luke feel oppressed. This is a social factor because people in society usually resent those whom oppress them or take away their freedom. Society has a general dislike for bullies like TT INTERACTIVE."}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 475, "chunk_id": "english_pack_475_0_3b0477c2", "content": "APPENDIX B: APPLYING THE ETHICAL DECISION MODEL | 457 Factor 7, Luke Broke a Promise Luke signed a contract with TT INTERACTIVE, promising not to make off-site backups without permission from the TT director. Luke breached this contract by making off-site backups. This is a social factor because society generally does not like it when people break promises. For example when a government party promises to lower taxes and does not deliver on the promise, people in society become resentful towards party. Factor 8, Luke’s boss pressured him Luke’s boss at Tangential asked him to use whatever he had learned from the previous project. Luke felt that his boss’ manner was odd. This was because the boss knew about Luke’s copy of the Altiora web portal project and was suggesting that Luke copy this work for the new Tangential web portal project. This is an employment factor because Luke’s boss suggested he take part in an un- professional act. Factor I, Luke was misled Luke was told that there would probably be a place for him at TT INTERACTIVE after he finished his university degree however TT INTERACTIVE did not have a place for him. This is an employment factor because the organisation, TT INTERACTIVE may have disappointed Luke. Personal Factors Factor 10, Luke committed plagiarism Luke committed plagiarism by copying the Altiora web portal project without"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 476, "chunk_id": "english_pack_476_0_45f8cade", "content": "458 | APPENDIX B: APPLYING THE ETHICAL DECISION MODEL giving credit to TT INTERACTIVE, the rightful owner and therefore committed plagiarism. This is a personal factor because Luke’s personal environment was once at Altiora University and universities are very strict on plagiarism. Factor 11, Luke was from an I.T (Information Technology) background Luke made off-site copies of the Altiora web portal project whilst at TT INTERACTIVE. Luke made these copies because he wanted to make sure he did not lose his work. Also when Luke copied TT INTERACTIVE’s web portal he said to himself, “Why reinvent the wheel?” These are personal factors because they come from Luke’s information technology background. Factor 12, Peer pressure Luke’s boss at Tangential subtly told Luke to copy what he could from TT INTERACTIVE’s web portal project. This is a personal factor because Luke’s boss applied peer pressure on Luke. It is part of what caused Luke to copy TT INTERACTIVE’s web portal project. Intrinsic Factors Factor 13, Luke’s personal gain Luke wanted the new project with Tangential to go successfully as this may lead to Luke getting ahead in the industry. Luke had ambitions to open his own studio in the hopes of earning enough money so he and his wife could move into a larger home. This is a personal factor because it is what motivated Luke to copy TT INTERACTIVE’s Altiora web portal project."}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 477, "chunk_id": "english_pack_477_0_1af79200", "content": "APPENDIX B: APPLYING THE ETHICAL DECISION MODEL | 459 Factor 14, Luke was a safe person Luke was a safe person by nature. This is what led him to making off-line backups of the Altiora web portal project. This is an intrinsic factor because it is to do with Luke’s own experiences and it is part of who he is as a person. Factor O, Luke’s mixed feelings toward TT INTERACTIVE Luke had mixed feelings about working at TT INTERACTIVE and felt he was taken advantage of. This is an intrinsic factor as it is part of Luke’s own experiences and Luke may have felt negatively towards TT INTERACTIVE because of it. Sample Solution 2 Legal factors The first subject I have in this list of legal factors is the intellectual property theft. The reason I have chosen this as the first legal priority is because while Luke was working at TT he was taking backups of his work off site which is legally the property of TT. Although he may have been under the assumption that he was doing the right thing for TT by making off site backups, it does not eliminate the fact that being ignorant of the law does not make it ok to break the law. The second legal factor which is still within relation of the first is the breach of agreement that Luke has committed. Being employed by TT and gaining the work experience he was achieving I’m sure Luke could have been intelligent enough"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 478, "chunk_id": "english_pack_478_0_8c68ffb7", "content": "460 | APPENDIX B: APPLYING THE ETHICAL DECISION MODEL to talk to the director about implementing other systems in regards to making backups off site and not take it into his own hands which has initiated this breach. The third legal factor is when Luke breached the privacy legislation. The time he was having a few drinks then he had mentioned to the Tangential manager that he still had backups of the work he done when he was working at TT interactive. This act is highly unethical. This action has created a negative reaction further down the lines when the Tangential manager calls him in for chat about the new project of the portal. The fourth is confidentiality as at the point when without hesitation Luke used his original backups from TT as the framework for his new project with Tangential & spoke to his boss about the work he had done with TT. The fifth legal factor is that Mr Vandenberg has breached his own Code of ethical conduct as he had committed the above breaches mentioned and never owned up to his mistake hence decreasing the integrity of himself. The sixth legal breach that has been committed is piracy at the time in the case study when Luke used the framework from TT as the framework for Tangential web portal. Professional factors The night that Luke was having few drinks with his manager from Tangential & he mentioned that he still had backups of the work he had done while he was employed by a rival competitor at TT interactive. As Months went by then one"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 479, "chunk_id": "english_pack_479_0_4d68c90b", "content": "APPENDIX B: APPLYING THE ETHICAL DECISION MODEL | 461 day Luke got called into the office by his manager at Tangential. The news of the new project they are beginning for the web portal gets informed to Luke. The next instant Luke catches hints from his employer about the backups he had from the last portal that he had designed at TT would be a great asset to the project. Could this be the reason Luke was given the opportunity to be the Lead worker of this project? Employment and Social factors Despite Luke received disparaging comments from fellow students he was still justifiably happy with the web portal produced by TT. The time Luke was working at TT INTERACTIVE he was doing long hours and earning the base wage minimum. The justification of this circumstance was that the fact that the experience he is receiving is much more treasured. However Luke still felt underappreciated and what some could say being de humanised slightly. No gratitude was being acknowledged towards the workers just to the future outcomes of the project being completed. The fact that Luke was an existing student at Altiora University assisted in TT interactive giving him the internship as they wanted someone who was familiar with the culture at the university."}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 480, "chunk_id": "english_pack_480_0_887c86ff", "content": "462 | APPENDIX B: APPLYING THE ETHICAL DECISION MODEL Personal factors Luke had great justification in the fact that getting the lead role for the project at Tangential would really assist in taking the path of being senior director then concluding to his dream of opening his own studio. His priority was that just after recently getting married Luke & his wife discussed that they would like to have a bigger place and possibly have children (family) one day. Intrinsic factors Luke has attended university for 4 years in total. Luke was required to do 1 year work experience as part of his multimedia degree. This was the time when TT interactive was doing the web portal for Altiora University. The entire project had taken the whole entire year that Luke was doing as work experience. This also gave him co-developer experience. Once Luke went back to university for his final year he was under the impression that he would have a job at TT however this was not the case as TT was fully staffed on the completion of Luke’s degree. None the less Luke gained employment at a competitor of TT called Tangential. As time goes by with his employment Luke becomes Team leader at Tangential. The opportunity arises that would help him get ahead in the industry. Further along down the line Luke becomes a senior"}
{"pack_id": "english_pack", "section_title": "Appendix B: Applying the Ethical Decision Model", "page": 481, "chunk_id": "english_pack_481_0_2bd09072", "content": "APPENDIX B: APPLYING THE ETHICAL DECISION MODEL | 463 project manager which then he decides to open and run his own studio as the director. Please note this sample is for illustrative purposes only."}
{"pack_id": "english_pack", "section_title": "Appendix C: Common scenarios", "page": 482, "chunk_id": "english_pack_482_0_f803cceb", "content": "464 | APPENDIX C: COMMON SCENARIOS APPENDIX C: COMMON SCENARIOS Appendix C: Common scenarios Tell us our competitor’s secrets Often people are hired for a job on the strength of what they have learned working for a competitor. The assumption, often unstated, is that the new-hire will bring specific knowledge of their former employer’s competitive advantage. In this situation, it is permissible to bring general knowledge of a competitor’s business to a new employer. You will have signed a legally binding employment contract with the first employer, and this prohibits the disclosure of any proprietary information to a third party without written permission. So you must not make copies of commercial-in-confidence material, and especially not sell that information to competitors, whether directly or by going to work for them. Severe penalties apply for proven breaches. A recent press report describes how a man was sued by his former employer because he took with him 17,000 followers of a promotional Twitter account when he left. The former"}
{"pack_id": "english_pack", "section_title": "Appendix C: Common scenarios", "page": 483, "chunk_id": "english_pack_483_0_53f27a4c", "content": "APPENDIX C: COMMON SCENARIOS | 465 employer alleges that each follower is worth $2.50 and was seeking damages. The man was working for a competitor by this time. Intellectual property protection or copyright applies to specific implementations of an idea. It is the implementation that is copyright. Organisations can try to patent or copyright an idea, but it is often problematic. An abstract idea can be implemented in any number of ways. If the implementation is sufficiently different from the original then copyright breach cannot be proven. The history of innovation is full of instances where good ideas have been thought of independently yet simultaneously. The underlying idea might be similar, but the way it is implemented will differ significantly. Work for us, but we won’t pay you It is common practice for unscrupulous employers to hire interns; enthusiastic, usually young developers on low or non- existent salary, on the understanding that the experience they are getting is adequate compensation. The employer exploits the intern by obliging them to work long hours, often at some considerable personal cost. Most people will burn out after a few months of this, and when that happens, they are discarded and replaced by a new wave of intern. While it may be true that you are getting experience that"}
{"pack_id": "english_pack", "section_title": "Appendix C: Common scenarios", "page": 484, "chunk_id": "english_pack_484_0_49d04cf4", "content": "466 | APPENDIX C: COMMON SCENARIOS will look good on your resume, as a rule you should not allow yourself to be exploited in this way, at least not for long. You deserve fair payment for the work you do. This may not be much money, given your lack of experience, but if you are creating something useful and profitable for your employer, you deserve to be paid. Any attempts to exploit you like this should be treated with the contempt it deserves (short-term internships may be acceptable). If an employer’s business model relies on using free, skilled labour, it is questionable. The costly patch It is common practice by some software development companies to release defective software to their clients and then charge them to fix the defects that should not be there in the first place. In some cases, the developer becomes aware that all their customers have a potential problem, yet instead of proactively sending out a patch to their customers, they wait for a customer to complain, and then charge them to fix the problem. It is understandable why a developer might want to use a strategy like this, but it is clearly unethical if the software has been sold on the understanding that it is defect free. It is most unlikely that the customer agreed to accept faulty goods that"}
{"pack_id": "english_pack", "section_title": "Appendix C: Common scenarios", "page": 485, "chunk_id": "english_pack_485_0_1136e0ab", "content": "APPENDIX C: COMMON SCENARIOS | 467 they would be required to pay twice for, or three times if there is a maintenance/support agreement in place. No, that doesn’t fit with our strategy Software development companies with an idea for a software product will often look for a client who is willing to finance the production of the software which will then be sold to as many of the client’s competitors as possible. If the client can be persuaded to pay full price, then so much the better. To secure the deal, the developer may offer a discount. The client probably realises that they are funding a project that will not only benefit them, but also their competitors. Perhaps they trust a non-disclosure agreement with the developer to safeguard their interests. This trust may prove to be misplaced. As development proceeds, there are sure to be times when the client says to the developer, thinking about it, we now want the software to do this or that (specific requirements). Unless this fits with the developer’s own product strategy, the client is likely to be told, no that cannot be done. It would be a rash or arrogant developer who then says, it cannot be done because it does not fit with our development strategy, though I have personally heard these words spoken in a client-developer meeting. If the client is paying for it, they are entitled to get what"}
{"pack_id": "english_pack", "section_title": "Appendix C: Common scenarios", "page": 486, "chunk_id": "english_pack_486_0_9f917c53", "content": "468 | APPENDIX C: COMMON SCENARIOS they are paying for. If the developer wants to go in a different direction, they should do it at their own expense, on their time. Two sets of accounts Some businesses that accept cash money have been known to keep two sets of accounts; one for the purposes of paying tax, the other to tell the full financial story strictly for in-house use. As a software professional working on an organisation’s accounts, you are likely to see information that your employer expects you to keep confidential. It may only be relatively minor matters, not necessarily a whole second set of book. Confidentiality of the employer-employee relationship is extremely important in professional practice. Like a doctor or lawyer, what you learn must not be disclosed to a third party except under certain prescribed circumstances. Becoming a whistle-blower is an extreme act, though sometimes it is justified. Before doing it, it is highly recommended that you seek the advice of your professional association. With good advice, your ethical concerns can be resolved without breaching your obligation to treat your employer’s information confidentially. You should be aware that whistle-blowers, regardless of how well-intentioned they be, almost invariably become despised and unemployable."}
{"pack_id": "english_pack", "section_title": "Appendix D: Software licensing", "page": 487, "chunk_id": "english_pack_487_0_0d72e4fb", "content": "APPENDIX D: SOFTWARE LICENSING | 469 APPENDIX D: SOFTWARE LICENSING Appendix D: Software licensing This section contains useful reference information for software developers wanting to understand the intricacies of software licensing. • An independent software author owns the copyright on the program • Enables author to regulate the copying, using and adapting of the program • Must specify conditions of use (license). • Several standard licenses are possible Categories of License • Exclusive rights • Standard licenses ◦ The GNU General Public License ◦ The BSD license ◦ The Artistic License"}
{"pack_id": "english_pack", "section_title": "Appendix D: Software licensing", "page": 488, "chunk_id": "english_pack_488_0_b5ee7537", "content": "470 | APPENDIX D: SOFTWARE LICENSING ◦ Public domain • Writing your own software license Exclusive rights • Author has the right to restrict copying, modification & distribution • Author may authorise others to distribute software, with or without charge. • The conditions of this authorisation is specified in the License. • License can be specific for one particular recipient (this is often the case with custom-made applications), but it can also be written in a generic way, like with most of the standard software sold in stores or available for download on the Internet. • If Author desires to sell their software, a carefully drafted license is essential. • If author desires to give their software freely, a License is still required to specify the conditions under which users may execute and distribute the software. • Two options; use a standard license, or write their own. Standard License • GNU General Public License, BSD license, Artistic License, Public domain."}
{"pack_id": "english_pack", "section_title": "Appendix D: Software licensing", "page": 489, "chunk_id": "english_pack_489_0_25ad5ede", "content": "APPENDIX D: SOFTWARE LICENSING | 471 • Many programs are distributed under standard licenses. • Advantage is everybody knows what is and is not permitted • After wide usage, the wording is likely to be accurate and legally binding. • Saves time and effort in producing own License. • May not be exactly what Author wants though. • It also saves the author from having to spend time and effort in • Most standard licenses allow 3rd parties to sell. GNU General Public License • The original and best known • Software may be used by anyone for commercial and non-commercial purposes • May be redistributed without restrictions • Conditional on including the source code • GPL restricts the creation of derived works (permitted but only if the derived work is also licensed under GPL) • This prevents GPL-licensed software from being transformed into a proprietary (Exclusive Rights) product (with secret source code). • Examples include Linux, MySQL, GCC compiler, EMACS editor and hundreds of other programs."}
{"pack_id": "english_pack", "section_title": "Appendix D: Software licensing", "page": 490, "chunk_id": "english_pack_490_0_e8a82246", "content": "472 | APPENDIX D: SOFTWARE LICENSING BSD License • The BSD license is very simple, therefore short. • Only condition is that people must mention the name of the Author if they incorporate the software in their own programs. • All other use and redistribution is permitted (including for commercial purposes) • Popular among Author’s who desire to have their work used by the largest number of people, and who do not object to others making money from their software • The operating systems FreeBSD and OpenBSD and the web server Apache are notable examples • A comparable license is the MIT license in which the user indemnifies the Author from any liability arising from the use Artistic License • Software may be used and redistributed without further restrictions. • Modifying the software is also permitted, but modified versions may only be redistributed if the modifications are freely available to all. • It is not permitted to sell software covered by the Artistic License (which is permitted by most other standard licenses)"}
{"pack_id": "english_pack", "section_title": "Appendix D: Software licensing", "page": 491, "chunk_id": "english_pack_491_0_87ecee30", "content": "APPENDIX D: SOFTWARE LICENSING | 473 • Notable example is the Perl interpreter (with which Perl scripts are executed) Public Domain • Strictly speaking not a license • Means that there is no copyright on the software whatsoever • Without restriction, the software can be used, distributed, modified and distributed by anyone, anywhere, anytime. • No restrictions on commercial expectation of public domain software • Not required to identify the original author • Author yields all rights and does not restrict anything third parties do with the software. • Author is not liable for damages. Writing your own software license • Author may write own License (or have lawyer draft) • It is notoriously difficult to anticipate all possible circumstances under which people will want to use and/ or distribute the program. For example; ◦ May someone put the program on a CD-ROM and sell that? ◦ Does it matter whether the CD-ROM contains a"}
{"pack_id": "english_pack", "section_title": "Appendix D: Software licensing", "page": 492, "chunk_id": "english_pack_492_0_8dffd9b0", "content": "474 | APPENDIX D: SOFTWARE LICENSING collection of software or only that program? ◦ May modified versions bear the same name? • Even if an Author has determined the conditions under which they are making the software available, it is very difficult to properly express these conditions in legally binding terms. ◦ A license such as “Permission is hereby granted to use this program in any way and for any purpose, to modify it and to distribute it” does not authorize third parties to distribute modified versions, although this was probably the intent of the author. • Unless Author has a compelling reason to do so and is prepared to engage a lawyer to write the License, it is recommended to use a standard license. Sources: IUS Mentis: Law & Technology Explained, and Karl Fogel (see Bibliography for details)"}
{"pack_id": "english_pack", "section_title": "Appendix E: Propaganda techniques", "page": 493, "chunk_id": "english_pack_493_0_67695559", "content": "APPENDIX E: PROPAGANDA TECHNIQUES | 475 APPENDIX E: PROPAGANDA TECHNIQUES Appendix E: Propaganda techniques People are inherently social creatures, and which makes us suggestible. Being open to the influence of others lies at the very heart of what it is to be human. Even for those among us who identify as introverts and claim not to need people, it is still a question of degree as to how suggestible we are. An essential skill of the ethical technologist, and indeed the self-aware citizen of the world, is to recognise when attempts are being made to manipulate us by playing on our emotions. This appendix outlines the seven principal ways that you can recognise an attempt to manipulate or brainwash you. The advertising industry and politicians have been using them for a long time. If you know what they are, you can avoid being unduly influenced. Forewarned is forearmed. If you know what propaganda looks and sounds like and how to deal rationally with it, you can then make an informed decision about whether to go along with it."}
{"pack_id": "english_pack", "section_title": "Appendix E: Propaganda techniques", "page": 494, "chunk_id": "english_pack_494_0_626aac00", "content": "476 | APPENDIX E: PROPAGANDA TECHNIQUES The techniques outlined here were developed by the Institute of Propaganda Analysis (IPS), a U.S.-based organization set up in 1937 by Mather, Filene and Miller. These techniques are as true today as they were then and will still be true in the future. Why? Because they are rooted in human social psychology, and we do not evolve very quickly. We are innately susceptible to these techniques. It is not culturally defined. The techniques include the following: • Name Calling • Glittering Generalities • Transfer • Testimonial • Plain Folks • Card Stacking • Band Wagon The techniques have one thing in common; they are designed to appeal to our emotions rather than to reason. Neuroscience tells us that our emotions are an aspect of our primitive brain, the part we have in common with animals. Reason is a function of our evolved brain, the part that developed in more recent times. A normally reasonable person becomes unreasonable when their emotions are inflamed. The best defence against any of these techniques is to stay rational and gather enough information from independent"}
{"pack_id": "english_pack", "section_title": "Appendix E: Propaganda techniques", "page": 495, "chunk_id": "english_pack_495_0_7b7e3fa7", "content": "APPENDIX E: PROPAGANDA TECHNIQUES | 477 sources to make an informed decision. The IPS suggests specific defences for each technique, as seen in italics below. Name calling Name calling is declaring something is bad without any real evidence. No further discussion or investigation is required. The matter is closed. The bad name is the conclusion that we should all accept and start repeating. It works to create fear and loathing towards the target, and it can be applied against individuals, groups, belief systems, religions, institutions and nations. Name calling is a substitute for a reasoned weighing up of the merits of something. It is characterised by a tone of scorn, sarcasm and ridicule. The best way to deal with this technique is to calmly ask yourself what does the name really mean? Is there a real connection between the idea and the name being used? Does the idea have merit if the name is left out? Glittering Generalities Propagandists as well as charismatic leaders are adept at using catchphrases that connect with deeply held values and beliefs in the audience. Little or no supporting evidence is given. Glittering generalities appeal to abstract ideas like honour, glory, love of country, desire for peace, security, freedom and family values. The words used are vague enough to mean what"}
{"pack_id": "english_pack", "section_title": "Appendix E: Propaganda techniques", "page": 496, "chunk_id": "english_pack_496_0_4e1ad9c0", "content": "478 | APPENDIX E: PROPAGANDA TECHNIQUES people want them to mean, but the implication is always favourable. No-one can prove it wrong because it says very little in concrete terms. The best defence is to calmly ask yourself what does the slogan or catch-phrase mean? Is there a real connection between the idea and the slogan being used? Does the idea have merit if the slogan is left out? Transfer The Transfer technique tries to extend the authority and approval of something or someone we respect to something the propagandist would have us believe. Symbols play an important role with this technique, for example flag waving or idealised images of womanhood or manhood, anything that can stir the emotions and win our approval. The best defence is to calmly ask yourself what exactly is the speaker trying to pitch? What does it mean? Is there a legitimate connection between the suggestion and the person or product? Does the proposal have any merit by itself? Try to do this independently of the convictions you already have about other persons or ideas. Testimonial Testimonials are an implementation of the Transfer technique, but where the respected or authoritative person themselves"}
{"pack_id": "english_pack", "section_title": "Appendix E: Propaganda techniques", "page": 497, "chunk_id": "english_pack_497_0_b6797cca", "content": "APPENDIX E: PROPAGANDA TECHNIQUES | 479 seeks to make the transfer rather than a third person. The respected person gives something their stamp of approval, essentially making an ethical appeal based on their authority as an expert to encourage the audience to follow their example. The best defence is to calmly ask yourself who exactly is this authority figure? Is there a good reason we should believe they are qualified to make this recommendation? Is there any merit to what is being proposed? The technique falls apart if you can see that the person is not actually an authority but somebody with a secret agenda, or show that other experts disagree with them. Plain Folks The Plain Folks technique presents a spokesperson from humble origins, a simple, decent, good-natured person who has our best interests at heart. The spokesperson uses common, everyday speech and mannerisms to get people to identify with them and so accept their point of view. The best defence is to calmly ask yourself is this person believable and trustworthy when removed from the situation being discussed? Are they trying to cover up anything? What are the facts of the situation? Try to consider the ideas contained in the proposal separately from the personality of the presenter."}
{"pack_id": "english_pack", "section_title": "Appendix E: Propaganda techniques", "page": 498, "chunk_id": "english_pack_498_0_e51fe5fc", "content": "480 | APPENDIX E: PROPAGANDA TECHNIQUES Bandwagon The Bandwagon technique aims to get you to follow the crowd. It creates the impression that something has widespread support, and plays on the instinct to be on the winning side. It also plays on feelings of loneliness or social isolation if these are present. The message simultaneously encourages those not on the bandwagon to join, and for those already on the bandwagon to stay on board. A variation is to say if you do not join now, you will be left behind, abandoned. Either way, a person is encouraged to get with the strength on-board the bandwagon. The best defence is to calmly ask yourself what exactly is the propagandist’s program? What are the pro’s and con’s of the program? Others might be supporting it, but is there a good reason for me to do so? Card Stacking In the Card Stacking technique, the propagandist cherry-picks only those facts that strongly support their case, while presenting their opponent’s case in the worst possible light. It is dishonest because we are being urged to accept a carefully selected sub-set of the truth as the whole truth. The technique is difficult to detect because it does not present all the information necessary to make an informed decision, while implying that this is the whole truth."}
{"pack_id": "english_pack", "section_title": "Appendix E: Propaganda techniques", "page": 499, "chunk_id": "english_pack_499_0_1bde8a1a", "content": "APPENDIX E: PROPAGANDA TECHNIQUES | 481 The best defence is to calmly ask yourself are the facts being distorted, or are they missing altogether? Does anyone else independently support this point of view? The ethical technologist Propaganda techniques like these have been successfully used in organisations to shape culture and instil values. Most of the time, there is nothing unethical about this. Sometimes though, the corporate culture is about profit above scruples. If you find yourself in such an environment, at the very least you should be aware of what kind of people you are associating with and becoming like. You would be well-advised to leave gracefully and as soon as possible."}
{"pack_id": "english_pack", "section_title": "Appendix F: Military technology", "page": 500, "chunk_id": "english_pack_500_0_5c678dc9", "content": "482 | APPENDIX F: MILITARY TECHNOLOGY APPENDIX F: MILITARY TECHNOLOGY Appendix F: Military technology No discussion is complete without mention of the dilemma of military technology. How can we reconcile the existence, if not the need for military technology in the world when its primary purpose appears to be to dehumanise, often in the most extreme ways? Kill-bots This truly is a dilemma that remains unresolved. This appendix tries to unravel the issues so that we may see them more clearly. The military ethicist Peter W. Singer of the Brookings Institute (not to be confused with Peter A. Singer who is a professor of Bioethics at Princeton University) concluded in a 2010 article in the Journal of Military Ethics that in a world of ‘killer apps’, robotic weapons that can function autonomously, it is necessary to open up a constructive dialogue on how to deal with the moral dilemmas created by this new category of weaponry."}
{"pack_id": "english_pack", "section_title": "Appendix F: Military technology", "page": 501, "chunk_id": "english_pack_501_0_d4d0b880", "content": "APPENDIX F: MILITARY TECHNOLOGY | 483 Peter W. Singer notes that throughout history, certain technological advances have been ‘game-changers’. For example the printing press, gunpowder, the steam engine, or the atomic bomb. Not only are the current military technologies game-changers, they are part of a cresting wave of advances that are coming at us thick and fast. These include directed energy weapons (Lasers), precision guided weapons (‘smart’ IEDs), nanotech and microbotics (The Diamond Age), bioagents and genetic weaponry (DNA bombs), chemical and hardware enhancements to the human body (IronMan meets Captain America), autonomous armed robots (Terminators), electromagnetic pulse weaponry (The Day After, Ocean’s 11), and space weaponry (Star Wars). These may seem to be the stuff of science-fiction, but all of them are currently in development and are likely to be deployed in active service around 2030 or sooner. History clearly shows us that many of the technologies that we use and depend on in everyday life have their origins as military technology that has become de-classified and then commercialised. Indeed, the modern phenomenon of computer technology owes much to trying to win World War II. For example, in the U.S. the ENIAC machines were developed to help the US Army with artillery aiming by quickly calculating ballistic trajectories. In Germany, Konrad Zuse and his Z series computers were helping the German war- effort in no small way. In Britain, the Colossus computer was developed to decode the German Enigma cipher that allowed"}
{"pack_id": "english_pack", "section_title": "Appendix F: Military technology", "page": 502, "chunk_id": "english_pack_502_0_d3d45b7b", "content": "484 | APPENDIX F: MILITARY TECHNOLOGY the allies to know where to find and destroy the U-boats that were taking such a toll on the supply convoy ships carrying materials across the Atlantic from the US to Britain. These were truly breakthrough, game-changing technologies. Highly secret at the time, in the 1950’s and beyond, much of this computer technology was later commercialised, leading st to the world as we know it now in the 21 Century. Indeed, wars and conflict throughout human history have been responsible for rapid advances in technology. It is a little- known fact that Leonardo da Vinci, known for his love of humanity, not to mention his art and science, was also a well- paid military engineer whose inventions helped the wealthy city states of Renaissance Italy defend themselves against plunderers. Us and them The tendency for one group of people to go to war with another group is deeply ingrained in human nature, as evolutionary psychology recognises. As a species, humans evolved in cooperative groups (extended families). Loyalty to the group was essential for survival because the scarcity of resources meant that one group would often get what it needed at the expense of another group, inevitably leading to conflict. We have all heard of the term ‘us and them’ and instinctively understand the concept of in-groups and out- groups."}
{"pack_id": "english_pack", "section_title": "Appendix F: Military technology", "page": 503, "chunk_id": "english_pack_503_0_821fe922", "content": "APPENDIX F: MILITARY TECHNOLOGY | 485 Notwithstanding these evolutionary factors, it can be strongly argued that people today need to be able to transcend these ancient patterns of behaviour, these instincts, by using our more recently evolved rational minds. Much of this book focuses on just this point. Realistically, instincts can never be gotten rid of or repressed; they can only be transcended or over-ridden by logic. One strategy is to transcend the ‘us and them’ mind-set that makes us see ‘them’ as sub-human and so be able to kill them in good conscience, with the more enlightened attitude that ‘us and them’ in the modern world is an illusion. We are all one species, all essentially the same under the skin, all of us members of the one big human family. If we widen our ‘circle of care’ as the other Peter Singer (from Princeton) suggests, from our immediate family to include an ever-widening circle of people in the world, then we will naturally come to act more compassionately towards everyone, not just our immediate family. Another strategy is put forward by Robert Wright in his 2001 book Nonzero: the logic of human destiny. He makes the compelling point that we are less likely to want to go to war against someone if we have an economic connection with them, such that by harming them, we harm ourselves. It does not make sense to hurt our own interests. The global economy st in the 21 century is a single interconnected entity. We can no longer act in isolation. The consequences of our actions are transmitted everywhere."}
{"pack_id": "english_pack", "section_title": "Appendix F: Military technology", "page": 504, "chunk_id": "english_pack_504_0_c426a9a5", "content": "486 | APPENDIX F: MILITARY TECHNOLOGY Wright quotes Charles Darwin to good effect: ‘As man advances in civilization, and small tribes are united into larger communities, the simplest reason would tell everyone that he ought to extend his social instincts and sympathies to all members of the same nation, though personally unknown to him. This point being once reached, there is only an artificial barrier to prevent his sympathies extending to the men of all nations and races.’ ― Charles Darwin, The Descent of Man. Wright’s and Darwin’s perspectives can help us to transcend the ‘us and them’ mentality that has kept humanity perpetually at war throughout our blood-stained history. It will take a long time for the world to change because there is immense inertia built-up in the system, but the observable trend suggests that the change will come in time. Perhaps not soon enough for some but come it will. In practical terms, where does this leave us now? We currently live in a world where war is still a reality. There are bad actors who would go on the offensive unless their intended victims are well-defended. If nations are going to safeguard their interests, there will be a continuing, though hopefully lessening need for military technology to support this imperative. Seven questions Professor Singer suggests these seven questions to help a technologist decide what an ethical course of action is:"}
{"pack_id": "english_pack", "section_title": "Appendix F: Military technology", "page": 505, "chunk_id": "english_pack_505_0_5d6b2bee", "content": "APPENDIX F: MILITARY TECHNOLOGY | 487 1. From whom is it ethical to take research and development money? From whom should one refuse to accept funding? 2. What attributes should one design into a new technology, such as its weaponization, autonomy or intelligence? What attributes should be limited or avoided? 3. What organizations and individuals should be allowed to buy and use the technology? Who should not? 4. What type of training or licensing should the users have? 5. When someone is harmed because of the technology’s actions, who is responsible? How is this determined? 6. Who should own the wealth of information the technology gathers about the world around them? Who should not? As a general principle, the abuse of something should not in itself prohibit the use of it. The potential for people to abuse something should not prevent it from being used in non- harmful ways. Motor cars and drugs would be two examples out of many. Military technology would be another. If the net good outweighs the net harm, a compelling argument exists to use it. Where to draw the line is often unclear. Each case must be considered individually and on its merits."}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 506, "chunk_id": "english_pack_506_0_3dd39a2e", "content": "488 | APPENDIX G: SELECT BIBLIOGRAPHIES APPENDIX G: SELECT BIBLIOGRAPHIES Appendix G: Select Bibliographies Select Bibliography from Carnegie Mellon University URLs are valid as of the publication date of this document. Alberts, Chris; Dorofee, Audrey; Killcrece, Georgia; Ruefle, Robin; & Zajicek, Mark. Defining Incident Management Processes for CSIRTs: A Work in Progress. CMU/ SEI-2004-TR-015 ADA453378. Software Engineering Institute, Carnegie Mellon University. 2004. https://resources.sei.cmu.edu/library/asset- view.cfm?assetid=7153 Alberts, Chris; Dorofee, Audrey; Ruefle, Robin; & Zajicek, Mark. An Introduction to the Mission Risk Diagnostic for Incident Management Capabilities (MRD-IMC). CMU/ SEI-2013-TN-015. Software Engineering Institute, Carnegie Mellon"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 507, "chunk_id": "english_pack_507_0_82a68bfc", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 489 University. 2013. https://resources.sei.cmu.edu/library/asset- view.cfm?assetid=91452 Barker, William C. Guideline for Identifying an Information System as a National Security System (NIST Special Publication 800-59). 2003. https://doi.org/10.6028/ NIST.SP.800-59 Cichonski, Paul; Millar, Tom; Grance, Tim; & Scarfone, Karen. Computer Security Incident Handling Guide (NIST Special Publication 800-61, Rev 2). 2012.https://csrc.nist.gov/ pubs/sp/800/61/r2/final Dempsey, Kelley; Sha Chawlaa, Nirali; Johnson, Arnold; Johnston, Ronald; Clay Jones, Alicia; Orebaugh, Angela; Scholl, Matthew; & Stine, Kevin. Information Security Continuous Monitoring (ISCM) for Federal Information Systems and Organizations (NIST Special Publication 800-137). 2010. https://csrc.nist.gov/pubs/sp/800/137/final Department of Homeland Security. DHS Federal Continuity Directive 1: Federal Executive Branch National Continuity Program and Requirements: Annex C. 2008. http://www.fema.gov/pdf/about/org/ncp/fcd1.pdf Department of Homeland Security. DHS Federal Continuity Directive 2: Federal Executive Branch Mission Essential Function and Primary Mission Essential Function Identification and Submission Process. 2008. Department of Homeland Security. A Roadmap for Cybersecurity Research. 2009. Department of Homeland Security. Department of"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 508, "chunk_id": "english_pack_508_0_a1a8231b", "content": "490 | APPENDIX G: SELECT BIBLIOGRAPHIES Homeland Security Federal Network Security Branch. Continuous Asset Evaluation, Situational Awareness, and Risk Scoring Reference Architecture Report (CAESARS). 2010. Department of Homeland Security. Cybersecurity Capability Validation (CCV) Assessment Method and Process Guidance Version 1.1. U.S Department of Homeland Security. 2012. Department of Homeland Security. IT Program Assessment: Department of Homeland Security (DHS) Analysis and Operations (A&O) Common Operating Picture (COP). U.S. Department of Homeland Security. 2012. http://www.dhs.gov/xlibrary/assets/mgmt/itpa-ao- cop2012.pdf Dorofee, Audrey; Killcrece, Georgia; Ruefle, Robin; & Zajicek, Mark. Incident Management Capability Metrics, Version 0.1. CMU/SEI-2007-TR-008 ADA468688. Software Engineering Institute, Carnegie Mellon University. 2007. ENISA. CSIRT A Step-by-Step Approach on How to Set Up a CSIRT. 2006. ENISA. CSIRT Good Practice Guide for Incident Management. 2010. https://www.enisa.europa.eu/ publications/good-practice-guide-for-incident-management Federal Financial Institutions Examination Council (FFIEC). IT Examination Handbook InfoBase. 2006. http://ithandbook.ffiec.gov/ Grance, Tim; Nolan, Tamara; Burke, Kristin; & Good,"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 509, "chunk_id": "english_pack_509_0_81237327", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 491 Travis. Guide to Test, Training, and Exercise Programs for IT Plans and Capabilities (NIST Special Publication 800-84). 2006. http://csrc.nist.gov/publications/nistpubs/800-84/ SP800-84.pdf Hash, Joan; Bartol, Nadya; Rollins, Holly; Robinson, Will; Abeles, John; & Batdorff, Steve. Integrating IT Security into the Capital Planning and Investment Control Process (NIST Special Publication 800-65). 2005. International Information Systems Security Certification Consortium (ISC)2. Certified Information Systems Security Professional (CISSP) Common Body of Knowledge (CBK). 2007. Information Security Forum. The Standard of Good Practice for Information Security. 2012. IT Governance Institute. Control Objectives for Information and related Technology (COBIT) 5. 2012. http://www.isaca.org/cobit Johnson, Arnold; Dempsey, Kelley; Ross, Ron; Gupta, Sarbari; & Bailey, Dennis. Guide for Security-Focused Configuration Management of Information Systems (NIST Special Publication 800-128). 2011. http://csrc.nist.gov/ publications/nistpubs/800-128/sp800-128.pdf Kent, Karen & Souppaya, Murugiah. Guide to Computer Security Log Management (NIST Special Publication 800-92). 2006. http://csrc.nist.gov/publications/nistpubs/ 800-92/SP800-92.pdf Kent, Karen; Chevalier, Suzanne; Grance, Tim; & Dang,"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 510, "chunk_id": "english_pack_510_0_eec67b99", "content": "492 | APPENDIX G: SELECT BIBLIOGRAPHIES Hung. Guide to Integrating Forensic Techniques into Incident Response (NIST Special Publication 800-86). 2006. http://csrc.nist.gov/publications/nistpubs/800-86/ SP800-86.pdf Killcrece, Georgia; Kossakowski, Klaus-Peter; Ruefle, Robin; & Zajicek, Mark. State of the Practice of Computer Security Incident Response Teams (CSIRTs). CMU/ SEI-2003-TR-001, ADA421664. Software Engineering Institute, Carnegie Mellon University. 2003. https://resources.sei.cmu.edu/library/asset- view.cfm?assetid=6571 Killcrece, Georgia; Kossakowski, Klaus-Peter; Ruefle, Robin; & Zajicek, Mark. Organizational Models for Computer Security Incident Response Teams (CSIRTs). CMU/SEI-2003-HB-001, ADA421684. Software Engineering Institute, Carnegie Mellon University. 2003. https://resources.sei.cmu.edu/library/asset- view.cfm?assetid=6295 Killcrece, Georgia; Kossakowski, Klaus-Peter; Ruefle, Robin; & Zajicek, Mark. CSIRT Services. Software Engineering Institute, Carnegie Mellon University. 2002. https://resources.sei.cmu.edu/library/asset- view.cfm?assetid=53046 Mell, Peter; Waltermire, David; Feldman, Larry; Booth, Harold; Ragland, Zach; Ouyang, Alfred; & McBride, Timothy. CAESARS Framework Extension: An Enterprise Continuous Monitoring Technical Reference Architecture"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 511, "chunk_id": "english_pack_511_0_9eb52178", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 493 (Second Draft). 2012. http://csrc.nist.gov/publications/ drafts/nistir-7756/Draft-NISTIR-7756_second-public- draft.pdf Mell, Peter; Bergeron, Tiffany; & Henning, David. Creating a Patch and Vulnerability Management Program (NIST Special Publication 800-40, Version 2.0). 2005. http://csrc.nist.gov/publications/nistpubs/800-40-Ver2/ SP800-40v2.pdf Mell, Peter; Kent, Karen; & Nusbaum, Joseph. Guide to Malware Incident Prevention and Handling (NIST Special Publication 800-83). 2005. http://csrc.nist.gov/publications/ nistpubs/800-83/SP800-83.pdf The National Archives and Records Administration. General Records Schedule 24—Information Technology Operations and Management Records. 2010. https://www.archives.gov/files/records-mgmt/grs/grs24.pdf National Institute of Standards and Technology. Standards for Security Categorization of Federal Information and Information Systems (FIPS PUB 199). 2004. http://csrc.nist.gov/publications/fips/fips199/FIPS- PUB-199-final.pdf National Institute of Standards and Technology. Minimum Security Requirements for Federal Information and Information Systems (FIPS PUB 200). 2006. http://csrc.nist.gov/publications/fips/fips200/FIPS-200-final- march.pdf National Institute of Standards and Technology, Joint Task"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 512, "chunk_id": "english_pack_512_0_555ece7e", "content": "494 | APPENDIX G: SELECT BIBLIOGRAPHIES Force Transformation Initiative. Recommended Security Controls for Federal Information Systems and Organizations (NIST Special Publication 800-53, Rev 3). 2009. http://csrc.nist.gov/publications/nistpubs/800-53-Rev3/ sp800-53-rev3-final_updated-errata_05-01-2010.pdf National Institute of Standards and Technology. NIST Special Publications, 800 Series. 2009. http://csrc.nist.gov/ publications/PubsSPs.html National Institute of Standards and Technology, Joint Task Force Transformation Initiative. Guide for Applying the Risk Management Framework to Federal Information Systems: A Security Lifecycle Approach (NIST Special Publication 800-37 Rev 1). 2010. http://csrc.nist.gov/publications/ nistpubs/800-37-rev1/sp800-37-rev1-final.pdf National Institute of Standards and Technology, Joint Task Force Transformation Initiative. Guide for Assessing the Security Controls in Federal Information Systems (NIST Special Publication 800-53A Rev 1). 2010. http://csrc.nist.gov/publications/nistpubs/800-53A-rev1/ sp800-53A-rev1-final.pdf National Institute of Standards and Technology, Joint Task Force Transformation Initiative. Managing Information Security Risk: Organization, Mission, and Information System View (NIST Special Publication 800-39). 2011. http://csrc.nist.gov/publications/nistpubs/800-39/ SP800-39-final.pdf National Institute of Standards and Technology. Computer"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 513, "chunk_id": "english_pack_513_0_4fcedb53", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 495 Security Incident Handling Guide (Draft) (NIST Special Publication 800-61 Rev 2 DRAFT). 2012. https://nvlpubs.nist.gov/nistpubs/SpecialPublications/ NIST.SP.800-61r2.pdf National Institute of Standards and Technology. Security and Privacy Controls for Federal Information Systems and Organizations (NIST Special Publication 800-53 Rev 4). 2013. https://nvlpubs.nist.gov/nistpubs/SpecialPublications/ NIST.SP.800-53r4.pdf Network Working Group. Expectations for Computer Security Incident Response. 1998. http://www.ietf.org/rfc/ rfc2350.txt Office of Management and Budget. Safeguarding Against and Responding to the Breach of Personally Identifiable Information (memorandum). 2007. https://www.whitehouse.gov/sites/whitehouse.gov/files/ omb/memoranda/2007/m07-16.pdf Reid, Gavin & Schieber, Dustin. CSIRT Case Classification (Example for Enterprise CSIRT). 2004. https://www.first.org/resources/guides/#CSIRT-Case- Classification-Example-for-enterprise-CSIRT Scarfone, Karen & Mell, Peter. Guide to Intrusion Detection and Prevention Systems (IDPS) (NIST Special Publication 800-94). 2007. http://csrc.nist.gov/publications/ nistpubs/800-94/SP800-94.pdf Scarfone, Karen; Souppaya, Murugiah; Cody, Amanda; & Orebaugh, Angela. Technical Guide to Information Security"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 514, "chunk_id": "english_pack_514_0_fb8cde9b", "content": "496 | APPENDIX G: SELECT BIBLIOGRAPHIES Testing and Assessment (NIST Special Publication 800-115). 2008. http://csrc.nist.gov/publications/nistpubs/800-115/ SP800-115.pdf Scarfone, Karen & Hoffman, Paul. Guidelines on Firewalls and Firewall Policy (NIST Special Publication 800-41, Rev 1). 2009. https://csrc.nist.gov/publications/detail/sp/800-41/ rev-1/final Stine, Kevin; Kissel, Rich; Barker, William C.; Fahlsing, Jim; & Gulick, Jessica. Volume I: Guide for Mapping Types of Information and Information Systems to Security Categories (NIST Special Publication 800-60 Rev 1). 2008. http://csrc.nist.gov/publications/nistpubs/800-60-rev1/ SP800-60_Vol1-Rev1.pdf Swanson, Marianne & Guttman, Barbara. Generally Accepted Principles and Practices for Securing Information Technology Systems (NIST Special Publication 800-14). 1996. http://csrc.nist.gov/publications/nistpubs/800-14/ 800-14.pdf Swanson, Marianne; Hash, Joan; & Bowen, Pauline. Guide for Developing Security Plans for Federal Information Systems (NIST Special Publication 800-18, Rev 1). 2006. http://csrc.nist.gov/publications/nistpubs/800-18-Rev1/ sp800-18-Rev1-final.pdf Swanson, Marianne; Bowen, Pauline; Wohl Phillips, Amy; Gallup, Dean; & Lynes, David. Contingency Planning Guide for Federal Information Systems (NIST Special Publication"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 515, "chunk_id": "english_pack_515_0_4de26599", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 497 800-34, Rev 1). 2010. http://csrc.nist.gov/publications/ nistpubs/800-34-rev1/sp800-34-rev1_errata-Nov11-2010.pdf Tracy, Miles; Jansen, Wayne; Scarfone, Karen; & Butterfield, Jason. Guidelines on Electronic Mail Security (NIST Special Publication 800-45 Version 2). 2007. http://csrc.nist.gov/ publications/nistpubs/800-45-version2/SP800-45v2.pdf[ West-Brown, Moira J.; Stikvoort, Don; Kossakowski, Klaus- Peter; Killcrece, Georgia; Ruefle, Robin; & Zajicek, Mark. Handbook for Computer Security Incident Response Teams (CSIRTs) (CMU/SEI-2003-HB-002, ADA413778). Software Engineering Institute, Carnegie Mellon University. 2003. https://resources.sei.cmu.edu/library/asset- view.cfm?assetid=6305 Wilson, Mark & Hash, Joan. Building an Information Technology Security Awareness and Training Program (NIST Special Publication 800-50). 2003. http://csrc.nist.gov/ publications/nistpubs/800-50/NIST-SP800-50.pdf Select Bibliography from Cambridge University Accenture Security. 2017 Cyber Threatscape Report: Midyear Cybersecurity Risk Review-Forecast and Remediation’s. Accenture Security, 2017. Advisen. Information Security and Cyber Risk Management. Seventh Annual Survey, 2017. Akamai. State of the internet/security: Q2 2017 Report."}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 516, "chunk_id": "english_pack_516_0_be4ff8fa", "content": "498 | APPENDIX G: SELECT BIBLIOGRAPHIES Allianz. A Guide to Cyber Risk. Allianz Global Corporate & Specialty White Paper, 2017. Amazon (1). “Amazon Simple Storage Service (S3) — Cloud Storage — AWS”. Amazon Web Services, Inc. 2018. Amazon (2). “Amazon EC2”. Aon. Global Cyber Market Overview, June 2017. BAE. “When cyber attacks meet financial crime”. Barth, Bradley. “DDoS attacks delay trains, halt transportation services in Sweden”. SC Magazine. October 16, 2017. BBC (1). “Qatar Crisis: What you need to know.” July 19, 2017. BBC (2). “Theresa May accuses Vladimir Putin of election meddling.” November 14, 2017. BBC (3). “NHS cyber- defender Marcus Hutchins to appear in U.S. court.” August 4, 2017. BBC (4). “Dark web markets boom after AlphaBay and Hansa Busts”. August 1, 2017. BBC (5). “South Korean firm’s ‘record’ ransom payment”, June 20, 2017. Berr, Jonathan. “’WannaCry’ ransomware attack losses could reach $4 billion”. CBS Moneywatch. May 16, 2017. Beazley (1). “Ransomware attacks steal headlines, but accidental data breaches remain a major cause of loss”. August 1, 2017. Beazley (2). “Technology, Media & Business Services First Party Computer Claims”. Blodget, Henry. “Amazon’s Cloud Crash Disaster Permanently Destroyed Many Customers’ Data”. Business Insider. April 28, 2011. Boey, Darren. “North Korean Hacker"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 517, "chunk_id": "english_pack_517_0_413a9a04", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 499 Group Linked to Taiwan Bank Cyber Heist.” Bloomberg Technology. October 17, 2017. Brook, Chris. “DDOS Attacks Can Cost Businesses up to $2.5 Million per Attack, Report Says”. Threat Post. May 2, 2017. Burgess, M. “What is GDPR? WIRED explains what you need to know”. Wired, January 2, 2018. Chappell, Bill. “’Petya’ Ransomware Hits at Least 65 Countries; Microsoft Trace it to Tax Software.” NPR. June 28, 2018. Cimpanu, Catalin. “95% of All Ransomware Payments were Cashed out via BTC-e Platform”. Bleeping Computer. July 27, 2017. Coles, Cameron. “Overview of Cloud Market in 2017 And Beyond”. Skyhigh. Comptroller and Auditor General. Investigation: WannaCry cyber attack and the NHS. National Audit Office. Department of Health. October 27, 2017. Council of Europe. International Co-operation under the Convention on Cybercrime. Project on Cybercrime. 18 August, 2017. Cybereason. Paying the Price of Destructive Cyber Attacks. Whitepaper, 2017. EMVco. “EMVCo Reports 6.1 Billion EMV Chip Payment Cards in Global Circulation”. June 5, 2017. European Commission. “Protection of personal data”. Europa, 2017. Europol. Internet Organised Crime Threat Assessment. 2017. Field, Tom. “The Blurred Lines Between Criminals and Nation-States”. Bank Info Security. June 19, 2017. Forester,"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 518, "chunk_id": "english_pack_518_0_2fd16278", "content": "500 | APPENDIX G: SELECT BIBLIOGRAPHIES Conner. “NotPetya ransomware outbreak cost Merck more than $300M per quarter”. Tech Republic. Finkle, Jim (1). “Your medical record is worth more to hackers than your credit card”. Reuters. September 24, 2014. Finkle, Jim.(2) “Cybersecurity Firm: North Korea Was Likely Behind Cyber Heist In Taiwan”. Business Insider. October 16, 2017. Gabel, Detlev and Hickman, Tim. K. Key definitions- Unlocking the EU General Data Protection Regulation. Whitecase publications, September 2017. Gammons, Brianna. “6 Must-Know Cybersecurity Statistics for 2017”. Barkly (Blog), January, 2017. Gartner (1). “Gartner Says Worldwide Public Cloud Services Market to Grow 18% in 2017”. 2017. Gartner (2). “Gartner Says Worldwide Information Security Spending Will Grow 7 Percent to Reach $86.4 Billion in 2017”. August 16, 2017. Gerstein, Josh. “Alleged leaker Reality Winner said she stuffed NSA report in her pantyhose”. Politico. September 27, 2017. Gibbs, Samuel (1). “Shadow Brokers threaten to unleash more hacking tools”. The Guardian. May 17, 2017. Gibbs, Samuel (2). “Game of Thrones: HBO hackers threaten leak of season finale”. The Guardian. August 21, 2017. Gogan, Marcell. “Insider Threat as the Main Security Threat in 2017”. TRIPWIRE. April 11, 2017. Google (1). “Google Cloud Computing, Hosting Services"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 519, "chunk_id": "english_pack_519_0_4500b85d", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 501 & Apis”. Google Cloud Platform. Google (2). “Cloud Locations”. Graham, Chris. “NHS cyber attack: Everything you need to know about ‘biggest ransomware’ offensive in history”. The Telegraph. May 20, 2017. Gray, Alistair. “U.S. banks to introduce new anti-fraud measures after Equifax Hack”. Financial Times. October 15, 2017. Greenberg, Andy (1). “How An Entire Nation Became Russia’s Test Lab for Cyberwar.” Wired. June 19, 2017. Greenberg, Andy (2). “The Biggest Dark Web Takedown Yet Sends Black Markets Reeling”. Wired. July 14, 2017. Greenberg, Andy (3). “No One Wants to Buy Those Stolen NSA-Linked ‘Cyberweapons’”. Wired. August 16, 2016. Greenough, J. “The ‘Internet of Things’ Will Be The World’s Most Massive Device Market And Save Companies Billions Of Dollars”. Business Insider. November 18, 2014. IBM. “IBM Blue Mix”. IBM. IBM X-Force Research. The weaponization of IoT devices: Rise of the thingbots. New York: IBM, 2017. IDC. “Worldwide Spending On Security Technology Forecast To Reach $81.7 Billion In 2017, According To New IDC Spending”. Research Press Release. March 29, 2017. Imperva. Global DDoS Threat Landscape Q1 2017. 2017 Information Commissioner’s Office. Guide to the General Data Protection Regulation. ICO, 2017."}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 520, "chunk_id": "english_pack_520_0_dbcd344a", "content": "502 | APPENDIX G: SELECT BIBLIOGRAPHIES Jolly, Jasper. “Massive hack at Equifax exposes personal records of Brits and 142m Americans”. CITY A.M. September 8, 2017. JLT. “Asia Moves Towards Tougher Data Breach Rules”. December 8, 2017. Johnson, Tim. “Here’s one tally of the losses from WannaCry ransomware attack”. McClatchy. Jun, Kwanwoo and Yousef, Nancy. “North Korea Suspected of Hacking U.S.- South Korean War Plans.” The Wall Street Journal. October 10, 2017. Kan, Michael. “Yahoo uncovered breach after probing a black market sale”. CIO. September 22, 2016. Kar, Ian. “The chip card transition in the U.S. has been a disaster”. Quartz. July 29, 2016. Kaspersky Lab (1). APT Trends Report Q2 2017. SECURELIST, 2017. Kaspersky Lab (2). KSN Report: Ransomware in 2016-2017. Security List,2017. Khalimonenko, Alexander, Oleg Kupreev, and Timur Ibragimov. DDoS attacks in Q2 2017. SecureList DDOS Reports. Khandelwal, Swati. “Hackers Stole $32 Million in Ethereum; 3rd Heist in 20 days”. The Hacker News. July 19. 2017. Kshetri, Nir and The Conversation. “Cryptocurrencies May Be a Dream Come True for Cyber-Extortionists”. Fortune. September 19, 2017. Lin, Adela, and Ondaatjie, Anusha. “Sri Lanka Makes Arrests In $60 Million Taiwanese Bank Cyberheist”."}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 521, "chunk_id": "english_pack_521_0_4cc69186", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 503 Bloomberg. October 12, 2017. Lloyds. Bitcoin: Risk Factors for Insurance. London: Lloyd’s Innovation Series, 2015. Ludwin, Adam. “How Anonymous is Bitcoin? A Backgrounder for Policymakers”. Coindesk. January 25, 2015. Morgan, Steve. “Global ransomware damage costs predicted to exceed $5 billion in 2017, up from $325 million in 2015”. CSO. May 23, 2017. McCrank (1), John. “Equifax says 15.2 million U.K. records exposed in cyber breach”. Reuters. October 10, 2017. McCrank (2), John and Saxena, Aparajita. “Equifax clears executives who sold shares after hack”. Reuters. November 3, 2017. Michael, Casey. “The Kremlin’s California Dream.” Slate. May 4, 2017. Microsoft Azure. “Cloud Locations”. Google Cloud Platform. National Association of Insurance Commissioners. “The National System of State Regulation and Cybersecurity”. December 12, 2017. Nakashima, Ellen. “Prosecutors to seek indictment against former NSA contractor as early as this week”. The Washington Post. February 6, 2017. National Audit Office. Investigation: WannaCry Cyber Attack and the NHS. Report by the Comptroller and Auditor General, Department of Health. HC 414 Session 2017–2019 October 27, 2017. Newton, Casey. “How A Typo Took Down S3, The Backbone Of The Internet”. The Verge. March 2, 2017. Nichols, Shaun. “AWS’s S3 Outage Was So Bad Amazon"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 522, "chunk_id": "english_pack_522_0_93e5c840", "content": "504 | APPENDIX G: SELECT BIBLIOGRAPHIES Couldn’t Get into Its Own Dashboard to Warn The World”. The Register. March 1, 2017 O’Conner, Fred. “NotPetya Still Roils Company’s Finances, Costing Organizations $1.2 Billion In Revenue”. Cybereason. November 9, 2017. Office of the Director of National Intelligence. “Assessing Russian Activities and Intentions in Recent U.S. Elections”. ICA, 2017- 01D. January 6, 2017. Oliphant, Roland and McGoogan, Cara. “NATO warns cyber-attacks ‘could trigger article 5’ as world reels from Ukraine hack.” The Telegraph. June 28, 2017. Paganini, Pierluigi. “Imperva Report Q2 2017- Over 75% Of DDoS Targets Were Hit Multiple Times”. Security Affairs. October 3, 2017. Palmer, Danny. “A massive cyberattack is hitting organizations around the world”. ZD Net. June 27, 2017. Perlroth, Nicole. “All 3 Billion Yahoo Accounts Were Affected by 2013 Attack”. New York Times. October 3, 2017. Popper, Nathaniel and Ruiz, Rebecca. “2 Leading Online Black Markets Are Shut Down by Authorities”. New York Times. July 20, 2017. PYMNTS. “Dark Web Down but Not Out”. August 21, 2017 Rayome, Alison. “33% of businesses hit by DDoS attack in"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 523, "chunk_id": "english_pack_523_0_08e91ee0", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 505 2017, double that of 2016”. Tech Republic. October 11, 2017. Right Scale. 2017. State Of the Cloud Report. Riley, Michael (1), Anita Sharpe and Jordan Robertson. “Equifax Suffered a Hack Almost Five Months Earlier Than the Date It Disclosed”. Bloomberg. September 18, 2017. Riley, Michael (2), Jordan Robertson and Anita Sharpe. “The Equifax Hack Has the Hallmarks of State-Sponsored Pros”. Bloomberg. September 29, 2017. Romanosky, Sasha, Lillian Ablonm Andreas Kuehn and Therese Jones. Content Analysis on Cyber Insurance. RAND Working Paper, September 2017. Shepardson, David. “Equifax failed to patch security vulnerability in March: former CEO”. Reuters. October 2, 2017. Shevchenko, Sergei, Hirman Muhammad bin Abu Bakar, and James Wong. “Taiwan Heist: Lazarus Tools and Ransomware”. BAE Threat Research (Blog). October 16, 2017. Solon, Olivia and Siddiqui, Sabrina. “Russia-backed Facebook posts ‘reached 126m Americans’ during U.S. election.” The Guardian. October 31, 2017. Sputnik News. “Chinese Phone App Leaks 2 Billion Private Numbers, High Officials’ Among Them”. May 14, 2017. Stecklow, Steve, Alexandra Harney, Anna Irrera and Jemima Kelly. “Chaos and hackers stalk investors on cryptocurrency exchanges”. Reuters. September 29, 2017."}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 524, "chunk_id": "english_pack_524_0_51fdcbcb", "content": "506 | APPENDIX G: SELECT BIBLIOGRAPHIES Symantec. Internet Security Threat Report. ISTR, 2017. Symantec. ISTR Ransomware 2017. July 2017. Symantec. “Attackers target dozens of global banks with new malware”. Symantec Official Blog. February 12, 2017. Symantec. Internet Security Threat Report: Financial Threats Review 2017. 2017. Symantec. “Attackers Target Dozens of Global Banks With New Malware”. Symantec Official Blog (Blog). The Conversation. “By concealing identities, cryptocurrencies fuel cybercrime”. Editorial. September 26, 2017. Thomson, Iain. “Virus (cough, cough Petya) goes postal at FedEx, shares halted”. The Register. June 28, 2017. Turner, Karen. “The Equifax hacks are a case study in why we need better data breach laws”. Vox. September 14, 2017. United State Department of the Treasury Financial Crimes Enforcement Network. “FinCEN Fines BTC-e Virtual Currency Exchange $110 Million for Facilitating Ransomware, Dark Net Drugs Sales”. FinCen. July 26, 2017. Viner, K. “How technology disrupted the truth.” The Guardian. July 12, 2016. Wolff, Josephine. “The New Economics of Cybercrime”. The Atlantic. June 7, 2017. Wolfram, Hedrick, Gerald Wong and Jaclyn Yeo. Cyber Risk in Asia-Pacific: The Case For Greater Transparency. OLIVER WYMAN, 2017. Woo, G.; 2017; Counterfactual Analysis of WannaCry Malware Attack. RMS Webinar, Nov 2017; and blog ‘Reimagining the WannaCry Cyberattack’"}
{"pack_id": "english_pack", "section_title": "Appendix G: Select Bibliographies", "page": 525, "chunk_id": "english_pack_525_0_c122572a", "content": "APPENDIX G: SELECT BIBLIOGRAPHIES | 507 Woodward, Matt. “How Much Does 1 Hour of Downtime Cost the Average Business?”. RAND Group."}
{"pack_id": "english_pack", "section_title": "Versioning History", "page": 526, "chunk_id": "english_pack_526_0_c0dbb1fb", "content": "508 | VERSIONING HISTORY VERSIONING HISTORY This page provides a record of changes made to this textbook. Each set of edits is acknowledged with a 0.1 increase in the version number. The exported files for this toolkit reflect the most recent version. If you find an error, please contact d.tuffley@griffith.edu.au Version Date Change Details Updated version Version July 2025 published on 1.1 Pressbooks platform 15 Published on Version September Amazon 1 2023 platform"}
